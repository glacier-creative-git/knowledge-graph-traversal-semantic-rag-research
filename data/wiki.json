{
  "metadata": {
    "created_at": "2025-08-09T14:47:20.059428",
    "total_articles": 39,
    "failed_articles": 0,
    "config": {
      "wikipedia": {
        "use_cached_articles": true,
        "topics": [
          "Machine learning",
          "Artificial intelligence",
          "Natural language processing",
          "Computer vision",
          "Deep learning",
          "Neural networks",
          "Data science",
          "Statistics"
        ],
        "articles_per_topic": 5,
        "max_article_length": 5000000,
        "min_article_length": 1000,
        "language": "en",
        "rate_limit_delay": 1.0,
        "retry_attempts": 3,
        "timeout_seconds": 30
      },
      "text_processing": {
        "clean_html": true,
        "remove_references": true,
        "remove_navigation": true,
        "remove_tables": true,
        "fix_encoding": true,
        "normalize_whitespace": true,
        "min_sentence_length": 10,
        "max_sentence_length": 500
      }
    }
  },
  "articles": [
    {
      "title": "Machine learning",
      "url": "https://en.wikipedia.org/wiki/Machine_learning",
      "raw_text": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\nStatistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \nFrom a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.\n\n\n== History ==\n\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\nThe earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\nBy the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".\nModern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.\n\n\n== Relationships to other fields ==\n\n\n=== Artificial intelligence ===\n\nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.\nHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.\nMachine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.\n\n\n=== Data compression ===\n\n\n=== Data mining ===\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\nMachine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).\n\n\n=== Generalization ===\nCharacterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\n\n\n=== Statistics ===\nMachine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.\nConventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.\nLeo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.\n\n\n=== Statistical physics ===\nAnalytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics.\n\n\n== Theory ==\n\nA core objective of a learner is to generalise from its experience. Generalisation in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning  model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalisation error.\nFor the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer.\nIn addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n\n\n== Approaches ==\n\nMachine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n\nSupervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\nUnsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\nReinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise.\nAlthough each algorithm has advantages and limitations, no single algorithm works for all problems.\n\n\n=== Supervised learning ===\n\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.\nTypes of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data.\nSimilarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n\n\n=== Unsupervised learning ===\n\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation.\nCluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\nA special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.\n\n\n=== Semi-supervised learning ===\n\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy.\nIn weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.\n\n\n=== Reinforcement learning ===\n\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n\n\n=== Dimensionality reduction ===\nDimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularisation.\n\n\n=== Other types ===\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning.\n\n\n==== Self-learning ====\nSelf-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n\nin situation s perform action a\nreceive a consequence situation s'\ncompute emotion of being in the consequence situation v(s')\nupdate crossbar memory  w'(a,s) = w(a,s) + v(s')\nIt is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour, in an environment that contains both desirable and undesirable situations.\n\n\n==== Feature learning ====\n\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering.\nManifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n\n\n==== Sparse dictionary learning ====\n\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.\n\n\n==== Anomaly detection ====\n\nIn data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.\nIn particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.\nThree broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n\n\n==== Robot learning ====\nRobot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML).\n\n\n==== Association rules ====\n\nAssociation rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".\nRule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule \n  \n    \n      \n        {\n        \n          o\n          n\n          i\n          o\n          n\n          s\n          ,\n          p\n          o\n          t\n          a\n          t\n          o\n          e\n          s\n        \n        }\n        ⇒\n        {\n        \n          b\n          u\n          r\n          g\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n  \n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\nLearning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.\nInductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\nInductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n\n\n== Models ==\nA machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions. By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.\nVarious types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.\n\n\n=== Artificial neural networks ===\n\nArtificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\nAn ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\nDeep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.\n\n\n=== Decision trees ===\n\nDecision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\n\n\n=== Random forest regression ===\nRandom forest regression (RFR) falls under umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting.  To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data of from training set. This random selection of RFR for training enables model to reduce bias predictions and achieve accuracy. RFR generates independent decision trees, and it can work on single output data as well multiple regressor task. This makes RFR compatible to be used in various application.\n\n\n=== Support-vector machines ===\n\nSupport-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\n\n=== Regression analysis ===\n\nRegression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\nMultivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional.\n\n\n=== Bayesian networks ===\n\nA Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n\n\n=== Gaussian processes ===\n\nA Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\nGiven a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\nGaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation.\n\n\n=== Genetic algorithms ===\n\nA genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.\n\n\n=== Belief functions ===\n\nThe theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\n\n\n=== Rule-based models ===\n\nRule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems, association rule learning, artificial immune systems, and other similar models. These methods extract patterns from data and evolve rules over time.\n\n\n=== Training models ===\nTypically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.\n\n\n==== Federated learning ====\n\nFederated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.\n\n\n== Applications ==\nThere are many applications for machine learning, including:\n\nIn 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behaviour of travellers. Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.\nRecent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.\nMachine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires. \nMachine learning is also emerging as a promising tool in geotechnical engineering, where it is used to support tasks such as ground classification, hazard prediction, and site characterization. Recent research emphasizes a move toward data-centric methods in this field, where machine learning is not a replacement for engineering judgment, but a way to enhance it using site-specific data and patterns.\n\n\n== Limitations ==\nAlthough machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.\nThe \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data. The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.\nIn 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.\nMachine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.\n\n\n=== Explainability ===\n\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\n\n\n=== Overfitting ===\n\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.\n\n\n=== Other limitations and vulnerabilities ===\nLearners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgements from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.\nAdversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.\nResearchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.\n\n\n== Model assessments ==\nClassification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.\nIn addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.\n\n\n== Ethics ==\n\n\n=== Bias ===\n\nDifferent machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.\nSystems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices. For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.\nWhile responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases. In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.\nLanguage models learned from data have been shown to contain human-like biases. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.\nIn an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\". In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas. Similar issues with recognising non-white people have been found in many other systems.\nBecause of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"\n\n\n=== Financial incentives ===\nThere are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.\n\n\n== Hardware ==\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.\n\n\n=== Tensor Processing Units (TPUs) ===\nTensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.\n\n\n=== Neuromorphic computing ===\nNeuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.\n\n\n==== Physical neural networks ====\nA physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.\n\n\n=== Embedded machine learning ===\nEmbedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers. Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimisation. Common optimisation techniques include pruning, quantization, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing.\n\n\n== Software ==\nSoftware suites containing a variety of machine learning algorithms include the following:\n\n\n=== Free and open-source software ===\n\n\n=== Proprietary software with free and open-source editions ===\nKNIME\nRapidMiner\n\n\n=== Proprietary software ===\n\n\n== Journals ==\nJournal of Machine Learning Research\nMachine Learning\nNature Machine Intelligence\nNeural Computation\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n\n\n== Conferences ==\nAAAI Conference on Artificial Intelligence\nAssociation for Computational Linguistics (ACL)\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)\nInternational Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)\nInternational Conference on Machine Learning (ICML)\nInternational Conference on Learning Representations (ICLR)\nInternational Conference on Intelligent Robots and Systems (IROS)\nConference on Knowledge Discovery and Data Mining (KDD)\nConference on Neural Information Processing Systems (NeurIPS)\n\n\n== See also ==\nAutomated machine learning – Process of automating the application of machine learning\nBig data – Extremely large or complex datasets\nDeep learning — branch of ML concerned with artificial neural networks\nDifferentiable programming – Programming paradigm\nList of datasets for machine-learning research\nM-theory (learning framework)\nMachine unlearning\nSolomonoff's theory of inductive inference – Mathematical theory\n\n\n== References ==\n\n\n== Sources ==\nDomingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.\nNilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\nPoole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.\n\n\n== Further reading ==\n\n\n== External links ==\nInternational Machine Learning Society\nmloss is an academic database of open-source machine learning software.",
      "cleaned_text": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance. ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics. Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning. The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period. The earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\". Modern-day machine learning has two objectives. One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions. As a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation. Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory. Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples). Characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms. Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field. Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be. Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest. Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning. Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics. A core objective of a learner is to generalise from its experience. Generalisation in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases. The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias-variance decomposition is one way to quantify generalisation error. For the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer. In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time. Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system: Supervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs. Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm works for all problems. Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task. Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity. A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself. Semi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets. Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularisation. Other approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning. Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion. The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: in situation s perform action a receive a consequence situation s' compute emotion of being in the consequence situation v(s') update crossbar memory w'(a,s) = w(a,s) + v(s') It is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour, in an environment that contains both desirable and undesirable situations. Several learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering. Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot. In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions. In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns. Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance to be generated by the model. Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML). Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\". Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems. Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule { o n i o n s , p o t a t o e s } ⇒ { b u r g e r } {\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}} found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions. Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs. Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set. A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions. By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned. Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection. Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules. An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times. The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition. Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making. Random forest regression (RFR) falls under umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data of from training set. This random selection of RFR for training enables model to reduce bias predictions and achieve accuracy. RFR generates independent decision trees, and it can work on single output data as well multiple regressor task. This makes RFR compatible to be used in various application. Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space. Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional. A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams. A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations. Given a set of observed points, or input-output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point. Gaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation. A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms. The theory of belief functions, also referred to as evidence theory or Dempster-Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g., Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches. Rule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems, association rule learning, artificial immune systems, and other similar models. These methods extract patterns from data and evolve rules over time. Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams. Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google. There are many applications for machine learning, including: In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behaviour of travellers. Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS. Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes. Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires. Machine learning is also emerging as a promising tool in geotechnical engineering, where it is used to support tasks such as ground classification, hazard prediction, and site characterization. Recent research emphasizes a move toward data-centric methods in this field, where machine learning is not a replacement for engineering judgment, but a way to enhance it using site-specific data and patterns. Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems. The \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data. The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes. In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users. Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves. Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation. Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is. Learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgements from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies. Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning. Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access. Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy. In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model. Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices. For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data. While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases. In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI. Language models learned from data have been shown to contain human-like biases. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language. In an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\". In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas. Similar issues with recognising non-white people have been found in many other systems. Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and-most importantly-it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated. Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months. Tensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments. Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures. A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses. Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers. Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimisation. Common optimisation techniques include pruning, quantization, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing. Software suites containing a variety of machine learning algorithms include the following: KNIME RapidMiner Journal of Machine Learning Research Machine Learning Nature Machine Intelligence Neural Computation IEEE Transactions on Pattern Analysis and Machine Intelligence AAAI Conference on Artificial Intelligence Association for Computational Linguistics (ACL) European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB) International Conference on Machine Learning (ICML) International Conference on Learning Representations (ICLR) International Conference on Intelligent Robots and Systems (IROS) Conference on Knowledge Discovery and Data Mining (KDD) Conference on Neural Information Processing Systems (NeurIPS)",
      "sentences": [
        "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions.",
        "Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.",
        "ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.",
        "The application of ML to business problems is known as predictive analytics.",
        "Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning.",
        "Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.",
        "From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.",
        "The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.",
        "The synonym self-teaching computers was also used in this time period.",
        "The earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.",
        "In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.",
        "Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.",
        "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.",
        "By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning.",
        "It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions.",
        "A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.",
        "Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.",
        "In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.",
        "Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms.",
        "This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\"",
        "is replaced with the question \"Can machines do what we (as thinking entities) can do?\".",
        "Modern-day machine learning has two objectives.",
        "One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models.",
        "A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles.",
        "A machine learning algorithm for stock trading may inform the trader of future potential predictions.",
        "As a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI).",
        "In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data.",
        "They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics.",
        "Probabilistic reasoning was also employed, especially in automated medical diagnosis.",
        "However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning.",
        "Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.",
        "By 1980, expert systems had come to dominate AI, and statistics was out of favour.",
        "Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.",
        "Neural networks research had been abandoned by AI and computer science around the same time.",
        "This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton.",
        "Their main success came in the mid-1980s with the reinvention of backpropagation.",
        "Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s.",
        "The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature.",
        "It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.",
        "Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases).",
        "Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy.",
        "Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge.",
        "Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.",
        "Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples.",
        "Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).",
        "Characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.",
        "Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns.",
        "According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.",
        "He also suggested the term data science as a placeholder to call the overall field.",
        "Conventional statistical analyses require the a priori selection of a model most suitable for the study data set.",
        "In addition, only significant or theoretically relevant variables based on previous experience are included for analysis.",
        "In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns.",
        "The more variables (input) used to train the model, the more accurate the ultimate model will be.",
        "Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.",
        "Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.",
        "Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks.",
        "Statistical physics is thus finding applications in the area of medical diagnostics.",
        "A core objective of a learner is to generalise from its experience.",
        "Generalisation in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set.",
        "The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.",
        "The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning model.",
        "Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms.",
        "Instead, probabilistic bounds on the performance are quite common.",
        "The bias-variance decomposition is one way to quantify generalisation error.",
        "For the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data.",
        "If the hypothesis is less complex than the function, then the model has under fitted the data.",
        "If the complexity of the model is increased in response, then the training error decreases.",
        "But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer.",
        "In addition to performance bounds, learning theorists study the time complexity and feasibility of learning.",
        "In computational learning theory, a computation is considered feasible if it can be done in polynomial time.",
        "There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time.",
        "Negative results show that certain classes cannot be learned in polynomial time.",
        "Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system: Supervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.",
        "Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input.",
        "Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).",
        "Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent).",
        "As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise.",
        "Although each algorithm has advantages and limitations, no single algorithm works for all problems.",
        "Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.",
        "The data, known as training data, consists of a set of training examples.",
        "Each training example has one or more inputs and the desired output, also known as a supervisory signal.",
        "In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix.",
        "Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.",
        "An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data.",
        "An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.",
        "Types of supervised-learning algorithms include active learning, classification and regression.",
        "Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range.",
        "For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email.",
        "In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data.",
        "Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are.",
        "It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.",
        "Unsupervised learning algorithms find structures in data that has not been labelled, classified or categorised.",
        "Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data.",
        "Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation.",
        "Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar.",
        "Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters.",
        "Other methods are based on estimated density and graph connectivity.",
        "A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.",
        "Semi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data).",
        "Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy.",
        "In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.",
        "Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximise some notion of cumulative reward.",
        "Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms.",
        "In reinforcement learning, the environment is typically represented as a Markov decision process (MDP).",
        "Many reinforcement learning algorithms use dynamic programming techniques.",
        "Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible.",
        "Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.",
        "Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.",
        "In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\".",
        "Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction.",
        "One of the popular methods of dimensionality reduction is principal component analysis (PCA).",
        "PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).",
        "The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularisation.",
        "Other approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system.",
        "For example, topic modelling, meta-learning.",
        "Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).",
        "It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward.",
        "Emotion is used as state evaluation of a self-learning agent.",
        "The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations.",
        "The system is driven by the interaction between cognition and emotion.",
        "There is neither a separate reinforcement input nor an advice input from the environment.",
        "The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation.",
        "The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment.",
        "After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour, in an environment that contains both desirable and undesirable situations.",
        "Several learning algorithms aim at discovering better representations of the inputs provided during training.",
        "Classic examples include principal component analysis and cluster analysis.",
        "Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions.",
        "This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution.",
        "This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.",
        "Feature learning can be either supervised or unsupervised.",
        "In supervised feature learning, features are learned using labelled input data.",
        "Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning.",
        "In unsupervised feature learning, features are learned with unlabelled input data.",
        "Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering.",
        "Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional.",
        "Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros.",
        "Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.",
        "Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features.",
        "It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.",
        "Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process.",
        "However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features.",
        "An alternative is to discover such features or representations through examination, without relying on explicit algorithms.",
        "Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix.",
        "The method is strongly NP-hard and difficult to solve approximately.",
        "A popular heuristic method for sparse dictionary learning is the k-SVD algorithm.",
        "Sparse dictionary learning has been applied in several contexts.",
        "In classification, the problem is to determine the class to which a previously unseen training example belongs.",
        "For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary.",
        "Sparse dictionary learning has also been applied in image de-noising.",
        "The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.",
        "In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.",
        "Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text.",
        "Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.",
        "In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity.",
        "This pattern does not adhere to the common statistical definition of an outlier as a rare object.",
        "Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately.",
        "Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.",
        "Three broad categories of anomaly detection techniques exist.",
        "Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set.",
        "Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection).",
        "Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance to be generated by the model.",
        "Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g.",
        "Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases.",
        "It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".",
        "Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge.",
        "The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system.",
        "This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.",
        "Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.",
        "Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.",
        "For example, the rule { o n i o n s , p o t a t o e s } ⇒ { b u r g e r } {\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}} found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat.",
        "Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements.",
        "In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics.",
        "In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.",
        "Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning.",
        "They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.",
        "Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses.",
        "Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples.",
        "Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.",
        "Inductive logic programming is particularly useful in bioinformatics and natural language processing.",
        "Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.",
        "Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.",
        "The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.",
        "A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data.",
        "During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions.",
        "By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.",
        "Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.",
        "Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains.",
        "Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.",
        "An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain.",
        "Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another.",
        "An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.",
        "In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs.",
        "The connections between artificial neurons are called \"edges\".",
        "Artificial neurons and edges typically have a weight that adjusts as learning proceeds.",
        "The weight increases or decreases the strength of the signal at a connection.",
        "Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold.",
        "Typically, artificial neurons are aggregated into layers.",
        "Different layers may perform different kinds of transformations on their inputs.",
        "Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.",
        "The original goal of the ANN approach was to solve problems in the same way that a human brain would.",
        "However, over time, attention moved to performing specific tasks, leading to deviations from biology.",
        "Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.",
        "Deep learning consists of multiple hidden layers in an artificial neural network.",
        "This approach tries to model the way the human brain processes light and sound into vision and hearing.",
        "Some successful applications of deep learning are computer vision and speech recognition.",
        "Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves).",
        "It is one of the predictive modelling approaches used in statistics, data mining, and machine learning.",
        "Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels.",
        "Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.",
        "In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making.",
        "In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.",
        "Random forest regression (RFR) falls under umbrella of decision tree-based models.",
        "RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting.",
        "To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data of from training set.",
        "This random selection of RFR for training enables model to reduce bias predictions and achieve accuracy.",
        "RFR generates independent decision trees, and it can work on single output data as well multiple regressor task.",
        "This makes RFR compatible to be used in various application.",
        "Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression.",
        "Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.",
        "An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting.",
        "In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.",
        "Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features.",
        "Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares.",
        "The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression.",
        "When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.",
        "Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously.",
        "This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model.",
        "It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional.",
        "A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG).",
        "For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms.",
        "Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.",
        "Efficient algorithms exist that perform inference and learning.",
        "Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks.",
        "Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.",
        "A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.",
        "Given a set of observed points, or input-output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.",
        "Gaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation.",
        "A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem.",
        "In machine learning, genetic algorithms were used in the 1980s and 1990s.",
        "Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.",
        "The theory of belief functions, also referred to as evidence theory or Dempster-Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and imprecise probability theories.",
        "These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g., Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities.",
        "However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification.",
        "These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.",
        "However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.",
        "Rule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data.",
        "It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity.",
        "Key RBML techniques includes learning classifier systems, association rule learning, artificial immune systems, and other similar models.",
        "These methods extract patterns from data and evolve rules over time.",
        "Typically, machine learning models require a high quantity of reliable data to perform accurate predictions.",
        "When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data.",
        "Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service.",
        "Overfitting is something to watch out for when training a machine learning model.",
        "Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions.",
        "Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives.",
        "Algorithmic bias is a potential result of data not being fully prepared for training.",
        "Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.",
        "Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server.",
        "This also increases efficiency by decentralising the training process to many devices.",
        "For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.",
        "There are many applications for machine learning, including: In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%.",
        "A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.",
        "Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.",
        "In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis.",
        "In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.",
        "In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists.",
        "In 2019 Springer Nature published the first research book created using machine learning.",
        "In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.",
        "Machine learning was recently applied to predict the pro-environmental behaviour of travellers.",
        "Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone.",
        "When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting.",
        "By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.",
        "Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.",
        "Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters.",
        "Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.",
        "Other applications have been focusing on pre evacuation decisions in building fires.",
        "Machine learning is also emerging as a promising tool in geotechnical engineering, where it is used to support tasks such as ground classification, hazard prediction, and site characterization.",
        "Recent research emphasizes a move toward data-centric methods in this field, where machine learning is not a replacement for engineering judgment, but a way to enhance it using site-specific data and patterns.",
        "Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.",
        "Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.",
        "The \"black box theory\" poses another yet significant challenge.",
        "Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.",
        "The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.",
        "In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.",
        "Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.",
        "Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.",
        "Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature.",
        "While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.",
        "It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.",
        "By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively.",
        "XAI may be an implementation of the social right to explanation.",
        "Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting.",
        "Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.",
        "Learners can also disappoint by \"learning the wrong lesson\".",
        "A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.",
        "A real-world example is that, unlike humans, current image classifiers often do not primarily make judgements from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects.",
        "Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.",
        "Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations.",
        "For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.",
        "Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.",
        "Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties.",
        "Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.",
        "Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set.",
        "In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model.",
        "In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.",
        "In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively.",
        "Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR).",
        "However, these rates are ratios that fail to reveal their numerators and denominators.",
        "Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment.",
        "Higher AUC is associated with a better performing model.",
        "Different machine learning approaches can suffer from different data biases.",
        "A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data.",
        "When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.",
        "Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.",
        "For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names.",
        "Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.",
        "Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.",
        "While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.",
        "In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world.",
        "Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.",
        "Language models learned from data have been shown to contain human-like biases.",
        "Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.",
        "In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.",
        "In an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\".",
        "In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy.",
        "The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas.",
        "Similar issues with recognising non-white people have been found in many other systems.",
        "Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.",
        "Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI.",
        "It's inspired by people, it's created by people, and-most importantly-it impacts people.",
        "It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"",
        "There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines.",
        "This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits.",
        "For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes.",
        "There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.",
        "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.",
        "By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.",
        "OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.",
        "Tensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads.",
        "Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference.",
        "They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models.",
        "TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency.",
        "Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.",
        "Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks.",
        "These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.",
        "A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses.",
        "The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations.",
        "It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.",
        "Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers.",
        "Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets.",
        "Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimisation.",
        "Common optimisation techniques include pruning, quantization, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing."
      ],
      "metadata": {
        "title": "Machine learning",
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "word_count": 8262,
        "char_count": 56285,
        "sentence_count": 352,
        "scraped_at": "2025-08-09T14:46:41.336367",
        "language": "en",
        "processing_time": 0.04005289077758789,
        "source_hash": "af66f519ed0d6eeebc0ddd3c1b37d239"
      }
    },
    {
      "title": "Quantum machine learning",
      "url": "https://en.wikipedia.org/wiki/Quantum_machine_learning",
      "raw_text": "Quantum machine learning (QML) is the study of quantum algorithms which solve machine learning tasks.\nThe most common use of the term refers to quantum algorithms for machine learning tasks which analyze classical data, sometimes called quantum-enhanced machine learning. QML algorithms use qubits and quantum operations to try to improve the space and time complexity of classical machine learning algortihms. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.\nThe term \"quantum machine learning\" is sometimes use to refer classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.\nQML also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.\nFurthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\n\n\n== Machine learning with quantum computers ==\nQuantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of QML algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices.\n\n\n=== Quantum associative memories and quantum pattern recognition ===\nAssociative (or content-addressable) memories are able to recognize stored content on the basis of a similarity measure, while random access memories are accessed by the address of stored information and not its content. As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition.\nTypical classical associative memories store p patterns in the \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{2})}\n  \n interactions (synapses) of a real,  symmetric energy matrix over a network of n artificial neurons. The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration.\nUnfortunately, classical associative memories are severely limited by the phenomenon of cross-talk. When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible. The number of storable patterns is typically limited by a linear function of the number of neurons, \n  \n    \n      \n        p\n        ≤\n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle p\\leq O(n)}\n  \n.\nQuantum associative memories (in their simplest realization) store patterns in a unitary matrix U acting on the Hilbert space of n qubits. Retrieval is realized by the unitary evolution of a fixed initial state to a quantum superposition of the desired patterns with probability distribution peaked on the most similar pattern to an input. By its very quantum nature, the retrieval process is thus probabilistic. Because quantum associative memories are free from cross-talk, however, spurious memories are never generated. Correspondingly, they have a superior capacity than classical ones. The number of parameters in the unitary matrix U is \n  \n    \n      \n        O\n        (\n        p\n        n\n        )\n      \n    \n    {\\displaystyle O(pn)}\n  \n. One can thus have efficient, spurious-memory-free quantum associative memories for any polynomial number of patterns.\n\n\n=== Linear algebra simulation with quantum amplitudes ===\nA number of quantum algorithms for machine learning are based on the idea of amplitude encoding, that is, to associate the amplitudes of a quantum state with the inputs and outputs of computations. Since a state of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n qubits is described by \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n complex amplitudes, this information encoding can allow for an exponentially compact representation. Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n, which amounts to a logarithmic time complexity in the number of amplitudes and thereby the dimension of the input.\nMany QML algorithms in this category are based on variations of the quantum algorithm for linear systems of equations (colloquially called HHL, after the paper's authors) which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix. One of these conditions is that a Hamiltonian which entry wise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse or low rank. For reference, any known classical algorithm for matrix inversion requires a number of operations that grows more than quadratically in the dimension of the matrix (e.g. \n  \n    \n      \n        O\n        \n          \n            \n              (\n              \n                n\n                \n                  2.373\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle O{\\mathord {\\left(n^{2.373}\\right)}}}\n  \n), but they are not restricted to sparse matrices.\nQuantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations, for example in least-squares linear regression, the least-squares version of support vector machines, and Gaussian processes.\nA crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset. Although efficient methods for state preparation are known for specific cases, this step easily hides the complexity of the task.\n\n\n=== Variational Quantum Algorithms (VQAs) ===\nIn a variational quantum algorithm, a classical computer optimizes the parameters used to prepare a quantum state, while a quantum computer is used to do the actual state preparation and measurement. VQAs are considered promising candidates for noisy intermediate-scale quantum computers. Variational quantum circuits (or parameterized quantum circuits) are a popular class of VQAs where the parameters are those used in a fixed quantum circuit. Researchers have studied VQCs to solve optimization problems and find the ground state energy of complex quantum systems, which were difficult to solve using a classical computer.\n\n\n=== Quantum binary classifier ===\nPattern reorganization is one of the important tasks of machine learning, binary classification is one of the tools or algorithms to find patterns. Binary classification is used in supervised learning and in unsupervised learning. In QML, classical bits are converted to qubits and they are mapped to Hilbert space; complex value data are used in a quantum binary classifier to use the advantage of Hilbert space. By exploiting the quantum mechanic properties such as superposition, entanglement, interference the quantum binary classifier produces the accurate result in short period of time.\n\n\n=== Quantum machine learning algorithms based on Grover search ===\nAnother approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on Grover's search algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the k-medians and the k-nearest neighbors algorithms. Other applications include quadratic speedups in the training of perceptrons.\nAn example of amplitude amplification being used in a machine learning algorithm is Grover's search algorithm minimization. In which a subroutine uses Grover's search algorithm to find an element less than some previously defined element. This can be done with an oracle that determines whether or not a state with a corresponding element is less than the predefined one. Grover's algorithm can then find an element such that our condition is met. The minimization is initialized by some random element in our data set, and iteratively does this subroutine to find the minimum element in the data set. This minimization is notably used in quantum k-medians, and it has a speed up of at least \n  \n    \n      \n        \n          \n            O\n          \n        \n        \n          (\n          \n            \n              \n                n\n                k\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\mathcal {O}}\\left({\\sqrt {\\frac {n}{k}}}\\right)}\n  \n compared to classical versions of k-medians, where \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is the number of data points and \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n is the number of clusters.\nAmplitude amplification is often combined with quantum walks to achieve the same quadratic speedup. Quantum walks have been proposed to enhance Google's PageRank algorithm as well as the performance of reinforcement learning agents in the projective simulation framework.\n\n\n=== Quantum-enhanced reinforcement learning ===\nIn quantum-enhanced reinforcement learning, a quantum agent interacts with a classical or quantum environment and occasionally receives rewards for its actions. In some situations, either because of the quantum processing capability of the agent, or due to the possibility to probe the environment in superpositions, a quantum speedup may be achieved. Implementations of these kinds of protocols have been proposed for systems of trapped ions and superconducting circuits. A quantum speedup of the agent's internal decision-making time has been experimentally demonstrated in trapped ions, while a quantum speedup of the learning time in a fully coherent (`quantum') interaction between agent and environment has been experimentally realized in a photonic setup.\n\n\n=== Quantum annealing ===\n\nQuantum annealing is an optimization technique used to determine the local minima and maxima of a function over a given set of candidate functions. This is a method of discretizing a function with many local minima or maxima in order to determine the observables of the function. The process can be distinguished from Simulated annealing by the Quantum tunneling process, by which particles tunnel through kinetic or potential barriers from a high state to a low state. Quantum annealing starts from a superposition of all possible states of a system, weighted equally. Then the time-dependent Schrödinger equation guides the time evolution of the system, serving to affect the amplitude of each state as time increases. Eventually, the ground state can be reached to yield the instantaneous Hamiltonian of the system.\n\n\n=== Quantum sampling techniques ===\nSampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society. Examples include deep learning, probabilistic programming, and other machine learning and artificial intelligence applications.\nA computationally hard problem, which is key for some relevant machine learning tasks, is the estimation of averages over probabilistic models defined in terms of a Boltzmann distribution. Sampling from generic probabilistic models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.\nSome research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks. The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms. Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset.\nThe D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures. Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks. The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets. In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward. Reverse annealing has been used as well to solve a fully connected quantum restricted Boltzmann machine.\nInspired by the success of Boltzmann machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed. Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines.\nQuantum annealing is not the only technology for sampling. In a prepare-and-measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing. The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first-order logic template. This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.\n\n\n=== Quantum neural networks ===\n\nQuantum analogues or generalizations of classical neural nets are often referred to as quantum neural networks. The term is claimed by a wide range of approaches, including the implementation and extension of neural networks using photons, layered variational circuits or quantum Ising-type models.\n\n\n=== Quantum Convolution Neural Network ===\nA novel design for multi-dimensional vectors that uses circuits as convolution filters is QCNN. It was inspired by the advantages of CNNs and the power of QML. It is made using a combination of a variational quantum circuit (VQC) and a deep neural network(DNN), fully utilizing the power of extremely parallel processing on a superposition of a quantum state with a finite number of qubits. The main strategy is to carry out an iterative optimization process in the NISQ devices, without the negative impact of noise, which is possibly incorporated into the circuit parameter, and without the need for quantum error correction.\nThe quantum circuit must effectively handle spatial information in order for QCNN to function as CNN. The convolution filter is the most basic technique for making use of spatial information. One or more quantum convolutional filters make up a quantum convolutional neural network (QCNN), and each of these filters transforms input data using a quantum circuit that can be created in an organized or randomized way. Three parts  that make up the quantum convolutional filter are:  the encoder, the parameterized quantum circuit (PQC), and the measurement. The quantum convolutional filter can be seen as an extension of the filter in the traditional CNN because it was designed with trainable parameters.\nQuantum neural networks take advantage of the hierarchical structures, and for each subsequent layer, the number of qubits from the preceding layer is decreased by a factor of two. For n input qubits, these structure have O(log(n)) layers, allowing for shallow circuit depth. Additionally, they are able to avoid \"barren plateau,\" one of the most significant issues with PQC-based algorithms, ensuring trainability. Despite the fact that the QCNN model does not include the corresponding quantum operation, the fundamental idea of the pooling layer is also offered to assure validity. In QCNN architecture, the pooling layer is typically placed between succeeding convolutional layers. Its function is to shrink the representation's spatial size while preserving crucial features, which allows it to reduce the number of parameters, streamline network computing, and manage over-fitting.  Such process can be accomplished applying full Tomography on the state to reduce it all the way down to one qubit and then processed it in subway. The most frequently used unit type in the pooling layer is max pooling, although there are other types as well. Similar to conventional feed-forward neural networks, the last module is a fully connected layer with full connections to all activations in the preceding layer. Translational invariance, which requires identical blocks of parameterized quantum gates within a layer, is a distinctive feature of the QCNN architecture.\n\n\n=== Fully quantum machine learning ===\nIn the most general case of QML, both the learning device and the system under study, as well as their interaction, are fully quantum. This section gives a few examples of results on this topic.\nOne class of problem that can benefit from the fully quantum approach is that of 'learning' unknown quantum states, processes or measurements, in the sense that one can subsequently reproduce them on another quantum system. For example, one may wish to learn a measurement that discriminates between two coherent states, given not a classical description of the states to be discriminated, but instead a set of example quantum systems prepared in these states. The naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information. This would only require classical learning. However, one can show that a fully quantum approach is strictly superior in this case. (This also relates to work on quantum pattern matching.) The problem of learning unitary transformations can be approached in a similar way.\nGoing beyond the specific problem of learning states and transformations, the task of clustering also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum. Finally, a general framework spanning supervised, unsupervised and reinforcement learning in the fully quantum setting was introduced in, where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning. Such a speedup in the reinforcement-learning paradigm has been experimentally demonstrated in a photonic setup.\n\n\n=== Explainable quantum machine learning ===\nThe need for models that can be understood by humans emerges in QML in analogy to classical machine learning and drives the research field of explainable QML (or XQML in analogy to XAI/XML). These efforts are often also referred to as Interpretable Machine Learning (IML, and by extension IQML). XQML/IQML can be considered as an alternative research direction instead of finding a quantum advantage. For example, XQML has been used in the context of mobile malware detection and classification. Quantum Shapley values have also been proposed to interpret gates within a circuit based on a game-theoretic approach. For this purpose, gates instead of features act as players in a coalitional game with a value function that depends on measurements of the quantum circuit of interest. Additionally, a quantum version of the classical technique known as LIME (Linear Interpretable Model-Agnostic Explanations) has also been proposed, known as Q-LIME.\n\n\n=== Quantum Kernel Methods and Generative Models ===\nQuantum kernel methods have emerged as particularly promising approaches for near-term applications. Large-scale benchmarking studies encompassing over 20,000 trained models have provided comprehensive insights into the effectiveness of fidelity quantum kernels (FQKs) and projected quantum kernels (PQKs) across diverse classification and regression tasks. These studies have revealed universal patterns that guide effective quantum kernel method design.\nIn the generative modeling domain, quantum generative adversarial networks and quantum circuit Born machines have shown particular promise for tabular data synthesis. Novel quantum generative models for tabular data have demonstrated performance improvements of 8.5% over leading classical models while using only 0.072% of the parameters, indicating significant potential for parameter-efficient learning.\n\n\n== Classical learning applied to quantum problems ==\n\nThe term \"quantum machine learning\" sometimes refers to classical machine learning performed on data from quantum systems. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other applications include learning Hamiltonians and automatically generating quantum experiments.\n\n\n== Quantum learning theory ==\nQuantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide. The framework is very similar to that of classical computational learning theory, but the learner in this case is a quantum information processing device, while the data may be either classical or quantum. Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider specific problems and to use quantum protocols to improve the time complexity of classical algorithms for these problems. Although quantum learning theory is still under development, partial results in this direction have been obtained.\nThe starting point in learning theory is typically a concept class, a set of possible concepts. Usually a concept is a function on some domain, such as \n  \n    \n      \n        {\n        0\n        ,\n        1\n        \n          }\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\{0,1\\}^{n}}\n  \n. For example, the concept class could be the set of disjunctive normal form (DNF) formulas on n bits or the set of Boolean circuits of some constant depth. The goal for the learner is to learn (exactly or approximately) an unknown target concept from this concept class. The learner may be actively interacting with the target concept, or passively receiving samples from it.\nIn active learning, a learner can make membership queries to the target concept c, asking for its value c(x) on inputs x chosen by the learner. The learner then has to reconstruct the exact target concept, with high probability. In the model of quantum exact learning, the learner can make membership queries in quantum superposition. If the complexity of the learner is measured by the number of membership queries it makes, then quantum exact learners can be polynomially more efficient than classical learners for some concept classes, but not more. If complexity is measured by the amount of time the learner uses, then there are concept classes that can be learned efficiently by quantum learners but not by classical learners (under plausible complexity-theoretic assumptions).\nA natural model of passive learning is Valiant's probably approximately correct (PAC) learning. Here the learner receives random examples (x,c(x)), where x is distributed according to some unknown distribution D. The learner's goal is to output a hypothesis function h such that h(x)=c(x) with high probability when x is drawn according to D. The learner has to be able to produce such an 'approximately correct' h for every D and every target concept c in its concept class. We can consider replacing the random examples by potentially more powerful quantum examples \n  \n    \n      \n        \n          ∑\n          \n            x\n          \n        \n        \n          \n            D\n            (\n            x\n            )\n          \n        \n        \n          |\n        \n        x\n        ,\n        c\n        (\n        x\n        )\n        ⟩\n      \n    \n    {\\displaystyle \\sum _{x}{\\sqrt {D(x)}}|x,c(x)\\rangle }\n  \n. In the PAC model (and the related agnostic model), this doesn't significantly reduce the number of examples needed: for every concept class, classical and quantum sample complexity are the same up to constant factors. However, for learning under some fixed distribution D, quantum examples can be very helpful, for example for learning DNF under the uniform distribution. When considering time complexity, there exist concept classes that can be PAC-learned efficiently by quantum learners, even from classical examples, but not by classical learners (again, under plausible complexity-theoretic assumptions).\nThis passive learning type is also the most common scheme in supervised learning: a learning algorithm typically takes the training examples fixed, without the ability to query the label of unlabelled examples. Outputting a hypothesis h is a step of induction. Classically, an inductive model splits into a training and an application phase: the model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase. In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources.\n\n\n== Implementations and experiments ==\nThe earliest experiments were conducted using the adiabatic D-Wave quantum computer, for instance, to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009. Many experiments followed on the same architecture, and leading tech companies have shown interest in the potential of QML for future technological implementations. In 2013, Google Research, NASA, and the Universities Space Research Association launched the Quantum Artificial Intelligence Lab which explores the use of the adiabatic D-Wave quantum computer. A more recent example trained a probabilistic generative models with arbitrary pairwise connectivity, showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits.\nUsing a different annealing technology based on nuclear magnetic resonance (NMR), a quantum Hopfield network was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation. NMR technology also enables universal quantum computing, and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state  quantum computer in 2015. The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space, the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal.\nPhotonic implementations are attracting more attention, not the least because they do not require extensive cooling. Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013. Using non-linear photonics to implement an all-optical linear classifier, a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule. A core building block in many learning algorithms is to calculate the distance between two vectors: this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in 2015.\nRecently, based on a neuromimetic approach, a novel ingredient has been added to the field of QML, in the form of a so-called quantum memristor, a quantized model of the standard classical memristor. This device can be constructed by means of a tunable resistor, weak measurements on the system, and a classical feed-forward mechanism. An implementation of a quantum memristor in superconducting circuits has been proposed, and an experiment with quantum dots performed. A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network.\nSince 2016, IBM has launched an online cloud-based platform for quantum software developers, called the IBM Q Experience. This platform consists of several fully operational quantum processors accessible via the IBM Web API. In doing so, the company is encouraging software developers to pursue new algorithms through a development environment with quantum capabilities. New architectures are being explored on an experimental basis, up to 32 qubits, using both trapped-ion and superconductive quantum computing methods.\nIn October 2019, it was noted that the introduction of Quantum Random Number Generators (QRNGs) to machine learning models including Neural Networks and Convolutional Neural Networks for random initial weight distribution and Random Forests for splitting processes had a profound effect on their ability when compared to the classical method of Pseudorandom Number Generators (PRNGs). However, in a more recent publication from 2021, these claims could not be reproduced for Neural Network weight initialization and no significant advantage of using QRNGs over PRNGs was found. The work also demonstrated that the generation of fair random numbers with a gate quantum computer is a non-trivial task on NISQ devices, and QRNGs are therefore typically much more difficult to use in practice than PRNGs.\nA paper published in December 2018 reported on an experiment using a trapped-ion system demonstrating a quantum speedup of the deliberation time of reinforcement learning agents employing internal quantum hardware.\nIn March 2021, a team of researchers from Austria, The Netherlands, the US and Germany reported the experimental demonstration of a quantum speedup of the learning time of reinforcement learning agents interacting fully quantumly with the environment. The relevant degrees of freedom of both agent and environment were realized on a compact and fully tunable integrated nanophotonic processor.\n\n\n== Breakthrough Solutions to Barren Plateaus ==\nThe barren plateau problem [1] where quantum algorithms encounter flat optimization landscapes—has seen significant theoretical and practical advances in 2025. Los Alamos National Laboratory researchers have provided the first mathematical characterization of why and when barren plateaus occur in variational quantum algorithms, establishing theoretical guarantees for algorithm scalability. \nThis work solves a key usability problem for quantum machine learning by providing rigorous theorems that predict whether a given architecture will remain trainable as it scales to larger quantum systems. The breakthrough eliminates the previous trial-and-error approach that had led to researcher fatigue in the field.\n\n\n== Skepticism ==\nWhile machine learning itself is now not only a research field but an economically significant and fast growing industry and quantum computing is a well established field of both theoretical and experimental research, QML remains a purely theoretical field of studies. Attempts to experimentally demonstrate concepts of QML remain insufficient. Further, another obstacle exists at the prediction stage because the outputs of quantum learning models are inherently random. This creates an often considerable overhead, as many executions of a quantum learning model have to be aggregated to obtain an actual prediction.\nMany of the leading scientists that extensively publish in the field of QML warn about the extensive hype around the topic and are very restrained if asked about its practical uses in the foreseeable future. Sophia Chen collected some of the statements made by well known scientists in the field:\n\n\"I think we haven't done our homework yet. This is an extremely new scientific field,\" - physicist Maria Schuld of Canada-based quantum computing startup Xanadu.\n“When mixing machine learning with ‘quantum,’ you catalyse a hype-condensate.” - Jacob Biamonte a contributor to the theory of quantum computation.\n\"There is a lot more work that needs to be done before claiming quantum machine learning will actually work,\" - computer scientist Iordanis Kerenidis, the head of quantum algorithms at the Silicon Valley-based quantum computing startup QC Ware.\n\"I have not seen a single piece of evidence that there exists a meaningful [machine learning] task for which it would make sense to use a quantum computer and not a classical computer,\" - physicist Ryan Sweke of the Free University of Berlin in Germany.\n“Don't fall for the hype!” -  Frank Zickert, who is the author of probably the most practical book related to the subject beware that ”quantum computers are far away from advancing machine learning for their representation ability”, and even speaking about evaluation and optimization for any kind of useful task quantum supremacy is not yet achieved. Furthermore, nobody among the active researchers in the field make any forecasts about when it could possibly become practical.\n\n\n== See also ==\nDifferentiable programming\nQuantum computing\nQuantum algorithm for linear systems of equations\nQuantum annealing\nQuantum neural network\nQuantum image\n\n\n== References ==",
      "cleaned_text": "Quantum machine learning (QML) is the study of quantum algorithms which solve machine learning tasks. The most common use of the term refers to quantum algorithms for machine learning tasks which analyze classical data, sometimes called quantum-enhanced machine learning. QML algorithms use qubits and quantum operations to try to improve the space and time complexity of classical machine learning algortihms. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data. The term \"quantum machine learning\" is sometimes use to refer classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments. QML also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa. Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\". Quantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of QML algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices. Associative (or content-addressable) memories are able to recognize stored content on the basis of a similarity measure, while random access memories are accessed by the address of stored information and not its content. As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition. Typical classical associative memories store p patterns in the O ( n 2 ) {\\displaystyle O(n^{2})} interactions (synapses) of a real, symmetric energy matrix over a network of n artificial neurons. The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration. Unfortunately, classical associative memories are severely limited by the phenomenon of cross-talk. When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible. The number of storable patterns is typically limited by a linear function of the number of neurons, p ≤ O ( n ) {\\displaystyle p\\leq O(n)} . Quantum associative memories (in their simplest realization) store patterns in a unitary matrix U acting on the Hilbert space of n qubits. Retrieval is realized by the unitary evolution of a fixed initial state to a quantum superposition of the desired patterns with probability distribution peaked on the most similar pattern to an input. By its very quantum nature, the retrieval process is thus probabilistic. Because quantum associative memories are free from cross-talk, however, spurious memories are never generated. Correspondingly, they have a superior capacity than classical ones. The number of parameters in the unitary matrix U is O ( p n ) {\\displaystyle O(pn)} . One can thus have efficient, spurious-memory-free quantum associative memories for any polynomial number of patterns. A number of quantum algorithms for machine learning are based on the idea of amplitude encoding, that is, to associate the amplitudes of a quantum state with the inputs and outputs of computations. Since a state of n {\\displaystyle n} qubits is described by 2 n {\\displaystyle 2^{n}} complex amplitudes, this information encoding can allow for an exponentially compact representation. Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits n {\\displaystyle n} , which amounts to a logarithmic time complexity in the number of amplitudes and thereby the dimension of the input. Many QML algorithms in this category are based on variations of the quantum algorithm for linear systems of equations (colloquially called HHL, after the paper's authors) which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix. One of these conditions is that a Hamiltonian which entry wise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse or low rank. For reference, any known classical algorithm for matrix inversion requires a number of operations that grows more than quadratically in the dimension of the matrix (e.g. O ( n 2.373 ) {\\displaystyle O{\\mathord {\\left(n^{2.373}\\right)}}} ), but they are not restricted to sparse matrices. Quantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations, for example in least-squares linear regression, the least-squares version of support vector machines, and Gaussian processes. A crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset. Although efficient methods for state preparation are known for specific cases, this step easily hides the complexity of the task. In a variational quantum algorithm, a classical computer optimizes the parameters used to prepare a quantum state, while a quantum computer is used to do the actual state preparation and measurement. VQAs are considered promising candidates for noisy intermediate-scale quantum computers. Variational quantum circuits (or parameterized quantum circuits) are a popular class of VQAs where the parameters are those used in a fixed quantum circuit. Researchers have studied VQCs to solve optimization problems and find the ground state energy of complex quantum systems, which were difficult to solve using a classical computer. Pattern reorganization is one of the important tasks of machine learning, binary classification is one of the tools or algorithms to find patterns. Binary classification is used in supervised learning and in unsupervised learning. In QML, classical bits are converted to qubits and they are mapped to Hilbert space; complex value data are used in a quantum binary classifier to use the advantage of Hilbert space. By exploiting the quantum mechanic properties such as superposition, entanglement, interference the quantum binary classifier produces the accurate result in short period of time. Another approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on Grover's search algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the k-medians and the k-nearest neighbors algorithms. Other applications include quadratic speedups in the training of perceptrons. An example of amplitude amplification being used in a machine learning algorithm is Grover's search algorithm minimization. In which a subroutine uses Grover's search algorithm to find an element less than some previously defined element. This can be done with an oracle that determines whether or not a state with a corresponding element is less than the predefined one. Grover's algorithm can then find an element such that our condition is met. The minimization is initialized by some random element in our data set, and iteratively does this subroutine to find the minimum element in the data set. This minimization is notably used in quantum k-medians, and it has a speed up of at least O ( n k ) {\\displaystyle {\\mathcal {O}}\\left({\\sqrt {\\frac {n}{k}}}\\right)} compared to classical versions of k-medians, where n {\\displaystyle n} is the number of data points and k {\\displaystyle k} is the number of clusters. Amplitude amplification is often combined with quantum walks to achieve the same quadratic speedup. Quantum walks have been proposed to enhance Google's PageRank algorithm as well as the performance of reinforcement learning agents in the projective simulation framework. In quantum-enhanced reinforcement learning, a quantum agent interacts with a classical or quantum environment and occasionally receives rewards for its actions. In some situations, either because of the quantum processing capability of the agent, or due to the possibility to probe the environment in superpositions, a quantum speedup may be achieved. Implementations of these kinds of protocols have been proposed for systems of trapped ions and superconducting circuits. A quantum speedup of the agent's internal decision-making time has been experimentally demonstrated in trapped ions, while a quantum speedup of the learning time in a fully coherent (`quantum') interaction between agent and environment has been experimentally realized in a photonic setup. Quantum annealing is an optimization technique used to determine the local minima and maxima of a function over a given set of candidate functions. This is a method of discretizing a function with many local minima or maxima in order to determine the observables of the function. The process can be distinguished from Simulated annealing by the Quantum tunneling process, by which particles tunnel through kinetic or potential barriers from a high state to a low state. Quantum annealing starts from a superposition of all possible states of a system, weighted equally. Then the time-dependent Schrödinger equation guides the time evolution of the system, serving to affect the amplitude of each state as time increases. Eventually, the ground state can be reached to yield the instantaneous Hamiltonian of the system. Sampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society. Examples include deep learning, probabilistic programming, and other machine learning and artificial intelligence applications. A computationally hard problem, which is key for some relevant machine learning tasks, is the estimation of averages over probabilistic models defined in terms of a Boltzmann distribution. Sampling from generic probabilistic models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects. Some research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks. The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms. Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset. The D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures. Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks. The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets. In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward. Reverse annealing has been used as well to solve a fully connected quantum restricted Boltzmann machine. Inspired by the success of Boltzmann machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed. Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines. Quantum annealing is not the only technology for sampling. In a prepare-and-measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing. The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first-order logic template. This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware. Quantum analogues or generalizations of classical neural nets are often referred to as quantum neural networks. The term is claimed by a wide range of approaches, including the implementation and extension of neural networks using photons, layered variational circuits or quantum Ising-type models. A novel design for multi-dimensional vectors that uses circuits as convolution filters is QCNN. It was inspired by the advantages of CNNs and the power of QML. It is made using a combination of a variational quantum circuit (VQC) and a deep neural network(DNN), fully utilizing the power of extremely parallel processing on a superposition of a quantum state with a finite number of qubits. The main strategy is to carry out an iterative optimization process in the NISQ devices, without the negative impact of noise, which is possibly incorporated into the circuit parameter, and without the need for quantum error correction. The quantum circuit must effectively handle spatial information in order for QCNN to function as CNN. The convolution filter is the most basic technique for making use of spatial information. One or more quantum convolutional filters make up a quantum convolutional neural network (QCNN), and each of these filters transforms input data using a quantum circuit that can be created in an organized or randomized way. Three parts that make up the quantum convolutional filter are: the encoder, the parameterized quantum circuit (PQC), and the measurement. The quantum convolutional filter can be seen as an extension of the filter in the traditional CNN because it was designed with trainable parameters. Quantum neural networks take advantage of the hierarchical structures, and for each subsequent layer, the number of qubits from the preceding layer is decreased by a factor of two. For n input qubits, these structure have O(log(n)) layers, allowing for shallow circuit depth. Additionally, they are able to avoid \"barren plateau,\" one of the most significant issues with PQC-based algorithms, ensuring trainability. Despite the fact that the QCNN model does not include the corresponding quantum operation, the fundamental idea of the pooling layer is also offered to assure validity. In QCNN architecture, the pooling layer is typically placed between succeeding convolutional layers. Its function is to shrink the representation's spatial size while preserving crucial features, which allows it to reduce the number of parameters, streamline network computing, and manage over-fitting. Such process can be accomplished applying full Tomography on the state to reduce it all the way down to one qubit and then processed it in subway. The most frequently used unit type in the pooling layer is max pooling, although there are other types as well. Similar to conventional feed-forward neural networks, the last module is a fully connected layer with full connections to all activations in the preceding layer. Translational invariance, which requires identical blocks of parameterized quantum gates within a layer, is a distinctive feature of the QCNN architecture. In the most general case of QML, both the learning device and the system under study, as well as their interaction, are fully quantum. This section gives a few examples of results on this topic. One class of problem that can benefit from the fully quantum approach is that of 'learning' unknown quantum states, processes or measurements, in the sense that one can subsequently reproduce them on another quantum system. For example, one may wish to learn a measurement that discriminates between two coherent states, given not a classical description of the states to be discriminated, but instead a set of example quantum systems prepared in these states. The naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information. This would only require classical learning. However, one can show that a fully quantum approach is strictly superior in this case. (This also relates to work on quantum pattern matching.) The problem of learning unitary transformations can be approached in a similar way. Going beyond the specific problem of learning states and transformations, the task of clustering also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum. Finally, a general framework spanning supervised, unsupervised and reinforcement learning in the fully quantum setting was introduced in, where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning. Such a speedup in the reinforcement-learning paradigm has been experimentally demonstrated in a photonic setup. The need for models that can be understood by humans emerges in QML in analogy to classical machine learning and drives the research field of explainable QML (or XQML in analogy to XAI/XML). These efforts are often also referred to as Interpretable Machine Learning (IML, and by extension IQML). XQML/IQML can be considered as an alternative research direction instead of finding a quantum advantage. For example, XQML has been used in the context of mobile malware detection and classification. Quantum Shapley values have also been proposed to interpret gates within a circuit based on a game-theoretic approach. For this purpose, gates instead of features act as players in a coalitional game with a value function that depends on measurements of the quantum circuit of interest. Additionally, a quantum version of the classical technique known as LIME (Linear Interpretable Model-Agnostic Explanations) has also been proposed, known as Q-LIME. Quantum kernel methods have emerged as particularly promising approaches for near-term applications. Large-scale benchmarking studies encompassing over 20,000 trained models have provided comprehensive insights into the effectiveness of fidelity quantum kernels (FQKs) and projected quantum kernels (PQKs) across diverse classification and regression tasks. These studies have revealed universal patterns that guide effective quantum kernel method design. In the generative modeling domain, quantum generative adversarial networks and quantum circuit Born machines have shown particular promise for tabular data synthesis. Novel quantum generative models for tabular data have demonstrated performance improvements of 8.5% over leading classical models while using only 0.072% of the parameters, indicating significant potential for parameter-efficient learning. The term \"quantum machine learning\" sometimes refers to classical machine learning performed on data from quantum systems. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other applications include learning Hamiltonians and automatically generating quantum experiments. Quantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide. The framework is very similar to that of classical computational learning theory, but the learner in this case is a quantum information processing device, while the data may be either classical or quantum. Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider specific problems and to use quantum protocols to improve the time complexity of classical algorithms for these problems. Although quantum learning theory is still under development, partial results in this direction have been obtained. The starting point in learning theory is typically a concept class, a set of possible concepts. Usually a concept is a function on some domain, such as { 0 , 1 } n {\\displaystyle \\{0,1\\}^{n}} . For example, the concept class could be the set of disjunctive normal form (DNF) formulas on n bits or the set of Boolean circuits of some constant depth. The goal for the learner is to learn (exactly or approximately) an unknown target concept from this concept class. The learner may be actively interacting with the target concept, or passively receiving samples from it. In active learning, a learner can make membership queries to the target concept c, asking for its value c(x) on inputs x chosen by the learner. The learner then has to reconstruct the exact target concept, with high probability. In the model of quantum exact learning, the learner can make membership queries in quantum superposition. If the complexity of the learner is measured by the number of membership queries it makes, then quantum exact learners can be polynomially more efficient than classical learners for some concept classes, but not more. If complexity is measured by the amount of time the learner uses, then there are concept classes that can be learned efficiently by quantum learners but not by classical learners (under plausible complexity-theoretic assumptions). A natural model of passive learning is Valiant's probably approximately correct (PAC) learning. Here the learner receives random examples (x,c(x)), where x is distributed according to some unknown distribution D. The learner's goal is to output a hypothesis function h such that h(x)=c(x) with high probability when x is drawn according to D. The learner has to be able to produce such an 'approximately correct' h for every D and every target concept c in its concept class. We can consider replacing the random examples by potentially more powerful quantum examples ∑ x D ( x ) | x , c ( x ) ⟩ {\\displaystyle \\sum _{x}{\\sqrt {D(x)}}|x,c(x)\\rangle } . In the PAC model (and the related agnostic model), this doesn't significantly reduce the number of examples needed: for every concept class, classical and quantum sample complexity are the same up to constant factors. However, for learning under some fixed distribution D, quantum examples can be very helpful, for example for learning DNF under the uniform distribution. When considering time complexity, there exist concept classes that can be PAC-learned efficiently by quantum learners, even from classical examples, but not by classical learners (again, under plausible complexity-theoretic assumptions). This passive learning type is also the most common scheme in supervised learning: a learning algorithm typically takes the training examples fixed, without the ability to query the label of unlabelled examples. Outputting a hypothesis h is a step of induction. Classically, an inductive model splits into a training and an application phase: the model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase. In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources. The earliest experiments were conducted using the adiabatic D-Wave quantum computer, for instance, to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009. Many experiments followed on the same architecture, and leading tech companies have shown interest in the potential of QML for future technological implementations. In 2013, Google Research, NASA, and the Universities Space Research Association launched the Quantum Artificial Intelligence Lab which explores the use of the adiabatic D-Wave quantum computer. A more recent example trained a probabilistic generative models with arbitrary pairwise connectivity, showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits. Using a different annealing technology based on nuclear magnetic resonance (NMR), a quantum Hopfield network was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation. NMR technology also enables universal quantum computing, and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state quantum computer in 2015. The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space, the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal. Photonic implementations are attracting more attention, not the least because they do not require extensive cooling. Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013. Using non-linear photonics to implement an all-optical linear classifier, a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule. A core building block in many learning algorithms is to calculate the distance between two vectors: this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in 2015. Recently, based on a neuromimetic approach, a novel ingredient has been added to the field of QML, in the form of a so-called quantum memristor, a quantized model of the standard classical memristor. This device can be constructed by means of a tunable resistor, weak measurements on the system, and a classical feed-forward mechanism. An implementation of a quantum memristor in superconducting circuits has been proposed, and an experiment with quantum dots performed. A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network. Since 2016, IBM has launched an online cloud-based platform for quantum software developers, called the IBM Q Experience. This platform consists of several fully operational quantum processors accessible via the IBM Web API. In doing so, the company is encouraging software developers to pursue new algorithms through a development environment with quantum capabilities. New architectures are being explored on an experimental basis, up to 32 qubits, using both trapped-ion and superconductive quantum computing methods. In October 2019, it was noted that the introduction of Quantum Random Number Generators (QRNGs) to machine learning models including Neural Networks and Convolutional Neural Networks for random initial weight distribution and Random Forests for splitting processes had a profound effect on their ability when compared to the classical method of Pseudorandom Number Generators (PRNGs). However, in a more recent publication from 2021, these claims could not be reproduced for Neural Network weight initialization and no significant advantage of using QRNGs over PRNGs was found. The work also demonstrated that the generation of fair random numbers with a gate quantum computer is a non-trivial task on NISQ devices, and QRNGs are therefore typically much more difficult to use in practice than PRNGs. A paper published in December 2018 reported on an experiment using a trapped-ion system demonstrating a quantum speedup of the deliberation time of reinforcement learning agents employing internal quantum hardware. In March 2021, a team of researchers from Austria, The Netherlands, the US and Germany reported the experimental demonstration of a quantum speedup of the learning time of reinforcement learning agents interacting fully quantumly with the environment. The relevant degrees of freedom of both agent and environment were realized on a compact and fully tunable integrated nanophotonic processor. The barren plateau problem where quantum algorithms encounter flat optimization landscapes-has seen significant theoretical and practical advances in 2025. Los Alamos National Laboratory researchers have provided the first mathematical characterization of why and when barren plateaus occur in variational quantum algorithms, establishing theoretical guarantees for algorithm scalability. This work solves a key usability problem for quantum machine learning by providing rigorous theorems that predict whether a given architecture will remain trainable as it scales to larger quantum systems. The breakthrough eliminates the previous trial-and-error approach that had led to researcher fatigue in the field. While machine learning itself is now not only a research field but an economically significant and fast growing industry and quantum computing is a well established field of both theoretical and experimental research, QML remains a purely theoretical field of studies. Attempts to experimentally demonstrate concepts of QML remain insufficient. Further, another obstacle exists at the prediction stage because the outputs of quantum learning models are inherently random. This creates an often considerable overhead, as many executions of a quantum learning model have to be aggregated to obtain an actual prediction. Many of the leading scientists that extensively publish in the field of QML warn about the extensive hype around the topic and are very restrained if asked about its practical uses in the foreseeable future. Sophia Chen collected some of the statements made by well known scientists in the field: \"I think we haven't done our homework yet. This is an extremely new scientific field,\" - physicist Maria Schuld of Canada-based quantum computing startup Xanadu. “When mixing machine learning with ‘quantum,’ you catalyse a hype-condensate.” - Jacob Biamonte a contributor to the theory of quantum computation. \"There is a lot more work that needs to be done before claiming quantum machine learning will actually work,\" - computer scientist Iordanis Kerenidis, the head of quantum algorithms at the Silicon Valley-based quantum computing startup QC Ware. \"I have not seen a single piece of evidence that there exists a meaningful [machine learning] task for which it would make sense to use a quantum computer and not a classical computer,\" - physicist Ryan Sweke of the Free University of Berlin in Germany. “Don't fall for the hype!” - Frank Zickert, who is the author of probably the most practical book related to the subject beware that ”quantum computers are far away from advancing machine learning for their representation ability”, and even speaking about evaluation and optimization for any kind of useful task quantum supremacy is not yet achieved. Furthermore, nobody among the active researchers in the field make any forecasts about when it could possibly become practical.",
      "sentences": [
        "Quantum machine learning (QML) is the study of quantum algorithms which solve machine learning tasks.",
        "The most common use of the term refers to quantum algorithms for machine learning tasks which analyze classical data, sometimes called quantum-enhanced machine learning.",
        "QML algorithms use qubits and quantum operations to try to improve the space and time complexity of classical machine learning algortihms.",
        "This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device.",
        "These routines can be more complex in nature and executed faster on a quantum computer.",
        "Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.",
        "The term \"quantum machine learning\" is sometimes use to refer classical machine learning methods applied to data generated from quantum experiments (i.e.",
        "machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.",
        "QML also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks.",
        "For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.",
        "Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".",
        "Quantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques.",
        "Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing.",
        "Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system.",
        "For example, the outcome of the measurement of a qubit reveals the result of a binary classification task.",
        "While many proposals of QML algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices.",
        "Associative (or content-addressable) memories are able to recognize stored content on the basis of a similarity measure, while random access memories are accessed by the address of stored information and not its content.",
        "As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition.",
        "The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration.",
        "Unfortunately, classical associative memories are severely limited by the phenomenon of cross-talk.",
        "When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible.",
        "The number of storable patterns is typically limited by a linear function of the number of neurons, p ≤ O ( n ) {\\displaystyle p\\leq O(n)} .",
        "Quantum associative memories (in their simplest realization) store patterns in a unitary matrix U acting on the Hilbert space of n qubits.",
        "Retrieval is realized by the unitary evolution of a fixed initial state to a quantum superposition of the desired patterns with probability distribution peaked on the most similar pattern to an input.",
        "By its very quantum nature, the retrieval process is thus probabilistic.",
        "Because quantum associative memories are free from cross-talk, however, spurious memories are never generated.",
        "Correspondingly, they have a superior capacity than classical ones.",
        "The number of parameters in the unitary matrix U is O ( p n ) {\\displaystyle O(pn)} .",
        "One can thus have efficient, spurious-memory-free quantum associative memories for any polynomial number of patterns.",
        "A number of quantum algorithms for machine learning are based on the idea of amplitude encoding, that is, to associate the amplitudes of a quantum state with the inputs and outputs of computations.",
        "Since a state of n {\\displaystyle n} qubits is described by 2 n {\\displaystyle 2^{n}} complex amplitudes, this information encoding can allow for an exponentially compact representation.",
        "Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector.",
        "The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits n {\\displaystyle n} , which amounts to a logarithmic time complexity in the number of amplitudes and thereby the dimension of the input.",
        "Many QML algorithms in this category are based on variations of the quantum algorithm for linear systems of equations (colloquially called HHL, after the paper's authors) which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix.",
        "One of these conditions is that a Hamiltonian which entry wise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse or low rank.",
        "For reference, any known classical algorithm for matrix inversion requires a number of operations that grows more than quadratically in the dimension of the matrix (e.g.",
        "O ( n 2.373 ) {\\displaystyle O{\\mathord {\\left(n^{2.373}\\right)}}} ), but they are not restricted to sparse matrices.",
        "Quantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations, for example in least-squares linear regression, the least-squares version of support vector machines, and Gaussian processes.",
        "A crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset.",
        "Although efficient methods for state preparation are known for specific cases, this step easily hides the complexity of the task.",
        "In a variational quantum algorithm, a classical computer optimizes the parameters used to prepare a quantum state, while a quantum computer is used to do the actual state preparation and measurement.",
        "VQAs are considered promising candidates for noisy intermediate-scale quantum computers.",
        "Variational quantum circuits (or parameterized quantum circuits) are a popular class of VQAs where the parameters are those used in a fixed quantum circuit.",
        "Researchers have studied VQCs to solve optimization problems and find the ground state energy of complex quantum systems, which were difficult to solve using a classical computer.",
        "Pattern reorganization is one of the important tasks of machine learning, binary classification is one of the tools or algorithms to find patterns.",
        "Binary classification is used in supervised learning and in unsupervised learning.",
        "In QML, classical bits are converted to qubits and they are mapped to Hilbert space; complex value data are used in a quantum binary classifier to use the advantage of Hilbert space.",
        "By exploiting the quantum mechanic properties such as superposition, entanglement, interference the quantum binary classifier produces the accurate result in short period of time.",
        "Another approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on Grover's search algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms.",
        "These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the k-medians and the k-nearest neighbors algorithms.",
        "Other applications include quadratic speedups in the training of perceptrons.",
        "An example of amplitude amplification being used in a machine learning algorithm is Grover's search algorithm minimization.",
        "In which a subroutine uses Grover's search algorithm to find an element less than some previously defined element.",
        "This can be done with an oracle that determines whether or not a state with a corresponding element is less than the predefined one.",
        "Grover's algorithm can then find an element such that our condition is met.",
        "The minimization is initialized by some random element in our data set, and iteratively does this subroutine to find the minimum element in the data set.",
        "This minimization is notably used in quantum k-medians, and it has a speed up of at least O ( n k ) {\\displaystyle {\\mathcal {O}}\\left({\\sqrt {\\frac {n}{k}}}\\right)} compared to classical versions of k-medians, where n {\\displaystyle n} is the number of data points and k {\\displaystyle k} is the number of clusters.",
        "Amplitude amplification is often combined with quantum walks to achieve the same quadratic speedup.",
        "Quantum walks have been proposed to enhance Google's PageRank algorithm as well as the performance of reinforcement learning agents in the projective simulation framework.",
        "In quantum-enhanced reinforcement learning, a quantum agent interacts with a classical or quantum environment and occasionally receives rewards for its actions.",
        "In some situations, either because of the quantum processing capability of the agent, or due to the possibility to probe the environment in superpositions, a quantum speedup may be achieved.",
        "Implementations of these kinds of protocols have been proposed for systems of trapped ions and superconducting circuits.",
        "A quantum speedup of the agent's internal decision-making time has been experimentally demonstrated in trapped ions, while a quantum speedup of the learning time in a fully coherent (`quantum') interaction between agent and environment has been experimentally realized in a photonic setup.",
        "Quantum annealing is an optimization technique used to determine the local minima and maxima of a function over a given set of candidate functions.",
        "This is a method of discretizing a function with many local minima or maxima in order to determine the observables of the function.",
        "The process can be distinguished from Simulated annealing by the Quantum tunneling process, by which particles tunnel through kinetic or potential barriers from a high state to a low state.",
        "Quantum annealing starts from a superposition of all possible states of a system, weighted equally.",
        "Then the time-dependent Schrödinger equation guides the time evolution of the system, serving to affect the amplitude of each state as time increases.",
        "Eventually, the ground state can be reached to yield the instantaneous Hamiltonian of the system.",
        "Sampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society.",
        "Examples include deep learning, probabilistic programming, and other machine learning and artificial intelligence applications.",
        "A computationally hard problem, which is key for some relevant machine learning tasks, is the estimation of averages over probabilistic models defined in terms of a Boltzmann distribution.",
        "Sampling from generic probabilistic models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become.",
        "Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.",
        "Some research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks.",
        "The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms.",
        "Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution.",
        "The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset.",
        "The D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures.",
        "Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks.",
        "The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets.",
        "In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality.",
        "The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications.",
        "Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward.",
        "Reverse annealing has been used as well to solve a fully connected quantum restricted Boltzmann machine.",
        "Inspired by the success of Boltzmann machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed.",
        "Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial.",
        "This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling.",
        "It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines.",
        "Quantum annealing is not the only technology for sampling.",
        "In a prepare-and-measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements.",
        "This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing.",
        "The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts.",
        "Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first-order logic template.",
        "This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.",
        "Quantum analogues or generalizations of classical neural nets are often referred to as quantum neural networks.",
        "The term is claimed by a wide range of approaches, including the implementation and extension of neural networks using photons, layered variational circuits or quantum Ising-type models.",
        "A novel design for multi-dimensional vectors that uses circuits as convolution filters is QCNN.",
        "It was inspired by the advantages of CNNs and the power of QML.",
        "It is made using a combination of a variational quantum circuit (VQC) and a deep neural network(DNN), fully utilizing the power of extremely parallel processing on a superposition of a quantum state with a finite number of qubits.",
        "The main strategy is to carry out an iterative optimization process in the NISQ devices, without the negative impact of noise, which is possibly incorporated into the circuit parameter, and without the need for quantum error correction.",
        "The quantum circuit must effectively handle spatial information in order for QCNN to function as CNN.",
        "The convolution filter is the most basic technique for making use of spatial information.",
        "One or more quantum convolutional filters make up a quantum convolutional neural network (QCNN), and each of these filters transforms input data using a quantum circuit that can be created in an organized or randomized way.",
        "Three parts that make up the quantum convolutional filter are: the encoder, the parameterized quantum circuit (PQC), and the measurement.",
        "The quantum convolutional filter can be seen as an extension of the filter in the traditional CNN because it was designed with trainable parameters.",
        "Quantum neural networks take advantage of the hierarchical structures, and for each subsequent layer, the number of qubits from the preceding layer is decreased by a factor of two.",
        "For n input qubits, these structure have O(log(n)) layers, allowing for shallow circuit depth.",
        "Additionally, they are able to avoid \"barren plateau,\" one of the most significant issues with PQC-based algorithms, ensuring trainability.",
        "Despite the fact that the QCNN model does not include the corresponding quantum operation, the fundamental idea of the pooling layer is also offered to assure validity.",
        "In QCNN architecture, the pooling layer is typically placed between succeeding convolutional layers.",
        "Its function is to shrink the representation's spatial size while preserving crucial features, which allows it to reduce the number of parameters, streamline network computing, and manage over-fitting.",
        "Such process can be accomplished applying full Tomography on the state to reduce it all the way down to one qubit and then processed it in subway.",
        "The most frequently used unit type in the pooling layer is max pooling, although there are other types as well.",
        "Similar to conventional feed-forward neural networks, the last module is a fully connected layer with full connections to all activations in the preceding layer.",
        "Translational invariance, which requires identical blocks of parameterized quantum gates within a layer, is a distinctive feature of the QCNN architecture.",
        "In the most general case of QML, both the learning device and the system under study, as well as their interaction, are fully quantum.",
        "This section gives a few examples of results on this topic.",
        "One class of problem that can benefit from the fully quantum approach is that of 'learning' unknown quantum states, processes or measurements, in the sense that one can subsequently reproduce them on another quantum system.",
        "For example, one may wish to learn a measurement that discriminates between two coherent states, given not a classical description of the states to be discriminated, but instead a set of example quantum systems prepared in these states.",
        "The naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information.",
        "This would only require classical learning.",
        "However, one can show that a fully quantum approach is strictly superior in this case.",
        "(This also relates to work on quantum pattern matching.)",
        "The problem of learning unitary transformations can be approached in a similar way.",
        "Going beyond the specific problem of learning states and transformations, the task of clustering also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum.",
        "Finally, a general framework spanning supervised, unsupervised and reinforcement learning in the fully quantum setting was introduced in, where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning.",
        "Such a speedup in the reinforcement-learning paradigm has been experimentally demonstrated in a photonic setup.",
        "The need for models that can be understood by humans emerges in QML in analogy to classical machine learning and drives the research field of explainable QML (or XQML in analogy to XAI/XML).",
        "These efforts are often also referred to as Interpretable Machine Learning (IML, and by extension IQML).",
        "XQML/IQML can be considered as an alternative research direction instead of finding a quantum advantage.",
        "For example, XQML has been used in the context of mobile malware detection and classification.",
        "Quantum Shapley values have also been proposed to interpret gates within a circuit based on a game-theoretic approach.",
        "For this purpose, gates instead of features act as players in a coalitional game with a value function that depends on measurements of the quantum circuit of interest.",
        "Additionally, a quantum version of the classical technique known as LIME (Linear Interpretable Model-Agnostic Explanations) has also been proposed, known as Q-LIME.",
        "Quantum kernel methods have emerged as particularly promising approaches for near-term applications.",
        "Large-scale benchmarking studies encompassing over 20,000 trained models have provided comprehensive insights into the effectiveness of fidelity quantum kernels (FQKs) and projected quantum kernels (PQKs) across diverse classification and regression tasks.",
        "These studies have revealed universal patterns that guide effective quantum kernel method design.",
        "In the generative modeling domain, quantum generative adversarial networks and quantum circuit Born machines have shown particular promise for tabular data synthesis.",
        "Novel quantum generative models for tabular data have demonstrated performance improvements of 8.5% over leading classical models while using only 0.072% of the parameters, indicating significant potential for parameter-efficient learning.",
        "The term \"quantum machine learning\" sometimes refers to classical machine learning performed on data from quantum systems.",
        "A basic example of this is quantum state tomography, where a quantum state is learned from measurement.",
        "Other applications include learning Hamiltonians and automatically generating quantum experiments.",
        "Quantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide.",
        "The framework is very similar to that of classical computational learning theory, but the learner in this case is a quantum information processing device, while the data may be either classical or quantum.",
        "Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider specific problems and to use quantum protocols to improve the time complexity of classical algorithms for these problems.",
        "Although quantum learning theory is still under development, partial results in this direction have been obtained.",
        "The starting point in learning theory is typically a concept class, a set of possible concepts.",
        "Usually a concept is a function on some domain, such as { 0 , 1 } n {\\displaystyle \\{0,1\\}^{n}} .",
        "For example, the concept class could be the set of disjunctive normal form (DNF) formulas on n bits or the set of Boolean circuits of some constant depth.",
        "The goal for the learner is to learn (exactly or approximately) an unknown target concept from this concept class.",
        "The learner may be actively interacting with the target concept, or passively receiving samples from it.",
        "In active learning, a learner can make membership queries to the target concept c, asking for its value c(x) on inputs x chosen by the learner.",
        "The learner then has to reconstruct the exact target concept, with high probability.",
        "In the model of quantum exact learning, the learner can make membership queries in quantum superposition.",
        "If the complexity of the learner is measured by the number of membership queries it makes, then quantum exact learners can be polynomially more efficient than classical learners for some concept classes, but not more.",
        "If complexity is measured by the amount of time the learner uses, then there are concept classes that can be learned efficiently by quantum learners but not by classical learners (under plausible complexity-theoretic assumptions).",
        "A natural model of passive learning is Valiant's probably approximately correct (PAC) learning.",
        "In the PAC model (and the related agnostic model), this doesn't significantly reduce the number of examples needed: for every concept class, classical and quantum sample complexity are the same up to constant factors.",
        "However, for learning under some fixed distribution D, quantum examples can be very helpful, for example for learning DNF under the uniform distribution.",
        "When considering time complexity, there exist concept classes that can be PAC-learned efficiently by quantum learners, even from classical examples, but not by classical learners (again, under plausible complexity-theoretic assumptions).",
        "This passive learning type is also the most common scheme in supervised learning: a learning algorithm typically takes the training examples fixed, without the ability to query the label of unlabelled examples.",
        "Outputting a hypothesis h is a step of induction.",
        "Classically, an inductive model splits into a training and an application phase: the model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase.",
        "In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources.",
        "The earliest experiments were conducted using the adiabatic D-Wave quantum computer, for instance, to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009.",
        "Many experiments followed on the same architecture, and leading tech companies have shown interest in the potential of QML for future technological implementations.",
        "In 2013, Google Research, NASA, and the Universities Space Research Association launched the Quantum Artificial Intelligence Lab which explores the use of the adiabatic D-Wave quantum computer.",
        "A more recent example trained a probabilistic generative models with arbitrary pairwise connectivity, showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits.",
        "Using a different annealing technology based on nuclear magnetic resonance (NMR), a quantum Hopfield network was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation.",
        "NMR technology also enables universal quantum computing, and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state quantum computer in 2015.",
        "The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit.",
        "The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image.",
        "Once the vectors are defined on the feature space, the quantum support vector machine was implemented to classify the unknown input vector.",
        "The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal.",
        "Photonic implementations are attracting more attention, not the least because they do not require extensive cooling.",
        "Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013.",
        "Using non-linear photonics to implement an all-optical linear classifier, a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule.",
        "A core building block in many learning algorithms is to calculate the distance between two vectors: this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in 2015.",
        "Recently, based on a neuromimetic approach, a novel ingredient has been added to the field of QML, in the form of a so-called quantum memristor, a quantized model of the standard classical memristor.",
        "This device can be constructed by means of a tunable resistor, weak measurements on the system, and a classical feed-forward mechanism.",
        "An implementation of a quantum memristor in superconducting circuits has been proposed, and an experiment with quantum dots performed.",
        "A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network.",
        "Since 2016, IBM has launched an online cloud-based platform for quantum software developers, called the IBM Q Experience.",
        "This platform consists of several fully operational quantum processors accessible via the IBM Web API.",
        "In doing so, the company is encouraging software developers to pursue new algorithms through a development environment with quantum capabilities.",
        "New architectures are being explored on an experimental basis, up to 32 qubits, using both trapped-ion and superconductive quantum computing methods.",
        "In October 2019, it was noted that the introduction of Quantum Random Number Generators (QRNGs) to machine learning models including Neural Networks and Convolutional Neural Networks for random initial weight distribution and Random Forests for splitting processes had a profound effect on their ability when compared to the classical method of Pseudorandom Number Generators (PRNGs).",
        "However, in a more recent publication from 2021, these claims could not be reproduced for Neural Network weight initialization and no significant advantage of using QRNGs over PRNGs was found.",
        "The work also demonstrated that the generation of fair random numbers with a gate quantum computer is a non-trivial task on NISQ devices, and QRNGs are therefore typically much more difficult to use in practice than PRNGs.",
        "A paper published in December 2018 reported on an experiment using a trapped-ion system demonstrating a quantum speedup of the deliberation time of reinforcement learning agents employing internal quantum hardware.",
        "In March 2021, a team of researchers from Austria, The Netherlands, the US and Germany reported the experimental demonstration of a quantum speedup of the learning time of reinforcement learning agents interacting fully quantumly with the environment.",
        "The relevant degrees of freedom of both agent and environment were realized on a compact and fully tunable integrated nanophotonic processor.",
        "The barren plateau problem where quantum algorithms encounter flat optimization landscapes-has seen significant theoretical and practical advances in 2025.",
        "Los Alamos National Laboratory researchers have provided the first mathematical characterization of why and when barren plateaus occur in variational quantum algorithms, establishing theoretical guarantees for algorithm scalability.",
        "This work solves a key usability problem for quantum machine learning by providing rigorous theorems that predict whether a given architecture will remain trainable as it scales to larger quantum systems.",
        "The breakthrough eliminates the previous trial-and-error approach that had led to researcher fatigue in the field.",
        "While machine learning itself is now not only a research field but an economically significant and fast growing industry and quantum computing is a well established field of both theoretical and experimental research, QML remains a purely theoretical field of studies.",
        "Attempts to experimentally demonstrate concepts of QML remain insufficient.",
        "Further, another obstacle exists at the prediction stage because the outputs of quantum learning models are inherently random.",
        "This creates an often considerable overhead, as many executions of a quantum learning model have to be aggregated to obtain an actual prediction.",
        "Many of the leading scientists that extensively publish in the field of QML warn about the extensive hype around the topic and are very restrained if asked about its practical uses in the foreseeable future.",
        "Sophia Chen collected some of the statements made by well known scientists in the field: \"I think we haven't done our homework yet.",
        "This is an extremely new scientific field,\" - physicist Maria Schuld of Canada-based quantum computing startup Xanadu.",
        "“When mixing machine learning with ‘quantum,’ you catalyse a hype-condensate.” - Jacob Biamonte a contributor to the theory of quantum computation.",
        "\"There is a lot more work that needs to be done before claiming quantum machine learning will actually work,\" - computer scientist Iordanis Kerenidis, the head of quantum algorithms at the Silicon Valley-based quantum computing startup QC Ware.",
        "\"I have not seen a single piece of evidence that there exists a meaningful [machine learning] task for which it would make sense to use a quantum computer and not a classical computer,\" - physicist Ryan Sweke of the Free University of Berlin in Germany.",
        "“Don't fall for the hype!” - Frank Zickert, who is the author of probably the most practical book related to the subject beware that ”quantum computers are far away from advancing machine learning for their representation ability”, and even speaking about evaluation and optimization for any kind of useful task quantum supremacy is not yet achieved.",
        "Furthermore, nobody among the active researchers in the field make any forecasts about when it could possibly become practical."
      ],
      "metadata": {
        "title": "Quantum machine learning",
        "url": "https://en.wikipedia.org/wiki/Quantum_machine_learning",
        "word_count": 5159,
        "char_count": 34474,
        "sentence_count": 209,
        "scraped_at": "2025-08-09T14:46:41.345003",
        "language": "en",
        "processing_time": 0.008199214935302734,
        "source_hash": "0c2e06579e6351b60baf3a1aac44f904"
      }
    },
    {
      "title": "Neural network (machine learning)",
      "url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
      "raw_text": "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks.\nA neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\n\n\n== Training ==\nNeural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data.\n\n\n== History ==\n\n\n=== Early work ===\nToday's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.\nHistorically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.\nWarren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.\nIn the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). \nIn 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research.\nR. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\"\nThe perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.\nThe first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.\n\n\n=== Deep learning breakthroughs in the 1960s and 1970s ===\nFundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\"\nThe first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\nIn 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning.\nNevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967).\nIn 1976 transfer learning was introduced in neural networks learning.\nDeep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.\n\n\n=== Backpropagation ===\nBackpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.\n\n\n=== Convolutional neural networks ===\nKunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision.\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.\nIn 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images.\nFrom 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.\n\n\n=== Recurrent neural networks ===\nOne origin of RNN was statistical mechanics. In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning. This was popularized as the Hopfield network by John Hopfield (1982). Another origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. The McCulloch and Pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.\nIn 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a Crossbar Adaptive Array, used direct recurrent connections from the output to the supervisor (teaching) inputs. In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations. Eliminating the external supervisor, it introduced the self-learning method in neural networks.  \nIn cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on the relation between cognition and emotion. Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion. In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation. It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology.\nTwo early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. \nIn the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, Jürgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\" which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\nIn 1991, Sepp Hochreiter's diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains. This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999. It became the default choice for RNN architecture.\nDuring 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models.\n\n\n=== Deep learning ===\nBetween 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly.\nIn October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.\nIn 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".\nRadial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.\nGenerative adversarial network (GAN) (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014–2018 period. The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here, the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).\nIn 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in December 2015. ResNet behaves like an open-gated Highway Net. \n\nDuring the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need.\nIt requires computation time that is quadratic in the size of the context window. Jürgen Schmidhuber's fast weight controller (1992) scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.\nTransformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.\n\n\n== Models ==\n\nANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.\nAn artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons.\n\n\n=== Artificial neurons ===\n\nANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.\nTo find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.\n\n\n=== Organization ===\nThe neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.\n\n\n=== Hyperparameter ===\n\nA hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.\n\n\n=== Learning ===\n\nLearning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.\n\n\n==== Learning rate ====\n\nThe learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.\n\n\n==== Cost function ====\nWhile it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) because it arises from the model (e.g. in a probabilistic model, the model's posterior probability can be used as an inverse cost).\n\n\n==== Backpropagation ====\n\nBackpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backpropagation calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.\n\n\n=== Learning paradigms ===\n\nMachine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task.\n\n\n==== Supervised learning ====\nSupervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\n\n\n==== Unsupervised learning ====\nIn unsupervised learning, input data is given along with the cost function, some function of the data \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\textstyle x}\n  \n and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model \n  \n    \n      \n        \n          f\n          (\n          x\n          )\n          =\n          a\n        \n      \n    \n    {\\displaystyle \\textstyle f(x)=a}\n  \n where \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\textstyle a}\n  \n is a constant and the cost \n  \n    \n      \n        \n          C\n          =\n          E\n          [\n          (\n          x\n          −\n          f\n          (\n          x\n          )\n          \n            )\n            \n              2\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\textstyle C=E[(x-f(x))^{2}]}\n  \n. Minimizing this cost produces a value of \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\textstyle a}\n  \n that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\textstyle x}\n  \n and \n  \n    \n      \n        \n          f\n          (\n          x\n          )\n        \n      \n    \n    {\\displaystyle \\textstyle f(x)}\n  \n, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\n\n\n==== Reinforcement learning ====\n\nIn applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.\nFormally, the environment is modeled as a Markov decision process (MDP) with states \n  \n    \n      \n        \n          \n            \n              s\n              \n                1\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              s\n              \n                n\n              \n            \n          \n          ∈\n          S\n        \n      \n    \n    {\\displaystyle \\textstyle {s_{1},...,s_{n}}\\in S}\n  \n and actions \n  \n    \n      \n        \n          \n            \n              a\n              \n                1\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              a\n              \n                m\n              \n            \n          \n          ∈\n          A\n        \n      \n    \n    {\\displaystyle \\textstyle {a_{1},...,a_{m}}\\in A}\n  \n. Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution \n  \n    \n      \n        \n          P\n          (\n          \n            c\n            \n              t\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(c_{t}|s_{t})}\n  \n, the observation distribution \n  \n    \n      \n        \n          P\n          (\n          \n            x\n            \n              t\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(x_{t}|s_{t})}\n  \n and the transition distribution \n  \n    \n      \n        \n          P\n          (\n          \n            s\n            \n              t\n              +\n              1\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          ,\n          \n            a\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(s_{t+1}|s_{t},a_{t})}\n  \n, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.\nANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\n\n\n==== Self-learning ====\nSelf-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation:\n\n In situation s perform action a;\n Receive consequence situation s';\n Compute emotion of being in consequence situation v(s');\n Update crossbar memory w'(a,s) = w(a,s) + v(s').\n\nThe backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it receives initial emotions (only once) about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.\n\n\n==== Neuroevolution ====\n\nNeuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".\n\n\n=== Stochastic neural network ===\nStochastic neural networks originating from Sherrington–Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions , or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.\n\n\n=== Topological deep learning ===\nTopological deep learning, first introduced in 2017, is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and high-order data. Initially rooted in algebraic topology, TDL has since evolved into a versatile framework incorporating tools from other mathematical disciplines, such as differential topology and geometric topology. As a successful example of mathematical deep learning, TDL continues to inspire advancements in mathematical artificial intelligence, fostering a mutually beneficial relationship between AI and mathematics.  \n\n\n=== Other ===\nIn a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation–maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.\n\n\n==== Modes ====\n\nTwo modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning, weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.\n\n\n== Types ==\n\nANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\nSome of the main breakthroughs include: \n\nConvolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; where long short-term memory avoids the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads;\nCompetitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input.\n\n\n== Network design ==\nUsing artificial neural networks requires an understanding of their characteristics.\n\nChoice of model: This depends on the data representation and the application. Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc.). Overly complex models learn slowly.\nLearning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.\nRobustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.\nNeural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras. scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras.\nHyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc. The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters:\n\n\n== Applications ==\nBecause of their ability to model and reproduce nonlinear processes, artificial neural networks have found applications in many disciplines. These include:\n\nFunction approximation, or regression analysis, (including time series prediction, fitness approximation, and modeling)\nData processing (including filtering, clustering, blind source separation, and compression)\nNonlinear system identification and control (including vehicle control, trajectory prediction, adaptive control, process control, and natural resource management)\nPattern recognition (including radar systems, face identification, signal classification, novelty detection, 3D reconstruction, object recognition, and sequential decision making)\nSequence recognition (including gesture, speech, and handwritten and printed text recognition)\nSensor data analysis (including image analysis)\nRobotics (including directing manipulators and prostheses)\nData mining (including knowledge discovery in databases)\nFinance (such as ex-ante models for specific financial long-run forecasts and artificial financial markets)\nQuantum chemistry\nGeneral game playing\nGenerative AI\nData visualization\nMachine translation\nSocial network filtering\nE-mail spam filtering\nMedical diagnosis\nANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.\nANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.\nANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.\nIt is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.\nBeyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.\n\n\n== Theoretical properties ==\n\n\n=== Computational power ===\nThe multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\nA specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.\n\n\n=== Capacity ===\nA model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\nTwo notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.\n\n\n=== Convergence ===\nModels may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.\nAnother issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction.\nThe convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fit target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.\n\n\n=== Generalization and statistics ===\n\nApplications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. \nTwo approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\n\nSupervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\nBy assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.\nThe softmax activation function is:\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \n            \n              e\n              \n                \n                  x\n                  \n                    i\n                  \n                \n              \n            \n            \n              \n                ∑\n                \n                  j\n                  =\n                  1\n                \n                \n                  c\n                \n              \n              \n                e\n                \n                  \n                    x\n                    \n                      j\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}}\n  \n\n\n== Criticism ==\n\n\n=== Training ===\nA common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.\nAny learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.\nDean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right).\n\n\n=== Theory ===\nA central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a \n\nsomething-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything. One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.\nTechnology writer Roger Bridgman commented:\n\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\nIn spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\n\nAlthough it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.\nBiological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\n\n\n=== Hardware ===\nLarge and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time.\nSome argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.\nNeuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.\n\n\n=== Practical counterexamples ===\nAnalyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.\n\n\n=== Hybrid approaches ===\nAdvocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.\n\n\n=== Dataset bias ===\nNeural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement. For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field. The program would penalize any resume with the word \"woman\" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.\n\n\n== Gallery ==\n\n\n== Recent advancements and future directions ==\nArtificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.\n\n\n=== Image processing ===\nIn the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance. This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.\n\n\n=== Speech recognition ===\nBy modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques. These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.\n\n\n=== Natural language processing ===\nIn natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content. This has implications for automated customer service, content moderation, and language understanding technologies.\n\n\n=== Control systems ===\nIn the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications.\n\n\n=== Finance ===\n\nANNs are used for stock market prediction and credit scoring: \n\nIn investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions.\nIn credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process.\nANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.\n\n\n=== Medicine ===\nANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning. In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs. Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.\n\n\n=== Content creation ===\nANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry, generative models are used to create personalized advertisements for consumers. Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020. Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== External links ==\n\nA Brief Introduction to Neural Networks (D. Kriesel) – Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.\nReview of Neural Networks in Materials Science Archived 7 June 2015 at the Wayback Machine\nArtificial Neural Networks Tutorial in three languages (Univ. Politécnica de Madrid)\nAnother introduction to ANN\nNext Generation of Neural Networks Archived 24 January 2011 at the Wayback Machine – Google Tech Talks\nPerformance of Neural Networks\nNeural Networks and Information Archived 9 July 2009 at the Wayback Machine\nSanderson G (5 October 2017). \"But what is a Neural Network?\". 3Blue1Brown. Archived from the original on 7 November 2021 – via YouTube.",
      "cleaned_text": "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks. A neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers. Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information. Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. Today's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing. Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. In the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research. R. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\" The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. The first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning. Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\" The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967). In 1976 transfer learning was introduced in neural networks learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. Kunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision. The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images. From 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments. One origin of RNN was statistical mechanics. In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning. This was popularized as the Hopfield network by John Hopfield (1982). Another origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. The McCulloch and Pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past. In 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a Crossbar Adaptive Array, used direct recurrent connections from the output to the supervisor (teaching) inputs. In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations. Eliminating the external supervisor, it introduced the self-learning method in neural networks. In cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on the relation between cognition and emotion. Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion. In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation. It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology. Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. In the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, Jürgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\" which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. In 1991, Sepp Hochreiter's diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains. This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999. It became the default choice for RNN architecture. During 1985-1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models. Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly. In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\". Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications. Generative adversarial network (GAN) (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014-2018 period. The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here, the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022). In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in December 2015. ResNet behaves like an open-gated Highway Net. During the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need. It requires computation time that is quadratic in the size of the context window. Jürgen Schmidhuber's fast weight controller (1992) scales linearly and was later shown to be equivalent to the unnormalized linear Transformer. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture. ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph. An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons. ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image. To find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image. The neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks. A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers. Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation. The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change. While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) because it arises from the model (e.g. in a probabilistic model, the model's posterior probability can be used as an inverse cost). Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backpropagation calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks. Machine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task. Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. In unsupervised learning, input data is given along with the cost function, some function of the data x {\\displaystyle extstyle x} and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model f ( x ) = a {\\displaystyle extstyle f(x)=a} where a {\\displaystyle extstyle a} is a constant and the cost C = E [ ( x − f ( x ) ) 2 ] {\\displaystyle extstyle C=E[(x-f(x))^{2}]} . Minimizing this cost produces a value of a {\\displaystyle extstyle a} that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between x {\\displaystyle extstyle x} and f ( x ) {\\displaystyle extstyle f(x)} , whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering. In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly. Formally, the environment is modeled as a Markov decision process (MDP) with states s 1 , . . . , s n ∈ S {\\displaystyle extstyle {s_{1},...,s_{n}}\\in S} and actions a 1 , . . . , a m ∈ A {\\displaystyle extstyle {a_{1},...,a_{m}}\\in A} . Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution P ( c t | s t ) {\\displaystyle extstyle P(c_{t}|s_{t})} , the observation distribution P ( x t | s t ) {\\displaystyle extstyle P(x_{t}|s_{t})} and the transition distribution P ( s t + 1 | s t , a t ) {\\displaystyle extstyle P(s_{t+1}|s_{t},a_{t})} , while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC. ANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks. Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation: In situation s perform action a; Receive consequence situation s'; Compute emotion of being in consequence situation v(s'); Update crossbar memory w'(a,s) = w(a,s) + v(s'). The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it receives initial emotions (only once) about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations. Neuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\". Stochastic neural networks originating from Sherrington-Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions , or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks. Topological deep learning, first introduced in 2017, is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and high-order data. Initially rooted in algebraic topology, TDL has since evolved into a versatile framework incorporating tools from other mathematical disciplines, such as differential topology and geometric topology. As a successful example of mathematical deep learning, TDL continues to inspire advancements in mathematical artificial intelligence, fostering a mutually beneficial relationship between AI and mathematics. In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks. Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning, weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set. ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers. Some of the main breakthroughs include: Convolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; where long short-term memory avoids the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads; Competitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input. Using artificial neural networks requires an understanding of their characteristics. Choice of model: This depends on the data representation and the application. Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc.). Overly complex models learn slowly. Learning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation. Robustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust. Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras. scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras. Hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc. The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters: Because of their ability to model and reproduce nonlinear processes, artificial neural networks have found applications in many disciplines. These include: Function approximation, or regression analysis, (including time series prediction, fitness approximation, and modeling) Data processing (including filtering, clustering, blind source separation, and compression) Nonlinear system identification and control (including vehicle control, trajectory prediction, adaptive control, process control, and natural resource management) Pattern recognition (including radar systems, face identification, signal classification, novelty detection, 3D reconstruction, object recognition, and sequential decision making) Sequence recognition (including gesture, speech, and handwritten and printed text recognition) Sensor data analysis (including image analysis) Robotics (including directing manipulators and prostheses) Data mining (including knowledge discovery in databases) Finance (such as ex-ante models for specific financial long-run forecasts and artificial financial markets) Quantum chemistry General game playing Generative AI Data visualization Machine translation Social network filtering E-mail spam filtering Medical diagnosis ANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information. ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions. ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level. It is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition. Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation. The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters. A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power. A model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity. Two notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity. Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical. Another issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction. The convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fit target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions. Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting. Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is: y i = e x i ∑ j = 1 c e x j {\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} A common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation. Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC. Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns-it should not learn to always turn right). A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything. One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. Technology writer Roger Bridgman commented: Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\". In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture. Biological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies. Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons - which require enormous CPU power and time. Some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days. Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU. Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture. Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. Neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement. For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field. The program would penalize any resume with the word \"woman\" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets. Artificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine. In the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance. This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging. By modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques. These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products. In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content. This has implications for automated customer service, content moderation, and language understanding technologies. In the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications. ANNs are used for stock market prediction and credit scoring: In investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions. In credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process. ANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies. ANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning. In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs. Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine. ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry, generative models are used to create personalized advertisements for consumers. Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020. Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.",
      "sentences": [
        "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks.",
        "A neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain.",
        "Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance.",
        "These are connected by edges, which model the synapses in the brain.",
        "Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons.",
        "The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function.",
        "The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.",
        "Typically, neurons are aggregated into layers.",
        "Different layers may perform different transformations on their inputs.",
        "A network is typically called a deep neural network if it has at least two hidden layers.",
        "Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence.",
        "They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.",
        "Neural networks are typically trained through empirical risk minimization.",
        "This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset.",
        "Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network.",
        "During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function.",
        "This method allows the network to generalize to unseen data.",
        "Today's deep neural networks are based on early work in statistics over 200 years ago.",
        "The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights.",
        "The sum of the products of the weights and the inputs is calculated at each node.",
        "The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights.",
        "This technique has been known for over two centuries as the method of least squares or linear regression.",
        "It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.",
        "Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors.",
        "Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism.",
        "Unlike the von Neumann model, connectionist computing does not separate memory and processing.",
        "Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks.",
        "This model paved the way for research to split into two approaches.",
        "One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.",
        "In the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning.",
        "It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network.",
        "Farley and Clark (1954) used computational machines to simulate a Hebbian network.",
        "Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).",
        "In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research.",
        "R. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\"",
        "However, \"they dropped the subject.\"",
        "The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding.",
        "This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.",
        "The first perceptrons did not have adaptive hidden units.",
        "However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer.",
        "Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight.",
        "Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.",
        "Fundamental research was conducted on ANNs in the 1960s and 1970s.",
        "The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965).",
        "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron.",
        "A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis.",
        "Superfluous hidden units are pruned using a separate validation set.",
        "Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\"",
        "The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.",
        "In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes.",
        "Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.",
        "In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.",
        "The rectifier has become the most popular activation function for deep learning.",
        "Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit.",
        "This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967).",
        "In 1976 transfer learning was introduced in neural networks learning.",
        "Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.",
        "Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes.",
        "The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.",
        "In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970).",
        "Ostrovski et al.",
        "republished it in 1971.",
        "Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm).",
        "In 1986, David E. Rumelhart et al.",
        "popularised backpropagation but did not cite the original work.",
        "Kunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs.",
        "CNNs have become an essential tool for computer vision.",
        "The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition.",
        "It used convolutions, weight sharing, and backpropagation.",
        "In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.",
        "In 1989, Yann LeCun et al.",
        "created a CNN called LeNet for recognizing handwritten ZIP codes on mail.",
        "Training required 3 days.",
        "In 1990, Wei Zhang implemented a CNN on optical computing hardware.",
        "In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms.",
        "LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images.",
        "From 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.",
        "One origin of RNN was statistical mechanics.",
        "In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning.",
        "This was popularized as the Hopfield network by John Hopfield (1982).",
        "Another origin of RNN was neuroscience.",
        "The word \"recurrent\" is used to describe loop-like structures in anatomy.",
        "In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex.",
        "Hebb considered \"reverberating circuit\" as an explanation for short-term memory.",
        "The McCulloch and Pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.",
        "In 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a Crossbar Adaptive Array, used direct recurrent connections from the output to the supervisor (teaching) inputs.",
        "In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations.",
        "Eliminating the external supervisor, it introduced the self-learning method in neural networks.",
        "In cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on the relation between cognition and emotion.",
        "Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion.",
        "In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation.",
        "It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology.",
        "Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology.",
        "In the 1980s, backpropagation did not work well for deep RNNs.",
        "To overcome this problem, in 1991, Jürgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\" which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation.",
        "In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.",
        "In 1991, Sepp Hochreiter's diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it.",
        "He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains.",
        "This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999.",
        "It became the default choice for RNN architecture.",
        "During 1985-1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm.",
        "These were designed for unsupervised learning of deep generative models.",
        "Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition.",
        "In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.",
        "It then won more contests.",
        "They also showed how max-pooling CNNs on GPU improved performance significantly.",
        "In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods.",
        "Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.",
        "In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images.",
        "Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".",
        "Radial basis function and wavelet networks were introduced in 2013.",
        "These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.",
        "Generative adversarial network (GAN) (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014-2018 period.",
        "The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.",
        "The first network is a generative model that models a probability distribution over output patterns.",
        "The second network learns by gradient descent to predict the reactions of the environment to these patterns.",
        "Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al.",
        "Here, the GAN generator is grown from small to large scale in a pyramidal fashion.",
        "Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.",
        "In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers.",
        "Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem.",
        "In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in December 2015.",
        "ResNet behaves like an open-gated Highway Net.",
        "During the 2010s, the seq2seq model was developed, and attention mechanisms were added.",
        "It led to the modern Transformer architecture in 2017 in Attention Is All You Need.",
        "It requires computation time that is quadratic in the size of the context window.",
        "Jürgen Schmidhuber's fast weight controller (1992) scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.",
        "Transformers have increasingly become the model of choice for natural language processing.",
        "Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.",
        "ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with.",
        "They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors.",
        "ANNs have the ability to learn and model non-linearities and complex relationships.",
        "This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others.",
        "The network forms a directed, weighted graph.",
        "An artificial neural network consists of simulated neurons.",
        "Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection.",
        "All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data.",
        "Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons.",
        "ANNs are composed of artificial neurons which are conceptually derived from biological neurons.",
        "Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons.",
        "The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons.",
        "The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.",
        "To find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron.",
        "We add a bias term to this sum.",
        "This weighted sum is sometimes called the activation.",
        "This weighted sum is then passed through a (usually nonlinear) activation function to produce the output.",
        "The initial inputs are external data, such as images and documents.",
        "The ultimate outputs accomplish the task, such as recognizing an object in an image.",
        "The neurons are typically organized into multiple layers, especially in deep learning.",
        "Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers.",
        "The layer that receives external data is the input layer.",
        "The layer that produces the ultimate result is the output layer.",
        "In between them are zero or more hidden layers.",
        "Single layer and unlayered networks are also used.",
        "Between two layers, multiple connection patterns are possible.",
        "They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer.",
        "They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer.",
        "Neurons with only such connections form a directed acyclic graph and are known as feedforward networks.",
        "Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.",
        "A hyperparameter is a constant parameter whose value is set before the learning process begins.",
        "The values of parameters are derived via learning.",
        "Examples of hyperparameters include learning rate, the number of hidden layers and batch size.",
        "The values of some hyperparameters can be dependent on those of other hyperparameters.",
        "For example, the size of some layers can depend on the overall number of layers.",
        "Learning is the adaptation of the network to better handle a task by considering sample observations.",
        "Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result.",
        "This is done by minimizing the observed errors.",
        "Learning is complete when examining additional observations does not usefully reduce the error rate.",
        "Even after learning, the error rate typically does not reach 0.",
        "If after learning, the error rate is too high, the network typically must be redesigned.",
        "Practically this is done by defining a cost function that is evaluated periodically during learning.",
        "As long as its output continues to decline, learning continues.",
        "The cost is frequently defined as a statistic whose value can only be approximated.",
        "The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small.",
        "Learning attempts to reduce the total of the differences across the observations.",
        "Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.",
        "The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation.",
        "A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy.",
        "Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability.",
        "In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate.",
        "The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change.",
        "A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.",
        "While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) because it arises from the model (e.g.",
        "in a probabilistic model, the model's posterior probability can be used as an inverse cost).",
        "Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning.",
        "The error amount is effectively divided among the connections.",
        "Technically, backpropagation calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights.",
        "The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.",
        "Machine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning.",
        "Each corresponds to a particular learning task.",
        "Supervised learning uses a set of paired inputs and desired outputs.",
        "The learning task is to produce the desired output for each input.",
        "In this case, the cost function is related to eliminating incorrect deductions.",
        "A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output.",
        "Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation).",
        "Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition).",
        "This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.",
        "In unsupervised learning, input data is given along with the cost function, some function of the data x {\\displaystyle extstyle x} and the network's output.",
        "The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables).",
        "Minimizing this cost produces a value of a {\\displaystyle extstyle a} that is equal to the mean of the data.",
        "The cost function can be much more complicated.",
        "Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.",
        "In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one.",
        "The goal is to win the game, i.e., generate the most positive (lowest cost) responses.",
        "In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost.",
        "At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules.",
        "The rules and the long-term cost usually only can be estimated.",
        "At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.",
        "Formally, the environment is modeled as a Markov decision process (MDP) with states s 1 , .",
        ", s n ∈ S {\\displaystyle extstyle {s_{1},...,s_{n}}\\in S} and actions a 1 , .",
        "Taken together, the two define a Markov chain (MC).",
        "The aim is to discover the lowest-cost MC.",
        "ANNs serve as the learning component in such applications.",
        "Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems.",
        "Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.",
        "Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA).",
        "It is a system with only one input, situation s, and only one output, action (or behavior) a.",
        "It has neither external advice input nor external reinforcement input from the environment.",
        "The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations.",
        "The system is driven by the interaction between cognition and emotion.",
        "The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation.",
        "The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it receives initial emotions (only once) about to be encountered situations in the behavioral environment.",
        "Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.",
        "Neuroevolution can create neural network topologies and weights using evolutionary computation.",
        "It is competitive with sophisticated gradient descent approaches.",
        "One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".",
        "Stochastic neural networks originating from Sherrington-Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions , or by giving them stochastic weights.",
        "This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima.",
        "Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.",
        "Topological deep learning, first introduced in 2017, is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and high-order data.",
        "Initially rooted in algebraic topology, TDL has since evolved into a versatile framework incorporating tools from other mathematical disciplines, such as differential topology and geometric topology.",
        "As a successful example of mathematical deep learning, TDL continues to inspire advancements in mathematical artificial intelligence, fostering a mutually beneficial relationship between AI and mathematics.",
        "In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost.",
        "Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other learning algorithms.",
        "Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.",
        "Two modes of learning are available: stochastic and batch.",
        "In stochastic learning, each input creates a weight adjustment.",
        "In batch learning, weights are adjusted based on a batch of inputs, accumulating errors over the batch.",
        "Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima.",
        "However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error.",
        "A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.",
        "ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.",
        "The simplest types have one or more static components, including number of units, number of layers, unit weights and topology.",
        "Dynamic types allow one or more of these to evolve via learning.",
        "The latter is much more complicated but can shorten learning periods and produce better results.",
        "Some types allow/require learning to be \"supervised\" by the operator, while others operate independently.",
        "Some types operate purely in hardware, while others are purely software and run on general purpose computers.",
        "Using artificial neural networks requires an understanding of their characteristics.",
        "Choice of model: This depends on the data representation and the application.",
        "Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc.).",
        "Overly complex models learn slowly.",
        "Learning algorithm: Numerous trade-offs exist between learning algorithms.",
        "Almost any algorithm will work well with the correct hyperparameters for training on a particular data set.",
        "However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.",
        "Robustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.",
        "Neural architecture search (NAS) uses machine learning to automate ANN design.",
        "Various approaches to NAS have designed networks that compare well with hand-designed systems.",
        "The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network.",
        "Available systems include AutoML and AutoKeras.",
        "scikit-learn library provides functions to help with building a deep network from scratch.",
        "We can then implement a deep network with TensorFlow or Keras.",
        "Hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc.",
        "The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters: Because of their ability to model and reproduce nonlinear processes, artificial neural networks have found applications in many disciplines.",
        "ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements.",
        "It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff.",
        "ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology.",
        "ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones.",
        "For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk.",
        "Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.",
        "ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems.",
        "In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems.",
        "Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.",
        "It is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.",
        "Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science.",
        "For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals.",
        "This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.",
        "The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem.",
        "However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.",
        "A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections.",
        "Further, the use of irrational values for weights results in a machine with super-Turing power.",
        "A model's \"capacity\" property corresponds to its ability to model any given function.",
        "It is related to the amount of information that can be stored in the network and to the notion of complexity.",
        "Two notions of capacity are known by the community.",
        "The information capacity and the VC Dimension.",
        "The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover.",
        "The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element.",
        "The information capacity captures the functions modelable by the network given any data as input.",
        "The second notion, is the VC dimension.",
        "VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances.",
        "This is, given input data in a specific form.",
        "As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a perceptron.",
        "The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.",
        "Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model.",
        "Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum.",
        "Thirdly, for sufficiently large data or parameters, some methods become impractical.",
        "Another issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction.",
        "The convergence behavior of certain types of ANN architectures are more understood than others.",
        "When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models.",
        "Another example is when parameters are small, it is observed that ANNs often fit target functions from low to high frequencies.",
        "This behavior is referred to as the spectral bias, or frequency principle, of neural networks.",
        "This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method.",
        "Deeper neural networks have been observed to be more biased towards low frequency functions.",
        "Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training.",
        "This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters.",
        "Two approaches address over-training.",
        "The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.",
        "The second is to use some form of regularization.",
        "This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.",
        "Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model.",
        "The MSE on a validation set can be used as an estimate for variance.",
        "This value can then be used to calculate the confidence interval of network output, assuming a normal distribution.",
        "A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.",
        "By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities.",
        "This is useful in classification as it gives a certainty measure on classifications.",
        "The softmax activation function is: y i = e x i ∑ j = 1 c e x j {\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} A common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.",
        "Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases.",
        "Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.",
        "Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.",
        "), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns-it should not learn to always turn right).",
        "A central claim of ANNs is that they embody new and powerful general principles for processing information.",
        "These principles are ill-defined.",
        "It is often claimed that they are emergent from the network itself.",
        "This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition.",
        "In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are.",
        "No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything.",
        "One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.",
        "Technology writer Roger Bridgman commented: Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?)",
        "but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".",
        "In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers.",
        "An unreadable table that a useful machine could read would still be well worth having.",
        "Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network.",
        "Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks.",
        "Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful.",
        "For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.",
        "Biological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance.",
        "Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.",
        "Large and effective neural networks require considerable computing resources.",
        "While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage.",
        "Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons - which require enormous CPU power and time.",
        "Some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before.",
        "The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.",
        "Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry.",
        "Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.",
        "Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network.",
        "Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful.",
        "For example, local vs. non-local learning and shallow vs. deep architecture.",
        "Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.",
        "Neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.",
        "These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute.",
        "This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement.",
        "For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field.",
        "The program would penalize any resume with the word \"woman\" or the name of any women's college.",
        "However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.",
        "Artificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications.",
        "Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.",
        "In the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation.",
        "For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance.",
        "This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.",
        "By modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion.",
        "Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques.",
        "These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.",
        "In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation.",
        "They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content.",
        "This has implications for automated customer service, content moderation, and language understanding technologies.",
        "In the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization.",
        "For instance, deep feedforward neural networks are important in system identification and control applications.",
        "ANNs are used for stock market prediction and credit scoring: In investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions.",
        "In credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process.",
        "ANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation.",
        "Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.",
        "ANNs are able to process and analyze vast medical datasets.",
        "They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning.",
        "In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs.",
        "Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management.",
        "Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.",
        "ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries.",
        "This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions.",
        "For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user.",
        "In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck.",
        "In the marketing industry, generative models are used to create personalized advertisements for consumers.",
        "Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020.",
        "Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game."
      ],
      "metadata": {
        "title": "Neural network (machine learning)",
        "url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
        "word_count": 7880,
        "char_count": 52748,
        "sentence_count": 382,
        "scraped_at": "2025-08-09T14:46:41.358212",
        "language": "en",
        "processing_time": 0.012722015380859375,
        "source_hash": "8a0c1003edb6daf2062575dfb1014bd9"
      }
    },
    {
      "title": "Attention (machine learning)",
      "url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
      "raw_text": "In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.\nUnlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.\nInspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\n\n\n== History ==\n\nAdditional surveys of the attention mechanism in deep learning are provided by Niu et al. and Soydaner.\nThe major breakthrough came with self-attention, where each element in the input sequence attends to all others, enabling the model to capture global dependencies. This idea was central to the Transformer architecture, which replaced recurrence with attention mechanisms. As a result, Transformers became the foundation for models like BERT, T5 and generative pre-trained transformers (GPT). \n\n\n== Overview ==\n\nThe modern era of machine attention was revitalized by grafting an attention mechanism (Fig 1.  orange) to an Encoder-Decoder.\n\nFigure 2 shows the internal step-by-step operation of the attention block (A) in Fig 1.\n\n\n=== Interpreting attention weights ===\nIn translating between languages, alignment is the process of matching words from the source sentence to words of the translated sentence. Networks that perform verbatim translation without regard to word order would show the highest scores along the (dominant) diagonal of the matrix. The off-diagonal dominance shows that the attention mechanism is more nuanced. \nConsider an example of translating I love you to French. On the first pass through the decoder, 94% of the attention weight is on the first English word I, so the network offers the word je. On the second pass of the decoder, 88% of the attention weight is on the third English word you, so it offers t'. On the last pass, 95% of the attention weight is on the second English word love, so it offers aime.\nIn the I love you example, the second word love is aligned with the third word aime. Stacking soft row vectors together for je, t', and aime yields an alignment matrix:\n\nSometimes, alignment can be multiple-to-multiple. For example, the English phrase look it up corresponds to cherchez-le. Thus, \"soft\" attention weights work better than \"hard\" attention weights (setting one attention weight to 1, and the others to 0), as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors, rather than \"the best one\", as there may not be a best hidden vector.\n\n\n== Variants ==\n\nMany variants of attention implement soft weights, such as\n\nfast weight programmers, or fast weight controllers (1992). A \"slow\" neural network outputs the \"fast\" weights of another neural network through outer products. The slow network learns by gradient descent. It was later renamed as \"linearized self-attention\".\nBahdanau-style attention, also referred to as additive attention,\nLuong-style attention, which is known as multiplicative attention,\nEarly attention mechanisms similar to modern self-attention were proposed using recurrent neural networks. However, the highly parallelizable self-attention was introduced in 2017 and successfully used in the Transformer model,\npositional attention and factorized positional attention.\nFor convolutional neural networks, attention mechanisms can be distinguished by the dimension on which they operate, namely: spatial attention, channel attention, or combinations.\nThese variants recombine the encoder-side inputs to redistribute those effects to each target output. Often, a correlation-style matrix of dot products provides the re-weighting coefficients.  In the figures below, W is the matrix of context attention weights, similar to the formula in Overview section above.\n\n\n== Optimizations ==\n\n\n=== Flash attention ===\nThe size of the attention matrix is proportional to the square of the number of input tokens. Therefore, when the input is long, calculating the attention matrix requires a lot of GPU memory. Flash attention is an implementation that reduces the memory needs and increases efficiency without sacrificing accuracy. It achieves this by partitioning the attention computation into smaller blocks that fit into the GPU's faster on-chip memory, reducing the need to store large intermediate matrices and thus lowering memory usage while increasing computational efficiency.\n\n\n=== FlexAttention ===\nFlexAttention is an attention kernel developed by Meta that allows users to modify attention scores prior to softmax and dynamically chooses the optimal attention algorithm.\n\n\n== Applications ==\nAttention is widely used in natural language processing, computer vision, and speech recognition. In NLP, it improves context understanding in tasks like question answering and summarization. In vision, visual attention helps models focus on relevant image regions, enhancing object detection and image captioning.\n\n\n=== Attention maps as explanations for vision transformers ===\n\nFrom the original paper on vision transformers (ViT), visualizing attention scores as a heat map (called saliency maps or attention maps) has become an important and routine way to inspect the decision making process of ViT models. One can compute the attention maps with respect to any attention head at any layer, while the deeper layers tend to show more semantically meaningful visualization. Attention rollout is a recursive algorithm to combine attention scores across all layers, by computing the dot product of successive attention maps.\nBecause vision transformers are typically trained in a self-supervised manner, attention maps are generally not class-sensitive. When a classification head attached to the ViT backbone, class-discriminative attention maps (CDAM) combines attention maps and gradients with respect to the class [CLS] token. Some class-sensitive interpretability methods originally developed for convolutional neural networks can be also applied to ViT, such as GradCAM, which back-propagates the gradients to the outputs of the final attention layer.\nUsing attention as basis of explanation for the transformers in language and vision is not without debate. While some pioneering papers analyzed and framed attention scores as explanations, higher attention scores do not always correlate with greater impact on model performances. \n\n\n== Mathematical representation ==\n\n\n=== Standard scaled dot-product attention ===\nFor matrices: \n  \n    \n      \n        Q\n        ∈\n        \n          \n            R\n          \n          \n            m\n            ×\n            \n              d\n              \n                k\n              \n            \n          \n        \n        ,\n        K\n        ∈\n        \n          \n            R\n          \n          \n            n\n            ×\n            \n              d\n              \n                k\n              \n            \n          \n        \n      \n    \n    {\\displaystyle Q\\in \\mathbb {R} ^{m\\times d_{k}},K\\in \\mathbb {R} ^{n\\times d_{k}}}\n  \n and \n  \n    \n      \n        V\n        ∈\n        \n          \n            R\n          \n          \n            n\n            ×\n            \n              d\n              \n                v\n              \n            \n          \n        \n      \n    \n    {\\displaystyle V\\in \\mathbb {R} ^{n\\times d_{v}}}\n  \n, the scaled dot-product, or QKV attention, is defined as:\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                Q\n                \n                  K\n                  \n                    T\n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        ∈\n        \n          \n            R\n          \n          \n            m\n            ×\n            \n              d\n              \n                v\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{T}}{\\sqrt {d_{k}}}}\\right)V\\in \\mathbb {R} ^{m\\times d_{v}}}\n  \n\nwhere \n  \n    \n      \n        \n          \n\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {}^{T}}\n  \n denotes transpose and the softmax function is applied independently to every row of its argument. The matrix \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n contains \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n queries, while matrices \n  \n    \n      \n        K\n        ,\n        V\n      \n    \n    {\\displaystyle K,V}\n  \n jointly contain an unordered set of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n key-value pairs. Value vectors in matrix \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n are weighted using the weights resulting from the softmax operation, so that the rows of the \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n-by-\n  \n    \n      \n        \n          d\n          \n            v\n          \n        \n      \n    \n    {\\displaystyle d_{v}}\n  \n output matrix are confined to the convex hull of the points in \n  \n    \n      \n        \n          \n            R\n          \n          \n            \n              d\n              \n                v\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{d_{v}}}\n  \n given by the rows of \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n.\nTo understand the permutation invariance and permutation equivariance properties of QKV attention, let \n  \n    \n      \n        A\n        ∈\n        \n          \n            R\n          \n          \n            m\n            ×\n            m\n          \n        \n      \n    \n    {\\displaystyle A\\in \\mathbb {R} ^{m\\times m}}\n  \n and \n  \n    \n      \n        B\n        ∈\n        \n          \n            R\n          \n          \n            n\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle B\\in \\mathbb {R} ^{n\\times n}}\n  \n be permutation matrices; and \n  \n    \n      \n        D\n        ∈\n        \n          \n            R\n          \n          \n            m\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle D\\in \\mathbb {R} ^{m\\times n}}\n  \n an arbitrary matrix. The softmax function is permutation equivariant in the sense that:\n\n  \n    \n      \n        \n          softmax\n        \n        (\n        A\n        D\n        B\n        )\n        =\n        A\n        \n        \n          softmax\n        \n        (\n        D\n        )\n        B\n      \n    \n    {\\displaystyle {\\text{softmax}}(ADB)=A\\,{\\text{softmax}}(D)B}\n  \n\nBy noting that the transpose of a permutation matrix is also its inverse, it follows that:\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        A\n        Q\n        ,\n        B\n        K\n        ,\n        B\n        V\n        )\n        =\n        A\n        \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n      \n    \n    {\\displaystyle {\\text{Attention}}(AQ,BK,BV)=A\\,{\\text{Attention}}(Q,K,V)}\n  \n\nwhich shows that QKV attention is equivariant with respect to re-ordering the queries (rows of \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n); and invariant to re-ordering of the key-value pairs in \n  \n    \n      \n        K\n        ,\n        V\n      \n    \n    {\\displaystyle K,V}\n  \n. These properties are inherited when applying linear transforms to the inputs and outputs of QKV attention blocks. For example, a simple self-attention function defined as:\n\n  \n    \n      \n        X\n        ↦\n        \n          Attention\n        \n        (\n        X\n        \n          T\n          \n            q\n          \n        \n        ,\n        X\n        \n          T\n          \n            k\n          \n        \n        ,\n        X\n        \n          T\n          \n            v\n          \n        \n        )\n      \n    \n    {\\displaystyle X\\mapsto {\\text{Attention}}(XT_{q},XT_{k},XT_{v})}\n  \n\nis permutation equivariant with respect to re-ordering the rows of the input matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n in a non-trivial way, because every row of the output is a function of all the rows of the input. Similar properties hold for multi-head attention, which is defined below.\n\n\n=== Masked attention ===\nWhen QKV attention is used as a building block for an autoregressive decoder, and when at training time all input and output matrices have \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n rows, a masked attention variant is used:\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                \n                  Q\n                  \n                    K\n                    \n                      T\n                    \n                  \n                \n                \n                  \n                    d\n                    \n                      k\n                    \n                  \n                \n              \n            \n            +\n            M\n          \n          )\n        \n        V\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{T}}{\\sqrt {d_{k}}}}+M\\right)V}\n  \n\nwhere the mask, \n  \n    \n      \n        M\n        ∈\n        \n          \n            R\n          \n          \n            n\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle M\\in \\mathbb {R} ^{n\\times n}}\n  \n is a strictly upper triangular matrix, with zeros on and below the diagonal and \n  \n    \n      \n        −\n        ∞\n      \n    \n    {\\displaystyle -\\infty }\n  \n in every element above the diagonal. The softmax output, also in \n  \n    \n      \n        \n          \n            R\n          \n          \n            n\n            ×\n            n\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{n\\times n}}\n  \n is then lower triangular, with zeros in all elements above the diagonal. The masking ensures that for all \n  \n    \n      \n        1\n        ≤\n        i\n        <\n        j\n        ≤\n        n\n      \n    \n    {\\displaystyle 1\\leq i<j\\leq n}\n  \n, row \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n of the attention output is independent of row \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n of any of the three input matrices. The permutation invariance and equivariance properties of standard QKV attention do not hold for the masked variant.\n\n\n=== Multi-head attention ===\n\nMulti-head attention\n\n  \n    \n      \n        \n          MultiHead\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          Concat\n        \n        (\n        \n          \n            head\n          \n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          \n            head\n          \n          \n            h\n          \n        \n        )\n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiHead}}(Q,K,V)={\\text{Concat}}({\\text{head}}_{1},...,{\\text{head}}_{h})W^{O}}\n  \n\nwhere each head is computed with QKV attention as:\n\n  \n    \n      \n        \n          \n            head\n          \n          \n            i\n          \n        \n        =\n        \n          Attention\n        \n        (\n        Q\n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        K\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        V\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\text{head}}_{i}={\\text{Attention}}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})}\n  \n\nand \n  \n    \n      \n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}\n  \n, and \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n are parameter matrices.\nThe permutation properties of (standard, unmasked) QKV attention apply here also. For permutation matrices, \n  \n    \n      \n        A\n        ,\n        B\n      \n    \n    {\\displaystyle A,B}\n  \n:\n\n  \n    \n      \n        \n          MultiHead\n        \n        (\n        A\n        Q\n        ,\n        B\n        K\n        ,\n        B\n        V\n        )\n        =\n        A\n        \n        \n          MultiHead\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n      \n    \n    {\\displaystyle {\\text{MultiHead}}(AQ,BK,BV)=A\\,{\\text{MultiHead}}(Q,K,V)}\n  \n\nfrom which we also see that multi-head self-attention:\n\n  \n    \n      \n        X\n        ↦\n        \n          MultiHead\n        \n        (\n        X\n        \n          T\n          \n            q\n          \n        \n        ,\n        X\n        \n          T\n          \n            k\n          \n        \n        ,\n        X\n        \n          T\n          \n            v\n          \n        \n        )\n      \n    \n    {\\displaystyle X\\mapsto {\\text{MultiHead}}(XT_{q},XT_{k},XT_{v})}\n  \n\nis equivariant with respect to re-ordering of the rows of input matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n.\n\n\n=== Bahdanau (additive) attention ===\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        (\n        tanh\n        ⁡\n        (\n        \n          W\n          \n            Q\n          \n        \n        Q\n        +\n        \n          W\n          \n            K\n          \n        \n        K\n        )\n        V\n        )\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}(\\tanh(W_{Q}Q+W_{K}K)V)}\n  \n\nwhere \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W_{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W_{K}}\n  \n are learnable weight matrices.\n\n\n=== Luong attention (general) ===\n\n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        (\n        Q\n        W\n        \n          K\n          \n            T\n          \n        \n        )\n        V\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}(QWK^{T})V}\n  \n\nwhere \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n is a learnable weight matrix.\n\n\n=== Self-attention ===\nSelf-attention is essentially the same as cross-attention, except that query, key, and value vectors all come from the same model. Both encoder and decoder can use self-attention, but with subtle differences.\nFor encoder self-attention, we can start with a simple encoder without self-attention, such as an \"embedding layer\", which simply converts each input word into a vector by a fixed lookup table. This gives a sequence of hidden vectors \n  \n    \n      \n        \n          h\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            1\n          \n        \n        ,\n        …\n      \n    \n    {\\displaystyle h_{0},h_{1},\\dots }\n  \n. These can then be applied to a dot-product attention mechanism, to obtain\n  \n    \n      \n        \n          \n            \n              \n                \n                  h\n                  \n                    0\n                  \n                  ′\n                \n              \n              \n                \n                =\n                \n                  A\n                  t\n                  t\n                  e\n                  n\n                  t\n                  i\n                  o\n                  n\n                \n                (\n                \n                  h\n                  \n                    0\n                  \n                \n                \n                  W\n                  \n                    Q\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    K\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    V\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  h\n                  \n                    1\n                  \n                  ′\n                \n              \n              \n                \n                =\n                \n                  A\n                  t\n                  t\n                  e\n                  n\n                  t\n                  i\n                  o\n                  n\n                \n                (\n                \n                  h\n                  \n                    1\n                  \n                \n                \n                  W\n                  \n                    Q\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    K\n                  \n                \n                ,\n                H\n                \n                  W\n                  \n                    V\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                ⋯\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}h_{0}'&=\\mathrm {Attention} (h_{0}W^{Q},HW^{K},HW^{V})\\\\h_{1}'&=\\mathrm {Attention} (h_{1}W^{Q},HW^{K},HW^{V})\\\\&\\cdots \\end{aligned}}}\n  \nor more succinctly, \n  \n    \n      \n        \n          H\n          ′\n        \n        =\n        \n          A\n          t\n          t\n          e\n          n\n          t\n          i\n          o\n          n\n        \n        (\n        H\n        \n          W\n          \n            Q\n          \n        \n        ,\n        H\n        \n          W\n          \n            K\n          \n        \n        ,\n        H\n        \n          W\n          \n            V\n          \n        \n        )\n      \n    \n    {\\displaystyle H'=\\mathrm {Attention} (HW^{Q},HW^{K},HW^{V})}\n  \n. This can be applied repeatedly, to obtain a multilayered encoder. This is the \"encoder self-attention\", sometimes called the \"all-to-all attention\", as the vector at every position can attend to every other.\n\n\n=== Masking ===\nFor decoder self-attention, all-to-all attention is inappropriate, because during the autoregressive decoding process, the decoder cannot attend to future outputs that has yet to be decoded. This can be solved by forcing the attention weights \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle w_{ij}=0}\n  \n for all \n  \n    \n      \n        i\n        <\n        j\n      \n    \n    {\\displaystyle i<j}\n  \n, called \"causal masking\". This attention mechanism is the \"causally masked self-attention\".\n\n\n== See also ==\nRecurrent neural network\nseq2seq\nTransformer (deep learning architecture)\nAttention\nDynamic neural network\n\n\n== References ==\n\n\n== External links ==\nOlah, Chris; Carter, Shan (September 8, 2016). \"Attention and Augmented Recurrent Neural Networks\". Distill. 1 (9). Distill Working Group. doi:10.23915/distill.00001.\nDan Jurafsky and James H. Martin (2022). Speech and Language Processing (3rd ed. draft, January 2022) — Chapter 10.4 (Attention) and Chapter 9.7 (Self-Attention Networks: Transformers)\nAlex Graves (2020). Attention and Memory in Deep Learning — video lecture from DeepMind / UCL",
      "cleaned_text": "In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size. Unlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme. Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state. Additional surveys of the attention mechanism in deep learning are provided by Niu et al. and Soydaner. The major breakthrough came with self-attention, where each element in the input sequence attends to all others, enabling the model to capture global dependencies. This idea was central to the Transformer architecture, which replaced recurrence with attention mechanisms. As a result, Transformers became the foundation for models like BERT, T5 and generative pre-trained transformers (GPT). The modern era of machine attention was revitalized by grafting an attention mechanism (Fig 1. orange) to an Encoder-Decoder. Figure 2 shows the internal step-by-step operation of the attention block (A) in Fig 1. In translating between languages, alignment is the process of matching words from the source sentence to words of the translated sentence. Networks that perform verbatim translation without regard to word order would show the highest scores along the (dominant) diagonal of the matrix. The off-diagonal dominance shows that the attention mechanism is more nuanced. Consider an example of translating I love you to French. On the first pass through the decoder, 94% of the attention weight is on the first English word I, so the network offers the word je. On the second pass of the decoder, 88% of the attention weight is on the third English word you, so it offers t'. On the last pass, 95% of the attention weight is on the second English word love, so it offers aime. In the I love you example, the second word love is aligned with the third word aime. Stacking soft row vectors together for je, t', and aime yields an alignment matrix: Sometimes, alignment can be multiple-to-multiple. For example, the English phrase look it up corresponds to cherchez-le. Thus, \"soft\" attention weights work better than \"hard\" attention weights (setting one attention weight to 1, and the others to 0), as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors, rather than \"the best one\", as there may not be a best hidden vector. Many variants of attention implement soft weights, such as fast weight programmers, or fast weight controllers (1992). A \"slow\" neural network outputs the \"fast\" weights of another neural network through outer products. The slow network learns by gradient descent. It was later renamed as \"linearized self-attention\". Bahdanau-style attention, also referred to as additive attention, Luong-style attention, which is known as multiplicative attention, Early attention mechanisms similar to modern self-attention were proposed using recurrent neural networks. However, the highly parallelizable self-attention was introduced in 2017 and successfully used in the Transformer model, positional attention and factorized positional attention. For convolutional neural networks, attention mechanisms can be distinguished by the dimension on which they operate, namely: spatial attention, channel attention, or combinations. These variants recombine the encoder-side inputs to redistribute those effects to each target output. Often, a correlation-style matrix of dot products provides the re-weighting coefficients. In the figures below, W is the matrix of context attention weights, similar to the formula in Overview section above. The size of the attention matrix is proportional to the square of the number of input tokens. Therefore, when the input is long, calculating the attention matrix requires a lot of GPU memory. Flash attention is an implementation that reduces the memory needs and increases efficiency without sacrificing accuracy. It achieves this by partitioning the attention computation into smaller blocks that fit into the GPU's faster on-chip memory, reducing the need to store large intermediate matrices and thus lowering memory usage while increasing computational efficiency. FlexAttention is an attention kernel developed by Meta that allows users to modify attention scores prior to softmax and dynamically chooses the optimal attention algorithm. Attention is widely used in natural language processing, computer vision, and speech recognition. In NLP, it improves context understanding in tasks like question answering and summarization. In vision, visual attention helps models focus on relevant image regions, enhancing object detection and image captioning. From the original paper on vision transformers (ViT), visualizing attention scores as a heat map (called saliency maps or attention maps) has become an important and routine way to inspect the decision making process of ViT models. One can compute the attention maps with respect to any attention head at any layer, while the deeper layers tend to show more semantically meaningful visualization. Attention rollout is a recursive algorithm to combine attention scores across all layers, by computing the dot product of successive attention maps. Because vision transformers are typically trained in a self-supervised manner, attention maps are generally not class-sensitive. When a classification head attached to the ViT backbone, class-discriminative attention maps (CDAM) combines attention maps and gradients with respect to the class [CLS] token. Some class-sensitive interpretability methods originally developed for convolutional neural networks can be also applied to ViT, such as GradCAM, which back-propagates the gradients to the outputs of the final attention layer. Using attention as basis of explanation for the transformers in language and vision is not without debate. While some pioneering papers analyzed and framed attention scores as explanations, higher attention scores do not always correlate with greater impact on model performances. For matrices: Q ∈ R m × d k , K ∈ R n × d k {\\displaystyle Q\\in \\mathbb {R} ^{m imes d_{k}},K\\in \\mathbb {R} ^{n imes d_{k}}} and V ∈ R n × d v {\\displaystyle V\\in \\mathbb {R} ^{n imes d_{v}}} , the scaled dot-product, or QKV attention, is defined as: Attention ( Q , K , V ) = softmax ( Q K T d k ) V ∈ R m × d v {\\displaystyle { ext{Attention}}(Q,K,V)={ ext{softmax}}\\left({\\frac {QK^{T}}{\\sqrt {d_{k}}}}\\right)V\\in \\mathbb {R} ^{m imes d_{v}}} where T {\\displaystyle {}^{T}} denotes transpose and the softmax function is applied independently to every row of its argument. The matrix Q {\\displaystyle Q} contains m {\\displaystyle m} queries, while matrices K , V {\\displaystyle K,V} jointly contain an unordered set of n {\\displaystyle n} key-value pairs. Value vectors in matrix V {\\displaystyle V} are weighted using the weights resulting from the softmax operation, so that the rows of the m {\\displaystyle m} -by- d v {\\displaystyle d_{v}} output matrix are confined to the convex hull of the points in R d v {\\displaystyle \\mathbb {R} ^{d_{v}}} given by the rows of V {\\displaystyle V} . To understand the permutation invariance and permutation equivariance properties of QKV attention, let A ∈ R m × m {\\displaystyle A\\in \\mathbb {R} ^{m imes m}} and B ∈ R n × n {\\displaystyle B\\in \\mathbb {R} ^{n imes n}} be permutation matrices; and D ∈ R m × n {\\displaystyle D\\in \\mathbb {R} ^{m imes n}} an arbitrary matrix. The softmax function is permutation equivariant in the sense that: softmax ( A D B ) = A softmax ( D ) B {\\displaystyle { ext{softmax}}(ADB)=A\\,{ ext{softmax}}(D)B} By noting that the transpose of a permutation matrix is also its inverse, it follows that: Attention ( A Q , B K , B V ) = A Attention ( Q , K , V ) {\\displaystyle { ext{Attention}}(AQ,BK,BV)=A\\,{ ext{Attention}}(Q,K,V)} which shows that QKV attention is equivariant with respect to re-ordering the queries (rows of Q {\\displaystyle Q} ); and invariant to re-ordering of the key-value pairs in K , V {\\displaystyle K,V} . These properties are inherited when applying linear transforms to the inputs and outputs of QKV attention blocks. For example, a simple self-attention function defined as: X ↦ Attention ( X T q , X T k , X T v ) {\\displaystyle X\\mapsto { ext{Attention}}(XT_{q},XT_{k},XT_{v})} is permutation equivariant with respect to re-ordering the rows of the input matrix X {\\displaystyle X} in a non-trivial way, because every row of the output is a function of all the rows of the input. Similar properties hold for multi-head attention, which is defined below. When QKV attention is used as a building block for an autoregressive decoder, and when at training time all input and output matrices have n {\\displaystyle n} rows, a masked attention variant is used: Attention ( Q , K , V ) = softmax ( Q K T d k + M ) V {\\displaystyle { ext{Attention}}(Q,K,V)={ ext{softmax}}\\left({\\frac {QK^{T}}{\\sqrt {d_{k}}}}+M\\right)V} where the mask, M ∈ R n × n {\\displaystyle M\\in \\mathbb {R} ^{n imes n}} is a strictly upper triangular matrix, with zeros on and below the diagonal and − ∞ {\\displaystyle -\\infty } in every element above the diagonal. The softmax output, also in R n × n {\\displaystyle \\mathbb {R} ^{n imes n}} is then lower triangular, with zeros in all elements above the diagonal. The masking ensures that for all 1 ≤ i < j ≤ n {\\displaystyle 1\\leq i<j\\leq n} , row i {\\displaystyle i} of the attention output is independent of row j {\\displaystyle j} of any of the three input matrices. The permutation invariance and equivariance properties of standard QKV attention do not hold for the masked variant. Multi-head attention MultiHead ( Q , K , V ) = Concat ( head 1 , . . . , head h ) W O {\\displaystyle { ext{MultiHead}}(Q,K,V)={ ext{Concat}}({ ext{head}}_{1},...,{ ext{head}}_{h})W^{O}} where each head is computed with QKV attention as: head i = Attention ( Q W i Q , K W i K , V W i V ) {\\displaystyle { ext{head}}_{i}={ ext{Attention}}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})} and W i Q , W i K , W i V {\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}} , and W O {\\displaystyle W^{O}} are parameter matrices. The permutation properties of (standard, unmasked) QKV attention apply here also. For permutation matrices, A , B {\\displaystyle A,B} : MultiHead ( A Q , B K , B V ) = A MultiHead ( Q , K , V ) {\\displaystyle { ext{MultiHead}}(AQ,BK,BV)=A\\,{ ext{MultiHead}}(Q,K,V)} from which we also see that multi-head self-attention: X ↦ MultiHead ( X T q , X T k , X T v ) {\\displaystyle X\\mapsto { ext{MultiHead}}(XT_{q},XT_{k},XT_{v})} is equivariant with respect to re-ordering of the rows of input matrix X {\\displaystyle X} . Attention ( Q , K , V ) = softmax ( tanh ⁡ ( W Q Q + W K K ) V ) {\\displaystyle { ext{Attention}}(Q,K,V)={ ext{softmax}}( anh(W_{Q}Q+W_{K}K)V)} where W Q {\\displaystyle W_{Q}} and W K {\\displaystyle W_{K}} are learnable weight matrices. Attention ( Q , K , V ) = softmax ( Q W K T ) V {\\displaystyle { ext{Attention}}(Q,K,V)={ ext{softmax}}(QWK^{T})V} where W {\\displaystyle W} is a learnable weight matrix. Self-attention is essentially the same as cross-attention, except that query, key, and value vectors all come from the same model. Both encoder and decoder can use self-attention, but with subtle differences. For encoder self-attention, we can start with a simple encoder without self-attention, such as an \"embedding layer\", which simply converts each input word into a vector by a fixed lookup table. This gives a sequence of hidden vectors h 0 , h 1 , ... {\\displaystyle h_{0},h_{1},\\dots } . These can then be applied to a dot-product attention mechanism, to obtain h 0 ′ = A t t e n t i o n ( h 0 W Q , H W K , H W V ) h 1 ′ = A t t e n t i o n ( h 1 W Q , H W K , H W V ) ⋯ {\\displaystyle {\\begin{aligned}h_{0}'&=\\mathrm {Attention} (h_{0}W^{Q},HW^{K},HW^{V})\\\\h_{1}'&=\\mathrm {Attention} (h_{1}W^{Q},HW^{K},HW^{V})\\\\&\\cdots \\end{aligned}}} or more succinctly, H ′ = A t t e n t i o n ( H W Q , H W K , H W V ) {\\displaystyle H'=\\mathrm {Attention} (HW^{Q},HW^{K},HW^{V})} . This can be applied repeatedly, to obtain a multilayered encoder. This is the \"encoder self-attention\", sometimes called the \"all-to-all attention\", as the vector at every position can attend to every other. For decoder self-attention, all-to-all attention is inappropriate, because during the autoregressive decoding process, the decoder cannot attend to future outputs that has yet to be decoded. This can be solved by forcing the attention weights w i j = 0 {\\displaystyle w_{ij}=0} for all i < j {\\displaystyle i<j} , called \"causal masking\". This attention mechanism is the \"causally masked self-attention\".",
      "sentences": [
        "In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence.",
        "In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence.",
        "More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.",
        "Unlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input.",
        "Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme.",
        "Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks.",
        "Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated.",
        "Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.",
        "Additional surveys of the attention mechanism in deep learning are provided by Niu et al.",
        "and Soydaner.",
        "The major breakthrough came with self-attention, where each element in the input sequence attends to all others, enabling the model to capture global dependencies.",
        "This idea was central to the Transformer architecture, which replaced recurrence with attention mechanisms.",
        "As a result, Transformers became the foundation for models like BERT, T5 and generative pre-trained transformers (GPT).",
        "The modern era of machine attention was revitalized by grafting an attention mechanism (Fig 1. orange) to an Encoder-Decoder.",
        "Figure 2 shows the internal step-by-step operation of the attention block (A) in Fig 1.",
        "In translating between languages, alignment is the process of matching words from the source sentence to words of the translated sentence.",
        "Networks that perform verbatim translation without regard to word order would show the highest scores along the (dominant) diagonal of the matrix.",
        "The off-diagonal dominance shows that the attention mechanism is more nuanced.",
        "Consider an example of translating I love you to French.",
        "On the first pass through the decoder, 94% of the attention weight is on the first English word I, so the network offers the word je.",
        "On the second pass of the decoder, 88% of the attention weight is on the third English word you, so it offers t'.",
        "On the last pass, 95% of the attention weight is on the second English word love, so it offers aime.",
        "In the I love you example, the second word love is aligned with the third word aime.",
        "Stacking soft row vectors together for je, t', and aime yields an alignment matrix: Sometimes, alignment can be multiple-to-multiple.",
        "For example, the English phrase look it up corresponds to cherchez-le.",
        "Thus, \"soft\" attention weights work better than \"hard\" attention weights (setting one attention weight to 1, and the others to 0), as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors, rather than \"the best one\", as there may not be a best hidden vector.",
        "Many variants of attention implement soft weights, such as fast weight programmers, or fast weight controllers (1992).",
        "A \"slow\" neural network outputs the \"fast\" weights of another neural network through outer products.",
        "The slow network learns by gradient descent.",
        "It was later renamed as \"linearized self-attention\".",
        "Bahdanau-style attention, also referred to as additive attention, Luong-style attention, which is known as multiplicative attention, Early attention mechanisms similar to modern self-attention were proposed using recurrent neural networks.",
        "However, the highly parallelizable self-attention was introduced in 2017 and successfully used in the Transformer model, positional attention and factorized positional attention.",
        "For convolutional neural networks, attention mechanisms can be distinguished by the dimension on which they operate, namely: spatial attention, channel attention, or combinations.",
        "These variants recombine the encoder-side inputs to redistribute those effects to each target output.",
        "Often, a correlation-style matrix of dot products provides the re-weighting coefficients.",
        "In the figures below, W is the matrix of context attention weights, similar to the formula in Overview section above.",
        "The size of the attention matrix is proportional to the square of the number of input tokens.",
        "Therefore, when the input is long, calculating the attention matrix requires a lot of GPU memory.",
        "Flash attention is an implementation that reduces the memory needs and increases efficiency without sacrificing accuracy.",
        "It achieves this by partitioning the attention computation into smaller blocks that fit into the GPU's faster on-chip memory, reducing the need to store large intermediate matrices and thus lowering memory usage while increasing computational efficiency.",
        "FlexAttention is an attention kernel developed by Meta that allows users to modify attention scores prior to softmax and dynamically chooses the optimal attention algorithm.",
        "Attention is widely used in natural language processing, computer vision, and speech recognition.",
        "In NLP, it improves context understanding in tasks like question answering and summarization.",
        "In vision, visual attention helps models focus on relevant image regions, enhancing object detection and image captioning.",
        "From the original paper on vision transformers (ViT), visualizing attention scores as a heat map (called saliency maps or attention maps) has become an important and routine way to inspect the decision making process of ViT models.",
        "One can compute the attention maps with respect to any attention head at any layer, while the deeper layers tend to show more semantically meaningful visualization.",
        "Attention rollout is a recursive algorithm to combine attention scores across all layers, by computing the dot product of successive attention maps.",
        "Because vision transformers are typically trained in a self-supervised manner, attention maps are generally not class-sensitive.",
        "When a classification head attached to the ViT backbone, class-discriminative attention maps (CDAM) combines attention maps and gradients with respect to the class [CLS] token.",
        "Some class-sensitive interpretability methods originally developed for convolutional neural networks can be also applied to ViT, such as GradCAM, which back-propagates the gradients to the outputs of the final attention layer.",
        "Using attention as basis of explanation for the transformers in language and vision is not without debate.",
        "While some pioneering papers analyzed and framed attention scores as explanations, higher attention scores do not always correlate with greater impact on model performances.",
        "The matrix Q {\\displaystyle Q} contains m {\\displaystyle m} queries, while matrices K , V {\\displaystyle K,V} jointly contain an unordered set of n {\\displaystyle n} key-value pairs.",
        "Value vectors in matrix V {\\displaystyle V} are weighted using the weights resulting from the softmax operation, so that the rows of the m {\\displaystyle m} -by- d v {\\displaystyle d_{v}} output matrix are confined to the convex hull of the points in R d v {\\displaystyle \\mathbb {R} ^{d_{v}}} given by the rows of V {\\displaystyle V} .",
        "To understand the permutation invariance and permutation equivariance properties of QKV attention, let A ∈ R m × m {\\displaystyle A\\in \\mathbb {R} ^{m imes m}} and B ∈ R n × n {\\displaystyle B\\in \\mathbb {R} ^{n imes n}} be permutation matrices; and D ∈ R m × n {\\displaystyle D\\in \\mathbb {R} ^{m imes n}} an arbitrary matrix.",
        "These properties are inherited when applying linear transforms to the inputs and outputs of QKV attention blocks.",
        "For example, a simple self-attention function defined as: X ↦ Attention ( X T q , X T k , X T v ) {\\displaystyle X\\mapsto { ext{Attention}}(XT_{q},XT_{k},XT_{v})} is permutation equivariant with respect to re-ordering the rows of the input matrix X {\\displaystyle X} in a non-trivial way, because every row of the output is a function of all the rows of the input.",
        "Similar properties hold for multi-head attention, which is defined below.",
        "The softmax output, also in R n × n {\\displaystyle \\mathbb {R} ^{n imes n}} is then lower triangular, with zeros in all elements above the diagonal.",
        "The masking ensures that for all 1 ≤ i < j ≤ n {\\displaystyle 1\\leq i<j\\leq n} , row i {\\displaystyle i} of the attention output is independent of row j {\\displaystyle j} of any of the three input matrices.",
        "The permutation invariance and equivariance properties of standard QKV attention do not hold for the masked variant.",
        "Multi-head attention MultiHead ( Q , K , V ) = Concat ( head 1 , .",
        "The permutation properties of (standard, unmasked) QKV attention apply here also.",
        "Self-attention is essentially the same as cross-attention, except that query, key, and value vectors all come from the same model.",
        "Both encoder and decoder can use self-attention, but with subtle differences.",
        "For encoder self-attention, we can start with a simple encoder without self-attention, such as an \"embedding layer\", which simply converts each input word into a vector by a fixed lookup table.",
        "This gives a sequence of hidden vectors h 0 , h 1 , ... {\\displaystyle h_{0},h_{1},\\dots } .",
        "This can be applied repeatedly, to obtain a multilayered encoder.",
        "This is the \"encoder self-attention\", sometimes called the \"all-to-all attention\", as the vector at every position can attend to every other.",
        "For decoder self-attention, all-to-all attention is inappropriate, because during the autoregressive decoding process, the decoder cannot attend to future outputs that has yet to be decoded.",
        "This can be solved by forcing the attention weights w i j = 0 {\\displaystyle w_{ij}=0} for all i < j {\\displaystyle i<j} , called \"causal masking\".",
        "This attention mechanism is the \"causally masked self-attention\"."
      ],
      "metadata": {
        "title": "Attention (machine learning)",
        "url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
        "word_count": 2223,
        "char_count": 13696,
        "sentence_count": 72,
        "scraped_at": "2025-08-09T14:46:41.364266",
        "language": "en",
        "processing_time": 0.005878925323486328,
        "source_hash": "bfa4aa76845d36953eb0b953f0d23fa7"
      }
    },
    {
      "title": "Transformer (deep learning architecture)",
      "url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)",
      "raw_text": "In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. \nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.\n\nThe modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\n\n\n== History ==\n\n\n=== Predecessors ===\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\nA key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\nHowever, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.\nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.\n\n\n=== Attention with seq2seq ===\n\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.\nA 380M-parameter model for machine translation uses two long short-term memories (LSTM). Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.\nThe RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\".\nThe relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.\nIn 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.\n\n\n=== Parallelizing attention ===\n\nSeq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title \"attention is all you need\". That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical. In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.\nIn 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.\n\n\n=== AI boom era ===\nAlready in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.\nIn language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model.\nStarting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models.\nSince 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The vision transformer, in turn, stimulated new developments in convolutional neural networks. Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024), and Sora (2024), use Transformers to analyse input data (like text prompts) by breaking it down into \"tokens\" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data.\n\n\n== Training ==\n\n\n=== Methods for stabilizing training ===\nThe plain transformer architecture had difficulty converging. In the original paper the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.\nA 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.\n\n\n=== Pretrain-finetune ===\nTransformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:\n\nlanguage modeling\nnext-sentence prediction\nquestion answering\nreading comprehension\nsentiment analysis\nparaphrasing\nThe T5 transformer report documents a large number of natural language pretraining tasks. Some examples are:\n\nrestoring or repairing incomplete or corrupted text. For example, the input, \"Thank you ~~ me to your party ~~ week\", might generate the output, \"Thank you for inviting me to your party last week\".\ntranslation between natural languages (machine translation)\njudging the pragmatic acceptability of natural language. For example, the following sentence might be judged \"not acceptable\", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well.\nNote that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.\n\n\n=== Tasks ===\n\nIn general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\". These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.\nIn a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: \n  \n    \n      \n        \n          Loss\n        \n        =\n        −\n        \n          ∑\n          \n            t\n            ∈\n            \n              masked tokens\n            \n          \n        \n        ln\n        ⁡\n        (\n        \n          probability of \n        \n        t\n        \n           conditional on its context\n        \n        )\n      \n    \n    {\\displaystyle {\\text{Loss}}=-\\sum _{t\\in {\\text{masked tokens}}}\\ln({\\text{probability of }}t{\\text{ conditional on its context}})}\n  \nand the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.\nIn an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.\nIn a prefixLM task, the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.\nNote that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model).\n\n\n== Architecture ==\nAll transformers have the same primary components:\n\nTokenizers, which convert text into tokens.\nEmbedding layer, which converts tokens and positions of the tokens into vector representations.\nTransformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. These consist of alternating attention and feedforward layers. There are two major types of transformer layers: encoder layers and decoder layers, with further variants.\nUn-embedding layer, which converts the final vector representations back to a probability distribution over the tokens.\nThe following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section.\nBy convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as \n  \n    \n      \n        x\n        W\n      \n    \n    {\\displaystyle xW}\n  \n.\n\n\n=== Tokenization ===\n\nAs the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences is a tokenizer.\nThe set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size \n  \n    \n      \n        \n          n\n          \n            vocabulary\n          \n        \n      \n    \n    {\\displaystyle n_{\\text{vocabulary}}}\n  \n. When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\".\nSome commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.\n\n\n=== Embedding ===\n\nEach token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n. For example, if the input token is \n  \n    \n      \n        3\n      \n    \n    {\\displaystyle 3}\n  \n, then the one-hot representation is \n  \n    \n      \n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        …\n        ]\n      \n    \n    {\\displaystyle [0,0,0,1,0,0,\\dots ]}\n  \n, and its embedding vector is\n  \n    \n      \n        \n          E\n          m\n          b\n          e\n          d\n        \n        (\n        3\n        )\n        =\n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        …\n        ]\n        M\n      \n    \n    {\\displaystyle \\mathrm {Embed} (3)=[0,0,0,1,0,0,\\dots ]M}\n  \nThe token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors.\nThe number of dimensions in an embedding vector is called hidden size or embedding size and written as \n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb}}}\n  \n. This size is written as \n  \n    \n      \n        \n          d\n          \n            model\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{model}}}\n  \n in the original Transformer paper.\n\n\n=== Un-embedding ===\nAn un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.\nThe un-embedding layer is a linear-softmax layer:\n  \n    \n      \n        \n          U\n          n\n          E\n          m\n          b\n          e\n          d\n        \n        (\n        x\n        )\n        =\n        \n          s\n          o\n          f\n          t\n          m\n          a\n          x\n        \n        (\n        x\n        W\n        +\n        b\n        )\n      \n    \n    {\\displaystyle \\mathrm {UnEmbed} (x)=\\mathrm {softmax} (xW+b)}\n  \nThe matrix has shape \n  \n    \n      \n        (\n        \n          d\n          \n            emb\n          \n        \n        ,\n        \n          n\n          \n            vocabulary\n          \n        \n        )\n      \n    \n    {\\displaystyle (d_{\\text{emb}},n_{\\text{vocabulary}})}\n  \n. The embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n and the un-embedding matrix \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n are sometimes required to be transposes of each other, a practice called weight tying.\n\n\n=== Positional encoding ===\n\nA positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This induces a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\".\nThe positional encoding is defined as a function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        →\n        \n          \n            R\n          \n          \n            d\n          \n        \n        ;\n        d\n        ∈\n        \n          Z\n        \n        ,\n        d\n        >\n        0\n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {R} ^{d};d\\in \\mathbb {Z} ,d>0}\n  \n, where \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n is a positive even integer. The full positional encoding defined in the original paper is:\n  \n    \n      \n        (\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n          \n        \n        ,\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n            +\n            1\n          \n        \n        )\n        =\n        (\n        sin\n        ⁡\n        (\n        θ\n        )\n        ,\n        cos\n        ⁡\n        (\n        θ\n        )\n        )\n        \n        ∀\n        k\n        ∈\n        {\n        0\n        ,\n        1\n        ,\n        …\n        ,\n        d\n        \n          /\n        \n        2\n        −\n        1\n        }\n      \n    \n    {\\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\\sin(\\theta ),\\cos(\\theta ))\\quad \\forall k\\in \\{0,1,\\ldots ,d/2-1\\}}\n  \nwhere \n  \n    \n      \n        θ\n        =\n        \n          \n            t\n            \n              r\n              \n                k\n              \n            \n          \n        \n        ,\n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle \\theta ={\\frac {t}{r^{k}}},r=N^{2/d}}\n  \n.\nHere, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is a free parameter that should be significantly larger than the biggest \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n that would be input into the positional encoding function. The original paper uses \n  \n    \n      \n        N\n        =\n        10000\n      \n    \n    {\\displaystyle N=10000}\n  \n.\nThe function is in a simpler form when written as a complex function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        →\n        \n          \n            C\n          \n          \n            d\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {C} ^{d/2}}\n  \n\n  \n    \n      \n        f\n        (\n        t\n        )\n        =\n        \n          \n            (\n            \n              e\n              \n                i\n                t\n                \n                  /\n                \n                \n                  r\n                  \n                    k\n                  \n                \n              \n            \n            )\n          \n          \n            k\n            =\n            0\n            ,\n            1\n            ,\n            …\n            ,\n            \n              \n                d\n                2\n              \n            \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle f(t)=\\left(e^{it/r^{k}}\\right)_{k=0,1,\\ldots ,{\\frac {d}{2}}-1}}\n  \nwhere \n  \n    \n      \n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle r=N^{2/d}}\n  \n.\nThe main reason for using this positional encoding function is that using it, shifts are linear transformations:\n  \n    \n      \n        f\n        (\n        t\n        +\n        Δ\n        t\n        )\n        =\n        \n          d\n          i\n          a\n          g\n        \n        (\n        f\n        (\n        Δ\n        t\n        )\n        )\n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle f(t+\\Delta t)=\\mathrm {diag} (f(\\Delta t))f(t)}\n  \nwhere \n  \n    \n      \n        Δ\n        t\n        ∈\n        \n          R\n        \n      \n    \n    {\\displaystyle \\Delta t\\in \\mathbb {R} }\n  \n is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.\nBy taking a linear sum, any convolution can also be implemented as linear transformations:\n  \n    \n      \n        \n          ∑\n          \n            j\n          \n        \n        \n          c\n          \n            j\n          \n        \n        f\n        (\n        t\n        +\n        Δ\n        \n          t\n          \n            j\n          \n        \n        )\n        =\n        \n          (\n          \n            \n              ∑\n              \n                j\n              \n            \n            \n              c\n              \n                j\n              \n            \n            \n            \n              d\n              i\n              a\n              g\n            \n            (\n            f\n            (\n            Δ\n            \n              t\n              \n                j\n              \n            \n            )\n            )\n          \n          )\n        \n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\sum _{j}c_{j}f(t+\\Delta t_{j})=\\left(\\sum _{j}c_{j}\\,\\mathrm {diag} (f(\\Delta t_{j}))\\right)f(t)}\n  \nfor any constants \n  \n    \n      \n        \n          c\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle c_{j}}\n  \n. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\"\nIn typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.\n\n\n=== Encoder-decoder (overview) ===\n\nLike earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.\nThe purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for \"mixing\" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).\nBoth the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps. These feed-forward layers contain most of the parameters in a Transformer model.\n\n\n=== Feedforward network ===\n\nThe feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons:\n  \n    \n      \n        \n          F\n          F\n          N\n        \n        (\n        x\n        )\n        =\n        ϕ\n        (\n        x\n        \n          W\n          \n            (\n            1\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            1\n            )\n          \n        \n        )\n        \n          W\n          \n            (\n            2\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {FFN} (x)=\\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}\n  \nwhere \n  \n    \n      \n        \n          W\n          \n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle W^{(1)}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle W^{(2)}}\n  \n are weight matrices and \n  \n    \n      \n        \n          b\n          \n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle b^{(1)}}\n  \n and  \n  \n    \n      \n        \n          b\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle b^{(2)}}\n  \n are bias vectors, and \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n is its activation function. The original Transformer used ReLU activation.\nThe number of neurons in the middle layer is called intermediate size (GPT), filter size (BERT), or feedforward size (BERT). It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: \n  \n    \n      \n        \n          d\n          \n            ffn\n          \n        \n        =\n        4\n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{ffn}}=4d_{\\text{emb}}}\n  \n.\n\n\n=== Scaled dot-product attention ===\n\n\n==== Attention head ====\n\nThe attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n, the key weights \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n, and the value weights \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n.\nThe module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length \n  \n    \n      \n        \n          ℓ\n          \n            seq, query\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, query}}}\n  \n, and each entry is a vector of dimension \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}}\n  \n. Similarly for the key and value sequences.\nFor each vector \n  \n    \n      \n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n      \n    \n    {\\displaystyle x_{i,{\\text{query}}}}\n  \n in the query sequence, it is multiplied by a matrix \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n to produce a query vector \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        =\n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle q_{i}=x_{i,{\\text{query}}}W^{Q}}\n  \n. The matrix of all query vectors is the query matrix:\n  \n    \n      \n        Q\n        =\n        \n          X\n          \n            query\n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle Q=X_{\\text{query}}W^{Q}}\n  \nSimilarly, we construct the key matrix \n  \n    \n      \n        K\n        =\n        \n          X\n          \n            key\n          \n        \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle K=X_{\\text{key}}W^{K}}\n  \n and the value matrix \n  \n    \n      \n        V\n        =\n        \n          X\n          \n            value\n          \n        \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle V=X_{\\text{value}}W^{V}}\n  \n.\nIt is usually the case that all \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{Q},W^{K},W^{V}}\n  \n are square matrices, meaning \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n        =\n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}=d_{\\text{query}}}\n  \n, etc.\nAttention weights are calculated using the query and key vectors: the attention weight \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n is the dot product between \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n and \n  \n    \n      \n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle k_{j}}\n  \n. The attention weights are divided by the square root of the dimension of the key vectors, \n  \n    \n      \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {d_{k}}}}\n  \n, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n are different matrices allows attention to be non-symmetric: if token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n attends to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        ⋅\n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle q_{i}\\cdot k_{j}}\n  \n is large), this does not necessarily mean that token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n will attend to token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            j\n          \n        \n        ⋅\n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{j}\\cdot k_{i}}\n  \n could be small). The output of the attention unit for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n is the weighted sum of the value vectors of all tokens, weighted by \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n, the attention from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to each token.\nThe attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n are defined as the matrices where the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \nth rows are vectors \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n, \n  \n    \n      \n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle k_{i}}\n  \n, and \n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle v_{i}}\n  \n respectively. Then we can represent the attention as\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        Q\n                        \n                          K\n                          \n                            \n                              T\n                            \n                          \n                        \n                      \n                      \n                        \n                          d\n                          \n                            k\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \n\nwhere the softmax is applied over each of the rows of the matrix.\nThe number of dimensions in a query vector is query size \n  \n    \n      \n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{query}}}\n  \n and similarly for the key size \n  \n    \n      \n        \n          d\n          \n            key\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{key}}}\n  \n and value size \n  \n    \n      \n        \n          d\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{value}}}\n  \n. The output dimension of an attention head is its head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n. The attention mechanism requires the following three equalities to hold:\n  \n    \n      \n        \n          ℓ\n          \n            seq, key\n          \n        \n        =\n        \n          ℓ\n          \n            seq, value\n          \n        \n        ,\n        \n        \n          d\n          \n            query\n          \n        \n        =\n        \n          d\n          \n            key\n          \n        \n        ,\n        \n        \n          d\n          \n            value\n          \n        \n        =\n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, key}}=\\ell _{\\text{seq, value}},\\;d_{\\text{query}}=d_{\\text{key}},\\;d_{\\text{value}}=d_{\\text{head}}}\n  \nbut is otherwise unconstrained.\nIf the attention head is used in a self-attention fashion, then \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        =\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}=X_{\\text{key}}=X_{\\text{value}}}\n  \n. If the attention head is used in a cross-attention fashion, then usually \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        ≠\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}\\neq X_{\\text{key}}=X_{\\text{value}}}\n  \n. It is theoretically possible for all three to be different, but that is rarely the case in practice.\n\n\n==== Multiheaded attention ====\n\nOne set of \n  \n    \n      \n        \n          (\n          \n            \n              W\n              \n                Q\n              \n            \n            ,\n            \n              W\n              \n                K\n              \n            \n            ,\n            \n              W\n              \n                V\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)}\n  \n matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\". Specifically, the query and key projection matrices, \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n , which are involved in the attention score computation, defines the \"relevance\". Meanwhile, the value projection matrix \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n, in combination with the part of the output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects. The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.\nConcretely, let the multiple attention heads be indexed by \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, then we have\n  \n    \n      \n        \n          MultiheadedAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            ∈\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        (\n        \n          Attention\n        \n        (\n        X\n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        X\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        X\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n        )\n        )\n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V}))W^{O}}\n  \n where the matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n is the concatenation of word embeddings, and the matrices \n  \n    \n      \n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}\n  \n are \"projection matrices\" owned by individual attention head \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, and \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n is a final projection matrix owned by the whole multi-headed attention head.\nIt is theoretically possible for each attention head to have a different head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n, but that is rarely the case in practice.\nAs an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:\n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n        =\n        768\n        ,\n        \n          n\n          \n            head\n          \n        \n        =\n        12\n        ,\n        \n          d\n          \n            head\n          \n        \n        =\n        64\n      \n    \n    {\\displaystyle d_{\\text{emb}}=768,n_{\\text{head}}=12,d_{\\text{head}}=64}\n  \nSince \n  \n    \n      \n        12\n        ×\n        64\n        =\n        768\n      \n    \n    {\\displaystyle 12\\times 64=768}\n  \n, its output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n        ∈\n        \n          \n            R\n          \n          \n            (\n            12\n            ×\n            64\n            )\n            ×\n            768\n          \n        \n      \n    \n    {\\displaystyle W^{O}\\in \\mathbb {R} ^{(12\\times 64)\\times 768}}\n  \n is a square matrix.\n\n\n==== Masked attention ====\nThe Transformer architecture is constructed to calculate output tokens iteratively. Assuming \n  \n    \n      \n        t\n        =\n        0\n      \n    \n    {\\displaystyle t=0}\n  \n refers to the calculation of the first output token \n  \n    \n      \n        i\n        =\n        0\n      \n    \n    {\\displaystyle i=0}\n  \n, for step \n  \n    \n      \n        t\n        >\n        0\n      \n    \n    {\\displaystyle t>0}\n  \n, the output token \n  \n    \n      \n        i\n        =\n        0\n      \n    \n    {\\displaystyle i=0}\n  \n shall remain constant. This ensures properties of the model similar to autoregressive models. Therefore, at every time step \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, the calculation for all outputs \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n should not have access to tokens at position \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n for \n  \n    \n      \n        j\n        >=\n        i\n      \n    \n    {\\displaystyle j>=i}\n  \n (as it naturally is the case for time step \n  \n    \n      \n        t\n        =\n        i\n      \n    \n    {\\displaystyle t=i}\n  \n, when tokens \n  \n    \n      \n        j\n        >\n        t\n      \n    \n    {\\displaystyle j>t}\n  \n are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n that is \n  \n    \n      \n        −\n        ∞\n      \n    \n    {\\displaystyle -\\infty }\n  \n at entries where the attention link must be cut, and \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n at other places:\n  \n    \n      \n        \n          \n            \n              \n                \n                  MaskedAttention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    M\n                    +\n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{MaskedAttention}}(Q,K,V)={\\text{softmax}}\\left(M+{\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \n The following matrix is commonly used in decoder self-attention modules, called \"causal masking\":\n  \n    \n      \n        \n          M\n          \n            causal\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  −\n                  ∞\n                \n                \n                  −\n                  ∞\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  −\n                  ∞\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  …\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{causal}}={\\begin{bmatrix}0&-\\infty &-\\infty &\\dots &-\\infty \\\\0&0&-\\infty &\\dots &-\\infty \\\\0&0&0&\\dots &-\\infty \\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix}}}\n  \n\nIn words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form \n  \n    \n      \n        P\n        \n          M\n          \n            causal\n          \n        \n        \n          P\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle PM_{\\text{causal}}P^{-1}}\n  \n, where \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is a random permutation matrix.\n\n\n=== Encoder ===\n\nAn encoder consists of an embedding layer, followed by multiple encoder layers.\nEach encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  given input vectors \n                \n              \n              \n                \n                  h\n                  \n                    0\n                  \n                \n                ,\n                \n                  h\n                  \n                    1\n                  \n                \n                ,\n                …\n              \n            \n            \n              \n                \n                  combine them into a matrix \n                \n                H\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            h\n                            \n                              0\n                            \n                          \n                        \n                      \n                      \n                        \n                          \n                            h\n                            \n                              1\n                            \n                          \n                        \n                      \n                      \n                        \n                          ⋮\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n            \n              \n                \n                  EncoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadedAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              0\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadedAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              1\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          ⋮\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{given input vectors }}&h_{0},h_{1},\\dots \\\\{\\text{combine them into a matrix }}H&={\\begin{bmatrix}h_{0}\\\\h_{1}\\\\\\vdots \\end{bmatrix}}\\\\{\\text{EncoderLayer}}(H)&={\\begin{bmatrix}{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{0})\\\\{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{1})\\\\\\vdots \\end{bmatrix}}\\\\\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n stands for \"feed-forward network\". We can more succinctly write it as\n  \n    \n      \n        \n          EncoderLayer\n        \n        (\n        H\n        )\n        =\n        \n          FFN\n        \n        (\n        \n          MultiheadedAttention\n        \n        (\n        H\n        ,\n        H\n        ,\n        H\n        )\n        )\n      \n    \n    {\\displaystyle {\\text{EncoderLayer}}(H)={\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H))}\n  \nwith the implicit convention that the \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n is applied to each row of the matrix individually.\nThe encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.\nAs the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.\n\n\n=== Decoder ===\n\nA decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.\nEach decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention.\nLike the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow. This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.\nIn contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.\nSchematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  H\n                  ′\n                \n              \n              \n                \n                =\n                \n                  MaskedMultiheadedAttention\n                \n                (\n                H\n                ,\n                H\n                ,\n                H\n                )\n              \n            \n            \n              \n                \n                  DecoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  FFN\n                \n                (\n                \n                  MultiheadedAttention\n                \n                (\n                \n                  H\n                  ′\n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                )\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}H'&={\\text{MaskedMultiheadedAttention}}(H,H,H)\\\\{\\text{DecoderLayer}}(H)&={\\text{FFN}}({\\text{MultiheadedAttention}}(H',H^{E},H^{E}))\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        \n          H\n          \n            E\n          \n        \n      \n    \n    {\\displaystyle H^{E}}\n  \n is the matrix with rows being the output vectors from the encoder.\nThe last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.\n\n\n=== Adapted architectures ===\nMany large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture. Early GPT models are decoder-only models trained to predict the next token in a sequence. BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence.\n\n\n== Full transformer architecture ==\n\n\n=== Sublayers ===\nEach encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.\n\nThe final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence.\nThe residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y = F(x) + x. The expression indicates that an output y is the sum of the transformation of input x (F(x)) and the input itself (x). Adding the input x can preserve the input information and avoid issues when the gradient of F(x) is close to zero.\nSimilarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector.\nThere are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is \n  \n    \n      \n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle \\mathrm {LayerNorm} (x+\\mathrm {Sublayer} (x))}\n  \nwhere \n  \n    \n      \n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\mathrm {Sublayer} (x)}\n  \n is the function implemented by the sublayer itself.\nIn the pre-LN convention, the output of each sublayer is\n  \n    \n      \n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle x+\\mathrm {Sublayer} (\\mathrm {LayerNorm} (x))}\n  \nThe original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence.\n\n\n=== Pseudocode ===\nThe following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from\n\ninput: Encoder input t_e\n       Decoder input t_d\noutput: Array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence))\n\n/* encoder */\nz_e ← encoder.tokenizer(t_e)\n\nfor each t in 1:length(z_e) do\n    z_e[t] ← encoder.embedding(z_e[t]) + encoder.positional_embedding(t)\n\nfor each l in 1:length(encoder.layers) do\n    layer ← encoder.layers[l]\n\n    /* first sublayer */\n    z_e_copy ← copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← layer.layer_norm(z_e[t])\n    z_e ← layer.multiheaded_attention(z_e, z_e, z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← z_e[t] + z_e_copy[t]\n\n    /* second sublayer */\n    z_e_copy ← copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← layer.layer_norm(z_e[t])\n    z_e ← layer.feedforward(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← z_e[t] + z_e_copy[t]\n\nfor each t in 1:length(z_e) do\n    z_e[t] ← encoder.final_layer_norm(z_e[t])\n\n/* decoder */\nz_d ← decoder.tokenizer(t_d)\n\nfor each t in 1:length(z_d) do\n    z_d[t] ← decoder.embedding(z_d[t]) + decoder.positional_embedding(t)\n\nfor each l in 1:length(decoder.layers) do\n        layer ← decoder.layers[l]\n\n        /* first sublayer */\n        z_d_copy ← copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← layer.layer_norm(z_d[t])\n        z_d ← layer.masked_multiheaded_attention(z_d, z_d, z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← z_d[t] + z_d_copy[t]\n\n        /* second sublayer */\n        z_d_copy ← copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← layer.layer_norm(z_d[t])\n        z_d ← layer.multiheaded_attention(z_d, z_e, z_e) \n        for each i in 1:length(z_d) do\n            z_d[t] ← z_d[t] + z_d_copy[t]\n\n        /* third sublayer */\n        z_d_copy ← copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← layer.layer_norm(z_d[t])\n        z_d ← layer.feedforward(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← z_d[t] + z_d_copy[t]\n\nz_d ← decoder.final_layer_norm(z_d)\n\noutput_distributions ← []\nfor each t in 1:length(z_d) do\n    output_distributions.append(decoder.unembed(z_d[t]))\n\nreturn output_distributions\n\n\n=== Terminology ===\nThe Transformer architecture, being modular, allows variations. Several common variations are described here.\nAn \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.\nA \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only.\nAn \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder.\nA \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form\n  \n    \n      \n        \n          M\n          \n            prefixLM\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    0\n                  \n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \n                    M\n                    \n                      causal\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{\\text{causal}}\\end{bmatrix}}}\n  \nwhere the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less \"sparsity\". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.\nThere are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.\n\n\n== Subsequent work ==\n\n\n=== Alternative activation functions ===\nThe original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU; both GPT-1 and BERT used GELU.\nAlternative activation functions are often used in combination with Gated Linear Units in the feedforward module.\n\n\n=== Alternative normalizations ===\nThe normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm which is used in the Llama series. Other examples include CapsuleNorm ScaleNorm, or FixNorm.\n\n\n=== Alternative positional encodings ===\nTransformers may use other positional encoding methods than sinusoidal.\nThe original Transformer paper reported using a learned positional encoding, but finding it not superior to the sinusoidal one. Later, found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.\n\n\n==== RoPE ====\nRoPE (rotary positional embedding), is best explained by considering a list of 2-dimensional vectors \n  \n    \n      \n        [\n        (\n        \n          x\n          \n            1\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            2\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            3\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        .\n        .\n        .\n        ]\n      \n    \n    {\\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}\n  \n. Now pick some angle \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. Then RoPE encoding is\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  cos\n                  ⁡\n                  m\n                  θ\n                \n                \n                  −\n                  sin\n                  ⁡\n                  m\n                  θ\n                \n              \n              \n                \n                  sin\n                  ⁡\n                  m\n                  θ\n                \n                \n                  cos\n                  ⁡\n                  m\n                  θ\n                \n              \n            \n            )\n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                \n              \n            \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  cos\n                  ⁡\n                  m\n                  θ\n                  −\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  sin\n                  ⁡\n                  m\n                  θ\n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  cos\n                  ⁡\n                  m\n                  θ\n                  +\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  sin\n                  ⁡\n                  m\n                  θ\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x_{m}^{(1)},x_{m}^{(2)},m{\\big )}={\\begin{pmatrix}\\cos m\\theta &-\\sin m\\theta \\\\\\sin m\\theta &\\cos m\\theta \\end{pmatrix}}{\\begin{pmatrix}x_{m}^{(1)}\\\\x_{m}^{(2)}\\\\\\end{pmatrix}}={\\begin{pmatrix}x_{m}^{(1)}\\cos m\\theta -x_{m}^{(2)}\\sin m\\theta \\\\x_{m}^{(2)}\\cos m\\theta +x_{m}^{(1)}\\sin m\\theta \\\\\\end{pmatrix}}}\n  \nEquivalently, if we write the 2-dimensional vectors as complex numbers \n  \n    \n      \n        \n          z\n          \n            m\n          \n        \n        :=\n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        +\n        i\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}\n  \n, then RoPE encoding is just multiplication by an angle:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          z\n          \n            m\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          e\n          \n            i\n            m\n            θ\n          \n        \n        \n          z\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}z_{m},m{\\big )}=e^{im\\theta }z_{m}}\n  \nFor a list of \n  \n    \n      \n        2\n        n\n      \n    \n    {\\displaystyle 2n}\n  \n-dimensional vectors, a RoPE encoder is defined by a sequence of angles \n  \n    \n      \n        \n          θ\n          \n            (\n            1\n            )\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          θ\n          \n            (\n            n\n            )\n          \n        \n      \n    \n    {\\displaystyle \\theta ^{(1)},...,\\theta ^{(n)}}\n  \n. Then the RoPE encoding is applied to each pair of coordinates.\nThe benefit of RoPE is that the dot-product between two vectors depends on their relative location only:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        \n          \n            )\n          \n        \n        =\n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        +\n        k\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        +\n        k\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x,m{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n{\\big )}={\\text{RoPE}}{\\big (}x,m+k{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n+k{\\big )}}\n  \n\nfor any integer \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n.\n\n\n==== ALiBi ====\nALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    s\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+sB\\right)V\\end{aligned}}}\n  \nHere, \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is a real number (\"scalar\"), and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is the linear bias matrix defined by\n  \n    \n      \n        B\n        =\n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  3\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  −\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  −\n                  2\n                \n                \n                  −\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  −\n                  3\n                \n                \n                  −\n                  2\n                \n                \n                  −\n                  1\n                \n                \n                  0\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle B={\\begin{pmatrix}0&1&2&3&\\cdots \\\\-1&0&1&2&\\cdots \\\\-2&-1&0&1&\\cdots \\\\-3&-2&-1&0&\\cdots \\\\\\vdots &\\vdots &\\vdots &\\vdots &\\ddots \\\\\\end{pmatrix}}}\n  \nin other words, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        j\n        −\n        i\n      \n    \n    {\\displaystyle B_{i,j}=j-i}\n  \n. The idea being that the linear bias matrix is a softened mask. Just as \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n represent full attention paid, and \n  \n    \n      \n        −\n        ∞\n      \n    \n    {\\displaystyle -\\infty }\n  \n represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.\nALiBi allows pretraining on short context windows, then fine-tuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).\n\n\n==== Relative Position Encodings ====\nRelative Position Encodings is similar to ALiBi, but more generic:\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+B\\right)V\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is a Toeplitz matrix, that is, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        \n          B\n          \n            \n              i\n              ′\n            \n            ,\n            \n              j\n              ′\n            \n          \n        \n      \n    \n    {\\displaystyle B_{i,j}=B_{i',j'}}\n  \n whenever \n  \n    \n      \n        i\n        −\n        j\n        =\n        \n          i\n          ′\n        \n        −\n        \n          j\n          ′\n        \n      \n    \n    {\\displaystyle i-j=i'-j'}\n  \n. This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\".\n\n\n=== Efficient implementation ===\nThe transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.\n\n\n==== KV caching ====\nWhen an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.\nIf a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.\n\n\n==== FlashAttention ====\nFlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow). See the page on softmax for details.\nAn improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.\nKey advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).\nBenchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.\n\n\n==== Multi-Query Attention ====\n\nMulti-Query Attention changes the multiheaded attention mechanism. Whereas normally,\n\n  \n    \n      \n        \n          MultiheadedAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            ∈\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\\right)W^{O}}\n  \nwith Multi-Query Attention, there is just one \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{K},W^{V}}\n  \n, thus:\n\n  \n    \n      \n        \n          MultiQueryAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            ∈\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiQueryAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\\right)W^{O}}\n  \n\nThis has a neutral effect on model quality and training speed, but increases inference speed.\nMore generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups.\n\nMultihead Latent Attention (MLA) is a low-rank approximation to standard MHA. Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces (\"latent space\"), one for query and one for key-value (KV vector). This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached.\n\n\n==== Speculative decoding ====\nSpeculative decoding is a method to accelerate token decoding. Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified. If the quickly computed tokens are incorrect, they are discarded and computed slowly.\nThe key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode, in the following sense.\nSuppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512. To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            512\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},...,x_{512}}\n  \n, taking time \n  \n    \n      \n        512\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 512T_{\\text{GPT-3}}}\n  \n. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n is indeed the token with the largest log-likelihood in the \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n-th output.\nIn speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose we use GPT-3-small to generate four speculative tokens: \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1},{\\tilde {x}}_{2},{\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n  \n. This only takes \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3-small\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3-small}}}\n  \n. These tokens are then run through the larger GPT-3 in one go. Suppose that \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1}}\n  \n and \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{2}}\n  \n are verified by GPT-3 as what it would have picked, then those are kept, but \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{3}}\n  \n is not, so \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n  \n are discarded, and GPT-3 is run on those. This would take \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3-small\n          \n        \n        +\n        3\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3-small}}+3T_{\\text{GPT-3}}}\n  \n, which might be shorter than \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3}}}\n  \n.\nFor non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.\n\nIn Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability. However, that vector can then be further processed by another Transformer block to predict the next token, and so on for arbitrarily many steps into the future. This trades off accuracy for speed, since each new token costs just one more Transformer block, rather than the entire stack.\n\n\n=== Sub-quadratic transformers ===\nTraining transformer-based architectures can be expensive, especially for long inputs. Many methods have been developed to attempt to address the issue. In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows. In the audio domain, SepTr decouples the attention in time and frequency domains. Long Range Arena (2020) is a standard benchmark for comparing the behavior of transformer architectures over long inputs.\n\n\n==== Alternative attention graphs ====\nThe standard attention graph is either all-to-all or causal, both of which scales as \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n where \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of tokens in a sequence.\nReformer (2020) reduces the computational load from \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n to \n  \n    \n      \n        O\n        (\n        N\n        ln\n        ⁡\n        N\n        )\n      \n    \n    {\\displaystyle O(N\\ln N)}\n  \n by using locality-sensitive hashing and reversible layers.\nSparse attention uses attention graphs that grows slower than \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n. For example, BigBird (2020) uses random small-world networks which grows as \n  \n    \n      \n        O\n        (\n        N\n        )\n      \n    \n    {\\displaystyle O(N)}\n  \n.\nOrdinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.\n\n\n==== Random Feature Attention ====\nRandom Feature Attention (2021) uses Fourier random features:\n  \n    \n      \n        φ\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              D\n            \n          \n        \n        [\n        cos\n        ⁡\n        ⟨\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        ⟩\n        ,\n        sin\n        ⁡\n        ⟨\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        ⟩\n        ,\n        ⋯\n        cos\n        ⁡\n        ⟨\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        ⟩\n        ,\n        sin\n        ⁡\n        ⟨\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        ⟩\n        \n          ]\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (x)={\\frac {1}{\\sqrt {D}}}[\\cos \\langle w_{1},x\\rangle ,\\sin \\langle w_{1},x\\rangle ,\\cdots \\cos \\langle w_{D},x\\rangle ,\\sin \\langle w_{D},x\\rangle ]^{T}}\n  \nwhere \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are independent samples from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          σ\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n. This choice of parameters satisfy \n  \n    \n      \n        \n          E\n        \n        [\n        ⟨\n        φ\n        (\n        x\n        )\n        ,\n        φ\n        (\n        y\n        )\n        ⟩\n        ]\n        =\n        \n          e\n          \n            −\n            \n              \n                \n                  ‖\n                  x\n                  −\n                  y\n                  \n                    ‖\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {E} [\\langle \\varphi (x),\\varphi (y)\\rangle ]=e^{-{\\frac {\\|x-y\\|^{2}}{2\\sigma ^{2}}}}}\n  \n, or \n  \n    \n      \n        \n          e\n          \n            ⟨\n            x\n            ,\n            y\n            ⟩\n            \n              /\n            \n            \n              σ\n              \n                2\n              \n            \n          \n        \n        =\n        \n          E\n        \n        [\n        ⟨\n        \n          e\n          \n            ‖\n            x\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            ‖\n            y\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        y\n        )\n        ⟩\n        ]\n        ≈\n        ⟨\n        \n          e\n          \n            ‖\n            x\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            ‖\n            y\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        y\n        )\n        ⟩\n      \n    \n    {\\displaystyle e^{\\langle x,y\\rangle /\\sigma ^{2}}=\\mathbb {E} [\\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle ]\\approx \\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle }\n  \nConsequently, the one-headed attention, with one query, can be written as \n  \n    \n      \n        \n          Attention\n        \n        (\n        q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        ≈\n        \n          \n            \n              φ\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                ∑\n                \n                  i\n                \n              \n              \n                e\n                \n                  ‖\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    ‖\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n              φ\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n              \n                v\n                \n                  i\n                \n                \n                  T\n                \n              \n            \n            \n              φ\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                ∑\n                \n                  i\n                \n              \n              \n                e\n                \n                  ‖\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    ‖\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n              φ\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Attention}}(q,K,V)={\\text{softmax}}\\left({\\frac {qK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx {\\frac {\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})v_{i}^{T}}{\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})}}}\n  \nwhere \n  \n    \n      \n        σ\n        =\n        \n          d\n          \n            K\n          \n          \n            1\n            \n              /\n            \n            4\n          \n        \n      \n    \n    {\\displaystyle \\sigma =d_{K}^{1/4}}\n  \n. Similarly for multiple queries, and for multiheaded attention.\nThis approximation can be computed in linear time, as we can compute the matrix \n  \n    \n      \n        φ\n        (\n        \n          k\n          \n            i\n          \n        \n        )\n        \n          v\n          \n            i\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (k_{i})v_{i}^{T}}\n  \n first, then multiply it with the query. In essence, we have managed to obtain a more precise version of \n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                Q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        ≈\n        Q\n        (\n        \n          K\n          \n            T\n          \n        \n        V\n        \n          /\n        \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx Q(K^{T}V/{\\sqrt {d_{k}}})}\n  \nPerformer (2022) uses the same Random Feature Attention, but \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are first independently sampled from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          σ\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n, then they are Gram-Schmidt processed.\n\n\n=== Multimodality ===\nTransformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality.\nMultimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning. The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.\nVision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.\nConformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.\nPerceivers are a variant of Transformers designed for multimodality.\nFor image generation, notable architectures are DALL-E 1 (2021), Parti (2022), Phenaki (2023), and Muse (2023). Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image. Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image. Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted. Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.\n\n\n== Applications ==\nThe transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, Gemini, AlbertAGPT, Claude, BERT, Grok, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including:\n\nmachine translation\ntime series prediction\ndocument summarization\ndocument generation\nnamed entity recognition (NER)\nwriting computer code based on requirements expressed in natural language.\nspeech-to-text\nBeyond traditional NLP, the transformer architecture has had success in other applications, such as:\n\nbiological sequence analysis\nvideo understanding\nprotein folding (such as AlphaFold)\nevaluating chess board positions. Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level.\n\n\n== See also ==\nseq2seq – Family of machine learning approaches\nPerceiver – Variant of Transformer designed for multimodal data\nVision transformer – Machine learning model for vision processing\nLarge language model – Type of machine learning model\nBERT (language model) – Series of language models developed by Google AI\nGenerative pre-trained transformer – Type of large language model\nT5 (language model) – Series of large language models developed by Google AI\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==",
      "cleaned_text": "In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets. The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers). For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens. A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers. However, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer. The idea of encoder-decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014. A 380M-parameter model for machine translation uses two long short-term memories (LSTM). Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq. These early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation. The RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\". The relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time. In 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop. Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title \"attention is all you need\". That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical. In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs. In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks. Already in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom. In language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder-RNN-decoder model by a Transformer-encoder-RNN-decoder model. Starting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models. Since 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The vision transformer, in turn, stimulated new developments in convolutional neural networks. Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024), and Sora (2024), use Transformers to analyse input data (like text prompts) by breaking it down into \"tokens\" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data. The plain transformer architecture had difficulty converging. In the original paper the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again. A 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup. Transformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include: language modeling next-sentence prediction question answering reading comprehension sentiment analysis paraphrasing The T5 transformer report documents a large number of natural language pretraining tasks. Some examples are: restoring or repairing incomplete or corrupted text. For example, the input, \"Thank you ~~ me to your party ~~ week\", might generate the output, \"Thank you for inviting me to your party last week\". translation between natural languages (machine translation) judging the pragmatic acceptability of natural language. For example, the following sentence might be judged \"not acceptable\", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well. Note that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture. In general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\". These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer. In a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: Loss = − ∑ t ∈ masked tokens ln ⁡ ( probability of t conditional on its context ) {\\displaystyle { ext{Loss}}=-\\sum _{t\\in { ext{masked tokens}}}\\ln({ ext{probability of }}t{ ext{ conditional on its context}})} and the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task. In an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks. In a prefixLM task, the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks. Note that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model). All transformers have the same primary components: Tokenizers, which convert text into tokens. Embedding layer, which converts tokens and positions of the tokens into vector representations. Transformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. These consist of alternating attention and feedforward layers. There are two major types of transformer layers: encoder layers and decoder layers, with further variants. Un-embedding layer, which converts the final vector representations back to a probability distribution over the tokens. The following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section. By convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as x W {\\displaystyle xW} . As the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences is a tokenizer. The set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size n vocabulary {\\displaystyle n_{ ext{vocabulary}}} . When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\". Some commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece. Each token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix M {\\displaystyle M} . For example, if the input token is 3 {\\displaystyle 3} , then the one-hot representation is [ 0 , 0 , 0 , 1 , 0 , 0 , ... ] {\\displaystyle [0,0,0,1,0,0,\\dots ]} , and its embedding vector is E m b e d ( 3 ) = [ 0 , 0 , 0 , 1 , 0 , 0 , ... ] M {\\displaystyle \\mathrm {Embed} (3)=[0,0,0,1,0,0,\\dots ]M} The token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors. The number of dimensions in an embedding vector is called hidden size or embedding size and written as d emb {\\displaystyle d_{ ext{emb}}} . This size is written as d model {\\displaystyle d_{ ext{model}}} in the original Transformer paper. An un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens. The un-embedding layer is a linear-softmax layer: U n E m b e d ( x ) = s o f t m a x ( x W + b ) {\\displaystyle \\mathrm {UnEmbed} (x)=\\mathrm {softmax} (xW+b)} The matrix has shape ( d emb , n vocabulary ) {\\displaystyle (d_{ ext{emb}},n_{ ext{vocabulary}})} . The embedding matrix M {\\displaystyle M} and the un-embedding matrix W {\\displaystyle W} are sometimes required to be transposes of each other, a practice called weight tying. A positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This induces a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\". The positional encoding is defined as a function of type f : R → R d ; d ∈ Z , d > 0 {\\displaystyle f:\\mathbb {R} o \\mathbb {R} ^{d};d\\in \\mathbb {Z} ,d>0} , where d {\\displaystyle d} is a positive even integer. The full positional encoding defined in the original paper is: ( f ( t ) 2 k , f ( t ) 2 k + 1 ) = ( sin ⁡ ( θ ) , cos ⁡ ( θ ) ) ∀ k ∈ { 0 , 1 , ... , d / 2 − 1 } {\\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\\sin( heta ),\\cos( heta ))\\quad \\forall k\\in \\{0,1,\\ldots ,d/2-1\\}} where θ = t r k , r = N 2 / d {\\displaystyle heta ={\\frac {t}{r^{k}}},r=N^{2/d}} . Here, N {\\displaystyle N} is a free parameter that should be significantly larger than the biggest k {\\displaystyle k} that would be input into the positional encoding function. The original paper uses N = 10000 {\\displaystyle N=10000} . The function is in a simpler form when written as a complex function of type f : R → C d / 2 {\\displaystyle f:\\mathbb {R} o \\mathbb {C} ^{d/2}} f ( t ) = ( e i t / r k ) k = 0 , 1 , ... , d 2 − 1 {\\displaystyle f(t)=\\left(e^{it/r^{k}}\\right)_{k=0,1,\\ldots ,{\\frac {d}{2}}-1}} where r = N 2 / d {\\displaystyle r=N^{2/d}} . The main reason for using this positional encoding function is that using it, shifts are linear transformations: f ( t + Δ t ) = d i a g ( f ( Δ t ) ) f ( t ) {\\displaystyle f(t+\\Delta t)=\\mathrm {diag} (f(\\Delta t))f(t)} where Δ t ∈ R {\\displaystyle \\Delta t\\in \\mathbb {R} } is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication. By taking a linear sum, any convolution can also be implemented as linear transformations: ∑ j c j f ( t + Δ t j ) = ( ∑ j c j d i a g ( f ( Δ t j ) ) ) f ( t ) {\\displaystyle \\sum _{j}c_{j}f(t+\\Delta t_{j})=\\left(\\sum _{j}c_{j}\\,\\mathrm {diag} (f(\\Delta t_{j}))\\right)f(t)} for any constants c j {\\displaystyle c_{j}} . This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\" In typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference. Like earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far. The purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for \"mixing\" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time). Both the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps. These feed-forward layers contain most of the parameters in a Transformer model. The feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons: F F N ( x ) = ϕ ( x W ( 1 ) + b ( 1 ) ) W ( 2 ) + b ( 2 ) {\\displaystyle \\mathrm {FFN} (x)=\\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}} where W ( 1 ) {\\displaystyle W^{(1)}} and W ( 2 ) {\\displaystyle W^{(2)}} are weight matrices and b ( 1 ) {\\displaystyle b^{(1)}} and b ( 2 ) {\\displaystyle b^{(2)}} are bias vectors, and ϕ {\\displaystyle \\phi } is its activation function. The original Transformer used ReLU activation. The number of neurons in the middle layer is called intermediate size (GPT), filter size (BERT), or feedforward size (BERT). It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: d ffn = 4 d emb {\\displaystyle d_{ ext{ffn}}=4d_{ ext{emb}}} . The attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights W Q {\\displaystyle W^{Q}} , the key weights W K {\\displaystyle W^{K}} , and the value weights W V {\\displaystyle W^{V}} . The module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length ℓ seq, query {\\displaystyle \\ell _{ ext{seq, query}}} , and each entry is a vector of dimension d emb, query {\\displaystyle d_{ ext{emb, query}}} . Similarly for the key and value sequences. For each vector x i , query {\\displaystyle x_{i,{ ext{query}}}} in the query sequence, it is multiplied by a matrix W Q {\\displaystyle W^{Q}} to produce a query vector q i = x i , query W Q {\\displaystyle q_{i}=x_{i,{ ext{query}}}W^{Q}} . The matrix of all query vectors is the query matrix: Q = X query W Q {\\displaystyle Q=X_{ ext{query}}W^{Q}} Similarly, we construct the key matrix K = X key W K {\\displaystyle K=X_{ ext{key}}W^{K}} and the value matrix V = X value W V {\\displaystyle V=X_{ ext{value}}W^{V}} . It is usually the case that all W Q , W K , W V {\\displaystyle W^{Q},W^{K},W^{V}} are square matrices, meaning d emb, query = d query {\\displaystyle d_{ ext{emb, query}}=d_{ ext{query}}} , etc. Attention weights are calculated using the query and key vectors: the attention weight a i j {\\displaystyle a_{ij}} from token i {\\displaystyle i} to token j {\\displaystyle j} is the dot product between q i {\\displaystyle q_{i}} and k j {\\displaystyle k_{j}} . The attention weights are divided by the square root of the dimension of the key vectors, d k {\\displaystyle {\\sqrt {d_{k}}}} , which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that W Q {\\displaystyle W^{Q}} and W K {\\displaystyle W^{K}} are different matrices allows attention to be non-symmetric: if token i {\\displaystyle i} attends to token j {\\displaystyle j} (i.e. q i ⋅ k j {\\displaystyle q_{i}\\cdot k_{j}} is large), this does not necessarily mean that token j {\\displaystyle j} will attend to token i {\\displaystyle i} (i.e. q j ⋅ k i {\\displaystyle q_{j}\\cdot k_{i}} could be small). The output of the attention unit for token i {\\displaystyle i} is the weighted sum of the value vectors of all tokens, weighted by a i j {\\displaystyle a_{ij}} , the attention from token i {\\displaystyle i} to each token. The attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices Q {\\displaystyle Q} , K {\\displaystyle K} and V {\\displaystyle V} are defined as the matrices where the i {\\displaystyle i} th rows are vectors q i {\\displaystyle q_{i}} , k i {\\displaystyle k_{i}} , and v i {\\displaystyle v_{i}} respectively. Then we can represent the attention as Attention ( Q , K , V ) = softmax ( Q K T d k ) V {\\displaystyle {\\begin{aligned}{ ext{Attention}}(Q,K,V)={ ext{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}} where the softmax is applied over each of the rows of the matrix. The number of dimensions in a query vector is query size d query {\\displaystyle d_{ ext{query}}} and similarly for the key size d key {\\displaystyle d_{ ext{key}}} and value size d value {\\displaystyle d_{ ext{value}}} . The output dimension of an attention head is its head dimension d head {\\displaystyle d_{ ext{head}}} . The attention mechanism requires the following three equalities to hold: ℓ seq, key = ℓ seq, value , d query = d key , d value = d head {\\displaystyle \\ell _{ ext{seq, key}}=\\ell _{ ext{seq, value}},\\;d_{ ext{query}}=d_{ ext{key}},\\;d_{ ext{value}}=d_{ ext{head}}} but is otherwise unconstrained. If the attention head is used in a self-attention fashion, then X query = X key = X value {\\displaystyle X_{ ext{query}}=X_{ ext{key}}=X_{ ext{value}}} . If the attention head is used in a cross-attention fashion, then usually X query ≠ X key = X value {\\displaystyle X_{ ext{query}} eq X_{ ext{key}}=X_{ ext{value}}} . It is theoretically possible for all three to be different, but that is rarely the case in practice. One set of ( W Q , W K , W V ) {\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)} matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\". Specifically, the query and key projection matrices, W Q {\\displaystyle W^{Q}} and W K {\\displaystyle W^{K}} , which are involved in the attention score computation, defines the \"relevance\". Meanwhile, the value projection matrix W V {\\displaystyle W^{V}} , in combination with the part of the output projection matrix W O {\\displaystyle W^{O}} , determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects. The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers. Concretely, let the multiple attention heads be indexed by i {\\displaystyle i} , then we have MultiheadedAttention ( Q , K , V ) = Concat i ∈ [ n heads ] ( Attention ( X W i Q , X W i K , X W i V ) ) W O {\\displaystyle { ext{MultiheadedAttention}}(Q,K,V)={ ext{Concat}}_{i\\in [n_{ ext{heads}}]}({ ext{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V}))W^{O}} where the matrix X {\\displaystyle X} is the concatenation of word embeddings, and the matrices W i Q , W i K , W i V {\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}} are \"projection matrices\" owned by individual attention head i {\\displaystyle i} , and W O {\\displaystyle W^{O}} is a final projection matrix owned by the whole multi-headed attention head. It is theoretically possible for each attention head to have a different head dimension d head {\\displaystyle d_{ ext{head}}} , but that is rarely the case in practice. As an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions: d emb = 768 , n head = 12 , d head = 64 {\\displaystyle d_{ ext{emb}}=768,n_{ ext{head}}=12,d_{ ext{head}}=64} Since 12 × 64 = 768 {\\displaystyle 12 imes 64=768} , its output projection matrix W O ∈ R ( 12 × 64 ) × 768 {\\displaystyle W^{O}\\in \\mathbb {R} ^{(12 imes 64) imes 768}} is a square matrix. The Transformer architecture is constructed to calculate output tokens iteratively. Assuming t = 0 {\\displaystyle t=0} refers to the calculation of the first output token i = 0 {\\displaystyle i=0} , for step t > 0 {\\displaystyle t>0} , the output token i = 0 {\\displaystyle i=0} shall remain constant. This ensures properties of the model similar to autoregressive models. Therefore, at every time step t {\\displaystyle t} , the calculation for all outputs i {\\displaystyle i} should not have access to tokens at position j {\\displaystyle j} for j >= i {\\displaystyle j>=i} (as it naturally is the case for time step t = i {\\displaystyle t=i} , when tokens j > t {\\displaystyle j>t} are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix M {\\displaystyle M} that is − ∞ {\\displaystyle -\\infty } at entries where the attention link must be cut, and 0 {\\displaystyle 0} at other places: MaskedAttention ( Q , K , V ) = softmax ( M + Q K T d k ) V {\\displaystyle {\\begin{aligned}{ ext{MaskedAttention}}(Q,K,V)={ ext{softmax}}\\left(M+{\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}} The following matrix is commonly used in decoder self-attention modules, called \"causal masking\": M causal = [ 0 − ∞ − ∞ ... − ∞ 0 0 − ∞ ... − ∞ 0 0 0 ... − ∞ ⋮ ⋮ ⋮ ⋱ ⋮ 0 0 0 ... 0 ] {\\displaystyle M_{ ext{causal}}={\\begin{bmatrix}0&-\\infty &-\\infty &\\dots &-\\infty \\\\0&0&-\\infty &\\dots &-\\infty \\\\0&0&0&\\dots &-\\infty \\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix}}} In words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form P M causal P − 1 {\\displaystyle PM_{ ext{causal}}P^{-1}} , where P {\\displaystyle P} is a random permutation matrix. An encoder consists of an embedding layer, followed by multiple encoder layers. Each encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have: given input vectors h 0 , h 1 , ... combine them into a matrix H = [ h 0 h 1 ⋮ ] EncoderLayer ( H ) = [ FFN ( MultiheadedAttention ( H , H , H ) 0 ) FFN ( MultiheadedAttention ( H , H , H ) 1 ) ⋮ ] {\\displaystyle {\\begin{aligned}{ ext{given input vectors }}&h_{0},h_{1},\\dots \\\\{ ext{combine them into a matrix }}H&={\\begin{bmatrix}h_{0}\\\\h_{1}\\\\\\vdots \\end{bmatrix}}\\\\{ ext{EncoderLayer}}(H)&={\\begin{bmatrix}{ ext{FFN}}({ ext{MultiheadedAttention}}(H,H,H)_{0})\\\\{ ext{FFN}}({ ext{MultiheadedAttention}}(H,H,H)_{1})\\\\\\vdots \\end{bmatrix}}\\\\\\end{aligned}}} where FFN {\\displaystyle { ext{FFN}}} stands for \"feed-forward network\". We can more succinctly write it as EncoderLayer ( H ) = FFN ( MultiheadedAttention ( H , H , H ) ) {\\displaystyle { ext{EncoderLayer}}(H)={ ext{FFN}}({ ext{MultiheadedAttention}}(H,H,H))} with the implicit convention that the FFN {\\displaystyle { ext{FFN}}} is applied to each row of the matrix individually. The encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder. As the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking. A decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer. Each decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention. Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow. This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked. In contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism. Schematically, we have: H ′ = MaskedMultiheadedAttention ( H , H , H ) DecoderLayer ( H ) = FFN ( MultiheadedAttention ( H ′ , H E , H E ) ) {\\displaystyle {\\begin{aligned}H'&={ ext{MaskedMultiheadedAttention}}(H,H,H)\\\\{ ext{DecoderLayer}}(H)&={ ext{FFN}}({ ext{MultiheadedAttention}}(H',H^{E},H^{E}))\\end{aligned}}} where H E {\\displaystyle H^{E}} is the matrix with rows being the output vectors from the encoder. The last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text. Many large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture. Early GPT models are decoder-only models trained to predict the next token in a sequence. BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence. Each encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network. The final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence. The residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y = F(x) + x. The expression indicates that an output y is the sum of the transformation of input x (F(x)) and the input itself (x). Adding the input x can preserve the input information and avoid issues when the gradient of F(x) is close to zero. Similarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector. There are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is L a y e r N o r m ( x + S u b l a y e r ( x ) ) {\\displaystyle \\mathrm {LayerNorm} (x+\\mathrm {Sublayer} (x))} where S u b l a y e r ( x ) {\\displaystyle \\mathrm {Sublayer} (x)} is the function implemented by the sublayer itself. In the pre-LN convention, the output of each sublayer is x + S u b l a y e r ( L a y e r N o r m ( x ) ) {\\displaystyle x+\\mathrm {Sublayer} (\\mathrm {LayerNorm} (x))} The original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence. The following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from input: Encoder input t_e Decoder input t_d output: Array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence)) /* encoder */ z_e ← encoder.tokenizer(t_e) for each t in 1:length(z_e) do z_e[t] ← encoder.embedding(z_e[t]) + encoder.positional_embedding(t) for each l in 1:length(encoder.layers) do layer ← encoder.layers[l] /* first sublayer */ z_e_copy ← copy(z_e) for each t in 1:length(z_e) do z_e[t] ← layer.layer_norm(z_e[t]) z_e ← layer.multiheaded_attention(z_e, z_e, z_e) for each t in 1:length(z_e) do z_e[t] ← z_e[t] + z_e_copy[t] /* second sublayer */ z_e_copy ← copy(z_e) for each t in 1:length(z_e) do z_e[t] ← layer.layer_norm(z_e[t]) z_e ← layer.feedforward(z_e) for each t in 1:length(z_e) do z_e[t] ← z_e[t] + z_e_copy[t] for each t in 1:length(z_e) do z_e[t] ← encoder.final_layer_norm(z_e[t]) /* decoder */ z_d ← decoder.tokenizer(t_d) for each t in 1:length(z_d) do z_d[t] ← decoder.embedding(z_d[t]) + decoder.positional_embedding(t) for each l in 1:length(decoder.layers) do layer ← decoder.layers[l] /* first sublayer */ z_d_copy ← copy(z_d) for each t in 1:length(z_d) do z_d[t] ← layer.layer_norm(z_d[t]) z_d ← layer.masked_multiheaded_attention(z_d, z_d, z_d) for each t in 1:length(z_d) do z_d[t] ← z_d[t] + z_d_copy[t] /* second sublayer */ z_d_copy ← copy(z_d) for each t in 1:length(z_d) do z_d[t] ← layer.layer_norm(z_d[t]) z_d ← layer.multiheaded_attention(z_d, z_e, z_e) for each i in 1:length(z_d) do z_d[t] ← z_d[t] + z_d_copy[t] /* third sublayer */ z_d_copy ← copy(z_d) for each t in 1:length(z_d) do z_d[t] ← layer.layer_norm(z_d[t]) z_d ← layer.feedforward(z_d) for each t in 1:length(z_d) do z_d[t] ← z_d[t] + z_d_copy[t] z_d ← decoder.final_layer_norm(z_d) output_distributions ← [] for each t in 1:length(z_d) do output_distributions.append(decoder.unembed(z_d[t])) return output_distributions The Transformer architecture, being modular, allows variations. Several common variations are described here. An \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder. A \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only. An \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder. A \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form M prefixLM = [ 0 − ∞ 0 M causal ] {\\displaystyle M_{ ext{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{ ext{causal}}\\end{bmatrix}}} where the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less \"sparsity\". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons. There are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder-RNN-decoder model by a Transformer-encoder-RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively. The original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU; both GPT-1 and BERT used GELU. Alternative activation functions are often used in combination with Gated Linear Units in the feedforward module. The normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm which is used in the Llama series. Other examples include CapsuleNorm ScaleNorm, or FixNorm. Transformers may use other positional encoding methods than sinusoidal. The original Transformer paper reported using a learned positional encoding, but finding it not superior to the sinusoidal one. Later, found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module. RoPE (rotary positional embedding), is best explained by considering a list of 2-dimensional vectors [ ( x 1 ( 1 ) , x 1 ( 2 ) ) , ( x 2 ( 1 ) , x 2 ( 2 ) ) , ( x 3 ( 1 ) , x 3 ( 2 ) ) , . . . ] {\\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]} . Now pick some angle θ {\\displaystyle heta } . Then RoPE encoding is RoPE ( x m ( 1 ) , x m ( 2 ) , m ) = ( cos ⁡ m θ − sin ⁡ m θ sin ⁡ m θ cos ⁡ m θ ) ( x m ( 1 ) x m ( 2 ) ) = ( x m ( 1 ) cos ⁡ m θ − x m ( 2 ) sin ⁡ m θ x m ( 2 ) cos ⁡ m θ + x m ( 1 ) sin ⁡ m θ ) {\\displaystyle { ext{RoPE}}{\\big (}x_{m}^{(1)},x_{m}^{(2)},m{\\big )}={\\begin{pmatrix}\\cos m heta &-\\sin m heta \\\\\\sin m heta &\\cos m heta \\end{pmatrix}}{\\begin{pmatrix}x_{m}^{(1)}\\\\x_{m}^{(2)}\\\\\\end{pmatrix}}={\\begin{pmatrix}x_{m}^{(1)}\\cos m heta -x_{m}^{(2)}\\sin m heta \\\\x_{m}^{(2)}\\cos m heta +x_{m}^{(1)}\\sin m heta \\\\\\end{pmatrix}}} Equivalently, if we write the 2-dimensional vectors as complex numbers z m := x m ( 1 ) + i x m ( 2 ) {\\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}} , then RoPE encoding is just multiplication by an angle: RoPE ( z m , m ) = e i m θ z m {\\displaystyle { ext{RoPE}}{\\big (}z_{m},m{\\big )}=e^{im heta }z_{m}} For a list of 2 n {\\displaystyle 2n} -dimensional vectors, a RoPE encoder is defined by a sequence of angles θ ( 1 ) , . . . , θ ( n ) {\\displaystyle heta ^{(1)},..., heta ^{(n)}} . Then the RoPE encoding is applied to each pair of coordinates. The benefit of RoPE is that the dot-product between two vectors depends on their relative location only: RoPE ( x , m ) T RoPE ( y , n ) = RoPE ( x , m + k ) T RoPE ( y , n + k ) {\\displaystyle { ext{RoPE}}{\\big (}x,m{\\big )}^{T}{ ext{RoPE}}{\\big (}y,n{\\big )}={ ext{RoPE}}{\\big (}x,m+k{\\big )}^{T}{ ext{RoPE}}{\\big (}y,n+k{\\big )}} for any integer k {\\displaystyle k} . ALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is Attention ( Q , K , V ) = softmax ( Q K T d k + s B ) V {\\displaystyle {\\begin{aligned}{ ext{Attention}}(Q,K,V)={ ext{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+sB\\right)V\\end{aligned}}} Here, s {\\displaystyle s} is a real number (\"scalar\"), and B {\\displaystyle B} is the linear bias matrix defined by B = ( 0 1 2 3 ⋯ − 1 0 1 2 ⋯ − 2 − 1 0 1 ⋯ − 3 − 2 − 1 0 ⋯ ⋮ ⋮ ⋮ ⋮ ⋱ ) {\\displaystyle B={\\begin{pmatrix}0&1&2&3&\\cdots \\\\-1&0&1&2&\\cdots \\\\-2&-1&0&1&\\cdots \\\\-3&-2&-1&0&\\cdots \\\\\\vdots &\\vdots &\\vdots &\\vdots &\\ddots \\\\\\end{pmatrix}}} in other words, B i , j = j − i {\\displaystyle B_{i,j}=j-i} . The idea being that the linear bias matrix is a softened mask. Just as 0 {\\displaystyle 0} represent full attention paid, and − ∞ {\\displaystyle -\\infty } represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction. ALiBi allows pretraining on short context windows, then fine-tuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located). Relative Position Encodings is similar to ALiBi, but more generic: Attention ( Q , K , V ) = softmax ( Q K T d k + B ) V {\\displaystyle {\\begin{aligned}{ ext{Attention}}(Q,K,V)={ ext{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+B\\right)V\\end{aligned}}} where B {\\displaystyle B} is a Toeplitz matrix, that is, B i , j = B i ′ , j ′ {\\displaystyle B_{i,j}=B_{i',j'}} whenever i − j = i ′ − j ′ {\\displaystyle i-j=i'-j'} . This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\". The transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models. When an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching. If a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots. FlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow). See the page on softmax for details. An improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention. Key advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA). Benchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8. Multi-Query Attention changes the multiheaded attention mechanism. Whereas normally, MultiheadedAttention ( Q , K , V ) = Concat i ∈ [ n heads ] ( Attention ( X W i Q , X W i K , X W i V ) ) W O {\\displaystyle { ext{MultiheadedAttention}}(Q,K,V)={ ext{Concat}}_{i\\in [n_{ ext{heads}}]}\\left({ ext{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\\right)W^{O}} with Multi-Query Attention, there is just one W K , W V {\\displaystyle W^{K},W^{V}} , thus: MultiQueryAttention ( Q , K , V ) = Concat i ∈ [ n heads ] ( Attention ( X W i Q , X W K , X W V ) ) W O {\\displaystyle { ext{MultiQueryAttention}}(Q,K,V)={ ext{Concat}}_{i\\in [n_{ ext{heads}}]}\\left({ ext{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\\right)W^{O}} This has a neutral effect on model quality and training speed, but increases inference speed. More generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups. Multihead Latent Attention (MLA) is a low-rank approximation to standard MHA. Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces (\"latent space\"), one for query and one for key-value (KV vector). This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached. Speculative decoding is a method to accelerate token decoding. Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified. If the quickly computed tokens are incorrect, they are discarded and computed slowly. The key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode, in the following sense. Suppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512. To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token x 1 , x 2 , . . . , x 512 {\\displaystyle x_{1},x_{2},...,x_{512}} , taking time 512 T GPT-3 {\\displaystyle 512T_{ ext{GPT-3}}} . However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each x t {\\displaystyle x_{t}} is indeed the token with the largest log-likelihood in the t {\\displaystyle t} -th output. In speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose we use GPT-3-small to generate four speculative tokens: x ~ 1 , x ~ 2 , x ~ 3 , x ~ 4 {\\displaystyle { ilde {x}}_{1},{ ilde {x}}_{2},{ ilde {x}}_{3},{ ilde {x}}_{4}} . This only takes 4 T GPT-3-small {\\displaystyle 4T_{ ext{GPT-3-small}}} . These tokens are then run through the larger GPT-3 in one go. Suppose that x ~ 1 {\\displaystyle { ilde {x}}_{1}} and x ~ 2 {\\displaystyle { ilde {x}}_{2}} are verified by GPT-3 as what it would have picked, then those are kept, but x ~ 3 {\\displaystyle { ilde {x}}_{3}} is not, so x ~ 3 , x ~ 4 {\\displaystyle { ilde {x}}_{3},{ ilde {x}}_{4}} are discarded, and GPT-3 is run on those. This would take 4 T GPT-3-small + 3 T GPT-3 {\\displaystyle 4T_{ ext{GPT-3-small}}+3T_{ ext{GPT-3}}} , which might be shorter than 4 T GPT-3 {\\displaystyle 4T_{ ext{GPT-3}}} . For non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used. In Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability. However, that vector can then be further processed by another Transformer block to predict the next token, and so on for arbitrarily many steps into the future. This trades off accuracy for speed, since each new token costs just one more Transformer block, rather than the entire stack. Training transformer-based architectures can be expensive, especially for long inputs. Many methods have been developed to attempt to address the issue. In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows. In the audio domain, SepTr decouples the attention in time and frequency domains. Long Range Arena (2020) is a standard benchmark for comparing the behavior of transformer architectures over long inputs. The standard attention graph is either all-to-all or causal, both of which scales as O ( N 2 ) {\\displaystyle O(N^{2})} where N {\\displaystyle N} is the number of tokens in a sequence. Reformer (2020) reduces the computational load from O ( N 2 ) {\\displaystyle O(N^{2})} to O ( N ln ⁡ N ) {\\displaystyle O(N\\ln N)} by using locality-sensitive hashing and reversible layers. Sparse attention uses attention graphs that grows slower than O ( N 2 ) {\\displaystyle O(N^{2})} . For example, BigBird (2020) uses random small-world networks which grows as O ( N ) {\\displaystyle O(N)} . Ordinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value. Random Feature Attention (2021) uses Fourier random features: φ ( x ) = 1 D [ cos ⁡ ⟨ w 1 , x ⟩ , sin ⁡ ⟨ w 1 , x ⟩ , ⋯ cos ⁡ ⟨ w D , x ⟩ , sin ⁡ ⟨ w D , x ⟩ ] T {\\displaystyle \\varphi (x)={\\frac {1}{\\sqrt {D}}}[\\cos \\langle w_{1},x\\rangle ,\\sin \\langle w_{1},x\\rangle ,\\cdots \\cos \\langle w_{D},x\\rangle ,\\sin \\langle w_{D},x\\rangle ]^{T}} where w 1 , . . . , w D {\\displaystyle w_{1},...,w_{D}} are independent samples from the normal distribution N ( 0 , σ 2 I ) {\\displaystyle N(0,\\sigma ^{2}I)} . This choice of parameters satisfy E [ ⟨ φ ( x ) , φ ( y ) ⟩ ] = e − ‖ x − y ‖ 2 2 σ 2 {\\displaystyle \\mathbb {E} [\\langle \\varphi (x),\\varphi (y)\\rangle ]=e^{-{\\frac {\\|x-y\\|^{2}}{2\\sigma ^{2}}}}} , or e ⟨ x , y ⟩ / σ 2 = E [ ⟨ e ‖ x ‖ 2 / 2 σ 2 φ ( x ) , e ‖ y ‖ 2 / 2 σ 2 φ ( y ) ⟩ ] ≈ ⟨ e ‖ x ‖ 2 / 2 σ 2 φ ( x ) , e ‖ y ‖ 2 / 2 σ 2 φ ( y ) ⟩ {\\displaystyle e^{\\langle x,y\\rangle /\\sigma ^{2}}=\\mathbb {E} [\\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle ]\\approx \\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle } Consequently, the one-headed attention, with one query, can be written as Attention ( q , K , V ) = softmax ( q K T d k ) V ≈ φ ( q ) T ∑ i e ‖ k i ‖ 2 / 2 σ 2 φ ( k i ) v i T φ ( q ) T ∑ i e ‖ k i ‖ 2 / 2 σ 2 φ ( k i ) {\\displaystyle { ext{Attention}}(q,K,V)={ ext{softmax}}\\left({\\frac {qK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx {\\frac {\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})v_{i}^{T}}{\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})}}} where σ = d K 1 / 4 {\\displaystyle \\sigma =d_{K}^{1/4}} . Similarly for multiple queries, and for multiheaded attention. This approximation can be computed in linear time, as we can compute the matrix φ ( k i ) v i T {\\displaystyle \\varphi (k_{i})v_{i}^{T}} first, then multiply it with the query. In essence, we have managed to obtain a more precise version of Attention ( Q , K , V ) = softmax ( Q K T d k ) V ≈ Q ( K T V / d k ) {\\displaystyle { ext{Attention}}(Q,K,V)={ ext{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx Q(K^{T}V/{\\sqrt {d_{k}}})} Performer (2022) uses the same Random Feature Attention, but w 1 , . . . , w D {\\displaystyle w_{1},...,w_{D}} are first independently sampled from the normal distribution N ( 0 , σ 2 I ) {\\displaystyle N(0,\\sigma ^{2}I)} , then they are Gram-Schmidt processed. Transformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality. Multimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning. The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned. Vision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer. Conformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer. Perceivers are a variant of Transformers designed for multimodality. For image generation, notable architectures are DALL-E 1 (2021), Parti (2022), Phenaki (2023), and Muse (2023). Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image. Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image. Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted. Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video. The transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, Gemini, AlbertAGPT, Claude, BERT, Grok, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including: machine translation time series prediction document summarization document generation named entity recognition (NER) writing computer code based on requirements expressed in natural language. speech-to-text Beyond traditional NLP, the transformer architecture has had success in other applications, such as: biological sequence analysis video understanding protein folding (such as AlphaFold) evaluating chess board positions. Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level.",
      "sentences": [
        "In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.",
        "At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.",
        "Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).",
        "Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.",
        "The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google.",
        "Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since.",
        "They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess.",
        "It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).",
        "For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs).",
        "A well-cited early example was the Elman network (1990).",
        "In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.",
        "A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling.",
        "One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units.",
        "Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks.",
        "LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.",
        "However, LSTM still used sequential processing, like most other RNNs.",
        "Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.",
        "Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window.",
        "The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input.",
        "One of its two networks has \"fast weights\" or \"dynamic links\" (1981).",
        "A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries.",
        "This was later shown to be equivalent to the unnormalized linear Transformer.",
        "The idea of encoder-decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.",
        "A 380M-parameter model for machine translation uses two long short-term memories (LSTM).",
        "Its architecture consists of two parts.",
        "The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector.",
        "The decoder is another LSTM that converts the vector into a sequence of tokens.",
        "Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM.",
        "Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.",
        "These early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed.",
        "Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved.",
        "This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output.",
        "If the input is long, then the output vector would not be able to contain all relevant information, degrading the output.",
        "As evidence, reversing the input sentence improved seq2seq translation.",
        "The RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily.",
        "The name is because it \"emulates searching through a source sentence during decoding a translation\".",
        "The relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.",
        "In 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation.",
        "The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM.",
        "It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.",
        "Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs.",
        "In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs.",
        "One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title \"attention is all you need\".",
        "That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical.",
        "In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.",
        "In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper.",
        "At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance.",
        "This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence.",
        "Its parallelizability was an important factor to its widespread use in large neural networks.",
        "Already in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles.",
        "Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.",
        "In language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec.",
        "It was followed by BERT (2018), an encoder-only Transformer model.",
        "In 2019 October, Google started using BERT to process search queries.",
        "In 2020, Google Translate replaced the previous RNN-encoder-RNN-decoder model by a Transformer-encoder-RNN-decoder model.",
        "Starting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation.",
        "In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models.",
        "Since 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal.",
        "The vision transformer, in turn, stimulated new developments in convolutional neural networks.",
        "The plain transformer architecture had difficulty converging.",
        "In the original paper the authors recommended using learning rate warmup.",
        "That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.",
        "A 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.",
        "Transformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset.",
        "The pretrain dataset is typically an unlabeled large corpus, such as The Pile.",
        "Tasks for pretraining and fine-tuning commonly include: language modeling next-sentence prediction question answering reading comprehension sentiment analysis paraphrasing The T5 transformer report documents a large number of natural language pretraining tasks.",
        "Some examples are: restoring or repairing incomplete or corrupted text.",
        "For example, the input, \"Thank you ~~ me to your party ~~ week\", might generate the output, \"Thank you for inviting me to your party last week\".",
        "translation between natural languages (machine translation) judging the pragmatic acceptability of natural language.",
        "For example, the following sentence might be judged \"not acceptable\", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well.",
        "Note that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.",
        "In general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\".",
        "These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.",
        "In a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context.",
        "The loss function for the task is typically sum of log-perplexities for the masked-out tokens: Loss = − ∑ t ∈ masked tokens ln ⁡ ( probability of t conditional on its context ) {\\displaystyle { ext{Loss}}=-\\sum _{t\\in { ext{masked tokens}}}\\ln({ ext{probability of }}t{ ext{ conditional on its context}})} and the model is trained to minimize this loss function.",
        "The BERT series of models are trained for masked token prediction and another task.",
        "In an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token.",
        "Then the first token is revealed and the model predicts the second token, and so on.",
        "The loss function for the task is still typically the same.",
        "The GPT series of models are trained by autoregressive tasks.",
        "In a prefixLM task, the sequence is divided into two parts.",
        "The first part is presented as context, and the model predicts the first token of the second part.",
        "Then that would be revealed, and the model predicts the second token, and so on.",
        "The loss function for the task is still typically the same.",
        "The T5 series of models are trained by prefixLM tasks.",
        "Note that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model).",
        "All transformers have the same primary components: Tokenizers, which convert text into tokens.",
        "Embedding layer, which converts tokens and positions of the tokens into vector representations.",
        "Transformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information.",
        "These consist of alternating attention and feedforward layers.",
        "There are two major types of transformer layers: encoder layers and decoder layers, with further variants.",
        "Un-embedding layer, which converts the final vector representations back to a probability distribution over the tokens.",
        "The following description follows exactly the Transformer as described in the original paper.",
        "There are variants, described in the following section.",
        "By convention, we write all vectors as row vectors.",
        "This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as x W {\\displaystyle xW} .",
        "As the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens.",
        "A token is an integer that represents a character, or a short segment of characters.",
        "On the input side, the input text is parsed into a token sequence.",
        "Similarly, on the output side, the output tokens are parsed back to text.",
        "The module doing the conversion between texts and token sequences is a tokenizer.",
        "The set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size n vocabulary {\\displaystyle n_{ ext{vocabulary}}} .",
        "When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\".",
        "Some commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.",
        "Each token is converted into an embedding vector via a lookup table.",
        "Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix M {\\displaystyle M} .",
        "The number of dimensions in an embedding vector is called hidden size or embedding size and written as d emb {\\displaystyle d_{ ext{emb}}} .",
        "This size is written as d model {\\displaystyle d_{ ext{model}}} in the original Transformer paper.",
        "An un-embedding layer is almost the reverse of an embedding layer.",
        "Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.",
        "The embedding matrix M {\\displaystyle M} and the un-embedding matrix W {\\displaystyle W} are sometimes required to be transposes of each other, a practice called weight tying.",
        "A positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence.",
        "This induces a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\".",
        "The positional encoding is defined as a function of type f : R → R d ; d ∈ Z , d > 0 {\\displaystyle f:\\mathbb {R} o \\mathbb {R} ^{d};d\\in \\mathbb {Z} ,d>0} , where d {\\displaystyle d} is a positive even integer.",
        "Here, N {\\displaystyle N} is a free parameter that should be significantly larger than the biggest k {\\displaystyle k} that would be input into the positional encoding function.",
        "The original paper uses N = 10000 {\\displaystyle N=10000} .",
        "This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.",
        "This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors.",
        "This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model.",
        "In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\"",
        "In typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.",
        "Like earlier seq2seq models, the original transformer model used an encoder-decoder architecture.",
        "The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.",
        "The purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism.",
        "the tokens generated so far during inference time).",
        "Both the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps.",
        "These feed-forward layers contain most of the parameters in a Transformer model.",
        "The original Transformer used ReLU activation.",
        "It is typically larger than the embedding size.",
        "For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: d ffn = 4 d emb {\\displaystyle d_{ ext{ffn}}=4d_{ ext{emb}}} .",
        "The attention mechanism used in the Transformer architecture are scaled dot-product attention units.",
        "For each unit, the transformer model learns three weight matrices: the query weights W Q {\\displaystyle W^{Q}} , the key weights W K {\\displaystyle W^{K}} , and the value weights W V {\\displaystyle W^{V}} .",
        "The module takes three sequences, a query sequence, a key sequence, and a value sequence.",
        "The query sequence is a sequence of length ℓ seq, query {\\displaystyle \\ell _{ ext{seq, query}}} , and each entry is a vector of dimension d emb, query {\\displaystyle d_{ ext{emb, query}}} .",
        "Similarly for the key and value sequences.",
        "For each vector x i , query {\\displaystyle x_{i,{ ext{query}}}} in the query sequence, it is multiplied by a matrix W Q {\\displaystyle W^{Q}} to produce a query vector q i = x i , query W Q {\\displaystyle q_{i}=x_{i,{ ext{query}}}W^{Q}} .",
        "The matrix of all query vectors is the query matrix: Q = X query W Q {\\displaystyle Q=X_{ ext{query}}W^{Q}} Similarly, we construct the key matrix K = X key W K {\\displaystyle K=X_{ ext{key}}W^{K}} and the value matrix V = X value W V {\\displaystyle V=X_{ ext{value}}W^{V}} .",
        "It is usually the case that all W Q , W K , W V {\\displaystyle W^{Q},W^{K},W^{V}} are square matrices, meaning d emb, query = d query {\\displaystyle d_{ ext{emb, query}}=d_{ ext{query}}} , etc.",
        "Attention weights are calculated using the query and key vectors: the attention weight a i j {\\displaystyle a_{ij}} from token i {\\displaystyle i} to token j {\\displaystyle j} is the dot product between q i {\\displaystyle q_{i}} and k j {\\displaystyle k_{j}} .",
        "The attention weights are divided by the square root of the dimension of the key vectors, d k {\\displaystyle {\\sqrt {d_{k}}}} , which stabilizes gradients during training, and passed through a softmax which normalizes the weights.",
        "The fact that W Q {\\displaystyle W^{Q}} and W K {\\displaystyle W^{K}} are different matrices allows attention to be non-symmetric: if token i {\\displaystyle i} attends to token j {\\displaystyle j} (i.e.",
        "q i ⋅ k j {\\displaystyle q_{i}\\cdot k_{j}} is large), this does not necessarily mean that token j {\\displaystyle j} will attend to token i {\\displaystyle i} (i.e.",
        "q j ⋅ k i {\\displaystyle q_{j}\\cdot k_{i}} could be small).",
        "The output of the attention unit for token i {\\displaystyle i} is the weighted sum of the value vectors of all tokens, weighted by a i j {\\displaystyle a_{ij}} , the attention from token i {\\displaystyle i} to each token.",
        "The attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations.",
        "The matrices Q {\\displaystyle Q} , K {\\displaystyle K} and V {\\displaystyle V} are defined as the matrices where the i {\\displaystyle i} th rows are vectors q i {\\displaystyle q_{i}} , k i {\\displaystyle k_{i}} , and v i {\\displaystyle v_{i}} respectively.",
        "The number of dimensions in a query vector is query size d query {\\displaystyle d_{ ext{query}}} and similarly for the key size d key {\\displaystyle d_{ ext{key}}} and value size d value {\\displaystyle d_{ ext{value}}} .",
        "The output dimension of an attention head is its head dimension d head {\\displaystyle d_{ ext{head}}} .",
        "The attention mechanism requires the following three equalities to hold: ℓ seq, key = ℓ seq, value , d query = d key , d value = d head {\\displaystyle \\ell _{ ext{seq, key}}=\\ell _{ ext{seq, value}},\\;d_{ ext{query}}=d_{ ext{key}},\\;d_{ ext{value}}=d_{ ext{head}}} but is otherwise unconstrained.",
        "If the attention head is used in a self-attention fashion, then X query = X key = X value {\\displaystyle X_{ ext{query}}=X_{ ext{key}}=X_{ ext{value}}} .",
        "If the attention head is used in a cross-attention fashion, then usually X query ≠ X key = X value {\\displaystyle X_{ ext{query}} eq X_{ ext{key}}=X_{ ext{value}}} .",
        "It is theoretically possible for all three to be different, but that is rarely the case in practice.",
        "One set of ( W Q , W K , W V ) {\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)} matrices is called an attention head, and each layer in a transformer model has multiple attention heads.",
        "While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\".",
        "Specifically, the query and key projection matrices, W Q {\\displaystyle W^{Q}} and W K {\\displaystyle W^{K}} , which are involved in the attention score computation, defines the \"relevance\".",
        "Meanwhile, the value projection matrix W V {\\displaystyle W^{V}} , in combination with the part of the output projection matrix W O {\\displaystyle W^{O}} , determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits.",
        "In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers.",
        "This allows the model to capture more complex and long-range dependencies in deeper layers.",
        "Many transformer attention heads encode relevance relations that are meaningful to humans.",
        "For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects.",
        "The computations for each attention head can be performed in parallel, which allows for fast processing.",
        "The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.",
        "It is theoretically possible for each attention head to have a different head dimension d head {\\displaystyle d_{ ext{head}}} , but that is rarely the case in practice.",
        "As an example, in the smallest GPT-2 model, there are only self-attention mechanisms.",
        "It has the following dimensions: d emb = 768 , n head = 12 , d head = 64 {\\displaystyle d_{ ext{emb}}=768,n_{ ext{head}}=12,d_{ ext{head}}=64} Since 12 × 64 = 768 {\\displaystyle 12 imes 64=768} , its output projection matrix W O ∈ R ( 12 × 64 ) × 768 {\\displaystyle W^{O}\\in \\mathbb {R} ^{(12 imes 64) imes 768}} is a square matrix.",
        "The Transformer architecture is constructed to calculate output tokens iteratively.",
        "Assuming t = 0 {\\displaystyle t=0} refers to the calculation of the first output token i = 0 {\\displaystyle i=0} , for step t > 0 {\\displaystyle t>0} , the output token i = 0 {\\displaystyle i=0} shall remain constant.",
        "This ensures properties of the model similar to autoregressive models.",
        "Therefore, at every time step t {\\displaystyle t} , the calculation for all outputs i {\\displaystyle i} should not have access to tokens at position j {\\displaystyle j} for j >= i {\\displaystyle j>=i} (as it naturally is the case for time step t = i {\\displaystyle t=i} , when tokens j > t {\\displaystyle j>t} are not yet calculated).",
        "A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero.",
        "As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form P M causal P − 1 {\\displaystyle PM_{ ext{causal}}P^{-1}} , where P {\\displaystyle P} is a random permutation matrix.",
        "An encoder consists of an embedding layer, followed by multiple encoder layers.",
        "Each encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer.",
        "It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually.",
        "The encoder layers are stacked.",
        "The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors.",
        "This sequence of vectors is processed by the second encoder, and so on.",
        "The output from the final encoder layer is then used by the decoder.",
        "As the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.",
        "A decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.",
        "Each decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network.",
        "The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders.",
        "This mechanism can also be called the encoder-decoder attention.",
        "Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings.",
        "The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow.",
        "This allows for autoregressive text generation.",
        "For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated.",
        "Thus, the self-attention module in the decoder is causally masked.",
        "In contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding.",
        "Consequently, there is no need for masking in the cross-attention mechanism.",
        "The last decoder is followed by a final un-embedding layer.",
        "to produce the output probabilities over the vocabulary.",
        "Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.",
        "Many large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture.",
        "Early GPT models are decoder-only models trained to predict the next token in a sequence.",
        "BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence.",
        "Each encoder layer contains 2 sublayers: the self-attention and the feedforward network.",
        "Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.",
        "The final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence.",
        "The residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y = F(x) + x.",
        "Adding the input x can preserve the input information and avoid issues when the gradient of F(x) is close to zero.",
        "Similarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector.",
        "There are two common conventions in use: the post-LN and the pre-LN convention.",
        "It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases.",
        "The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence.",
        "Several common variations are described here.",
        "An \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text.",
        "This is usually used for text embedding and representation learning for downstream applications.",
        "BERT is encoder-only.",
        "They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.",
        "A \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to.",
        "Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network.",
        "This is usually used for text generation and instruction following.",
        "The models in the GPT series and Chinchilla series are decoder-only.",
        "An \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc.",
        "They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc.",
        "This is also usually used for text generation and instruction following.",
        "The models in the T5 series are encoder-decoder.",
        "A \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking.",
        "Specifically, it has mask of the form M prefixLM = [ 0 − ∞ 0 M causal ] {\\displaystyle M_{ ext{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{ ext{causal}}\\end{bmatrix}}} where the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix.",
        "They resemble encoder-decoder models, but has less \"sparsity\".",
        "Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.",
        "There are also mixed seq2seq models.",
        "For example, in 2020, Google Translate replaced the previous RNN-encoder-RNN-decoder model by a Transformer-encoder-RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.",
        "The original transformer uses ReLU activation function.",
        "Other activation functions were developed.",
        "The Llama series and PaLM used SwiGLU; both GPT-1 and BERT used GELU.",
        "Alternative activation functions are often used in combination with Gated Linear Units in the feedforward module.",
        "The normalization used in the Transformer can be different from LayerNorm.",
        "One example is RMSNorm which is used in the Llama series.",
        "Other examples include CapsuleNorm ScaleNorm, or FixNorm.",
        "Transformers may use other positional encoding methods than sinusoidal.",
        "The original Transformer paper reported using a learned positional encoding, but finding it not superior to the sinusoidal one.",
        "Later, found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.",
        "Now pick some angle θ {\\displaystyle heta } .",
        "Then the RoPE encoding is applied to each pair of coordinates.",
        "ALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer.",
        "Instead, it is an additional positional encoder that is directly plugged into the attention mechanism.",
        "The idea being that the linear bias matrix is a softened mask.",
        "Just as 0 {\\displaystyle 0} represent full attention paid, and − ∞ {\\displaystyle -\\infty } represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.",
        "ALiBi allows pretraining on short context windows, then fine-tuning on longer context windows.",
        "Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).",
        "This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\".",
        "The transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch.",
        "Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.",
        "When an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same.",
        "The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token.",
        "PagedAttention applies memory paging to KV caching.",
        "If a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk.",
        "The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.",
        "FlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU.",
        "It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow).",
        "See the page on softmax for details.",
        "An improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths.",
        "It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.",
        "Key advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).",
        "Benchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch.",
        "Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.",
        "Multi-Query Attention changes the multiheaded attention mechanism.",
        "More generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair.",
        "MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups.",
        "Multihead Latent Attention (MLA) is a low-rank approximation to standard MHA.",
        "Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces (\"latent space\"), one for query and one for key-value (KV vector).",
        "This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached.",
        "Speculative decoding is a method to accelerate token decoding.",
        "Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified.",
        "If the quickly computed tokens are incorrect, they are discarded and computed slowly.",
        "The key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode, in the following sense.",
        "Suppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512.",
        "To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token x 1 , x 2 , .",
        ", x 512 {\\displaystyle x_{1},x_{2},...,x_{512}} , taking time 512 T GPT-3 {\\displaystyle 512T_{ ext{GPT-3}}} .",
        "However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each x t {\\displaystyle x_{t}} is indeed the token with the largest log-likelihood in the t {\\displaystyle t} -th output.",
        "In speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model.",
        "For example, suppose we use GPT-3-small to generate four speculative tokens: x ~ 1 , x ~ 2 , x ~ 3 , x ~ 4 {\\displaystyle { ilde {x}}_{1},{ ilde {x}}_{2},{ ilde {x}}_{3},{ ilde {x}}_{4}} .",
        "This only takes 4 T GPT-3-small {\\displaystyle 4T_{ ext{GPT-3-small}}} .",
        "These tokens are then run through the larger GPT-3 in one go.",
        "Suppose that x ~ 1 {\\displaystyle { ilde {x}}_{1}} and x ~ 2 {\\displaystyle { ilde {x}}_{2}} are verified by GPT-3 as what it would have picked, then those are kept, but x ~ 3 {\\displaystyle { ilde {x}}_{3}} is not, so x ~ 3 , x ~ 4 {\\displaystyle { ilde {x}}_{3},{ ilde {x}}_{4}} are discarded, and GPT-3 is run on those.",
        "This would take 4 T GPT-3-small + 3 T GPT-3 {\\displaystyle 4T_{ ext{GPT-3-small}}+3T_{ ext{GPT-3}}} , which might be shorter than 4 T GPT-3 {\\displaystyle 4T_{ ext{GPT-3}}} .",
        "For non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.",
        "In Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability.",
        "However, that vector can then be further processed by another Transformer block to predict the next token, and so on for arbitrarily many steps into the future.",
        "This trades off accuracy for speed, since each new token costs just one more Transformer block, rather than the entire stack.",
        "Training transformer-based architectures can be expensive, especially for long inputs.",
        "Many methods have been developed to attempt to address the issue.",
        "In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows.",
        "In the audio domain, SepTr decouples the attention in time and frequency domains.",
        "Long Range Arena (2020) is a standard benchmark for comparing the behavior of transformer architectures over long inputs.",
        "The standard attention graph is either all-to-all or causal, both of which scales as O ( N 2 ) {\\displaystyle O(N^{2})} where N {\\displaystyle N} is the number of tokens in a sequence.",
        "Sparse attention uses attention graphs that grows slower than O ( N 2 ) {\\displaystyle O(N^{2})} .",
        "Ordinary transformers require a memory size that is quadratic in the size of the context window.",
        "Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.",
        ", w D {\\displaystyle w_{1},...,w_{D}} are independent samples from the normal distribution N ( 0 , σ 2 I ) {\\displaystyle N(0,\\sigma ^{2}I)} .",
        "Similarly for multiple queries, and for multiheaded attention.",
        "This approximation can be computed in linear time, as we can compute the matrix φ ( k i ) v i T {\\displaystyle \\varphi (k_{i})v_{i}^{T}} first, then multiply it with the query.",
        ", w D {\\displaystyle w_{1},...,w_{D}} are first independently sampled from the normal distribution N ( 0 , σ 2 I ) {\\displaystyle N(0,\\sigma ^{2}I)} , then they are Gram-Schmidt processed.",
        "Transformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality.",
        "Multimodal models can either be trained from scratch, or by finetuning.",
        "A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning.",
        "The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer.",
        "Only the linear layer is finetuned.",
        "Vision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.",
        "Conformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e.",
        "broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.",
        "Perceivers are a variant of Transformers designed for multimodality.",
        "Unlike later models, DALL-E is not a diffusion model.",
        "Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image.",
        "Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image.",
        "Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens.",
        "During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted.",
        "Phenaki is a text-to-video model.",
        "It is a bidirectional masked transformer conditioned on pre-computed text tokens.",
        "The generated tokens are then decoded to a video.",
        "The transformer has had great success in natural language processing (NLP).",
        "Many large language models such as GPT-2, GPT-3, GPT-4, Gemini, AlbertAGPT, Claude, BERT, Grok, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including: machine translation time series prediction document summarization document generation named entity recognition (NER) writing computer code based on requirements expressed in natural language.",
        "speech-to-text Beyond traditional NLP, the transformer architecture has had success in other applications, such as: biological sequence analysis video understanding protein folding (such as AlphaFold) evaluating chess board positions.",
        "Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level."
      ],
      "metadata": {
        "title": "Transformer (deep learning architecture)",
        "url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)",
        "word_count": 9220,
        "char_count": 57548,
        "sentence_count": 316,
        "scraped_at": "2025-08-09T14:46:41.391535",
        "language": "en",
        "processing_time": 0.026695966720581055,
        "source_hash": "8c36acffd3397af17fb0957c3bd9ab4d"
      }
    },
    {
      "title": "Artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
      "raw_text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human.\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n\n\n== Goals ==\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\n\n\n=== Reasoning and problem-solving ===\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\n\n\n=== Knowledge representation ===\n\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.\n\n\n=== Planning and decision-making ===\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.\nIn classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\nA Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.\n\n\n=== Learning ===\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.\n\nThere are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.\nComputational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\n\n\n=== Natural language processing ===\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.\nEarly work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\nModern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.\n\n\n=== Perception ===\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\nThe field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception.\n\n\n=== Social intelligence ===\n\nAffective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\nHowever, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.\n\n\n=== General intelligence ===\nA machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.\n\n\n== Techniques ==\nAI research uses a wide variety of techniques to accomplish the goals above.\n\n\n=== Search and optimization ===\nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.\n\n\n==== State space search ====\nState space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.\n\n\n==== Local search ====\n Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.\nGradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm.\nAnother type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\n\n\n=== Logic ===\nFormal logic is used for reasoning and knowledge representation.\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.\nFuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains.\n\n\n=== Probabilistic methods for uncertain reasoning ===\n\nMany problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\nBayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\n\n\n=== Classifiers and statistical learning methods ===\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\nThere are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.\nNeural networks are also used as classifiers.\n\n\n=== Artificial neural networks ===\n\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.\nLearning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.\nIn feedforward neural networks the signal passes in only one direction. The term perceptron typically refers to a single-layer neural network. In contrast, deep learning uses many layers. Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem. Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.\n\n\n=== Deep learning ===\n\nDeep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.\n\n\n=== GPT ===\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.\nCurrent models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.\n\n\n=== Hardware and software ===\n\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.\nThe transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang.\n\n\n== Applications ==\nAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO).\n\n\n=== Health and medicine ===\n\nThe application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.\nFor medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.\n\n\n=== Games ===\n\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.\n\n\n=== Mathematics ===\nLarge language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.\nAlternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve all from Google DeepMind, Llemma from EleutherAI or Julius.\nWhen natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.   \nSome models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.\nTopological deep learning integrates various topological approaches.\n\n\n=== Finance ===\nFinance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.\nAccording to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"\n\n\n=== Military ===\n\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.\nAI has been used in military operations in Iraq, Syria, Israel and Ukraine.\n\n\n=== Generative AI ===\n\n\n=== Agents ===\n\nAI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.\n\n\n=== Sexuality ===\nApplications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika).  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.\nAI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.\n\n\n=== Other industry-specific tasks ===\nThere are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\nAI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.\nIn agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\nDuring the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.\n\n\n== Ethics ==\n\nAI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.\n\n\n=== Risks and harm ===\n\n\n==== Privacy and copyright ====\n\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\nAI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\nSensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.\nAI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.\n\n\n==== Dominance by tech giants ====\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.\n\n\n==== Power needs and environmental impacts ====\n\nIn January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.\nProdigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.\nIn 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million. Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.\nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.\nAfter the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.\nAlthough most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.\nOn 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.\nIn 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.\n\n\n==== Misinformation ====\n\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.\nIn the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.\nAI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.\n\n\n==== Algorithmic bias and fairness ====\n\nMachine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases.\nOn June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\n\n\n==== Lack of transparency ====\n\nMany AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.\nPeople who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.\nDARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.\n\n\n==== Bad actors and weaponized AI ====\n\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.\nThere are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.\n\n\n==== Technological unemployment ====\n\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.\n\n\n==== Existential risk ====\n\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.\nFirst, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".\nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.\nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\nIn May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.\nIn 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".\nSome other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.\n\n\n=== Ethical machines and alignment ===\n\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\nThe field of machine ethics is also called computational morality,\nand was founded at an AAAI symposium in 2005.\nOther approaches include Wendell Wallach's \"artificial moral agents\" and Stuart J. Russell's three principles for developing provably beneficial machines.\n\n\n=== Open source ===\n\nActive organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.\n\n\n=== Frameworks ===\nArtificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:\n\nRespect the dignity of individual people\nConnect with other people sincerely, openly, and inclusively\nCare for the wellbeing of everyone\nProtect social values, justice, and the public interest\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.\nThe UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.\n\n\n=== Regulation ===\n\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.\n\n\n== History ==\n\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.\nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\nFor many specific tasks, other methods were abandoned.\nDeep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.\n\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\nIn the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.\n\n\n== Philosophy ==\n\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI.\n\n\n=== Defining artificial intelligence ===\n\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"\n\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".\nMcCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\nAnother definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\nSome authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".\nThere has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.\n\n\n=== Evaluating approaches to AI ===\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n\n\n==== Symbolic AI and its limits ====\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n\n\n==== Neat vs. scruffy ====\n\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.\n\n\n==== Soft vs. hard computing ====\n\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n\n\n==== Narrow vs. general AI ====\n\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n\n\n=== Machine consciousness, sentience, and mind ===\n\nThere is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n\n\n==== Consciousness ====\n\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\n\n\n==== Computationalism and functionalism ====\n\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.\n\n\n==== AI welfare and rights ====\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.\n\n\n== Future ==\n\n\n=== Superintelligence and the singularity ===\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.\n\n\n=== Transhumanism ===\n\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.\nEdward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.\n\n\n== In fiction ==\n\nThought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\nIsaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\n\n== See also ==\nArtificial consciousness – Field in cognitive science\nArtificial intelligence and elections – Use and impact of AI on political elections\nArtificial intelligence content detection – Software to detect AI-generated content\nAssociation for the Advancement of Artificial Intelligence (AAAI)\nBehavior selection algorithm – Algorithm that selects actions for intelligent agents\nBusiness process automation – Automation of business processes\nCase-based reasoning – Process of solving new problems based on the solutions of similar past problems\nComputational intelligence – Ability of a computer to learn a specific task from data or experimental observation\nDigital immortality – Hypothetical concept of storing a personality in digital form\nEmergent algorithm – Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets\nGlossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence\nIntelligence amplification – Use of information technology to augment human intelligence\nIntelligent agent – Software agent which acts autonomously\nIntelligent automation – Software process that combines robotic process automation and artificial intelligence\nList of artificial intelligence journals\nList of artificial intelligence projects\nMind uploading – Hypothetical process of digitally emulating a brain\nOrganoid intelligence – Use of brain cells and brain organoids for intelligent computing\nRobotic process automation – Form of business process automation technology\nThe Last Day – 1967 Welsh science fiction novel\nWetware computer – Computer composed of organic material\nDARWIN EU - A European Union initiative coordinated by the European Medicines Agency (EMA) to generate and utilize real-world evidence (RWE) to support the evaluation and supervision of medicines across the EU.\n\n\n== Explanatory notes ==\n\n\n== References ==\n\n\n=== AI textbooks ===\nThe two most widely used textbooks in 2023 (see the Open Syllabus):\n\nRussell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474.\nRich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5.\nThe four most widely used AI textbooks in 2008:\n\nOther textbooks:\n\nErtel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7.\nCiaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3.\n\n\n=== History of AI ===\n\n\n=== Other sources ===\n\n\n== Further reading ==\n\n\n== External links ==\n\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.",
      "cleaned_text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)-AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications. An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences-there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned. Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. Natural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering. Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications. Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input. The field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception. Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human-computer interaction. However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject. A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence. AI research uses a wide variety of techniques to accomplish the goals above. AI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search. State space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal. Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position. Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally. Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm. Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation. Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails). Formal logic is used for reasoning and knowledge representation. Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\"). Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules. Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved. Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages. Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true. Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains. Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design. Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation-maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters). The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience. There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s. The naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability. Neural networks are also used as classifiers. An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers. Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function. In feedforward neural networks the signal passes in only one direction. The term perceptron typically refers to a single-layer neural network. In contrast, deep learning uses many layers. Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem. Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects. Deep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces. Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012-2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant. The transistor density in integrated circuits has been observed to roughly double every 18 months-a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang. AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO). The application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients. For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold. Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions. Large language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems. Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve all from Google DeepMind, Llemma from EleutherAI or Julius. When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025. Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics. Topological deep learning integrates various topological approaches. Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years. According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\" Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous. AI has been used in military operations in Iraq, Syria, Israel and Ukraine. AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks. Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika). AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns. AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors. There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management. AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions. In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water. Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation. During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages. AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning. Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright. AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency. Sensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy. AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\" Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors. The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace. In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation. Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources - from nuclear energy to geothermal to fusion. The tech firms argue that - in the long view - AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms. A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all. In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million. Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers. In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power - enough for 800,000 homes - of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation. After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban. Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI. On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors. In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300-500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it. YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem. In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks. AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models. Machine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases. On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon. COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different-the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data. A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\" Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive. Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women. There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws. At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed. Many AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist. It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading. People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used. DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems. Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts. Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states. A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots. AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier-AI facial recognition systems are already being used for mass surveillance in China. There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours. Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment. In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement. It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways. First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\". Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI-and that regulators who do will only benefit vested interests.\" Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research. Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk. Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas. The field of machine ethics is also called computational morality, and was founded at an AAAI symposium in 2005. Other approaches include Wendell Wallach's \"artificial moral agents\" and Stuart J. Russell's three principles for developing provably beneficial machines. Active organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows: Respect the dignity of individual people Connect with other people sincerely, openly, and inclusively Care for the wellbeing of everyone Protect social values, justice, and the public interest Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks. Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers. The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities. The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories. In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\". In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI. The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible. The field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s. Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed. In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began. Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks. AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect). However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s. Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field. For many specific tasks, other methods were abandoned. Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015-2019. In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI. Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\" Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine-and no other philosophical discussion is required, or may not even be possible. Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\". There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text. No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers. Symbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\" However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him. The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches. \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both. Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively. There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction. David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like. Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam. Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind. It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society. In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own. Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited. A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\". However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do. Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger. Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence. Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction. A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture. Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity. Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.",
      "sentences": [
        "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making.",
        "It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.",
        "However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"",
        "Various subfields of AI research are centered around particular goals and the use of particular tools.",
        "The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.",
        "To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.",
        "AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.",
        "Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)-AI that can complete virtually any cognitive task at least as well as a human.",
        "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters.",
        "Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques.",
        "This growth accelerated further after 2017 with the transformer architecture.",
        "In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom.",
        "Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.",
        "The general problem of simulating (or creating) intelligence has been broken into subproblems.",
        "These consist of particular traits or capabilities that researchers expect an intelligent system to display.",
        "The traits described below have received the most attention and cover the scope of AI research.",
        "Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.",
        "By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.",
        "Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow.",
        "Even humans rarely use the step-by-step deduction that early AI research could model.",
        "They solve most of their problems using fast, intuitive judgments.",
        "Accurate and efficient reasoning is an unsolved problem.",
        "Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts.",
        "Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.",
        "A knowledge base is a body of knowledge represented in a form that can be used by a program.",
        "An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.",
        "Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.",
        "Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).",
        "There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.",
        "An \"agent\" is anything that perceives and takes actions in the world.",
        "A rational agent has goals or preferences and takes actions to make them happen.",
        "In automated planning, the agent has a specific goal.",
        "In automated decision-making, the agent has preferences-there are some situations it would prefer to be in, and some situations it is trying to avoid.",
        "The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it.",
        "For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur.",
        "It can then choose the action with the maximum expected utility.",
        "In classical planning, the agent knows exactly what the effect of any action will be.",
        "In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\").",
        "It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.",
        "In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved.",
        "These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.",
        "Information value theory can be used to weigh the value of exploratory or experimental actions.",
        "The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.",
        "A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action.",
        "A policy associates a decision with each possible state.",
        "The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.",
        "Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.",
        "Machine learning is the study of programs that can improve their performance on a given task automatically.",
        "It has been a part of AI from the beginning.",
        "There are several kinds of machine learning.",
        "Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.",
        "Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).",
        "In reinforcement learning, the agent is rewarded for good responses and punished for bad ones.",
        "The agent learns to choose responses that are classified as \"good\".",
        "Transfer learning is when the knowledge gained from one problem is applied to a new problem.",
        "Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.",
        "Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.",
        "Natural language processing (NLP) allows programs to read, write and communicate in human languages.",
        "Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.",
        "Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem).",
        "Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.",
        "Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others.",
        "In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.",
        "Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world.",
        "Computer vision is the ability to analyze visual input.",
        "The field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception.",
        "Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.",
        "For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human-computer interaction.",
        "However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents.",
        "Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.",
        "A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.",
        "AI research uses a wide variety of techniques to accomplish the goals above.",
        "AI can solve many problems by intelligently searching through many possible solutions.",
        "There are two very different kinds of search used in AI: state space search and local search.",
        "State space search searches through a tree of possible states to try to find a goal state.",
        "For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.",
        "Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers.",
        "The result is a search that is too slow or never completes.",
        "\"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.",
        "Adversarial search is used for game-playing programs, such as chess or Go.",
        "It searches through a tree of possible moves and countermoves, looking for a winning position.",
        "Local search uses mathematical optimization to find a solution to a problem.",
        "It begins with some form of guess and refines it incrementally.",
        "Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function.",
        "Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm.",
        "Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.",
        "Distributed search processes can coordinate via swarm intelligence algorithms.",
        "Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).",
        "Formal logic is used for reasoning and knowledge representation.",
        "Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").",
        "Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).",
        "Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.",
        "Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms.",
        "In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.",
        "In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.",
        "Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable.",
        "However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete.",
        "Moreover, its efficiency is competitive with computation in other symbolic programming languages.",
        "Fuzzy logic assigns a \"degree of truth\" between 0 and 1.",
        "It can therefore handle propositions that are vague and partially true.",
        "Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.",
        "Other specialized versions of logic have been developed to describe many complex domains.",
        "Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information.",
        "AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.",
        "Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory.",
        "These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.",
        "Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).",
        "The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand.",
        "Classifiers are functions that use pattern matching to determine the closest match.",
        "They can be fine-tuned based on chosen examples using supervised learning.",
        "Each pattern (also called an \"observation\") is labeled with a certain predefined class.",
        "All the observations combined with their class labels are known as a data set.",
        "When a new observation is received, that observation is classified based on previous experience.",
        "There are many kinds of classifiers in use.",
        "The decision tree is the simplest and most widely used symbolic machine learning algorithm.",
        "K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.",
        "The naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.",
        "Neural networks are also used as classifiers.",
        "An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain.",
        "It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data.",
        "There is an input, at least one hidden layer of nodes and an output.",
        "Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer.",
        "A network is typically called a deep neural network if it has at least 2 hidden layers.",
        "Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training.",
        "The most common training technique is the backpropagation algorithm.",
        "Neural networks learn to model complex relationships between inputs and outputs and find patterns in data.",
        "In theory, a neural network can learn any function.",
        "In feedforward neural networks the signal passes in only one direction.",
        "The term perceptron typically refers to a single-layer neural network.",
        "In contrast, deep learning uses many layers.",
        "Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events.",
        "Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem.",
        "Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns.",
        "This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.",
        "Deep learning uses several layers of neurons between the network's inputs and outputs.",
        "The multiple layers can progressively extract higher-level features from the raw input.",
        "For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.",
        "Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others.",
        "The reason that deep learning performs so well in so many applications is not known as of 2021.",
        "The sudden success of deep learning in 2012-2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.",
        "Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences.",
        "Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet.",
        "The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation).",
        "Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token.",
        "Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF).",
        "Current GPT models are prone to generating falsehoods called \"hallucinations\".",
        "These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems.",
        "Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.",
        "Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI.",
        "Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.",
        "Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.",
        "The transistor density in integrated circuits has been observed to roughly double every 18 months-a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it.",
        "Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang.",
        "The deployment of AI may be overseen by a chief automation officer (CAO).",
        "The application of AI in medicine and medical research has the potential to increase patient care and quality of life.",
        "Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.",
        "For medical research, AI is an important tool for processing and integrating big data.",
        "This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.",
        "It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.",
        "New AI tools can deepen the understanding of biomedically relevant pathways.",
        "For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.",
        "In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.",
        "In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments.",
        "Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease).",
        "They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.",
        "Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.",
        "Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.",
        "In 2011, in a Jeopardy!",
        "quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy!",
        "champions, Brad Rutter and Ken Jennings, by a significant margin.",
        "In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.",
        "Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.",
        "Other programs handle imperfect-information games, such as the poker-playing program Pluribus.",
        "DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.",
        "In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.",
        "In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.",
        "In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.",
        "Large language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics.",
        "These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations.",
        "They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.",
        "A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.",
        "One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result.",
        "The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems.",
        "In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.",
        "Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve all from Google DeepMind, Llemma from EleutherAI or Julius.",
        "When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks.",
        "The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.",
        "Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.",
        "Topological deep learning integrates various topological approaches.",
        "Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.",
        "According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services.",
        "He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"",
        "Various countries are deploying AI military applications.",
        "The main applications enhance command and control, communications, sensors, integration and interoperability.",
        "Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.",
        "AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.",
        "AI has been used in military operations in Iraq, Syria, Israel and Ukraine.",
        "AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals.",
        "These agents can interact with users, their environment, or other agents.",
        "AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics.",
        "AI agents operate within the constraints of their programming, available computational resources, and hardware limitations.",
        "This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities.",
        "In real-world applications, AI agents often face time constraints for decision-making and action execution.",
        "Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training.",
        "Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.",
        "Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika).",
        "AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.",
        "AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.",
        "There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions.",
        "In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.",
        "A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.",
        "AI applications for evacuation and disaster management are growing.",
        "AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media.",
        "Furthermore, AI can provide real-time information on the evacuation conditions.",
        "In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments.",
        "Agronomists use AI to conduct research and development.",
        "AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.",
        "Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\"",
        "For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy.",
        "Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.",
        "During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.",
        "AI has potential benefits and potential risks.",
        "AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\".",
        "However, as the use of AI has become widespread, several unintended consequences and risks have been identified.",
        "In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.",
        "Machine learning algorithms require large amounts of data.",
        "The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.",
        "AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties.",
        "The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.",
        "Sensitive user data collected may include online activity records, geolocation data, video, or audio.",
        "For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.",
        "Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.",
        "AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.",
        "Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness.",
        "Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"",
        "Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\".",
        "Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".",
        "Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file.",
        "In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.",
        "Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.",
        "The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.",
        "Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.",
        "In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.",
        "This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency.",
        "The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.",
        "Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities.",
        "There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power.",
        "Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source.",
        "A ChatGPT search involves the use of 10 times the electrical energy as a Google search.",
        "The large firms are in haste to find power sources - from nuclear energy to geothermal to fusion.",
        "The tech firms argue that - in the long view - AI will be eventually kinder to the environment, but they need the energy now.",
        "AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.",
        "A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.",
        "Data centers' need for more and more electrical power is such that they might max out the electrical grid.",
        "The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.",
        "In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers.",
        "In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million.",
        "Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.",
        "In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years.",
        "Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission.",
        "If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power - enough for 800,000 homes - of energy will be produced.",
        "The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.",
        "The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan.",
        "Closed since 2022, the plant is planned to be reopened in October 2025.",
        "The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.",
        "After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.",
        "Taiwan aims to phase out nuclear power by 2025.",
        "On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.",
        "Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI.",
        "Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.",
        "On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.",
        "According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.",
        "In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons.",
        "By 2035, these emissions could rise to 300-500 million tonnes depending on what measures will be taken.",
        "This is below 1.5% of the energy sector emissions.",
        "The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.",
        "YouTube, Facebook and others use recommender systems to guide users to more content.",
        "These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching).",
        "The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it.",
        "Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.",
        "This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.",
        "The AI program had correctly learned to maximize its goal, but the result was harmful to society.",
        "After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.",
        "In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s.",
        "It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda.",
        "AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.",
        "AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.",
        "Machine learning applications will be biased if they learn from biased data.",
        "The developers may not be aware that the bias exists.",
        "Bias can be introduced by the way training data is selected and by the way a model is deployed.",
        "If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.",
        "The field of fairness studies how to prevent harms from algorithmic biases.",
        "On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black.",
        "The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\".",
        "Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\".",
        "Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.",
        "COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist.",
        "In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants.",
        "Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different-the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.",
        "In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.",
        "A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\").",
        "The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".",
        "Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"",
        "Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past.",
        "If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future.",
        "If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist.",
        "Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past.",
        "It is descriptive rather than prescriptive.",
        "Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.",
        "There are various conflicting definitions and mathematical models of fairness.",
        "These notions depend on ethical assumptions, and are influenced by beliefs about society.",
        "One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities.",
        "Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible.",
        "Procedural fairness focuses on the decision process rather than the outcome.",
        "The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders.",
        "The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them.",
        "Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.",
        "At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.",
        "Many AI systems are so complex that their designers cannot explain how they reach their decisions.",
        "Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs.",
        "But some popular explainability techniques exist.",
        "It is impossible to be certain that a program is operating correctly if no one knows how exactly it works.",
        "There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended.",
        "For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale.",
        "Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia.",
        "Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data.",
        "The correlation between asthma and low risk of dying from pneumonia was real, but misleading.",
        "People who have been harmed by an algorithm's decision have a right to an explanation.",
        "Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make.",
        "Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.",
        "Industry experts noted that this is an unsolved problem with no solution in sight.",
        "Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.",
        "DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.",
        "Several approaches aim to address the transparency problem.",
        "SHAP enables to visualise the contribution of each feature to the output.",
        "LIME can locally approximate a model's outputs with a simpler, interpretable model.",
        "Multitask learning provides a large number of outputs in addition to the target classification.",
        "These other outputs can help developers deduce what the network has learned.",
        "Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.",
        "For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.",
        "Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.",
        "A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.",
        "Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.",
        "Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person.",
        "In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.",
        "By 2015, over fifty countries were reported to be researching battlefield robots.",
        "AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways.",
        "Face and voice recognition allow widespread surveillance.",
        "Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding.",
        "Recommendation systems can precisely target propaganda and misinformation for maximum effect.",
        "Deepfakes and generative AI aid in producing misinformation.",
        "Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets.",
        "It lowers the cost and difficulty of digital warfare and advanced spyware.",
        "All these technologies have been available since 2020 or earlier-AI facial recognition systems are already being used for mass surveillance in China.",
        "There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen.",
        "For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.",
        "Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.",
        "In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.",
        "A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.",
        "Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\".",
        "The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.",
        "In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.",
        "Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".",
        "Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.",
        "From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.",
        "It has been argued AI will become so powerful that humanity may irreversibly lose control of it.",
        "This could, as physicist Stephen Hawking stated, \"spell the end of the human race\".",
        "This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character.",
        "These sci-fi scenarios are misleading in several ways.",
        "First, AI does not require human-like sentience to be an existential risk.",
        "Modern AI programs are given specific goals and use learning and intelligence to achieve them.",
        "Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer).",
        "Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\"",
        "In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".",
        "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk.",
        "The essential parts of civilization are not physical.",
        "Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe.",
        "The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.",
        "The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.",
        "Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.",
        "In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\".",
        "He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.",
        "In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".",
        "Some other researchers were more optimistic.",
        "AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\"",
        "While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"",
        "Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI-and that regulators who do will only benefit vested interests.\"",
        "Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"",
        "In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.",
        "However, after 2016, the study of current and future risks and possible solutions became a serious area of research.",
        "Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans.",
        "Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.",
        "Machines with intelligence have the potential to use their intelligence to make ethical decisions.",
        "The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.",
        "The field of machine ethics is also called computational morality, and was founded at an AAAI symposium in 2005.",
        "Other approaches include Wendell Wallach's \"artificial moral agents\" and Stuart J. Russell's three principles for developing provably beneficial machines.",
        "Active organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta.",
        "Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available.",
        "Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.",
        "Open-weight models are useful for research and innovation but can also be misused.",
        "Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective.",
        "Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed.",
        "They recommend pre-release audits and cost-benefit analyses.",
        "Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system.",
        "Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.",
        "The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages.",
        "It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.",
        "The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.",
        "The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.",
        "According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.",
        "Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.",
        "Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam.",
        "Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.",
        "The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.",
        "Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.",
        "In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.",
        "In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics.",
        "In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\".",
        "It was adopted by the European Union, the United States, the United Kingdom, and other signatories.",
        "In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".",
        "A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.",
        "In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".",
        "In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.",
        "28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.",
        "In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.",
        "The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity.",
        "The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning.",
        "This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\".",
        "They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.",
        "The field of AI research was founded at a workshop at Dartmouth College in 1956.",
        "The attendees became the leaders of AI research in the 1960s.",
        "They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.",
        "Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.",
        "Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.",
        "In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".",
        "In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".",
        "They had, however, underestimated the difficulty of the problem.",
        "In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects.",
        "Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.",
        "The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.",
        "In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts.",
        "By 1985, the market for AI had reached over a billion dollars.",
        "At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.",
        "However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.",
        "Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts.",
        "In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches.",
        "Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive.",
        "Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.",
        "But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others.",
        "In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.",
        "AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems.",
        "This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).",
        "By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).",
        "However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines.",
        "Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.",
        "Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.",
        "For many specific tasks, other methods were abandoned.",
        "Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet).",
        "Deep learning's success led to an enormous increase in interest and funding in AI.",
        "The amount of machine learning research (measured by total publications) increased by 50% in the years 2015-2019.",
        "In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues.",
        "The alignment problem became a serious field of academic study.",
        "In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest.",
        "In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player.",
        "The program taught only the game's rules and developed a strategy by itself.",
        "GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.",
        "ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.",
        "It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.",
        "These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research.",
        "According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".",
        "About 800,000 \"AI\"-related U.S. job openings existed in 2022.",
        "According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.",
        "Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.",
        "Another major focus has been whether machines can be conscious, and the associated ethical implications.",
        "Many other topics in philosophy are relevant to AI, such as epistemology and free will.",
        "Rapid advancements have intensified public discussions on the philosophy and ethics of AI.",
        "Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"",
        "He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".",
        "He devised the Turing test, which measures the ability of a machine to simulate human conversation.",
        "Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\".",
        "Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"",
        "Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.",
        "However, they are critical that the test requires the machine to imitate humans.",
        "\"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\"",
        "AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".",
        "McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\".",
        "Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\".",
        "The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.",
        "These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine-and no other philosophical discussion is required, or may not even be possible.",
        "Another definition has been adopted by Google, a major practitioner in the field of AI.",
        "This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.",
        "Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".",
        "There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.",
        "No established unifying theory or paradigm has guided AI research for most of its history.",
        "The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\").",
        "This approach is mostly sub-symbolic, soft and narrow.",
        "Critics argue that these questions may have to be revisited by future generations of AI researchers.",
        "Symbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics.",
        "They were highly successful at \"intelligent\" tasks such as algebra or IQ tests.",
        "In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"",
        "However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning.",
        "Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.",
        "Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.",
        "Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.",
        "The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias.",
        "Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision.",
        "The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.",
        "\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks).",
        "\"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems.",
        "Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work.",
        "This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant.",
        "Modern AI has elements of both.",
        "Finding a provably correct or optimal solution is intractable for many important problems.",
        "Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation.",
        "Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.",
        "AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.",
        "General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions.",
        "The sub-field of artificial general intelligence studies this area exclusively.",
        "There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do.",
        "This issue considers the internal experiences of the machine, rather than its external behavior.",
        "Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence.",
        "Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"",
        "However, the question has become central to the philosophy of mind.",
        "It is also typically the central question at issue in artificial intelligence in fiction.",
        "David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.",
        "The easy problem is understanding how the brain processes signals, makes plans and controls behavior.",
        "The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion).",
        "While human information processing is easy to explain, human subjective experience is difficult to explain.",
        "For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.",
        "Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing.",
        "Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem.",
        "This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.",
        "Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"",
        "Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.",
        "It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.",
        "But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.",
        "Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.",
        "Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.",
        "In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems.",
        "Similarly to the legal status of companies, it would have conferred rights but also responsibilities.",
        "Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios.",
        "They also noted that robots lacked the autonomy to take part in society on their own.",
        "Progress in AI increased interest in the topic.",
        "Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny.",
        "They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.",
        "A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.",
        "If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself.",
        "The improved software would be even better at improving itself, leading to what I. J.",
        "Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".",
        "However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.",
        "Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either.",
        "This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.",
        "Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.",
        "Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.",
        "A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters.",
        "In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.",
        "Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer.",
        "Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.",
        "Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer.",
        "This appears in Karel Čapek's R.U.R., the films A.I.",
        "Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick.",
        "Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence."
      ],
      "metadata": {
        "title": "Artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "word_count": 12699,
        "char_count": 83589,
        "sentence_count": 562,
        "scraped_at": "2025-08-09T14:46:47.045717",
        "language": "en",
        "processing_time": 0.030408859252929688,
        "source_hash": "2550c8dab055b47c5ed3f367d943ad62"
      }
    },
    {
      "title": "Artificial general intelligence",
      "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
      "raw_text": "Artificial general intelligence (AGI)—sometimes called human‑level intelligence AI—is a type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks.\nSome researchers argue that state‑of‑the‑art large language models (LLMs) already exhibit signs of AGI‑level capability, while others maintain that genuine AGI has not yet been achieved. Beyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin.\nUnlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming. The concept does not, in principle, require the system to be an autonomous agent; a static model—such as a highly capable large language model—or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved.\nCreating AGI is a primary goal of AI research and of companies such as OpenAI, Google, and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries.\nThe timeline for achieving human‑level intelligence AI remains deeply contested. Recent surveys of AI researchers give median forecasts ranging from the late 2020s to mid‑century, while still recording significant numbers who expect arrival much sooner—or never at all. There is debate on the exact definition of AGI and regarding whether modern LLMs such as GPT-4 are early forms of emerging AGI. AGI is a common topic in science fiction and futures studies.\nContention exists over whether AGI represents an existential risk. Many AI experts have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be in too remote a stage to present such a risk.\n\n\n== Terminology ==\nAGI is also known as strong AI, full AI, human-level AI, human-level intelligent AI, or general intelligent action.\nSome academic sources reserve the term \"strong AI\" for computer programs that will experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities. Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.\nRelated concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.\nA framework for classifying AGI by performance and autonomy was proposed in 2023 by Google DeepMind researchers. They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans). Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous).\n\n\n== Characteristics ==\n\nVarious popular definitions of intelligence have been proposed. One of the leading proposals is the Turing test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches.\n\n\n=== Intelligence traits ===\nResearchers generally hold that a system is required to do all of the following to be regarded as an AGI:\n\nreason, use strategy, solve puzzles, and make judgments under uncertainty\nrepresent knowledge, including common sense knowledge\nplan\nlearn\ncommunicate in natural language\nif necessary, integrate these skills in completion of any given goal\nMany interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.\nComputer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree.\n\n\n=== Physical traits ===\nOther capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:\n\nthe ability to sense (e.g. see, hear, etc.), and\nthe ability to act (e.g. move and manipulate objects, change location to explore, etc.)\nThis includes the ability to detect and respond to hazard.\nAlthough the ability to sense (e.g. see, hear, etc.) and the ability to act (e.g. move and manipulate objects, change location to explore, etc.) can be desirable for some intelligent systems, these physical capabilities are not strictly required for an entity to qualify as AGI—particularly under the thesis that large language models (LLMs) may already be or become AGI. Even from a less optimistic perspective on LLMs, there is no firm requirement for an AGI to have a human-like form; being a silicon-based computational system is sufficient, provided it can process input (language) from the external world in place of human senses. This interpretation aligns with the understanding that AGI has never been proscribed a particular physical embodiment and thus does not demand a capacity for locomotion or traditional \"eyes and ears\".  It can be regarded as sufficient for an intelligent computer to interact with other systems, to invoke or regulate them, to achieve specific goals, including altering a physical environment, as HAL in  2001: A Space Odyssey was both programmed and tasked to.\n\n\n=== Tests for human-level AGI ===\nSeveral tests meant to confirm human-level AGI have been considered, including:\n\nThe Turing Test (Turing)\nProposed by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence\", this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human-like responses. The machine passes the test if it can convince the judge it is human a significant fraction of the time. Turing proposed this as a practical measure of machine intelligence, focusing on the ability to produce human-like responses rather than on the internal workings of the machine.\nTuring described the test as follows:\nThe idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be expert about machines, must be taken in by the pretence.\nIn 2014, a chatbot named Eugene Goostman, designed to imitate a 13-year-old Ukrainian boy, reportedly passed a Turing Test event by convincing 33% of judges that it was human. However, this claim was met with significant skepticism from the AI research community, who questioned the test's implementation and its relevance to AGI.\nIn 2023, it was claimed that \"AI is closer to ever\" to passing the Turing test, though the article's authors reinforced that imitation (as \"large language models\" ever closer to passing the test are built upon) is not synonymous with \"intelligence\".  Further, as AI intelligence and human intelligence may differ, \"passing the Turing test is good evidence a system is intelligent, failing it is not good evidence a system is not intelligent.\"\nA 2024 study suggested that GPT-4 was identified as human 54% of the time in a randomized, controlled version of the Turing Test—surpassing older chatbots like ELIZA while still falling behind actual humans (67%).\nA 2025 pre‑registered, three‑party Turing‑test study by Cameron R. Jones and Benjamin K. Bergen showed that GPT-4.5 was judged to be the human in 73% of five‑minute text conversations—surpassing the 67% humanness rate of real confederates and meeting the researchers’ criterion for having passed the test.\nThe Robot College Student Test (Goertzel)\nA machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.\nThe Employment Test (Nilsson)\nA machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food and marketing.\nThe Ikea test (Marcus)\nAlso known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.\nThe Coffee Test (Wozniak)\nA machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. Robots developed by Figure AI and other robotics companies can perform tasks like this.\nThe Modern Turing Test (Suleyman)\nAn AI model is given $100,000 and has to obtain $1 million.\n\n\n=== AI-complete problems ===\n\nA problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that in order to solve it, one would need to implement AGI, because the solution is beyond the capabilities of a purpose-specific algorithm.\nThere are many problems that have been conjectured to require general intelligence to solve as well as humans. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.\nHowever, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning.\n\n\n== History ==\n\n\n=== Classical AI ===\n\nModern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\"\nTheir predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\".\nSeveral classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.\nHowever, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\".\n\n\n=== Narrow AI research ===\n\nIn the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018, development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.\n\nAt the turn of the century, many mainstream AI researchers hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.\nHowever, even at the time, this was disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the symbol grounding hypothesis by stating: The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).\n\n\n=== Modern artificial general intelligence research ===\nThe term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. A mathematical formalism of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises \"the ability to satisfy goals in a wide range of environments\". This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, was also called universal artificial intelligence.\nThe term AGI was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.\nAs of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do.\n\n\n=== Feasibility ===\n\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.\nA further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions?\nMost AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.  John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI.\nA report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.\nIn 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: \"Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\" Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking.\nBlaise Agüera y Arcas and Peter Norvig wrote in 2023 the article \"Artificial General Intelligence Is Already Here\", arguing that frontier models had already achieved a significant level of general intelligence. They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\".\n2023 also marked the emergence of large multimodal models (large language models capable of processing or generating multiple modalities such as text, audio, and images). As of 2025, large language models (LLMs) have been adapted to generate both music and images. Voice‑synthesis systems built on transformer LLMs—such as Suno AI’s Bark model—can sing, and several music‑generation platforms (e.g. Suno and Udio) build their services on modified LLM backbones.\nThe same year, OpenAI released GPT‑4o image generation, integrating native image synthesis directly into ChatGPT rather than relying on a separate diffusion‑based art model, as with DALL-E.\nLLM‑style foundation models are likewise being repurposed for robotics. Nvidia’s open‑source Isaac GR00T N1 and Google DeepMind’s Robotic Transformer 2 (RT‑2) are first trained with language‑model objectives and then fine‑tuned to handle vision‑language‑action control for embodied robots.\nIn 2024, OpenAI released o1-preview, the first of a series of models that \"spend more time thinking before they respond\". According to Mira Murati, this ability to think before responding represents a new, additional paradigm. It improves model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power.\nAn OpenAI employee, Vahid Kazemi, claimed in 2024 that the company had achieved AGI, stating, \"In my opinion, we have already achieved AGI and it's even more clear with O1.\" Kazemi clarified that while the AI is not yet \"better than any human at any task\", it is \"better than most humans at most tasks.\" He also addressed criticisms that large language models (LLMs) merely follow predefined patterns, comparing their learning process to the scientific method of observing, hypothesizing, and verifying. These statements have sparked debate, as they rely on a broad and unconventional definition of AGI—traditionally understood as AI that matches human intelligence across all domains. Critics argue that, while OpenAI's models demonstrate remarkable versatility, they may not fully meet this standard. Notably, Kazemi's comments came shortly after OpenAI removed \"AGI\" from the terms of its partnership with Microsoft, prompting speculation about the company's strategic intentions.\n\n\n=== Timescales ===\n\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.\nIn the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.\nIn 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave.\nIn 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.\nIn 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.\nIn the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.\nIn 2022, DeepMind developed Gato, a \"general-purpose\" system capable of performing more than 600 different tasks.\nIn 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.\nIn 2023, AI researcher Geoffrey Hinton stated that:\n\nThe idea that this stuff could actually get smarter than people – a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.He estimated in 2024 (with low confidence) that systems smarter than humans could appear within 5 to 20 years and stressed the attendant existential risks.\nIn May 2023, Demis Hassabis similarly said that \"The progress in the last few years has been pretty incredible\", and that he sees no reason why it would slow down, expecting AGI within a decade or even a few years. In March 2024, Nvidia's CEO, Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans. In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\".\n\n\n== Whole brain emulation ==\n\nWhile the development of transformer models like in ChatGPT is considered the most promising path to AGI, whole brain emulation can serve as an alternative approach. With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device. The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain. Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes. It has been discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.\n\n\n=== Early estimates ===\n For low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).\nIn 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps). (For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" – a measure used to rate current supercomputers – then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n\n\n=== Current research ===\nThe Human Brain Project, an EU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessible atlas of the human brain. In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain.\n\n\n=== Criticisms of simulation-based approaches ===\nThe artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.\nA fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel proposes virtual embodiment (like in metaverses like Second Life) as an option, but it is unknown whether this would be sufficient.\n\n\n== Philosophical perspective ==\n\n\n=== \"Strong AI\" as defined in philosophy ===\nIn 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument. He proposed a distinction between two hypotheses about artificial intelligence:\n\nStrong AI hypothesis: An artificial intelligence system can have \"a mind\" and \"consciousness\".\nWeak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness.\nThe first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be precisely identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.\nIn contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\". This is not the same as Searle's strong AI, unless it is assumed that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.\nMainstream AI is most interested in how a program behaves. According to Russell and Norvig, \"as long as the program works, they don't care if you call it real or a simulation.\" If the program can behave as if it has a mind, then there is no need to know if it actually has mind – indeed, there would be no way to tell. For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\". Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things.\n\n\n=== Consciousness ===\n\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts.\nSelf-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger is able to be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as ‘you’ within self-modeling frameworks.\nThese traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals. Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights. Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue.\n\n\n== Benefits ==\nAGI could improve productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society.\nAGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.\n\n\n=== Advancements in medicine and healthcare ===\nAGI would improve healthcare by making medical diagnostics faster, cheaper, and more accurate. AI-driven systems can analyse patient data and detect diseases at an early stage. This means patients will get diagnosed quicker and be able to seek medical attention before their medical condition gets worse. AGI systems could also recommend personalised treatment plans based on genetics and medical history.\nAdditionally, AGI could accelerate drug discovery by simulating molecular interactions, reducing the time it takes to develop new medicines for conditions like cancer and Alzheimer's. In hospitals, AGI-powered robotic assistants could assist in surgeries, monitor patients, and provide real-time medical support. It could also be used in elderly care, helping aging populations maintain independence through AI-powered caregivers and health-monitoring systems.\nBy evaluating large datasets, AGI can assist in developing personalised treatment plans tailored to individual patient needs. This approach ensures that therapies are optimised based on a patient's unique medical history and genetic profile, improving outcomes and reducing adverse effects.\n\n\n=== Advancements in science and technology ===\nAGI can become a tool for scientific research and innovation. In fields such as physics and mathematics, AGI could help solve complex problems that require massive computational power, such as modeling quantum systems, understanding dark matter, or proving mathematical theorems. Problems that have remained unsolved for decades may be solved with AGI.\nAGI could also drive technological breakthroughs that could reshape society. It can do this by optimising engineering designs, discovering new materials, and improving automation. For example, AI is already playing a role in developing more efficient renewable energy sources and optimising supply chains in manufacturing. Future AGI systems could push these innovations even further.\n\n\n=== Enhancing education and productivity ===\nAGI can personalize education by creating learning programs that are specific to each student's strengths, weaknesses, and interests. Unlike traditional teaching methods, AI-driven tutoring systems could adapt lessons in real-time, ensuring students understand difficult concepts before moving on.\nIn the workplace, AGI could automate repetitive tasks, freeing up workers for more creative and strategic roles. It could also improve efficiency across industries by optimising logistics, enhancing cybersecurity, and streamlining business operations. If properly managed, the wealth generated by AGI-driven automation could reduce the need for people to work for a living. Working may become optional.\n\n\n=== Mitigating global crises ===\nAGI could play a crucial role in preventing and managing global threats. It could help governments and organizations predict and respond to natural disasters more effectively, using real-time data analysis to forecast hurricanes, earthquakes, and pandemics. By analyzing vast datasets from satellites, sensors, and historical records, AGI could improve early warning systems, enabling faster disaster response and minimising casualties.\nIn climate science, AGI could develop new models for reducing carbon emissions, optimising energy resources, and mitigating climate change effects. It could also enhance weather prediction accuracy, allowing policymakers to implement more effective environmental regulations. Additionally, AGI could help regulate emerging technologies that carry significant risks, such as nanotechnology and bioengineering, by analysing complex systems and predicting unintended consequences. Furthermore, AGI could assist in cybersecurity by detecting and mitigating large-scale cyber threats, protecting critical infrastructure, and preventing digital warfare.\n\n\n=== Revitalising environmental conservation and biodiversity ===\nAGI could significantly contribute to preserving the environment and protecting endangered species. By analyzing satellite imagery, climate data, and wildlife patterns, AGI systems could identify environmental threats earlier and recommend targeted conservation strategies. AGI could help optimize land use, monitor illegal activities like poaching or deforestation in real-time, and support global efforts to restore ecosystems. Advanced predictive models developed by AGI could also assist in reversing biodiversity loss, ensuring the survival of critical species and maintaining ecological balance.\n\n\n=== Enhancing space exploration and colonization ===\nAGI could revolutionize humanity’s ability to explore and settle beyond Earth. With its advanced problem-solving skills, AGI could autonomously manage complex space missions, including navigation, resource management, and emergency response. It could accelerate the design of life support systems, habitats, and spacecraft optimized for extraterrestrial environments. Furthermore, AGI could support efforts to colonize planets like Mars by simulating survival scenarios and helping humans adapt to new worlds, dramatically expanding the possibilities for interplanetary civilization.\n\n\n== Risks ==\n\n\n=== Existential risks ===\n\nAGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\". The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench it, preventing moral progress. Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime. There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe. Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\".\n\n\n==== Risk of loss of control and human extinction ====\nThe thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman.\nIn 2014, Stephen Hawking criticized widespread indifference:\n\nSo, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.\nThe skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\". On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions.\nMany scholars who are concerned about existential risk advocate for more research into solving the \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race (which could lead to a race to the bottom of safety precautions in order to release products before competitors), and the use of AI in weapon systems.\nThe thesis that AI can pose existential risk also has detractors. Skeptics usually say that AGI is unlikely in the short-term, or that concerns about AGI distract from other issues related to current AI. Former Google fraud czar Shuman Ghosemajumder considers that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear.\nSkeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI, Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products.\nIn 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"\n\n\n=== Mass unemployment ===\n\nResearchers from OpenAI estimated that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\". They consider office workers to be the most exposed, for example mathematicians, accountants or web designers. AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies.\nCritics argue that AGI will complement rather than replace humans, and that automation displaces work in the short term but not in the long term.\nAccording to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:\n\nEveryone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequalityElon Musk argued in 2021 that the automation of society will require governments to adopt a universal basic income (UBI). Hinton similarly advised the UK government in 2025 to adopt a UBI as a response to AI-induced unemployment. In 2023, Hinton said \"I’m a socialist [...] I think that private ownership of the media, and of the ‘means of computation’, is not good.\"\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Sources ==\n\n\n== Further reading ==\n\n\n== External links ==\nThe AGI portal maintained by Pei Wang",
      "cleaned_text": "Artificial general intelligence (AGI)-sometimes called human‑level intelligence AI-is a type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks. Some researchers argue that state‑of‑the‑art large language models (LLMs) already exhibit signs of AGI‑level capability, while others maintain that genuine AGI has not yet been achieved. Beyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin. Unlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming. The concept does not, in principle, require the system to be an autonomous agent; a static model-such as a highly capable large language model-or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved. Creating AGI is a primary goal of AI research and of companies such as OpenAI, Google, and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries. The timeline for achieving human‑level intelligence AI remains deeply contested. Recent surveys of AI researchers give median forecasts ranging from the late 2020s to mid‑century, while still recording significant numbers who expect arrival much sooner-or never at all. There is debate on the exact definition of AGI and regarding whether modern LLMs such as GPT-4 are early forms of emerging AGI. AGI is a common topic in science fiction and futures studies. Contention exists over whether AGI represents an existential risk. Many AI experts have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be in too remote a stage to present such a risk. AGI is also known as strong AI, full AI, human-level AI, human-level intelligent AI, or general intelligent action. Some academic sources reserve the term \"strong AI\" for computer programs that will experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities. Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans. Related concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution. A framework for classifying AGI by performance and autonomy was proposed in 2023 by Google DeepMind researchers. They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans). Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous). Various popular definitions of intelligence have been proposed. One of the leading proposals is the Turing test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches. Researchers generally hold that a system is required to do all of the following to be regarded as an AGI: reason, use strategy, solve puzzles, and make judgments under uncertainty represent knowledge, including common sense knowledge plan learn communicate in natural language if necessary, integrate these skills in completion of any given goal Many interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy. Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree. Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include: the ability to sense (e.g. see, hear, etc.), and the ability to act (e.g. move and manipulate objects, change location to explore, etc.) This includes the ability to detect and respond to hazard. Although the ability to sense (e.g. see, hear, etc.) and the ability to act (e.g. move and manipulate objects, change location to explore, etc.) can be desirable for some intelligent systems, these physical capabilities are not strictly required for an entity to qualify as AGI-particularly under the thesis that large language models (LLMs) may already be or become AGI. Even from a less optimistic perspective on LLMs, there is no firm requirement for an AGI to have a human-like form; being a silicon-based computational system is sufficient, provided it can process input (language) from the external world in place of human senses. This interpretation aligns with the understanding that AGI has never been proscribed a particular physical embodiment and thus does not demand a capacity for locomotion or traditional \"eyes and ears\". It can be regarded as sufficient for an intelligent computer to interact with other systems, to invoke or regulate them, to achieve specific goals, including altering a physical environment, as HAL in 2001: A Space Odyssey was both programmed and tasked to. Several tests meant to confirm human-level AGI have been considered, including: The Turing Test (Turing) Proposed by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence\", this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human-like responses. The machine passes the test if it can convince the judge it is human a significant fraction of the time. Turing proposed this as a practical measure of machine intelligence, focusing on the ability to produce human-like responses rather than on the internal workings of the machine. Turing described the test as follows: The idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be expert about machines, must be taken in by the pretence. In 2014, a chatbot named Eugene Goostman, designed to imitate a 13-year-old Ukrainian boy, reportedly passed a Turing Test event by convincing 33% of judges that it was human. However, this claim was met with significant skepticism from the AI research community, who questioned the test's implementation and its relevance to AGI. In 2023, it was claimed that \"AI is closer to ever\" to passing the Turing test, though the article's authors reinforced that imitation (as \"large language models\" ever closer to passing the test are built upon) is not synonymous with \"intelligence\". Further, as AI intelligence and human intelligence may differ, \"passing the Turing test is good evidence a system is intelligent, failing it is not good evidence a system is not intelligent.\" A 2024 study suggested that GPT-4 was identified as human 54% of the time in a randomized, controlled version of the Turing Test-surpassing older chatbots like ELIZA while still falling behind actual humans (67%). A 2025 pre‑registered, three‑party Turing‑test study by Cameron R. Jones and Benjamin K. Bergen showed that GPT-4.5 was judged to be the human in 73% of five‑minute text conversations-surpassing the 67% humanness rate of real confederates and meeting the researchers’ criterion for having passed the test. The Robot College Student Test (Goertzel) A machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes. The Employment Test (Nilsson) A machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food and marketing. The Ikea test (Marcus) Also known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly. The Coffee Test (Wozniak) A machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. Robots developed by Figure AI and other robotics companies can perform tasks like this. The Modern Turing Test (Suleyman) An AI model is given $100,000 and has to obtain $1 million. A problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that in order to solve it, one would need to implement AGI, because the solution is beyond the capabilities of a purpose-specific algorithm. There are many problems that have been conjectured to require general intelligence to solve as well as humans. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance. However, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning. Modern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\" Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\". Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI. However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\". In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018, development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years. At the turn of the century, many mainstream AI researchers hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts. However, even at the time, this was disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the symbol grounding hypothesis by stating: The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) - nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer). The term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. A mathematical formalism of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises \"the ability to satisfy goals in a wide range of environments\". This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, was also called universal artificial intelligence. The term AGI was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. As of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do. As of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI. A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about. In 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: \"Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\" Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking. Blaise Agüera y Arcas and Peter Norvig wrote in 2023 the article \"Artificial General Intelligence Is Already Here\", arguing that frontier models had already achieved a significant level of general intelligence. They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\". 2023 also marked the emergence of large multimodal models (large language models capable of processing or generating multiple modalities such as text, audio, and images). As of 2025, large language models (LLMs) have been adapted to generate both music and images. Voice‑synthesis systems built on transformer LLMs-such as Suno AI’s Bark model-can sing, and several music‑generation platforms (e.g. Suno and Udio) build their services on modified LLM backbones. The same year, OpenAI released GPT‑4o image generation, integrating native image synthesis directly into ChatGPT rather than relying on a separate diffusion‑based art model, as with DALL-E. LLM‑style foundation models are likewise being repurposed for robotics. Nvidia’s open‑source Isaac GR00T N1 and Google DeepMind’s Robotic Transformer 2 (RT‑2) are first trained with language‑model objectives and then fine‑tuned to handle vision‑language‑action control for embodied robots. In 2024, OpenAI released o1-preview, the first of a series of models that \"spend more time thinking before they respond\". According to Mira Murati, this ability to think before responding represents a new, additional paradigm. It improves model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power. An OpenAI employee, Vahid Kazemi, claimed in 2024 that the company had achieved AGI, stating, \"In my opinion, we have already achieved AGI and it's even more clear with O1.\" Kazemi clarified that while the AI is not yet \"better than any human at any task\", it is \"better than most humans at most tasks.\" He also addressed criticisms that large language models (LLMs) merely follow predefined patterns, comparing their learning process to the scientific method of observing, hypothesizing, and verifying. These statements have sparked debate, as they rely on a broad and unconventional definition of AGI-traditionally understood as AI that matches human intelligence across all domains. Critics argue that, while OpenAI's models demonstrate remarkable versatility, they may not fully meet this standard. Notably, Kazemi's comments came shortly after OpenAI removed \"AGI\" from the terms of its partnership with Microsoft, prompting speculation about the company's strategic intentions. Progress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs. In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16-26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert. In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave. In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27. In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system. In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API. In 2022, DeepMind developed Gato, a \"general-purpose\" system capable of performing more than 600 different tasks. In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems. In 2023, AI researcher Geoffrey Hinton stated that: The idea that this stuff could actually get smarter than people - a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.He estimated in 2024 (with low confidence) that systems smarter than humans could appear within 5 to 20 years and stressed the attendant existential risks. In May 2023, Demis Hassabis similarly said that \"The progress in the last few years has been pretty incredible\", and that he sees no reason why it would slow down, expecting AGI within a decade or even a few years. In March 2024, Nvidia's CEO, Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans. In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\". While the development of transformer models like in ChatGPT is considered the most promising path to AGI, whole brain emulation can serve as an alternative approach. With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device. The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain. Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes. It has been discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it. For low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS). In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps). (For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" - a measure used to rate current supercomputers - then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued. The Human Brain Project, an EU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessible atlas of the human brain. In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain. The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes. A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel proposes virtual embodiment (like in metaverses like Second Life) as an option, but it is unknown whether this would be sufficient. In 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument. He proposed a distinction between two hypotheses about artificial intelligence: Strong AI hypothesis: An artificial intelligence system can have \"a mind\" and \"consciousness\". Weak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness. The first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be precisely identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks. In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\". This is not the same as Searle's strong AI, unless it is assumed that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope. Mainstream AI is most interested in how a program behaves. According to Russell and Norvig, \"as long as the program works, they don't care if you call it real or a simulation.\" If the program can behave as if it has a mind, then there is no need to know if it actually has mind - indeed, there would be no way to tell. For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\". Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence: Sentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"-an operating system or debugger is able to be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)-but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns-occasionally referring to themselves using second-person constructs such as ‘you’ within self-modeling frameworks. These traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals. Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights. Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue. AGI could improve productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society. AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life. AGI would improve healthcare by making medical diagnostics faster, cheaper, and more accurate. AI-driven systems can analyse patient data and detect diseases at an early stage. This means patients will get diagnosed quicker and be able to seek medical attention before their medical condition gets worse. AGI systems could also recommend personalised treatment plans based on genetics and medical history. Additionally, AGI could accelerate drug discovery by simulating molecular interactions, reducing the time it takes to develop new medicines for conditions like cancer and Alzheimer's. In hospitals, AGI-powered robotic assistants could assist in surgeries, monitor patients, and provide real-time medical support. It could also be used in elderly care, helping aging populations maintain independence through AI-powered caregivers and health-monitoring systems. By evaluating large datasets, AGI can assist in developing personalised treatment plans tailored to individual patient needs. This approach ensures that therapies are optimised based on a patient's unique medical history and genetic profile, improving outcomes and reducing adverse effects. AGI can become a tool for scientific research and innovation. In fields such as physics and mathematics, AGI could help solve complex problems that require massive computational power, such as modeling quantum systems, understanding dark matter, or proving mathematical theorems. Problems that have remained unsolved for decades may be solved with AGI. AGI could also drive technological breakthroughs that could reshape society. It can do this by optimising engineering designs, discovering new materials, and improving automation. For example, AI is already playing a role in developing more efficient renewable energy sources and optimising supply chains in manufacturing. Future AGI systems could push these innovations even further. AGI can personalize education by creating learning programs that are specific to each student's strengths, weaknesses, and interests. Unlike traditional teaching methods, AI-driven tutoring systems could adapt lessons in real-time, ensuring students understand difficult concepts before moving on. In the workplace, AGI could automate repetitive tasks, freeing up workers for more creative and strategic roles. It could also improve efficiency across industries by optimising logistics, enhancing cybersecurity, and streamlining business operations. If properly managed, the wealth generated by AGI-driven automation could reduce the need for people to work for a living. Working may become optional. AGI could play a crucial role in preventing and managing global threats. It could help governments and organizations predict and respond to natural disasters more effectively, using real-time data analysis to forecast hurricanes, earthquakes, and pandemics. By analyzing vast datasets from satellites, sensors, and historical records, AGI could improve early warning systems, enabling faster disaster response and minimising casualties. In climate science, AGI could develop new models for reducing carbon emissions, optimising energy resources, and mitigating climate change effects. It could also enhance weather prediction accuracy, allowing policymakers to implement more effective environmental regulations. Additionally, AGI could help regulate emerging technologies that carry significant risks, such as nanotechnology and bioengineering, by analysing complex systems and predicting unintended consequences. Furthermore, AGI could assist in cybersecurity by detecting and mitigating large-scale cyber threats, protecting critical infrastructure, and preventing digital warfare. AGI could significantly contribute to preserving the environment and protecting endangered species. By analyzing satellite imagery, climate data, and wildlife patterns, AGI systems could identify environmental threats earlier and recommend targeted conservation strategies. AGI could help optimize land use, monitor illegal activities like poaching or deforestation in real-time, and support global efforts to restore ecosystems. Advanced predictive models developed by AGI could also assist in reversing biodiversity loss, ensuring the survival of critical species and maintaining ecological balance. AGI could revolutionize humanity’s ability to explore and settle beyond Earth. With its advanced problem-solving skills, AGI could autonomously manage complex space missions, including navigation, resource management, and emergency response. It could accelerate the design of life support systems, habitats, and spacecraft optimized for extraterrestrial environments. Furthermore, AGI could support efforts to colonize planets like Mars by simulating survival scenarios and helping humans adapt to new worlds, dramatically expanding the possibilities for interplanetary civilization. AGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\". The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench it, preventing moral progress. Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime. There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe. Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\". The thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman. In 2014, Stephen Hawking criticized widespread indifference: So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here-we'll leave the lights on?' Probably not-but this is more or less what is happening with AI.The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities. The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\". On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions. Many scholars who are concerned about existential risk advocate for more research into solving the \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race (which could lead to a race to the bottom of safety precautions in order to release products before competitors), and the use of AI in weapon systems. The thesis that AI can pose existential risk also has detractors. Skeptics usually say that AGI is unlikely in the short-term, or that concerns about AGI distract from other issues related to current AI. Former Google fraud czar Shuman Ghosemajumder considers that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI, Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products. In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\" Researchers from OpenAI estimated that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\". They consider office workers to be the most exposed, for example mathematicians, accountants or web designers. AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies. Critics argue that AGI will complement rather than replace humans, and that automation displaces work in the short term but not in the long term. According to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed: Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequalityElon Musk argued in 2021 that the automation of society will require governments to adopt a universal basic income (UBI). Hinton similarly advised the UK government in 2025 to adopt a UBI as a response to AI-induced unemployment. In 2023, Hinton said \"I’m a socialist [...] I think that private ownership of the media, and of the ‘means of computation’, is not good.\"",
      "sentences": [
        "Artificial general intelligence (AGI)-sometimes called human‑level intelligence AI-is a type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks.",
        "Some researchers argue that state‑of‑the‑art large language models (LLMs) already exhibit signs of AGI‑level capability, while others maintain that genuine AGI has not yet been achieved.",
        "Beyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin.",
        "Unlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming.",
        "The concept does not, in principle, require the system to be an autonomous agent; a static model-such as a highly capable large language model-or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved.",
        "Creating AGI is a primary goal of AI research and of companies such as OpenAI, Google, and Meta.",
        "A 2020 survey identified 72 active AGI research and development projects across 37 countries.",
        "The timeline for achieving human‑level intelligence AI remains deeply contested.",
        "Recent surveys of AI researchers give median forecasts ranging from the late 2020s to mid‑century, while still recording significant numbers who expect arrival much sooner-or never at all.",
        "There is debate on the exact definition of AGI and regarding whether modern LLMs such as GPT-4 are early forms of emerging AGI.",
        "AGI is a common topic in science fiction and futures studies.",
        "Contention exists over whether AGI represents an existential risk.",
        "Many AI experts have stated that mitigating the risk of human extinction posed by AGI should be a global priority.",
        "Others find the development of AGI to be in too remote a stage to present such a risk.",
        "AGI is also known as strong AI, full AI, human-level AI, human-level intelligent AI, or general intelligent action.",
        "Some academic sources reserve the term \"strong AI\" for computer programs that will experience sentience or consciousness.",
        "In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities.",
        "Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.",
        "Related concepts include artificial superintelligence and transformative AI.",
        "An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.",
        "A framework for classifying AGI by performance and autonomy was proposed in 2023 by Google DeepMind researchers.",
        "They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman.",
        "For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e.",
        "an artificial superintelligence) is similarly defined but with a threshold of 100%.",
        "They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans).",
        "Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous).",
        "Various popular definitions of intelligence have been proposed.",
        "One of the leading proposals is the Turing test.",
        "However, there are other well-known definitions, and some researchers disagree with the more popular approaches.",
        "Researchers generally hold that a system is required to do all of the following to be regarded as an AGI: reason, use strategy, solve puzzles, and make judgments under uncertainty represent knowledge, including common sense knowledge plan learn communicate in natural language if necessary, integrate these skills in completion of any given goal Many interdisciplinary approaches (e.g.",
        "cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.",
        "Computer-based systems that exhibit many of these capabilities exist (e.g.",
        "see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent).",
        "There is debate about whether modern AI systems possess them to an adequate degree.",
        "Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression.",
        "These include: the ability to sense (e.g.",
        "see, hear, etc.",
        "), and the ability to act (e.g.",
        "move and manipulate objects, change location to explore, etc.)",
        "This includes the ability to detect and respond to hazard.",
        "Although the ability to sense (e.g.",
        "see, hear, etc.)",
        "and the ability to act (e.g.",
        "move and manipulate objects, change location to explore, etc.)",
        "can be desirable for some intelligent systems, these physical capabilities are not strictly required for an entity to qualify as AGI-particularly under the thesis that large language models (LLMs) may already be or become AGI.",
        "Even from a less optimistic perspective on LLMs, there is no firm requirement for an AGI to have a human-like form; being a silicon-based computational system is sufficient, provided it can process input (language) from the external world in place of human senses.",
        "This interpretation aligns with the understanding that AGI has never been proscribed a particular physical embodiment and thus does not demand a capacity for locomotion or traditional \"eyes and ears\".",
        "It can be regarded as sufficient for an intelligent computer to interact with other systems, to invoke or regulate them, to achieve specific goals, including altering a physical environment, as HAL in 2001: A Space Odyssey was both programmed and tasked to.",
        "Several tests meant to confirm human-level AGI have been considered, including: The Turing Test (Turing) Proposed by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence\", this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human-like responses.",
        "The machine passes the test if it can convince the judge it is human a significant fraction of the time.",
        "Turing proposed this as a practical measure of machine intelligence, focusing on the ability to produce human-like responses rather than on the internal workings of the machine.",
        "Turing described the test as follows: The idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing.",
        "A considerable portion of a jury, who should not be expert about machines, must be taken in by the pretence.",
        "In 2014, a chatbot named Eugene Goostman, designed to imitate a 13-year-old Ukrainian boy, reportedly passed a Turing Test event by convincing 33% of judges that it was human.",
        "However, this claim was met with significant skepticism from the AI research community, who questioned the test's implementation and its relevance to AGI.",
        "In 2023, it was claimed that \"AI is closer to ever\" to passing the Turing test, though the article's authors reinforced that imitation (as \"large language models\" ever closer to passing the test are built upon) is not synonymous with \"intelligence\".",
        "Further, as AI intelligence and human intelligence may differ, \"passing the Turing test is good evidence a system is intelligent, failing it is not good evidence a system is not intelligent.\"",
        "A 2024 study suggested that GPT-4 was identified as human 54% of the time in a randomized, controlled version of the Turing Test-surpassing older chatbots like ELIZA while still falling behind actual humans (67%).",
        "A 2025 pre‑registered, three‑party Turing‑test study by Cameron R. Jones and Benjamin K. Bergen showed that GPT-4.5 was judged to be the human in 73% of five‑minute text conversations-surpassing the 67% humanness rate of real confederates and meeting the researchers’ criterion for having passed the test.",
        "The Robot College Student Test (Goertzel) A machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree.",
        "LLMs can now pass university degree-level exams without even attending the classes.",
        "The Employment Test (Nilsson) A machine performs an economically important job at least as well as humans in the same job.",
        "AIs are now replacing humans in many roles as varied as fast food and marketing.",
        "The Ikea test (Marcus) Also known as the Flat Pack Furniture Test.",
        "An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.",
        "The Coffee Test (Wozniak) A machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons.",
        "Robots developed by Figure AI and other robotics companies can perform tasks like this.",
        "The Modern Turing Test (Suleyman) An AI model is given $100,000 and has to obtain $1 million.",
        "A problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that in order to solve it, one would need to implement AGI, because the solution is beyond the capabilities of a purpose-specific algorithm.",
        "There are many problems that have been conjectured to require general intelligence to solve as well as humans.",
        "Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.",
        "All of these problems need to be solved simultaneously in order to reach human-level machine performance.",
        "However, many of these tasks can now be performed by modern large language models.",
        "According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning.",
        "Modern AI research began in the mid-1950s.",
        "The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades.",
        "AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\"",
        "Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001.",
        "AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time.",
        "He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\".",
        "Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.",
        "However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project.",
        "Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\".",
        "In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\".",
        "In response to this and the success of expert systems, both industry and government pumped money into the field.",
        "However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled.",
        "For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken.",
        "By the 1990s, AI researchers had a reputation for making vain promises.",
        "They became reluctant to make predictions at all and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\".",
        "In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms.",
        "These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry.",
        "As of 2018, development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.",
        "At the turn of the century, many mainstream AI researchers hoped that strong AI could be developed by combining programs that solve various sub-problems.",
        "Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs.",
        "Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.",
        "However, even at the time, this was disputed.",
        "For example, Stevan Harnad of Princeton University concluded his 1990 paper on the symbol grounding hypothesis by stating: The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between.",
        "If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up.",
        "A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) - nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).",
        "The term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations.",
        "A mathematical formalism of AGI was proposed by Marcus Hutter in 2000.",
        "Named AIXI, the proposed AGI agent maximises \"the ability to satisfy goals in a wide range of environments\".",
        "This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, was also called universal artificial intelligence.",
        "The term AGI was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002.",
        "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\".",
        "The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog.",
        "The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov.",
        "MIT presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.",
        "As of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences.",
        "However, increasingly more researchers are interested in open-ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do.",
        "As of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community.",
        "While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist.",
        "AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\".",
        "This prediction failed to come true.",
        "Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\".",
        "Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.",
        "A further challenge is the lack of clarity in defining what intelligence entails.",
        "Does it require consciousness?",
        "Must it display the ability to set goals as well as pursue them?",
        "Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge?",
        "Are facilities such as planning, reasoning, and causal understanding required?",
        "Does intelligence require explicitly replicating the brain and its specific faculties?",
        "Does it require emotions?",
        "Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.",
        "John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "AI experts' views on the feasibility of AGI wax and wane.",
        "Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081.",
        "Of the experts, 16.5% answered with \"never\" when asked the same question but with a 90% confidence instead.",
        "Further current AGI progress considerations can be found above Tests for confirming human-level AGI.",
        "A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\".",
        "They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.",
        "In 2023, Microsoft researchers published a detailed evaluation of GPT-4.",
        "They concluded: \"Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"",
        "Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking.",
        "Blaise Agüera y Arcas and Peter Norvig wrote in 2023 the article \"Artificial General Intelligence Is Already Here\", arguing that frontier models had already achieved a significant level of general intelligence.",
        "They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\".",
        "2023 also marked the emergence of large multimodal models (large language models capable of processing or generating multiple modalities such as text, audio, and images).",
        "As of 2025, large language models (LLMs) have been adapted to generate both music and images.",
        "Voice‑synthesis systems built on transformer LLMs-such as Suno AI’s Bark model-can sing, and several music‑generation platforms (e.g.",
        "Suno and Udio) build their services on modified LLM backbones.",
        "The same year, OpenAI released GPT‑4o image generation, integrating native image synthesis directly into ChatGPT rather than relying on a separate diffusion‑based art model, as with DALL-E. LLM‑style foundation models are likewise being repurposed for robotics.",
        "Nvidia’s open‑source Isaac GR00T N1 and Google DeepMind’s Robotic Transformer 2 (RT‑2) are first trained with language‑model objectives and then fine‑tuned to handle vision‑language‑action control for embodied robots.",
        "In 2024, OpenAI released o1-preview, the first of a series of models that \"spend more time thinking before they respond\".",
        "According to Mira Murati, this ability to think before responding represents a new, additional paradigm.",
        "It improves model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power.",
        "An OpenAI employee, Vahid Kazemi, claimed in 2024 that the company had achieved AGI, stating, \"In my opinion, we have already achieved AGI and it's even more clear with O1.\"",
        "Kazemi clarified that while the AI is not yet \"better than any human at any task\", it is \"better than most humans at most tasks.\"",
        "He also addressed criticisms that large language models (LLMs) merely follow predefined patterns, comparing their learning process to the scientific method of observing, hypothesizing, and verifying.",
        "These statements have sparked debate, as they rely on a broad and unconventional definition of AGI-traditionally understood as AI that matches human intelligence across all domains.",
        "Critics argue that, while OpenAI's models demonstrate remarkable versatility, they may not fully meet this standard.",
        "Notably, Kazemi's comments came shortly after OpenAI removed \"AGI\" from the terms of its partnership with Microsoft, prompting speculation about the company's strategic intentions.",
        "Progress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop.",
        "Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress.",
        "For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.",
        "In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century.",
        "As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near (i.e.",
        "between 2015 and 2045) was plausible.",
        "Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid.",
        "A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16-26 years for modern and historical predictions alike.",
        "That paper has been criticized for how it categorized opinions as expert or non-expert.",
        "In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers).",
        "AlexNet was regarded as the initial ground-breaker of the current deep learning wave.",
        "In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others.",
        "At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade.",
        "An adult comes to about 100 on average.",
        "Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.",
        "In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training.",
        "According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.",
        "In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\".",
        "OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.",
        "In 2022, DeepMind developed Gato, a \"general-purpose\" system capable of performing more than 600 different tasks.",
        "In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law.",
        "This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.",
        "In 2023, AI researcher Geoffrey Hinton stated that: The idea that this stuff could actually get smarter than people - a few people believed that, [...].",
        "But most people thought it was way off.",
        "And I thought it was way off.",
        "I thought it was 30 to 50 years or even longer away.",
        "Obviously, I no longer think that.He estimated in 2024 (with low confidence) that systems smarter than humans could appear within 5 to 20 years and stressed the attendant existential risks.",
        "In May 2023, Demis Hassabis similarly said that \"The progress in the last few years has been pretty incredible\", and that he sees no reason why it would slow down, expecting AGI within a decade or even a few years.",
        "In March 2024, Nvidia's CEO, Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans.",
        "In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\".",
        "While the development of transformer models like in ChatGPT is considered the most promising path to AGI, whole brain emulation can serve as an alternative approach.",
        "With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device.",
        "The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain.",
        "Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes.",
        "It has been discussed in artificial intelligence research as an approach to strong AI.",
        "Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.",
        "For low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain.",
        "Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons.",
        "The brain of a three-year-old child has about 1015 synapses (1 quadrillion).",
        "This number declines with age, stabilizing by adulthood.",
        "Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion).",
        "An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).",
        "In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps).",
        "(For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" - a measure used to rate current supercomputers - then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.)",
        "He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.",
        "The Human Brain Project, an EU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessible atlas of the human brain.",
        "In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain.",
        "The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons.",
        "A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline.",
        "The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate.",
        "In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.",
        "A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning.",
        "If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body).",
        "Goertzel proposes virtual embodiment (like in metaverses like Second Life) as an option, but it is unknown whether this would be sufficient.",
        "In 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument.",
        "He proposed a distinction between two hypotheses about artificial intelligence: Strong AI hypothesis: An artificial intelligence system can have \"a mind\" and \"consciousness\".",
        "Weak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness.",
        "The first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test.",
        "The behaviour of a \"weak AI\" machine would be precisely identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience.",
        "This usage is also common in academic AI research and textbooks.",
        "In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\".",
        "This is not the same as Searle's strong AI, unless it is assumed that consciousness is necessary for human-level AGI.",
        "Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.",
        "Mainstream AI is most interested in how a program behaves.",
        "According to Russell and Norvig, \"as long as the program works, they don't care if you call it real or a simulation.\"",
        "If the program can behave as if it has a mind, then there is no need to know if it actually has mind - indeed, there would be no way to tell.",
        "For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\".",
        "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"",
        "Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things.",
        "Consciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence: Sentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions.",
        "Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience.",
        "Determining why and how subjective experience arises is known as the hard problem of consciousness.",
        "Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious.",
        "If we are not conscious, then it doesn't feel like anything.",
        "Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\"",
        "However, we are unlikely to ask \"what does it feel like to be a toaster?\"",
        "Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not.",
        "In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts.",
        "Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts.",
        "This is opposed to simply being the \"subject of one's thought\"-an operating system or debugger is able to be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)-but this is not what people typically mean when they use the term \"self-awareness\".",
        "In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns-occasionally referring to themselves using second-person constructs such as ‘you’ within self-modeling frameworks.",
        "These traits have a moral dimension.",
        "AI sentience would give rise to concerns of welfare and legal protection, similarly to animals.",
        "Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights.",
        "Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue.",
        "AGI could improve productivity and efficiency in most jobs.",
        "For example, in public health, AGI could accelerate medical research, notably against cancer.",
        "It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics.",
        "It could offer fun, cheap and personalized education.",
        "The need to work to subsist could become obsolete if the wealth produced is properly redistributed.",
        "This also raises the question of the place of humans in a radically automated society.",
        "AGI could also help to make rational decisions, and to anticipate and prevent disasters.",
        "It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks.",
        "If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.",
        "AGI would improve healthcare by making medical diagnostics faster, cheaper, and more accurate.",
        "AI-driven systems can analyse patient data and detect diseases at an early stage.",
        "This means patients will get diagnosed quicker and be able to seek medical attention before their medical condition gets worse.",
        "AGI systems could also recommend personalised treatment plans based on genetics and medical history.",
        "Additionally, AGI could accelerate drug discovery by simulating molecular interactions, reducing the time it takes to develop new medicines for conditions like cancer and Alzheimer's.",
        "In hospitals, AGI-powered robotic assistants could assist in surgeries, monitor patients, and provide real-time medical support.",
        "It could also be used in elderly care, helping aging populations maintain independence through AI-powered caregivers and health-monitoring systems.",
        "By evaluating large datasets, AGI can assist in developing personalised treatment plans tailored to individual patient needs.",
        "This approach ensures that therapies are optimised based on a patient's unique medical history and genetic profile, improving outcomes and reducing adverse effects.",
        "AGI can become a tool for scientific research and innovation.",
        "In fields such as physics and mathematics, AGI could help solve complex problems that require massive computational power, such as modeling quantum systems, understanding dark matter, or proving mathematical theorems.",
        "Problems that have remained unsolved for decades may be solved with AGI.",
        "AGI could also drive technological breakthroughs that could reshape society.",
        "It can do this by optimising engineering designs, discovering new materials, and improving automation.",
        "For example, AI is already playing a role in developing more efficient renewable energy sources and optimising supply chains in manufacturing.",
        "Future AGI systems could push these innovations even further.",
        "AGI can personalize education by creating learning programs that are specific to each student's strengths, weaknesses, and interests.",
        "Unlike traditional teaching methods, AI-driven tutoring systems could adapt lessons in real-time, ensuring students understand difficult concepts before moving on.",
        "In the workplace, AGI could automate repetitive tasks, freeing up workers for more creative and strategic roles.",
        "It could also improve efficiency across industries by optimising logistics, enhancing cybersecurity, and streamlining business operations.",
        "If properly managed, the wealth generated by AGI-driven automation could reduce the need for people to work for a living.",
        "Working may become optional.",
        "AGI could play a crucial role in preventing and managing global threats.",
        "It could help governments and organizations predict and respond to natural disasters more effectively, using real-time data analysis to forecast hurricanes, earthquakes, and pandemics.",
        "By analyzing vast datasets from satellites, sensors, and historical records, AGI could improve early warning systems, enabling faster disaster response and minimising casualties.",
        "In climate science, AGI could develop new models for reducing carbon emissions, optimising energy resources, and mitigating climate change effects.",
        "It could also enhance weather prediction accuracy, allowing policymakers to implement more effective environmental regulations.",
        "Additionally, AGI could help regulate emerging technologies that carry significant risks, such as nanotechnology and bioengineering, by analysing complex systems and predicting unintended consequences.",
        "Furthermore, AGI could assist in cybersecurity by detecting and mitigating large-scale cyber threats, protecting critical infrastructure, and preventing digital warfare.",
        "AGI could significantly contribute to preserving the environment and protecting endangered species.",
        "By analyzing satellite imagery, climate data, and wildlife patterns, AGI systems could identify environmental threats earlier and recommend targeted conservation strategies.",
        "AGI could help optimize land use, monitor illegal activities like poaching or deforestation in real-time, and support global efforts to restore ecosystems.",
        "Advanced predictive models developed by AGI could also assist in reversing biodiversity loss, ensuring the survival of critical species and maintaining ecological balance.",
        "AGI could revolutionize humanity’s ability to explore and settle beyond Earth.",
        "With its advanced problem-solving skills, AGI could autonomously manage complex space missions, including navigation, resource management, and emergency response.",
        "It could accelerate the design of life support systems, habitats, and spacecraft optimized for extraterrestrial environments.",
        "Furthermore, AGI could support efforts to colonize planets like Mars by simulating survival scenarios and helping humans adapt to new worlds, dramatically expanding the possibilities for interplanetary civilization.",
        "AGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\".",
        "The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future.",
        "Notably, it could be used to spread and preserve the set of values of whoever develops it.",
        "If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench it, preventing moral progress.",
        "Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime.",
        "There is also a risk for the machines themselves.",
        "If machines that are sentient or otherwise worthy of moral consideration are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe.",
        "Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\".",
        "The thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman.",
        "In 2014, Stephen Hawking criticized widespread indifference: So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right?",
        "If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here-we'll leave the lights on?'",
        "Probably not-but this is more or less what is happening with AI.The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities.",
        "The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated.",
        "As a result, the gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.",
        "The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans.",
        "He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\".",
        "On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals.",
        "And that this does not require having emotions.",
        "Many scholars who are concerned about existential risk advocate for more research into solving the \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?",
        "Solving the control problem is complicated by the AI arms race (which could lead to a race to the bottom of safety precautions in order to release products before competitors), and the use of AI in weapon systems.",
        "The thesis that AI can pose existential risk also has detractors.",
        "Skeptics usually say that AGI is unlikely in the short-term, or that concerns about AGI distract from other issues related to current AI.",
        "Former Google fraud czar Shuman Ghosemajumder considers that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear.",
        "Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God.",
        "Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI, Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products.",
        "In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"",
        "Researchers from OpenAI estimated that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\".",
        "They consider office workers to be the most exposed, for example mathematicians, accountants or web designers.",
        "AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies.",
        "Critics argue that AGI will complement rather than replace humans, and that automation displaces work in the short term but not in the long term.",
        "According to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed: Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution.",
        "So far, the trend seems to be toward the second option, with technology driving ever-increasing inequalityElon Musk argued in 2021 that the automation of society will require governments to adopt a universal basic income (UBI).",
        "Hinton similarly advised the UK government in 2025 to adopt a UBI as a response to AI-induced unemployment.",
        "In 2023, Hinton said \"I’m a socialist [...] I think that private ownership of the media, and of the ‘means of computation’, is not good.\""
      ],
      "metadata": {
        "title": "Artificial general intelligence",
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "word_count": 7030,
        "char_count": 45790,
        "sentence_count": 316,
        "scraped_at": "2025-08-09T14:46:47.056041",
        "language": "en",
        "processing_time": 0.009894371032714844,
        "source_hash": "307c2cadd3c9d4f187b304a9c607979a"
      }
    },
    {
      "title": "Generative artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Generative_artificial_intelligence",
      "raw_text": "Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTXV and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.\nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.\n\n\n== History ==\n\n\n=== Early history ===\nThe first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is trained on a text corpus, it can then be used as a probabilistic text generator.\nComputers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\n\n\n=== Generative neural networks (2014–2019) ===\n\nSince its inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.\nIn 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.\nThe new generative models introduced during this period allowed for large neural networks to be trained using unsupervised learning or semi-supervised learning, rather than the supervised learning typical of discriminative models. Unsupervised learning removed the need for humans to manually label data, allowing for larger networks to be trained.\n\n\n=== Generative AI boom (2020–) ===\n\nIn March 2020, the release of 15.ai, a free web application created by an anonymous MIT researcher that could generate convincing character voices using minimal training data, marked one of the earliest popular use cases of generative AI. The platform is credited as the first mainstream service to popularize AI voice cloning (audio deepfakes) in memes and content creation, influencing subsequent developments in voice AI technology.\nIn 2021, the emergence of DALL-E, a transformer-based pixel generative model, marked an advance in AI-generated imagery. This was followed by the releases of Midjourney and Stable Diffusion in 2022, which further democratized access to high-quality artificial intelligence art creation from natural language prompts. These systems demonstrated unprecedented capabilities in generating photorealistic images, artwork, and designs based on text descriptions, leading to widespread adoption among artists, designers, and the general public.\nIn late 2022, the public release of ChatGPT revolutionized the accessibility and application of generative AI for general-purpose text-based tasks. The system's ability to engage in natural conversations, generate creative content, assist with coding, and perform various analytical tasks captured global attention and sparked widespread discussion about AI's potential impact on work, education, and creativity.\nIn March 2023, GPT-4's release represented another jump in generative AI capabilities. A team from Microsoft Research controversially argued that it \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\" However, this assessment was contested by other scholars who maintained that generative AI remained \"still far from reaching the benchmark of 'general human intelligence'\" as of 2023. Later in 2023, Meta released ImageBind, an AI model combining multiple modalities including text, images, video, thermal data, 3D data, audio, and motion, paving the way for more immersive generative AI applications.\nIn December 2023, Google unveiled Gemini, a multimodal AI model available in four versions: Ultra, Pro, Flash, and Nano. The company integrated Gemini Pro into its Bard chatbot and announced plans for \"Bard Advanced\" powered by the larger Gemini Ultra model. In February 2024, Google unified Bard and Duet AI under the Gemini brand, launching a mobile app on Android and integrating the service into the Google app on iOS.\nIn March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus. The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google. In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.\n\nAsia–Pacific countries are significantly more optimistic than Western societies about generative AI and show higher adoption rates. Despite expressing concerns about privacy and the pace of change, in a 2024 survey, 68% of Asia-Pacific respondents believed that AI was having a positive impact on the world, compared to 57% globally. According to a survey by SAS and Coleman Parkes Research, China in particular has emerged as a global leader in generative AI adoption, with 83% of Chinese respondents using the technology, exceeding both the global average of 54% and the U.S. rate of 65%. This leadership is further evidenced by China's intellectual property developments in the field, with a UN report revealing that Chinese entities filed over 38,000 generative AI patents from 2014 to 2023, substantially surpassing the United States in patent applications. A 2024 survey on the Chinese social app Soul reported that 18% of respondents born after 2000 used generative AI \"almost every day\", and that over 60% of respondents like or love AI-generated content, while less than 3% dislike or hate it.\n\n\n== Applications ==\nNotable types of generative AI models include generative pre-trained transformers (GPTs), generative adversarial networks (GANs), and variational autoencoders (VAEs). Generative AI systems are multimodal if they can process multiple types of inputs or generate multiple types of outputs. For example, GPT-4o can both process and generate text, images and audio.\nGenerative AI has made its appearance in a wide variety of industries, radically changing the dynamics of content creation, analysis, and delivery. In healthcare, for instance, generative AI accelerates drug discovery by creating molecular structures with target characteristics and generates radiology images for training diagnostic models. This ability not only enables faster and cheaper development but also enhances medical decision-making. In finance, generative AI services help create datasets and automate reports using natural language. It automates content creation, produces synthetic financial data, and tailors customer communications. It also powers chatbots and virtual agents. Collectively, these technologies enhance efficiency, reduce operational costs, and support data-driven decision-making in financial institutions. The media industry makes use of generative AI for numerous creative activities such as music composition, scriptwriting, video editing, and digital art. The educational sector is impacted as well, since the tools make learning personalized through creating quizzes, study aids, and essay composition. Both the teachers and the learners benefit from AI-based platforms that suit various learning patterns. In the educational field, in Colombia, student use of Meta's generative AI programs resulted in a decline in scores.\n\n\n=== Text and software code ===\n\nGenerative AI systems trained on words or word tokens include GPT-3, GPT-4, GPT-4o, LaMDA, LLaMA, BLOOM, Gemini, Claude and others (see List of large language models). They are capable of natural language processing, machine translation, and natural language generation and can be used as foundation models for other tasks. Data sets include BookCorpus, Wikipedia, and others (see List of text corpora).\nIn addition to natural language text, large language models can be trained on programming language text, allowing them to generate source code for new computer programs. Examples include OpenAI Codex, Tabnine, GitHub Copilot, Microsoft Copilot, and VS Code fork Cursor.\nSome AI assistants help candidates cheat during online coding interviews by providing code, improvements, and explanations. Their clandestine interfaces minimize the need for eye movements that would expose cheating to the interviewer.\n\n\n=== Images ===\n\nProducing high-quality visual art is a prominent application of generative AI. Generative AI systems trained on sets of images with text captions include Imagen, DALL-E, Midjourney, Adobe Firefly, FLUX.1, Stable Diffusion and others (see Artificial intelligence art, Generative art, and Synthetic media). They are commonly used for text-to-image generation and neural style transfer. Datasets include LAION-5B and others (see List of datasets in computer vision and image processing).\n\n\n=== Audio ===\n\nGenerative AI can also be trained extensively on audio clips to produce natural-sounding speech synthesis and text-to-speech capabilities. An early pioneer in this field was 15.ai, launched in March 2020, which demonstrated the ability to clone character voices using as little as 15 seconds of training data. The website gained widespread attention for its ability to generate emotionally expressive speech for various fictional characters, though it was later taken offline in 2022 due to copyright concerns. Commercial alternatives subsequently emerged, including ElevenLabs' context-aware synthesis tools and Meta Platform's Voicebox.\nGenerative AI systems such as MusicLM and MusicGen can also be trained on the audio waveforms of recorded music along with text annotations, in order to generate new musical samples based on text descriptions such as a calming violin melody backed by a distorted guitar riff.\nAudio deepfakes of music lyrics have been generated, like the song Savages, which used AI to mimic rapper Jay-Z's vocals. Music artist's instrumentals and lyrics are copyrighted but their voices are not protected from regenerative AI yet, raising a debate about whether artists should get royalties from audio deepfakes.\nMany AI music generators have been created that can be generated using a text phrase, genre options, and looped libraries of bars and riffs.\n\n\n=== Video ===\n\nGenerative AI trained on annotated video can generate temporally-coherent, detailed and photorealistic video clips. Examples include Sora by OpenAI, Runway, Make-A-Video by Meta Platforms and the open source LTX Video by Lightricks .\n\n\n=== Robotics ===\nGenerative AI can also be trained on the motions of a robotic system to generate new trajectories for motion planning or navigation. For example, UniPi from Google Research uses prompts like \"pick up blue bowl\" or \"wipe plate with yellow sponge\" to control movements of a robot arm. Multimodal vision-language-action models such as Google's RT-2 can perform rudimentary reasoning in response to user prompts and visual input, such as picking up a toy dinosaur when given the prompt pick up the extinct animal at a table filled with toy animals and other objects.\n\n\n=== 3D modeling ===\n\nArtificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate 3D modeling. AI-based CAD libraries could also be developed using linked open data of schematics and diagrams. AI CAD assistants are used as tools to help streamline workflow.\n\n\n== Software and hardware ==\n\nGenerative AI models are used to power chatbot products such as ChatGPT, programming tools such as GitHub Copilot, text-to-image products such as Midjourney, and text-to-video products such as Runway Gen-2. Generative AI features have been integrated into a variety of existing commercially available products such as Microsoft Office (Microsoft Copilot), Google Photos, and the Adobe Suite (Adobe Firefly). Many generative AI models are also available as open-source software, including Stable Diffusion and the LLaMA language model.\nSmaller generative AI models with up to a few billion parameters can run on smartphones, embedded devices, and personal computers. For example, LLaMA-7B (a version with 7 billion parameters) can run on a Raspberry Pi 4 and one version of Stable Diffusion can run on an iPhone 11.\nLarger models with tens of billions of parameters can run on laptop or desktop computers. To achieve an acceptable speed, models of this size may require accelerators such as the GPU chips produced by NVIDIA and AMD or the Neural Engine included in Apple silicon products. For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC.\nThe advantages of running generative AI locally include protection of privacy and intellectual property, and avoidance of rate limiting and censorship. The subreddit r/LocalLLaMA in particular focuses on using consumer-grade gaming graphics cards through such techniques as compression. That forum is one of only two sources Andrej Karpathy trusts for language model benchmarks. Yann LeCun has advocated open-source models for their value to vertical applications and for improving AI safety.\nLanguage models with hundreds of billions of parameters, such as GPT-4 or PaLM, typically run on datacenter computers equipped with arrays of GPUs (such as NVIDIA's H100) or AI accelerator chips (such as Google's TPU). These very large models are typically accessed as cloud services over the Internet.\nIn 2022, the United States New Export Controls on Advanced Computing and Semiconductors to China imposed restrictions on exports to China of GPU and AI accelerator chips used for generative AI. Chips such as the NVIDIA A800 and the Biren Technology BR104 were developed to meet the requirements of the sanctions.\nThere is free software on the market capable of recognizing text generated by generative artificial intelligence (such as GPTZero), as well as images, audio or video coming from it. Potential mitigation strategies for detecting generative AI content include digital watermarking, content authentication, information retrieval, and machine learning classifier models. Despite claims of accuracy, both free and paid AI text detectors have frequently produced false positives, mistakenly accusing students of submitting AI-generated work.\n\n\n=== Generative models and training techniques ===\n\n\n==== Generative adversarial networks ====\n\nGenerative adversarial networks (GANs) are an influential generative modeling technique. GANs consist of two neural networks—the generator and the discriminator—trained simultaneously in a competitive setting. The generator creates synthetic data by transforming random noise into samples that resemble the training dataset. The discriminator is trained to distinguish the authentic data from synthetic data produced by the generator. The two models engage in a minimax game: the generator aims to create increasingly realistic data to \"fool\" the discriminator, while the discriminator improves its ability to distinguish real from fake data. This continuous training setup enables the generator to produce high-quality and realistic outputs.\n\n\n==== Variational autoencoders ====\n\nVariational autoencoders (VAEs) are deep learning models that probabilistically encode data. They are typically used for tasks such as noise reduction from images, data compression, identifying unusual patterns, and facial recognition. Unlike standard autoencoders, which compress input data into a fixed latent representation, VAEs model the latent space as a probability distribution, allowing for smooth sampling and interpolation between data points. The encoder (\"recognition model\") maps input data to a latent space, producing means and variances that define a probability distribution. The decoder (\"generative model\") samples from this latent distribution and attempts to reconstruct the original input. VAEs optimize a loss function that includes both the reconstruction error and a Kullback–Leibler divergence term, which ensures the latent space follows a known prior distribution. VAEs are particularly suitable for tasks that require structured but smooth latent spaces, although they may create blurrier images than GANs. They are used for applications like image generation, data interpolation and anomaly detection.\n\n\n===== Transformers =====\nTransformers became the foundation for many powerful generative models, most notably the generative pre-trained transformer (GPT) series developed by OpenAI. They marked a major shift in natural language processing by replacing traditional recurrent and convolutional models. This architecture allows models to process entire sequences simultaneously and capture long-range dependencies more efficiently. The self-attention mechanism enables the model to capture the significance of every word in a sequence when predicting the subsequent word, thus improving its contextual understanding. Unlike recurrent neural networks, transformers process all the tokens in parallel, which improves the training efficiency and scalability. Transformers are typically pre-trained on enormous corpora in a self-supervised manner, prior to being fine-tuned.\n\n\n== Law and regulation ==\n\nIn the United States, a group of companies including OpenAI, Alphabet, and Meta signed a voluntary agreement with the Biden administration in July 2023 to watermark AI-generated content. In October 2023, Executive Order 14110 applied the Defense Production Act to require all US companies to report information to the federal government when training certain high-impact AI models.\nIn the European Union, the proposed Artificial Intelligence Act includes requirements to disclose copyrighted material used to train generative AI systems, and to label any AI-generated output as such.\nIn China, the Interim Measures for the Management of Generative AI Services introduced by the Cyberspace Administration of China regulates any public-facing generative AI. It includes requirements to watermark generated images or videos, regulations on training data and label quality, restrictions on personal data collection, and a guideline that generative AI services must \"adhere to socialist core values\".\n\n\n=== Copyright ===\n\n\n==== Training with copyrighted content ====\nGenerative AI systems such as ChatGPT and Midjourney are trained on large, publicly available datasets that include copyrighted works. AI developers have argued that such training is protected under fair use, while copyright holders have argued that it infringes their rights.\nProponents of fair use training have argued that it is a transformative use and does not involve making copies of copyrighted works available to the public. Critics have argued that image generators such as Midjourney can create nearly-identical copies of some copyrighted images, and that generative AI programs compete with the content they are trained on.\nAs of 2024, several lawsuits related to the use of copyrighted material in training are ongoing.\nGetty Images has sued Stability AI over the use of its images to train Stable Diffusion. Both the Authors Guild and The New York Times have sued Microsoft and OpenAI over the use of their works to train ChatGPT.\n\n\n==== Copyright of AI-generated content ====\nA separate question is whether AI-generated works can qualify for copyright protection. The United States Copyright Office has ruled that works created by artificial intelligence without any human input cannot be copyrighted, because they lack human authorship. Some legal professionals have suggested that Naruto v. Slater (2018), in which the U.S. 9th Circuit Court of Appeals held that non-humans cannot be copyright holders of artistic works, could be a potential precedent in copyright litigation over works created by generative AI. However, the office has also begun taking public input to determine if these rules need to be refined for generative AI.\nIn January 2025, the United States Copyright Office (USCO) released extensive guidance regarding the use of AI tools in the creative process, and established that \"...generative AI systems also offer tools that similarly allow users to exert control. [These] can enable the user to control the selection and placement of individual creative elements. Whether such modifications rise to the minimum standard of originality required under Feist will depend on a case-by-case determination. In those cases where they do, the output should be copyrightable\" Subsequently, the USCO registered the first visual artwork to be composed of entirely AI-generated materials, titled \"A Single Piece of American Cheese\".\n\n\n== Concerns ==\n\nThe development of generative AI has raised concerns from governments, businesses, and individuals, resulting in protests, legal actions, calls to pause AI experiments, and actions by multiple governments. In a July 2023 briefing of the United Nations Security Council, Secretary-General António Guterres stated \"Generative AI has enormous potential for good and evil at scale\", that AI may \"turbocharge global development\" and contribute between $10 and $15 trillion to the global economy by 2030, but that its malicious use \"could cause horrific levels of death and destruction, widespread trauma, and deep psychological damage on an unimaginable scale\". In addition, generative AI has a significant carbon footprint.\n\n\n=== Academic honesty ===\nGenerative AI can be used to generate and modify academic prose, to paraphrasing sources, and translate languages. The use of generative AI in a classroom setting can be a form of academic plagiarism. Some schools have banned ChatGPT and similar tools. \nA commonly proposed use for teachers is grading and giving feedback. Companies like Pearson and ETS use AI to score grammar, mechanics, usage, and style, but not for main ideas or overall structure. The National Council of Teachers of English says machine scoring makes students feel their writing isn't worth reading. AI scoring has also given unfair results for students from different ethnic backgrounds.\n\n\n=== Job losses ===\n\nFrom the early days of the development of AI, there have been arguments put forward by ELIZA creator Joseph Weizenbaum and others about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculations and qualitative, value-based judgements. In April 2023, it was reported that image generation AI has resulted in 70% of the jobs for video game illustrators in China being lost. In July 2023, developments in generative AI contributed to the 2023 Hollywood labor disputes. Fran Drescher, president of the Screen Actors Guild, declared that \"artificial intelligence poses an existential threat to creative professions\" during the 2023 SAG-AFTRA strike. Voice generation AI has been seen as a potential challenge to the voice acting sector.\nThe intersection of AI and employment concerns among underrepresented groups globally remains a critical facet. While AI promises efficiency enhancements and skill acquisition, concerns about job displacement and biased recruiting processes persist among these groups, as outlined in surveys by Fast Company. To leverage AI for a more equitable society, proactive steps encompass mitigating biases, advocating transparency, respecting privacy and consent, and embracing diverse teams and ethical considerations. Strategies involve redirecting policy emphasis on regulation, inclusive design, and education's potential for personalized teaching to maximize benefits while minimizing harms.\n\n\n=== Racial and gender bias ===\nGenerative AI models can reflect and amplify any cultural bias present in the underlying data. For example, a language model might assume that doctors and judges are male, and that secretaries or nurses are female, if those biases are common in the training data. Similarly, an image model prompted with the text \"a photo of a CEO\" might disproportionately generate images of white male CEOs, if trained on a racially biased data set. A number of methods for mitigating bias have been attempted, such as altering input prompts and reweighting training data.\n\n\n=== Deepfakes ===\n\nDeepfakes (a portmanteau of \"deep learning\" and \"fake\") are AI-generated media that take a person in an existing image or video and replace them with someone else's likeness using artificial neural networks. Deepfakes have garnered widespread attention and concerns for their uses in deepfake celebrity pornographic videos, revenge porn, fake news, hoaxes, health disinformation, financial fraud, and covert foreign election interference. This has elicited responses from both industry and government to detect and limit their use.\nIn July 2023, the fact-checking company Logically found that the popular generative AI models Midjourney, DALL-E 2 and Stable Diffusion would produce plausible disinformation images when prompted to do so, such as images of electoral fraud in the United States and Muslim women supporting India's Hindu nationalist Bharatiya Janata Party.\nIn April 2024, a paper proposed to use blockchain (distributed ledger technology) to promote \"transparency, verifiability, and decentralization in AI development and usage\".\n\n\n==== Audio deepfakes ====\n\nInstances of users abusing software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals have raised ethical concerns over voice generation AI. In response, companies such as ElevenLabs have stated that they would work on mitigating potential abuse through safeguards and identity verification.\nConcerns and fandoms have spawned from AI-generated music. The same software used to clone voices has been used on famous musicians' voices to create songs that mimic their voices, gaining both tremendous popularity and criticism. Similar techniques have also been used to create improved quality or full-length versions of songs that have been leaked or have yet to be released.\nGenerative AI has also been used to create new digital artist personalities, with some of these receiving enough attention to receive record deals at major labels. The developers of these virtual artists have also faced their fair share of criticism for their personified programs, including backlash for \"dehumanizing\" an artform, and also creating artists which create unrealistic or immoral appeals to their audiences.\n\n\n=== Illegal imagery ===\n\nMany websites that allow explicit AI generated images or videos have been created, and this has been used to create illegal content, such as rape, child sexual abuse material, necrophilia, and zoophilia.\n\n\n=== Cybercrime ===\nGenerative AI's ability to create realistic fake content has been exploited in numerous types of cybercrime, including phishing scams. Deepfake video and audio have been used to create disinformation and fraud. In 2020, former Google click fraud czar Shuman Ghosemajumder argued that once deepfake videos become perfectly realistic, they would stop appearing remarkable to viewers, potentially leading to uncritical acceptance of false information. Additionally, large language models and other forms of text-generation AI have been used to create fake reviews of e-commerce websites to boost ratings. Cybercriminals have created large language models focused on fraud, including WormGPT and FraudGPT.\nA 2023 study showed that generative AI can be vulnerable to jailbreaks, reverse psychology and prompt injection attacks, enabling attackers to obtain help with harmful requests, such as for crafting social engineering and phishing attacks. Additionally, other researchers have demonstrated that open-source models can be fine-tuned to remove their safety restrictions at low cost.\n\n\n=== Reliance on industry giants ===\nTraining frontier AI models requires an enormous amount of computing power. Usually only Big Tech companies have the financial resources to make such investments. Smaller start-ups such as Cohere and OpenAI end up buying access to data centers from Google and Microsoft respectively.\n\n\n=== Energy and environment ===\n\nAI has a significant carbon footprint due to growing energy consumption from both training and usage. Scientists and journalists have expressed concerns about the environmental impact that the development and deployment of generative models are having: high CO2 emissions, large amounts of freshwater used for data centers, and high amounts of electricity usage. There is also concern that these impacts may increase as these models are incorporated into widely used search engines such as Google Search and Bing, as chatbots and other applications become more popular, and as models need to be retrained.\nThe carbon footprint of generative AI globally is estimated to be growing steadily, with potential annual emissions ranging from 18.21 to 245.94 million tons of CO2 by 2035, with the highest estimates for 2035 nearing the impact of the United States beef industry on emissions (currently estimated to emit 257.5 million tons annually as of 2024).\nProposed mitigation strategies include factoring potential environmental costs prior to model development or data collection, increasing efficiency of data centers to reduce electricity/energy usage, building more efficient machine learning models, minimizing the number of times that models need to be retrained, developing a government-directed framework for auditing the environmental impact of these models, regulating for transparency of these models, regulating their energy and water usage, encouraging researchers to publish data on their models' carbon footprint, and increasing the number of subject matter experts who understand both machine learning and climate science.\n\n\n=== Content quality ===\n\nThe New York Times defines slop as analogous to spam: \"shoddy or unwanted A.I. content in social media, art, books, and ... in search results.\" Journalists have expressed concerns about the scale of low-quality generated content with respect to social media content moderation, the monetary incentives from social media companies to spread such content, false political messaging, spamming of scientific research paper submissions, increased time and effort to find higher quality or desired content on the Internet, the indexing of generated content by search engines, and on journalism itself.\nA paper published by researchers at Amazon Web Services AI Labs found that over 57% of sentences from a sample of over 6 billion sentences from Common Crawl, a snapshot of web pages, were machine translated. Many of these automated translations were seen as lower quality, especially for sentences that were translated into at least three languages. Many lower-resource languages (ex. Wolof, Xhosa) were translated across more languages than higher-resource languages (ex. English, French).\nIn September 2024, Robyn Speer, the author of wordfreq, an open source database that calculated word frequencies based on text from the Internet, announced that she had stopped updating the data for several reasons: high costs for obtaining data from Reddit and Twitter, excessive focus on generative AI compared to other methods in the natural language processing community, and that \"generative AI has polluted the data\".\nThe adoption of generative AI tools led to an explosion of AI-generated content across multiple domains. A study from University College London estimated that in 2023, more than 60,000 scholarly articles—over 1% of all publications—were likely written with LLM assistance. According to Stanford University's Institute for Human-Centered AI, approximately 17.5% of newly published computer science papers and 16.9% of peer review text now incorporate content generated by LLMs. Many academic disciplines have concerns about the factual reliability of academic content generated by AI.\nVisual content follows a similar trend. Since the launch of DALL-E 2 in 2022, it is estimated that an average of 34 million images have been created daily. As of August 2023, more than 15 billion images had been generated using text-to-image algorithms, with 80% of these created by models based on Stable Diffusion.\nIf AI-generated content is included in new data crawls from the Internet for additional training of AI models, defects in the resulting models may occur. Training an AI model exclusively on the output of another AI model produces a lower-quality model. Repeating this process, where each new model is trained on the previous model's output, leads to progressive degradation and eventually results in a \"model collapse\" after multiple iterations. Tests have been conducted with pattern recognition of handwritten letters and with pictures of human faces. As a consequence, the value of data collected from genuine human interactions with systems may become increasingly valuable in the presence of LLM-generated content in data crawled from the Internet.\nOn the other side, synthetic data is often used as an alternative to data produced by real-world events. Such data can be deployed to validate mathematical models and to train machine learning models while preserving user privacy, including for structured data. The approach is not limited to text generation; image generation has been employed to train computer vision models.\n\n\n=== Misuse in journalism ===\n\nGenerative AI's potential to generate a large amount of content with little effort is also affecting journalism. In January 2023, Futurism.com broke the story that CNET had been using an undisclosed internal AI tool to write at least 77 of its stories; after the news broke, CNET posted corrections to 41 of the stories. In April 2023, Die Aktuelle published an AI-generated fake interview of Michael Schumacher. In May 2024, Futurism noted that a content management system video by AdVon Commerce, which had used generative AI to produce articles for many of the aforementioned outlets, appeared to show that they \"had produced tens of thousands of articles for more than 150 publishers.\" In 2025, a report from the American Sunlight Project stated that Pravda network was publishing as many as 10,000 articles a day, and concluded that much of this content aimed to push Russian narratives into large language models through their training data.\nIn June 2024, Reuters Institute published its Digital News Report for 2024. In a survey of people in America and Europe, Reuters Institute reports that 52% and 47% respectively are uncomfortable with news produced by \"mostly AI with some human oversight\", and 23% and 15% respectively report being comfortable. 42% of Americans and 33% of Europeans reported that they were comfortable with news produced by \"mainly human with some help from AI\". The results of global surveys reported that people were more uncomfortable with news topics including politics (46%), crime (43%), and local news (37%) produced by AI than other news topics.\n\n\n== Detection and awareness ==\n\nOnline users have falsely assumed media of using generative artificial intelligence for content, such as video games Little Droid and Catly.\nDue to various concerns about citizens' unknowingly consuming generative AI media content, proponents argue for labeling such content to provide context. The Cyberspace Administration of China issued rules obligating service providers to labeling this content online.\nThe popularity of ChatGPT caused the emergence of tools that detect whether content was AI-generated, such as GPTZero, but the risk of false accusations (false positives) has remained a concern. Digital watermarking allows to reach high detection accuracy by subtly altering the generated content in a way that can be detected by software, but without being noticeable by users. OpenAI developed in 2023 a digital watermarking tool that allowed to detect content generated by ChatGPT with an estimated accuracy of 99.9%, when given enough text. But OpenAI chose not to release it, worrying that users would switch to competitor products, and arguing that digital watermarking can be circumvented by bad actors, for example with superficial rephrasing. Google's digital watermarking tool called SynthID was integrated in 2025 into products like Gemini, Imagen and Veo. Google also created the portal SynthID detector for users to check whether text, images or videos were produced with Google's generative AI products.\n\n\n== See also ==\n\nArtificial general intelligence – Type of AI with wide-ranging abilities\nArtificial imagination – Artificial simulation of human imagination\nArtificial intelligence art – Visual media created with AIPages displaying short descriptions of redirect targets\nArtificial life – Field of study\nChatbot – Program that simulates conversation\nComputational creativity – Multidisciplinary endeavour\nGenerative adversarial network – Deep learning method\nGenerative pre-trained transformer – Type of large language model\nLarge language model – Type of machine learning model\nLists of open-source artificial intelligence software\nMusic and artificial intelligence – Usage of artificial intelligence to generate music\nGenerative AI pornography – Explicit material produced by generative AI\nProcedural generation – Method in which data is created algorithmically as opposed to manually\nRetrieval-augmented generation – Type of information retrieval using LLMs\nStochastic parrot – Term used in machine learning\n\n\n== References ==\n\n\n== Further reading ==\nHe, Ran; Cao, Jie; Tan, Tieniu (2025). \"Generative Artificial Intelligence: A Historical Perspective\". National Science Review. 12 (5): nwaf050. doi:10.1093/nsr/nwaf050. PMC 11970245. PMID 40191253.\nJames Gleick, \"The Parrot in the Machine\" (review of Emily M. Bender and Alex Hanna, The AI Con: How to Fight Big Tech's Hype and Create the Future We Want, Harper, 274 pp.; and James Boyle, The Line: AI and the Future of Personhood, MIT Press, 326 pp.), The New York Review of Books, vol. LXXII, no. 12 (24 July 2025), pp. 43–46. \"[C]hatbox 'writing' has a bland, regurgitated quality. Textures are flattened, sharp edges are sanded. No chatbox could ever have said that April is the cruelest month or that fog comes on little cat feet (though they might now, because one of their chief skills is plagiarism). And when synthetically extruded text turns out wrong, it can be comically wrong. When a movie fan asked Google whether a certain actor was in Heat, he received this 'AI Overview': 'No, Angelina Jolie is not in heat.'\" (p. 44.)",
      "cleaned_text": "Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts. Generative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTXV and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu. Generative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works. Generative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. The first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is trained on a text corpus, it can then be used as a probabilistic text generator. Computers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings. The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft. Since its inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling. In 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images. In 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model. The new generative models introduced during this period allowed for large neural networks to be trained using unsupervised learning or semi-supervised learning, rather than the supervised learning typical of discriminative models. Unsupervised learning removed the need for humans to manually label data, allowing for larger networks to be trained. In March 2020, the release of 15.ai, a free web application created by an anonymous MIT researcher that could generate convincing character voices using minimal training data, marked one of the earliest popular use cases of generative AI. The platform is credited as the first mainstream service to popularize AI voice cloning (audio deepfakes) in memes and content creation, influencing subsequent developments in voice AI technology. In 2021, the emergence of DALL-E, a transformer-based pixel generative model, marked an advance in AI-generated imagery. This was followed by the releases of Midjourney and Stable Diffusion in 2022, which further democratized access to high-quality artificial intelligence art creation from natural language prompts. These systems demonstrated unprecedented capabilities in generating photorealistic images, artwork, and designs based on text descriptions, leading to widespread adoption among artists, designers, and the general public. In late 2022, the public release of ChatGPT revolutionized the accessibility and application of generative AI for general-purpose text-based tasks. The system's ability to engage in natural conversations, generate creative content, assist with coding, and perform various analytical tasks captured global attention and sparked widespread discussion about AI's potential impact on work, education, and creativity. In March 2023, GPT-4's release represented another jump in generative AI capabilities. A team from Microsoft Research controversially argued that it \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\" However, this assessment was contested by other scholars who maintained that generative AI remained \"still far from reaching the benchmark of 'general human intelligence'\" as of 2023. Later in 2023, Meta released ImageBind, an AI model combining multiple modalities including text, images, video, thermal data, 3D data, audio, and motion, paving the way for more immersive generative AI applications. In December 2023, Google unveiled Gemini, a multimodal AI model available in four versions: Ultra, Pro, Flash, and Nano. The company integrated Gemini Pro into its Bard chatbot and announced plans for \"Bard Advanced\" powered by the larger Gemini Ultra model. In February 2024, Google unified Bard and Duet AI under the Gemini brand, launching a mobile app on Android and integrating the service into the Google app on iOS. In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus. The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google. In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis. Asia-Pacific countries are significantly more optimistic than Western societies about generative AI and show higher adoption rates. Despite expressing concerns about privacy and the pace of change, in a 2024 survey, 68% of Asia-Pacific respondents believed that AI was having a positive impact on the world, compared to 57% globally. According to a survey by SAS and Coleman Parkes Research, China in particular has emerged as a global leader in generative AI adoption, with 83% of Chinese respondents using the technology, exceeding both the global average of 54% and the U.S. rate of 65%. This leadership is further evidenced by China's intellectual property developments in the field, with a UN report revealing that Chinese entities filed over 38,000 generative AI patents from 2014 to 2023, substantially surpassing the United States in patent applications. A 2024 survey on the Chinese social app Soul reported that 18% of respondents born after 2000 used generative AI \"almost every day\", and that over 60% of respondents like or love AI-generated content, while less than 3% dislike or hate it. Notable types of generative AI models include generative pre-trained transformers (GPTs), generative adversarial networks (GANs), and variational autoencoders (VAEs). Generative AI systems are multimodal if they can process multiple types of inputs or generate multiple types of outputs. For example, GPT-4o can both process and generate text, images and audio. Generative AI has made its appearance in a wide variety of industries, radically changing the dynamics of content creation, analysis, and delivery. In healthcare, for instance, generative AI accelerates drug discovery by creating molecular structures with target characteristics and generates radiology images for training diagnostic models. This ability not only enables faster and cheaper development but also enhances medical decision-making. In finance, generative AI services help create datasets and automate reports using natural language. It automates content creation, produces synthetic financial data, and tailors customer communications. It also powers chatbots and virtual agents. Collectively, these technologies enhance efficiency, reduce operational costs, and support data-driven decision-making in financial institutions. The media industry makes use of generative AI for numerous creative activities such as music composition, scriptwriting, video editing, and digital art. The educational sector is impacted as well, since the tools make learning personalized through creating quizzes, study aids, and essay composition. Both the teachers and the learners benefit from AI-based platforms that suit various learning patterns. In the educational field, in Colombia, student use of Meta's generative AI programs resulted in a decline in scores. Generative AI systems trained on words or word tokens include GPT-3, GPT-4, GPT-4o, LaMDA, LLaMA, BLOOM, Gemini, Claude and others (see List of large language models). They are capable of natural language processing, machine translation, and natural language generation and can be used as foundation models for other tasks. Data sets include BookCorpus, Wikipedia, and others (see List of text corpora). In addition to natural language text, large language models can be trained on programming language text, allowing them to generate source code for new computer programs. Examples include OpenAI Codex, Tabnine, GitHub Copilot, Microsoft Copilot, and VS Code fork Cursor. Some AI assistants help candidates cheat during online coding interviews by providing code, improvements, and explanations. Their clandestine interfaces minimize the need for eye movements that would expose cheating to the interviewer. Producing high-quality visual art is a prominent application of generative AI. Generative AI systems trained on sets of images with text captions include Imagen, DALL-E, Midjourney, Adobe Firefly, FLUX.1, Stable Diffusion and others (see Artificial intelligence art, Generative art, and Synthetic media). They are commonly used for text-to-image generation and neural style transfer. Datasets include LAION-5B and others (see List of datasets in computer vision and image processing). Generative AI can also be trained extensively on audio clips to produce natural-sounding speech synthesis and text-to-speech capabilities. An early pioneer in this field was 15.ai, launched in March 2020, which demonstrated the ability to clone character voices using as little as 15 seconds of training data. The website gained widespread attention for its ability to generate emotionally expressive speech for various fictional characters, though it was later taken offline in 2022 due to copyright concerns. Commercial alternatives subsequently emerged, including ElevenLabs' context-aware synthesis tools and Meta Platform's Voicebox. Generative AI systems such as MusicLM and MusicGen can also be trained on the audio waveforms of recorded music along with text annotations, in order to generate new musical samples based on text descriptions such as a calming violin melody backed by a distorted guitar riff. Audio deepfakes of music lyrics have been generated, like the song Savages, which used AI to mimic rapper Jay-Z's vocals. Music artist's instrumentals and lyrics are copyrighted but their voices are not protected from regenerative AI yet, raising a debate about whether artists should get royalties from audio deepfakes. Many AI music generators have been created that can be generated using a text phrase, genre options, and looped libraries of bars and riffs. Generative AI trained on annotated video can generate temporally-coherent, detailed and photorealistic video clips. Examples include Sora by OpenAI, Runway, Make-A-Video by Meta Platforms and the open source LTX Video by Lightricks . Generative AI can also be trained on the motions of a robotic system to generate new trajectories for motion planning or navigation. For example, UniPi from Google Research uses prompts like \"pick up blue bowl\" or \"wipe plate with yellow sponge\" to control movements of a robot arm. Multimodal vision-language-action models such as Google's RT-2 can perform rudimentary reasoning in response to user prompts and visual input, such as picking up a toy dinosaur when given the prompt pick up the extinct animal at a table filled with toy animals and other objects. Artificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate 3D modeling. AI-based CAD libraries could also be developed using linked open data of schematics and diagrams. AI CAD assistants are used as tools to help streamline workflow. Generative AI models are used to power chatbot products such as ChatGPT, programming tools such as GitHub Copilot, text-to-image products such as Midjourney, and text-to-video products such as Runway Gen-2. Generative AI features have been integrated into a variety of existing commercially available products such as Microsoft Office (Microsoft Copilot), Google Photos, and the Adobe Suite (Adobe Firefly). Many generative AI models are also available as open-source software, including Stable Diffusion and the LLaMA language model. Smaller generative AI models with up to a few billion parameters can run on smartphones, embedded devices, and personal computers. For example, LLaMA-7B (a version with 7 billion parameters) can run on a Raspberry Pi 4 and one version of Stable Diffusion can run on an iPhone 11. Larger models with tens of billions of parameters can run on laptop or desktop computers. To achieve an acceptable speed, models of this size may require accelerators such as the GPU chips produced by NVIDIA and AMD or the Neural Engine included in Apple silicon products. For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC. The advantages of running generative AI locally include protection of privacy and intellectual property, and avoidance of rate limiting and censorship. The subreddit r/LocalLLaMA in particular focuses on using consumer-grade gaming graphics cards through such techniques as compression. That forum is one of only two sources Andrej Karpathy trusts for language model benchmarks. Yann LeCun has advocated open-source models for their value to vertical applications and for improving AI safety. Language models with hundreds of billions of parameters, such as GPT-4 or PaLM, typically run on datacenter computers equipped with arrays of GPUs (such as NVIDIA's H100) or AI accelerator chips (such as Google's TPU). These very large models are typically accessed as cloud services over the Internet. In 2022, the United States New Export Controls on Advanced Computing and Semiconductors to China imposed restrictions on exports to China of GPU and AI accelerator chips used for generative AI. Chips such as the NVIDIA A800 and the Biren Technology BR104 were developed to meet the requirements of the sanctions. There is free software on the market capable of recognizing text generated by generative artificial intelligence (such as GPTZero), as well as images, audio or video coming from it. Potential mitigation strategies for detecting generative AI content include digital watermarking, content authentication, information retrieval, and machine learning classifier models. Despite claims of accuracy, both free and paid AI text detectors have frequently produced false positives, mistakenly accusing students of submitting AI-generated work. Generative adversarial networks (GANs) are an influential generative modeling technique. GANs consist of two neural networks-the generator and the discriminator-trained simultaneously in a competitive setting. The generator creates synthetic data by transforming random noise into samples that resemble the training dataset. The discriminator is trained to distinguish the authentic data from synthetic data produced by the generator. The two models engage in a minimax game: the generator aims to create increasingly realistic data to \"fool\" the discriminator, while the discriminator improves its ability to distinguish real from fake data. This continuous training setup enables the generator to produce high-quality and realistic outputs. Variational autoencoders (VAEs) are deep learning models that probabilistically encode data. They are typically used for tasks such as noise reduction from images, data compression, identifying unusual patterns, and facial recognition. Unlike standard autoencoders, which compress input data into a fixed latent representation, VAEs model the latent space as a probability distribution, allowing for smooth sampling and interpolation between data points. The encoder (\"recognition model\") maps input data to a latent space, producing means and variances that define a probability distribution. The decoder (\"generative model\") samples from this latent distribution and attempts to reconstruct the original input. VAEs optimize a loss function that includes both the reconstruction error and a Kullback-Leibler divergence term, which ensures the latent space follows a known prior distribution. VAEs are particularly suitable for tasks that require structured but smooth latent spaces, although they may create blurrier images than GANs. They are used for applications like image generation, data interpolation and anomaly detection. Transformers became the foundation for many powerful generative models, most notably the generative pre-trained transformer (GPT) series developed by OpenAI. They marked a major shift in natural language processing by replacing traditional recurrent and convolutional models. This architecture allows models to process entire sequences simultaneously and capture long-range dependencies more efficiently. The self-attention mechanism enables the model to capture the significance of every word in a sequence when predicting the subsequent word, thus improving its contextual understanding. Unlike recurrent neural networks, transformers process all the tokens in parallel, which improves the training efficiency and scalability. Transformers are typically pre-trained on enormous corpora in a self-supervised manner, prior to being fine-tuned. In the United States, a group of companies including OpenAI, Alphabet, and Meta signed a voluntary agreement with the Biden administration in July 2023 to watermark AI-generated content. In October 2023, Executive Order 14110 applied the Defense Production Act to require all US companies to report information to the federal government when training certain high-impact AI models. In the European Union, the proposed Artificial Intelligence Act includes requirements to disclose copyrighted material used to train generative AI systems, and to label any AI-generated output as such. In China, the Interim Measures for the Management of Generative AI Services introduced by the Cyberspace Administration of China regulates any public-facing generative AI. It includes requirements to watermark generated images or videos, regulations on training data and label quality, restrictions on personal data collection, and a guideline that generative AI services must \"adhere to socialist core values\". Generative AI systems such as ChatGPT and Midjourney are trained on large, publicly available datasets that include copyrighted works. AI developers have argued that such training is protected under fair use, while copyright holders have argued that it infringes their rights. Proponents of fair use training have argued that it is a transformative use and does not involve making copies of copyrighted works available to the public. Critics have argued that image generators such as Midjourney can create nearly-identical copies of some copyrighted images, and that generative AI programs compete with the content they are trained on. As of 2024, several lawsuits related to the use of copyrighted material in training are ongoing. Getty Images has sued Stability AI over the use of its images to train Stable Diffusion. Both the Authors Guild and The New York Times have sued Microsoft and OpenAI over the use of their works to train ChatGPT. A separate question is whether AI-generated works can qualify for copyright protection. The United States Copyright Office has ruled that works created by artificial intelligence without any human input cannot be copyrighted, because they lack human authorship. Some legal professionals have suggested that Naruto v. Slater (2018), in which the U.S. 9th Circuit Court of Appeals held that non-humans cannot be copyright holders of artistic works, could be a potential precedent in copyright litigation over works created by generative AI. However, the office has also begun taking public input to determine if these rules need to be refined for generative AI. In January 2025, the United States Copyright Office (USCO) released extensive guidance regarding the use of AI tools in the creative process, and established that \"...generative AI systems also offer tools that similarly allow users to exert control. [These] can enable the user to control the selection and placement of individual creative elements. Whether such modifications rise to the minimum standard of originality required under Feist will depend on a case-by-case determination. In those cases where they do, the output should be copyrightable\" Subsequently, the USCO registered the first visual artwork to be composed of entirely AI-generated materials, titled \"A Single Piece of American Cheese\". The development of generative AI has raised concerns from governments, businesses, and individuals, resulting in protests, legal actions, calls to pause AI experiments, and actions by multiple governments. In a July 2023 briefing of the United Nations Security Council, Secretary-General António Guterres stated \"Generative AI has enormous potential for good and evil at scale\", that AI may \"turbocharge global development\" and contribute between $10 and $15 trillion to the global economy by 2030, but that its malicious use \"could cause horrific levels of death and destruction, widespread trauma, and deep psychological damage on an unimaginable scale\". In addition, generative AI has a significant carbon footprint. Generative AI can be used to generate and modify academic prose, to paraphrasing sources, and translate languages. The use of generative AI in a classroom setting can be a form of academic plagiarism. Some schools have banned ChatGPT and similar tools. A commonly proposed use for teachers is grading and giving feedback. Companies like Pearson and ETS use AI to score grammar, mechanics, usage, and style, but not for main ideas or overall structure. The National Council of Teachers of English says machine scoring makes students feel their writing isn't worth reading. AI scoring has also given unfair results for students from different ethnic backgrounds. From the early days of the development of AI, there have been arguments put forward by ELIZA creator Joseph Weizenbaum and others about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculations and qualitative, value-based judgements. In April 2023, it was reported that image generation AI has resulted in 70% of the jobs for video game illustrators in China being lost. In July 2023, developments in generative AI contributed to the 2023 Hollywood labor disputes. Fran Drescher, president of the Screen Actors Guild, declared that \"artificial intelligence poses an existential threat to creative professions\" during the 2023 SAG-AFTRA strike. Voice generation AI has been seen as a potential challenge to the voice acting sector. The intersection of AI and employment concerns among underrepresented groups globally remains a critical facet. While AI promises efficiency enhancements and skill acquisition, concerns about job displacement and biased recruiting processes persist among these groups, as outlined in surveys by Fast Company. To leverage AI for a more equitable society, proactive steps encompass mitigating biases, advocating transparency, respecting privacy and consent, and embracing diverse teams and ethical considerations. Strategies involve redirecting policy emphasis on regulation, inclusive design, and education's potential for personalized teaching to maximize benefits while minimizing harms. Generative AI models can reflect and amplify any cultural bias present in the underlying data. For example, a language model might assume that doctors and judges are male, and that secretaries or nurses are female, if those biases are common in the training data. Similarly, an image model prompted with the text \"a photo of a CEO\" might disproportionately generate images of white male CEOs, if trained on a racially biased data set. A number of methods for mitigating bias have been attempted, such as altering input prompts and reweighting training data. Deepfakes (a portmanteau of \"deep learning\" and \"fake\") are AI-generated media that take a person in an existing image or video and replace them with someone else's likeness using artificial neural networks. Deepfakes have garnered widespread attention and concerns for their uses in deepfake celebrity pornographic videos, revenge porn, fake news, hoaxes, health disinformation, financial fraud, and covert foreign election interference. This has elicited responses from both industry and government to detect and limit their use. In July 2023, the fact-checking company Logically found that the popular generative AI models Midjourney, DALL-E 2 and Stable Diffusion would produce plausible disinformation images when prompted to do so, such as images of electoral fraud in the United States and Muslim women supporting India's Hindu nationalist Bharatiya Janata Party. In April 2024, a paper proposed to use blockchain (distributed ledger technology) to promote \"transparency, verifiability, and decentralization in AI development and usage\". Instances of users abusing software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals have raised ethical concerns over voice generation AI. In response, companies such as ElevenLabs have stated that they would work on mitigating potential abuse through safeguards and identity verification. Concerns and fandoms have spawned from AI-generated music. The same software used to clone voices has been used on famous musicians' voices to create songs that mimic their voices, gaining both tremendous popularity and criticism. Similar techniques have also been used to create improved quality or full-length versions of songs that have been leaked or have yet to be released. Generative AI has also been used to create new digital artist personalities, with some of these receiving enough attention to receive record deals at major labels. The developers of these virtual artists have also faced their fair share of criticism for their personified programs, including backlash for \"dehumanizing\" an artform, and also creating artists which create unrealistic or immoral appeals to their audiences. Many websites that allow explicit AI generated images or videos have been created, and this has been used to create illegal content, such as rape, child sexual abuse material, necrophilia, and zoophilia. Generative AI's ability to create realistic fake content has been exploited in numerous types of cybercrime, including phishing scams. Deepfake video and audio have been used to create disinformation and fraud. In 2020, former Google click fraud czar Shuman Ghosemajumder argued that once deepfake videos become perfectly realistic, they would stop appearing remarkable to viewers, potentially leading to uncritical acceptance of false information. Additionally, large language models and other forms of text-generation AI have been used to create fake reviews of e-commerce websites to boost ratings. Cybercriminals have created large language models focused on fraud, including WormGPT and FraudGPT. A 2023 study showed that generative AI can be vulnerable to jailbreaks, reverse psychology and prompt injection attacks, enabling attackers to obtain help with harmful requests, such as for crafting social engineering and phishing attacks. Additionally, other researchers have demonstrated that open-source models can be fine-tuned to remove their safety restrictions at low cost. Training frontier AI models requires an enormous amount of computing power. Usually only Big Tech companies have the financial resources to make such investments. Smaller start-ups such as Cohere and OpenAI end up buying access to data centers from Google and Microsoft respectively. AI has a significant carbon footprint due to growing energy consumption from both training and usage. Scientists and journalists have expressed concerns about the environmental impact that the development and deployment of generative models are having: high CO2 emissions, large amounts of freshwater used for data centers, and high amounts of electricity usage. There is also concern that these impacts may increase as these models are incorporated into widely used search engines such as Google Search and Bing, as chatbots and other applications become more popular, and as models need to be retrained. The carbon footprint of generative AI globally is estimated to be growing steadily, with potential annual emissions ranging from 18.21 to 245.94 million tons of CO2 by 2035, with the highest estimates for 2035 nearing the impact of the United States beef industry on emissions (currently estimated to emit 257.5 million tons annually as of 2024). Proposed mitigation strategies include factoring potential environmental costs prior to model development or data collection, increasing efficiency of data centers to reduce electricity/energy usage, building more efficient machine learning models, minimizing the number of times that models need to be retrained, developing a government-directed framework for auditing the environmental impact of these models, regulating for transparency of these models, regulating their energy and water usage, encouraging researchers to publish data on their models' carbon footprint, and increasing the number of subject matter experts who understand both machine learning and climate science. The New York Times defines slop as analogous to spam: \"shoddy or unwanted A.I. content in social media, art, books, and ... in search results.\" Journalists have expressed concerns about the scale of low-quality generated content with respect to social media content moderation, the monetary incentives from social media companies to spread such content, false political messaging, spamming of scientific research paper submissions, increased time and effort to find higher quality or desired content on the Internet, the indexing of generated content by search engines, and on journalism itself. A paper published by researchers at Amazon Web Services AI Labs found that over 57% of sentences from a sample of over 6 billion sentences from Common Crawl, a snapshot of web pages, were machine translated. Many of these automated translations were seen as lower quality, especially for sentences that were translated into at least three languages. Many lower-resource languages (ex. Wolof, Xhosa) were translated across more languages than higher-resource languages (ex. English, French). In September 2024, Robyn Speer, the author of wordfreq, an open source database that calculated word frequencies based on text from the Internet, announced that she had stopped updating the data for several reasons: high costs for obtaining data from Reddit and Twitter, excessive focus on generative AI compared to other methods in the natural language processing community, and that \"generative AI has polluted the data\". The adoption of generative AI tools led to an explosion of AI-generated content across multiple domains. A study from University College London estimated that in 2023, more than 60,000 scholarly articles-over 1% of all publications-were likely written with LLM assistance. According to Stanford University's Institute for Human-Centered AI, approximately 17.5% of newly published computer science papers and 16.9% of peer review text now incorporate content generated by LLMs. Many academic disciplines have concerns about the factual reliability of academic content generated by AI. Visual content follows a similar trend. Since the launch of DALL-E 2 in 2022, it is estimated that an average of 34 million images have been created daily. As of August 2023, more than 15 billion images had been generated using text-to-image algorithms, with 80% of these created by models based on Stable Diffusion. If AI-generated content is included in new data crawls from the Internet for additional training of AI models, defects in the resulting models may occur. Training an AI model exclusively on the output of another AI model produces a lower-quality model. Repeating this process, where each new model is trained on the previous model's output, leads to progressive degradation and eventually results in a \"model collapse\" after multiple iterations. Tests have been conducted with pattern recognition of handwritten letters and with pictures of human faces. As a consequence, the value of data collected from genuine human interactions with systems may become increasingly valuable in the presence of LLM-generated content in data crawled from the Internet. On the other side, synthetic data is often used as an alternative to data produced by real-world events. Such data can be deployed to validate mathematical models and to train machine learning models while preserving user privacy, including for structured data. The approach is not limited to text generation; image generation has been employed to train computer vision models. Generative AI's potential to generate a large amount of content with little effort is also affecting journalism. In January 2023, Futurism.com broke the story that CNET had been using an undisclosed internal AI tool to write at least 77 of its stories; after the news broke, CNET posted corrections to 41 of the stories. In April 2023, Die Aktuelle published an AI-generated fake interview of Michael Schumacher. In May 2024, Futurism noted that a content management system video by AdVon Commerce, which had used generative AI to produce articles for many of the aforementioned outlets, appeared to show that they \"had produced tens of thousands of articles for more than 150 publishers.\" In 2025, a report from the American Sunlight Project stated that Pravda network was publishing as many as 10,000 articles a day, and concluded that much of this content aimed to push Russian narratives into large language models through their training data. In June 2024, Reuters Institute published its Digital News Report for 2024. In a survey of people in America and Europe, Reuters Institute reports that 52% and 47% respectively are uncomfortable with news produced by \"mostly AI with some human oversight\", and 23% and 15% respectively report being comfortable. 42% of Americans and 33% of Europeans reported that they were comfortable with news produced by \"mainly human with some help from AI\". The results of global surveys reported that people were more uncomfortable with news topics including politics (46%), crime (43%), and local news (37%) produced by AI than other news topics. Online users have falsely assumed media of using generative artificial intelligence for content, such as video games Little Droid and Catly. Due to various concerns about citizens' unknowingly consuming generative AI media content, proponents argue for labeling such content to provide context. The Cyberspace Administration of China issued rules obligating service providers to labeling this content online. The popularity of ChatGPT caused the emergence of tools that detect whether content was AI-generated, such as GPTZero, but the risk of false accusations (false positives) has remained a concern. Digital watermarking allows to reach high detection accuracy by subtly altering the generated content in a way that can be detected by software, but without being noticeable by users. OpenAI developed in 2023 a digital watermarking tool that allowed to detect content generated by ChatGPT with an estimated accuracy of 99.9%, when given enough text. But OpenAI chose not to release it, worrying that users would switch to competitor products, and arguing that digital watermarking can be circumvented by bad actors, for example with superficial rephrasing. Google's digital watermarking tool called SynthID was integrated in 2025 into products like Gemini, Imagen and Veo. Google also created the portal SynthID detector for users to check whether text, images or videos were produced with Google's generative AI products.",
      "sentences": [
        "Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data.",
        "These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.",
        "Generative AI tools have become more common since the AI boom in the 2020s.",
        "This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs).",
        "Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTXV and Sora.",
        "Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.",
        "Generative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes.",
        "Even if used ethically, it may lead to mass replacement of human jobs.",
        "The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.",
        "Generative AI is used across many industries.",
        "Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.",
        "The first example of an algorithmically generated media is likely the Markov chain.",
        "Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century.",
        "Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains.",
        "Once a Markov chain is trained on a text corpus, it can then be used as a probabilistic text generator.",
        "Computers were needed to go beyond Markov chains.",
        "By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.",
        "The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal.",
        "Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s.",
        "They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.",
        "Since its inception, the field of machine learning has used both discriminative models and generative models to model and predict data.",
        "Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks.",
        "Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.",
        "In 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images.",
        "These deep generative models were the first to output not only class labels for images but also entire images.",
        "In 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018.",
        "This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.",
        "The new generative models introduced during this period allowed for large neural networks to be trained using unsupervised learning or semi-supervised learning, rather than the supervised learning typical of discriminative models.",
        "Unsupervised learning removed the need for humans to manually label data, allowing for larger networks to be trained.",
        "In March 2020, the release of 15.ai, a free web application created by an anonymous MIT researcher that could generate convincing character voices using minimal training data, marked one of the earliest popular use cases of generative AI.",
        "The platform is credited as the first mainstream service to popularize AI voice cloning (audio deepfakes) in memes and content creation, influencing subsequent developments in voice AI technology.",
        "In 2021, the emergence of DALL-E, a transformer-based pixel generative model, marked an advance in AI-generated imagery.",
        "This was followed by the releases of Midjourney and Stable Diffusion in 2022, which further democratized access to high-quality artificial intelligence art creation from natural language prompts.",
        "These systems demonstrated unprecedented capabilities in generating photorealistic images, artwork, and designs based on text descriptions, leading to widespread adoption among artists, designers, and the general public.",
        "In late 2022, the public release of ChatGPT revolutionized the accessibility and application of generative AI for general-purpose text-based tasks.",
        "The system's ability to engage in natural conversations, generate creative content, assist with coding, and perform various analytical tasks captured global attention and sparked widespread discussion about AI's potential impact on work, education, and creativity.",
        "In March 2023, GPT-4's release represented another jump in generative AI capabilities.",
        "A team from Microsoft Research controversially argued that it \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"",
        "However, this assessment was contested by other scholars who maintained that generative AI remained \"still far from reaching the benchmark of 'general human intelligence'\" as of 2023.",
        "Later in 2023, Meta released ImageBind, an AI model combining multiple modalities including text, images, video, thermal data, 3D data, audio, and motion, paving the way for more immersive generative AI applications.",
        "In December 2023, Google unveiled Gemini, a multimodal AI model available in four versions: Ultra, Pro, Flash, and Nano.",
        "The company integrated Gemini Pro into its Bard chatbot and announced plans for \"Bard Advanced\" powered by the larger Gemini Ultra model.",
        "In February 2024, Google unified Bard and Duet AI under the Gemini brand, launching a mobile app on Android and integrating the service into the Google app on iOS.",
        "In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus.",
        "The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google.",
        "In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.",
        "Asia-Pacific countries are significantly more optimistic than Western societies about generative AI and show higher adoption rates.",
        "Despite expressing concerns about privacy and the pace of change, in a 2024 survey, 68% of Asia-Pacific respondents believed that AI was having a positive impact on the world, compared to 57% globally.",
        "According to a survey by SAS and Coleman Parkes Research, China in particular has emerged as a global leader in generative AI adoption, with 83% of Chinese respondents using the technology, exceeding both the global average of 54% and the U.S. rate of 65%.",
        "This leadership is further evidenced by China's intellectual property developments in the field, with a UN report revealing that Chinese entities filed over 38,000 generative AI patents from 2014 to 2023, substantially surpassing the United States in patent applications.",
        "A 2024 survey on the Chinese social app Soul reported that 18% of respondents born after 2000 used generative AI \"almost every day\", and that over 60% of respondents like or love AI-generated content, while less than 3% dislike or hate it.",
        "Generative AI systems are multimodal if they can process multiple types of inputs or generate multiple types of outputs.",
        "For example, GPT-4o can both process and generate text, images and audio.",
        "Generative AI has made its appearance in a wide variety of industries, radically changing the dynamics of content creation, analysis, and delivery.",
        "In healthcare, for instance, generative AI accelerates drug discovery by creating molecular structures with target characteristics and generates radiology images for training diagnostic models.",
        "This ability not only enables faster and cheaper development but also enhances medical decision-making.",
        "In finance, generative AI services help create datasets and automate reports using natural language.",
        "It automates content creation, produces synthetic financial data, and tailors customer communications.",
        "It also powers chatbots and virtual agents.",
        "Collectively, these technologies enhance efficiency, reduce operational costs, and support data-driven decision-making in financial institutions.",
        "The media industry makes use of generative AI for numerous creative activities such as music composition, scriptwriting, video editing, and digital art.",
        "The educational sector is impacted as well, since the tools make learning personalized through creating quizzes, study aids, and essay composition.",
        "Both the teachers and the learners benefit from AI-based platforms that suit various learning patterns.",
        "In the educational field, in Colombia, student use of Meta's generative AI programs resulted in a decline in scores.",
        "Generative AI systems trained on words or word tokens include GPT-3, GPT-4, GPT-4o, LaMDA, LLaMA, BLOOM, Gemini, Claude and others (see List of large language models).",
        "They are capable of natural language processing, machine translation, and natural language generation and can be used as foundation models for other tasks.",
        "Data sets include BookCorpus, Wikipedia, and others (see List of text corpora).",
        "In addition to natural language text, large language models can be trained on programming language text, allowing them to generate source code for new computer programs.",
        "Examples include OpenAI Codex, Tabnine, GitHub Copilot, Microsoft Copilot, and VS Code fork Cursor.",
        "Some AI assistants help candidates cheat during online coding interviews by providing code, improvements, and explanations.",
        "Their clandestine interfaces minimize the need for eye movements that would expose cheating to the interviewer.",
        "Producing high-quality visual art is a prominent application of generative AI.",
        "Generative AI systems trained on sets of images with text captions include Imagen, DALL-E, Midjourney, Adobe Firefly, FLUX.1, Stable Diffusion and others (see Artificial intelligence art, Generative art, and Synthetic media).",
        "They are commonly used for text-to-image generation and neural style transfer.",
        "Datasets include LAION-5B and others (see List of datasets in computer vision and image processing).",
        "Generative AI can also be trained extensively on audio clips to produce natural-sounding speech synthesis and text-to-speech capabilities.",
        "An early pioneer in this field was 15.ai, launched in March 2020, which demonstrated the ability to clone character voices using as little as 15 seconds of training data.",
        "The website gained widespread attention for its ability to generate emotionally expressive speech for various fictional characters, though it was later taken offline in 2022 due to copyright concerns.",
        "Commercial alternatives subsequently emerged, including ElevenLabs' context-aware synthesis tools and Meta Platform's Voicebox.",
        "Generative AI systems such as MusicLM and MusicGen can also be trained on the audio waveforms of recorded music along with text annotations, in order to generate new musical samples based on text descriptions such as a calming violin melody backed by a distorted guitar riff.",
        "Audio deepfakes of music lyrics have been generated, like the song Savages, which used AI to mimic rapper Jay-Z's vocals.",
        "Music artist's instrumentals and lyrics are copyrighted but their voices are not protected from regenerative AI yet, raising a debate about whether artists should get royalties from audio deepfakes.",
        "Many AI music generators have been created that can be generated using a text phrase, genre options, and looped libraries of bars and riffs.",
        "Generative AI trained on annotated video can generate temporally-coherent, detailed and photorealistic video clips.",
        "Examples include Sora by OpenAI, Runway, Make-A-Video by Meta Platforms and the open source LTX Video by Lightricks .",
        "Generative AI can also be trained on the motions of a robotic system to generate new trajectories for motion planning or navigation.",
        "For example, UniPi from Google Research uses prompts like \"pick up blue bowl\" or \"wipe plate with yellow sponge\" to control movements of a robot arm.",
        "Multimodal vision-language-action models such as Google's RT-2 can perform rudimentary reasoning in response to user prompts and visual input, such as picking up a toy dinosaur when given the prompt pick up the extinct animal at a table filled with toy animals and other objects.",
        "Artificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate 3D modeling.",
        "AI-based CAD libraries could also be developed using linked open data of schematics and diagrams.",
        "AI CAD assistants are used as tools to help streamline workflow.",
        "Generative AI models are used to power chatbot products such as ChatGPT, programming tools such as GitHub Copilot, text-to-image products such as Midjourney, and text-to-video products such as Runway Gen-2.",
        "Generative AI features have been integrated into a variety of existing commercially available products such as Microsoft Office (Microsoft Copilot), Google Photos, and the Adobe Suite (Adobe Firefly).",
        "Many generative AI models are also available as open-source software, including Stable Diffusion and the LLaMA language model.",
        "Smaller generative AI models with up to a few billion parameters can run on smartphones, embedded devices, and personal computers.",
        "For example, LLaMA-7B (a version with 7 billion parameters) can run on a Raspberry Pi 4 and one version of Stable Diffusion can run on an iPhone 11.",
        "Larger models with tens of billions of parameters can run on laptop or desktop computers.",
        "To achieve an acceptable speed, models of this size may require accelerators such as the GPU chips produced by NVIDIA and AMD or the Neural Engine included in Apple silicon products.",
        "For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC.",
        "The advantages of running generative AI locally include protection of privacy and intellectual property, and avoidance of rate limiting and censorship.",
        "The subreddit r/LocalLLaMA in particular focuses on using consumer-grade gaming graphics cards through such techniques as compression.",
        "That forum is one of only two sources Andrej Karpathy trusts for language model benchmarks.",
        "Yann LeCun has advocated open-source models for their value to vertical applications and for improving AI safety.",
        "Language models with hundreds of billions of parameters, such as GPT-4 or PaLM, typically run on datacenter computers equipped with arrays of GPUs (such as NVIDIA's H100) or AI accelerator chips (such as Google's TPU).",
        "These very large models are typically accessed as cloud services over the Internet.",
        "In 2022, the United States New Export Controls on Advanced Computing and Semiconductors to China imposed restrictions on exports to China of GPU and AI accelerator chips used for generative AI.",
        "Chips such as the NVIDIA A800 and the Biren Technology BR104 were developed to meet the requirements of the sanctions.",
        "There is free software on the market capable of recognizing text generated by generative artificial intelligence (such as GPTZero), as well as images, audio or video coming from it.",
        "Potential mitigation strategies for detecting generative AI content include digital watermarking, content authentication, information retrieval, and machine learning classifier models.",
        "Despite claims of accuracy, both free and paid AI text detectors have frequently produced false positives, mistakenly accusing students of submitting AI-generated work.",
        "Generative adversarial networks (GANs) are an influential generative modeling technique.",
        "GANs consist of two neural networks-the generator and the discriminator-trained simultaneously in a competitive setting.",
        "The generator creates synthetic data by transforming random noise into samples that resemble the training dataset.",
        "The discriminator is trained to distinguish the authentic data from synthetic data produced by the generator.",
        "The two models engage in a minimax game: the generator aims to create increasingly realistic data to \"fool\" the discriminator, while the discriminator improves its ability to distinguish real from fake data.",
        "This continuous training setup enables the generator to produce high-quality and realistic outputs.",
        "Variational autoencoders (VAEs) are deep learning models that probabilistically encode data.",
        "They are typically used for tasks such as noise reduction from images, data compression, identifying unusual patterns, and facial recognition.",
        "Unlike standard autoencoders, which compress input data into a fixed latent representation, VAEs model the latent space as a probability distribution, allowing for smooth sampling and interpolation between data points.",
        "The encoder (\"recognition model\") maps input data to a latent space, producing means and variances that define a probability distribution.",
        "The decoder (\"generative model\") samples from this latent distribution and attempts to reconstruct the original input.",
        "VAEs optimize a loss function that includes both the reconstruction error and a Kullback-Leibler divergence term, which ensures the latent space follows a known prior distribution.",
        "VAEs are particularly suitable for tasks that require structured but smooth latent spaces, although they may create blurrier images than GANs.",
        "They are used for applications like image generation, data interpolation and anomaly detection.",
        "Transformers became the foundation for many powerful generative models, most notably the generative pre-trained transformer (GPT) series developed by OpenAI.",
        "They marked a major shift in natural language processing by replacing traditional recurrent and convolutional models.",
        "This architecture allows models to process entire sequences simultaneously and capture long-range dependencies more efficiently.",
        "The self-attention mechanism enables the model to capture the significance of every word in a sequence when predicting the subsequent word, thus improving its contextual understanding.",
        "Unlike recurrent neural networks, transformers process all the tokens in parallel, which improves the training efficiency and scalability.",
        "Transformers are typically pre-trained on enormous corpora in a self-supervised manner, prior to being fine-tuned.",
        "In the United States, a group of companies including OpenAI, Alphabet, and Meta signed a voluntary agreement with the Biden administration in July 2023 to watermark AI-generated content.",
        "In October 2023, Executive Order 14110 applied the Defense Production Act to require all US companies to report information to the federal government when training certain high-impact AI models.",
        "In the European Union, the proposed Artificial Intelligence Act includes requirements to disclose copyrighted material used to train generative AI systems, and to label any AI-generated output as such.",
        "In China, the Interim Measures for the Management of Generative AI Services introduced by the Cyberspace Administration of China regulates any public-facing generative AI.",
        "It includes requirements to watermark generated images or videos, regulations on training data and label quality, restrictions on personal data collection, and a guideline that generative AI services must \"adhere to socialist core values\".",
        "Generative AI systems such as ChatGPT and Midjourney are trained on large, publicly available datasets that include copyrighted works.",
        "AI developers have argued that such training is protected under fair use, while copyright holders have argued that it infringes their rights.",
        "Proponents of fair use training have argued that it is a transformative use and does not involve making copies of copyrighted works available to the public.",
        "Critics have argued that image generators such as Midjourney can create nearly-identical copies of some copyrighted images, and that generative AI programs compete with the content they are trained on.",
        "As of 2024, several lawsuits related to the use of copyrighted material in training are ongoing.",
        "Getty Images has sued Stability AI over the use of its images to train Stable Diffusion.",
        "Both the Authors Guild and The New York Times have sued Microsoft and OpenAI over the use of their works to train ChatGPT.",
        "A separate question is whether AI-generated works can qualify for copyright protection.",
        "The United States Copyright Office has ruled that works created by artificial intelligence without any human input cannot be copyrighted, because they lack human authorship.",
        "Some legal professionals have suggested that Naruto v. Slater (2018), in which the U.S. 9th Circuit Court of Appeals held that non-humans cannot be copyright holders of artistic works, could be a potential precedent in copyright litigation over works created by generative AI.",
        "However, the office has also begun taking public input to determine if these rules need to be refined for generative AI.",
        "In January 2025, the United States Copyright Office (USCO) released extensive guidance regarding the use of AI tools in the creative process, and established that \"...generative AI systems also offer tools that similarly allow users to exert control.",
        "[These] can enable the user to control the selection and placement of individual creative elements.",
        "Whether such modifications rise to the minimum standard of originality required under Feist will depend on a case-by-case determination.",
        "In those cases where they do, the output should be copyrightable\" Subsequently, the USCO registered the first visual artwork to be composed of entirely AI-generated materials, titled \"A Single Piece of American Cheese\".",
        "The development of generative AI has raised concerns from governments, businesses, and individuals, resulting in protests, legal actions, calls to pause AI experiments, and actions by multiple governments.",
        "In a July 2023 briefing of the United Nations Security Council, Secretary-General António Guterres stated \"Generative AI has enormous potential for good and evil at scale\", that AI may \"turbocharge global development\" and contribute between $10 and $15 trillion to the global economy by 2030, but that its malicious use \"could cause horrific levels of death and destruction, widespread trauma, and deep psychological damage on an unimaginable scale\".",
        "In addition, generative AI has a significant carbon footprint.",
        "Generative AI can be used to generate and modify academic prose, to paraphrasing sources, and translate languages.",
        "The use of generative AI in a classroom setting can be a form of academic plagiarism.",
        "Some schools have banned ChatGPT and similar tools.",
        "A commonly proposed use for teachers is grading and giving feedback.",
        "Companies like Pearson and ETS use AI to score grammar, mechanics, usage, and style, but not for main ideas or overall structure.",
        "The National Council of Teachers of English says machine scoring makes students feel their writing isn't worth reading.",
        "AI scoring has also given unfair results for students from different ethnic backgrounds.",
        "From the early days of the development of AI, there have been arguments put forward by ELIZA creator Joseph Weizenbaum and others about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculations and qualitative, value-based judgements.",
        "In April 2023, it was reported that image generation AI has resulted in 70% of the jobs for video game illustrators in China being lost.",
        "In July 2023, developments in generative AI contributed to the 2023 Hollywood labor disputes.",
        "Fran Drescher, president of the Screen Actors Guild, declared that \"artificial intelligence poses an existential threat to creative professions\" during the 2023 SAG-AFTRA strike.",
        "Voice generation AI has been seen as a potential challenge to the voice acting sector.",
        "The intersection of AI and employment concerns among underrepresented groups globally remains a critical facet.",
        "While AI promises efficiency enhancements and skill acquisition, concerns about job displacement and biased recruiting processes persist among these groups, as outlined in surveys by Fast Company.",
        "To leverage AI for a more equitable society, proactive steps encompass mitigating biases, advocating transparency, respecting privacy and consent, and embracing diverse teams and ethical considerations.",
        "Strategies involve redirecting policy emphasis on regulation, inclusive design, and education's potential for personalized teaching to maximize benefits while minimizing harms.",
        "Generative AI models can reflect and amplify any cultural bias present in the underlying data.",
        "For example, a language model might assume that doctors and judges are male, and that secretaries or nurses are female, if those biases are common in the training data.",
        "Similarly, an image model prompted with the text \"a photo of a CEO\" might disproportionately generate images of white male CEOs, if trained on a racially biased data set.",
        "A number of methods for mitigating bias have been attempted, such as altering input prompts and reweighting training data.",
        "Deepfakes (a portmanteau of \"deep learning\" and \"fake\") are AI-generated media that take a person in an existing image or video and replace them with someone else's likeness using artificial neural networks.",
        "Deepfakes have garnered widespread attention and concerns for their uses in deepfake celebrity pornographic videos, revenge porn, fake news, hoaxes, health disinformation, financial fraud, and covert foreign election interference.",
        "This has elicited responses from both industry and government to detect and limit their use.",
        "In July 2023, the fact-checking company Logically found that the popular generative AI models Midjourney, DALL-E 2 and Stable Diffusion would produce plausible disinformation images when prompted to do so, such as images of electoral fraud in the United States and Muslim women supporting India's Hindu nationalist Bharatiya Janata Party.",
        "In April 2024, a paper proposed to use blockchain (distributed ledger technology) to promote \"transparency, verifiability, and decentralization in AI development and usage\".",
        "Instances of users abusing software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals have raised ethical concerns over voice generation AI.",
        "In response, companies such as ElevenLabs have stated that they would work on mitigating potential abuse through safeguards and identity verification.",
        "Concerns and fandoms have spawned from AI-generated music.",
        "The same software used to clone voices has been used on famous musicians' voices to create songs that mimic their voices, gaining both tremendous popularity and criticism.",
        "Similar techniques have also been used to create improved quality or full-length versions of songs that have been leaked or have yet to be released.",
        "Generative AI has also been used to create new digital artist personalities, with some of these receiving enough attention to receive record deals at major labels.",
        "The developers of these virtual artists have also faced their fair share of criticism for their personified programs, including backlash for \"dehumanizing\" an artform, and also creating artists which create unrealistic or immoral appeals to their audiences.",
        "Many websites that allow explicit AI generated images or videos have been created, and this has been used to create illegal content, such as rape, child sexual abuse material, necrophilia, and zoophilia.",
        "Generative AI's ability to create realistic fake content has been exploited in numerous types of cybercrime, including phishing scams.",
        "Deepfake video and audio have been used to create disinformation and fraud.",
        "In 2020, former Google click fraud czar Shuman Ghosemajumder argued that once deepfake videos become perfectly realistic, they would stop appearing remarkable to viewers, potentially leading to uncritical acceptance of false information.",
        "Additionally, large language models and other forms of text-generation AI have been used to create fake reviews of e-commerce websites to boost ratings.",
        "Cybercriminals have created large language models focused on fraud, including WormGPT and FraudGPT.",
        "A 2023 study showed that generative AI can be vulnerable to jailbreaks, reverse psychology and prompt injection attacks, enabling attackers to obtain help with harmful requests, such as for crafting social engineering and phishing attacks.",
        "Additionally, other researchers have demonstrated that open-source models can be fine-tuned to remove their safety restrictions at low cost.",
        "Training frontier AI models requires an enormous amount of computing power.",
        "Usually only Big Tech companies have the financial resources to make such investments.",
        "Smaller start-ups such as Cohere and OpenAI end up buying access to data centers from Google and Microsoft respectively.",
        "AI has a significant carbon footprint due to growing energy consumption from both training and usage.",
        "Scientists and journalists have expressed concerns about the environmental impact that the development and deployment of generative models are having: high CO2 emissions, large amounts of freshwater used for data centers, and high amounts of electricity usage.",
        "There is also concern that these impacts may increase as these models are incorporated into widely used search engines such as Google Search and Bing, as chatbots and other applications become more popular, and as models need to be retrained.",
        "The carbon footprint of generative AI globally is estimated to be growing steadily, with potential annual emissions ranging from 18.21 to 245.94 million tons of CO2 by 2035, with the highest estimates for 2035 nearing the impact of the United States beef industry on emissions (currently estimated to emit 257.5 million tons annually as of 2024).",
        "The New York Times defines slop as analogous to spam: \"shoddy or unwanted A.I.",
        "content in social media, art, books, and ... in search results.\"",
        "Journalists have expressed concerns about the scale of low-quality generated content with respect to social media content moderation, the monetary incentives from social media companies to spread such content, false political messaging, spamming of scientific research paper submissions, increased time and effort to find higher quality or desired content on the Internet, the indexing of generated content by search engines, and on journalism itself.",
        "A paper published by researchers at Amazon Web Services AI Labs found that over 57% of sentences from a sample of over 6 billion sentences from Common Crawl, a snapshot of web pages, were machine translated.",
        "Many of these automated translations were seen as lower quality, especially for sentences that were translated into at least three languages.",
        "Many lower-resource languages (ex.",
        "Wolof, Xhosa) were translated across more languages than higher-resource languages (ex.",
        "English, French).",
        "In September 2024, Robyn Speer, the author of wordfreq, an open source database that calculated word frequencies based on text from the Internet, announced that she had stopped updating the data for several reasons: high costs for obtaining data from Reddit and Twitter, excessive focus on generative AI compared to other methods in the natural language processing community, and that \"generative AI has polluted the data\".",
        "The adoption of generative AI tools led to an explosion of AI-generated content across multiple domains.",
        "A study from University College London estimated that in 2023, more than 60,000 scholarly articles-over 1% of all publications-were likely written with LLM assistance.",
        "According to Stanford University's Institute for Human-Centered AI, approximately 17.5% of newly published computer science papers and 16.9% of peer review text now incorporate content generated by LLMs.",
        "Many academic disciplines have concerns about the factual reliability of academic content generated by AI.",
        "Visual content follows a similar trend.",
        "Since the launch of DALL-E 2 in 2022, it is estimated that an average of 34 million images have been created daily.",
        "As of August 2023, more than 15 billion images had been generated using text-to-image algorithms, with 80% of these created by models based on Stable Diffusion.",
        "If AI-generated content is included in new data crawls from the Internet for additional training of AI models, defects in the resulting models may occur.",
        "Training an AI model exclusively on the output of another AI model produces a lower-quality model.",
        "Repeating this process, where each new model is trained on the previous model's output, leads to progressive degradation and eventually results in a \"model collapse\" after multiple iterations.",
        "Tests have been conducted with pattern recognition of handwritten letters and with pictures of human faces.",
        "As a consequence, the value of data collected from genuine human interactions with systems may become increasingly valuable in the presence of LLM-generated content in data crawled from the Internet.",
        "On the other side, synthetic data is often used as an alternative to data produced by real-world events.",
        "Such data can be deployed to validate mathematical models and to train machine learning models while preserving user privacy, including for structured data.",
        "The approach is not limited to text generation; image generation has been employed to train computer vision models.",
        "Generative AI's potential to generate a large amount of content with little effort is also affecting journalism.",
        "In January 2023, Futurism.com broke the story that CNET had been using an undisclosed internal AI tool to write at least 77 of its stories; after the news broke, CNET posted corrections to 41 of the stories.",
        "In April 2023, Die Aktuelle published an AI-generated fake interview of Michael Schumacher.",
        "In May 2024, Futurism noted that a content management system video by AdVon Commerce, which had used generative AI to produce articles for many of the aforementioned outlets, appeared to show that they \"had produced tens of thousands of articles for more than 150 publishers.\"",
        "In 2025, a report from the American Sunlight Project stated that Pravda network was publishing as many as 10,000 articles a day, and concluded that much of this content aimed to push Russian narratives into large language models through their training data.",
        "In June 2024, Reuters Institute published its Digital News Report for 2024.",
        "In a survey of people in America and Europe, Reuters Institute reports that 52% and 47% respectively are uncomfortable with news produced by \"mostly AI with some human oversight\", and 23% and 15% respectively report being comfortable.",
        "42% of Americans and 33% of Europeans reported that they were comfortable with news produced by \"mainly human with some help from AI\".",
        "Online users have falsely assumed media of using generative artificial intelligence for content, such as video games Little Droid and Catly.",
        "Due to various concerns about citizens' unknowingly consuming generative AI media content, proponents argue for labeling such content to provide context.",
        "The Cyberspace Administration of China issued rules obligating service providers to labeling this content online.",
        "The popularity of ChatGPT caused the emergence of tools that detect whether content was AI-generated, such as GPTZero, but the risk of false accusations (false positives) has remained a concern.",
        "Digital watermarking allows to reach high detection accuracy by subtly altering the generated content in a way that can be detected by software, but without being noticeable by users.",
        "OpenAI developed in 2023 a digital watermarking tool that allowed to detect content generated by ChatGPT with an estimated accuracy of 99.9%, when given enough text.",
        "But OpenAI chose not to release it, worrying that users would switch to competitor products, and arguing that digital watermarking can be circumvented by bad actors, for example with superficial rephrasing.",
        "Google's digital watermarking tool called SynthID was integrated in 2025 into products like Gemini, Imagen and Veo.",
        "Google also created the portal SynthID detector for users to check whether text, images or videos were produced with Google's generative AI products."
      ],
      "metadata": {
        "title": "Generative artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Generative_artificial_intelligence",
        "word_count": 5679,
        "char_count": 38240,
        "sentence_count": 241,
        "scraped_at": "2025-08-09T14:46:47.063648",
        "language": "en",
        "processing_time": 0.007154941558837891,
        "source_hash": "3cc4d49651a5e2d754c0b9e14dadcfce"
      }
    },
    {
      "title": "A.I. Artificial Intelligence",
      "url": "https://en.wikipedia.org/wiki/A.I._Artificial_Intelligence",
      "raw_text": "A.I. Artificial Intelligence (or simply A.I.) is a 2001 American science fiction drama film directed by Steven Spielberg. The screenplay by Spielberg and screen story by Ian Watson are loosely based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. Set in a futuristic society, the film stars Haley Joel Osment as David, a childlike android uniquely programmed with the ability to love. Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt star in supporting roles.\nDevelopment of A.I. originally began after producer and director Stanley Kubrick acquired the rights to Aldiss's story in the early 1970s. Kubrick hired a series of writers, including Aldiss, Bob Shaw, Ian Watson and Sara Maitland, until the mid-1990s. The film languished in development hell for years, partly because Kubrick felt that computer-generated imagery was not advanced enough to create the David character, which he believed no child actor would convincingly portray. In 1995, Kubrick handed A.I. to Spielberg, but the film did not gain momentum until Kubrick died in 1999. Spielberg remained close to Watson's treatment for the screenplay and dedicated the film to Kubrick.\nA.I. Artificial Intelligence was released on June 29, 2001, by Warner Bros. Pictures in North America. It received generally positive reviews from critics and grossed $235.9 million against a budget of $90–100 million. It was also nominated for Best Visual Effects and Best Original Score (for John Williams) at the 74th Academy Awards. In a 2016 BBC poll of 177 critics around the world, A.I. Artificial Intelligence was voted the eighty-third greatest film since 2000. It has since been called one of Spielberg's best works and one of the greatest films of the 21st century, and of all time.\n\n\n== Plot ==\nIn the 22nd century, rising sea levels from global warming have wiped out coastal cities and altered the world's climate. With the human population in decline, advanced nations have created humanoid robots called mechas to fulfill various roles in society.\nIn Madison, New Jersey, David, an 11-year-old prototype mecha child capable of experiencing love, is given to Henry Swinton and his wife Monica, whose son Martin is in suspended animation after contracting a rare disease. Initially uncomfortable with David, Monica eventually warms to him and activates his imprinting protocol. Wanting her to love him in return, he befriends Teddy, Martin's old robotic teddy bear.\nAfter Martin is unexpectedly cured of his disease and brought home, he jealously goads David into cutting off a piece of Monica's hair. That night, David enters his adoptive parents' room, but as Monica turns over, the scissors accidentally poke her in the eye. While Henry attends to her wounds, Teddy picks up the lock of hair from the floor and places it in his pocket. During a pool party, one of Martin's friends pokes David with a knife, triggering his self-protection programming. David grabs Martin, causing both of them to fall into the pool. While Martin is rescued, David is accused of endangering others.\nHenry convinces Monica to return David to his creators for destruction. En route, she instead spares David by abandoning him in the woods full of scrap metal and obsolete mechas. Now accompanied solely by Teddy, David recalls The Adventures of Pinocchio and decides to find the Blue Fairy to become human, which he believes will regain Monica's love.\nDavid and Teddy are captured by the \"Flesh Fair\", a traveling circus-like event at which obsolete mechas are destroyed in front of jeering crowds. About to be destroyed himself, David pleads for his life, and the audience revolts and allows David to escape with Gigolo Joe, a prostitute mecha on the run after being framed for murder. David, Teddy and Joe go to the decadent resort town of Rouge City, where \"Dr. Know\", a holographic answer engine, directs them to the top of Rockefeller Center in the flooded ruins of New York City and provides fairy tale information that David interprets as suggesting that a Blue Fairy can help him.\nAbove the ruins of New York, David meets Professor Hobby, his creator, who tells him that their meeting demonstrates David's ability to love and desire. David finds copies of himself, including female variants called \"Darlene\", ready to be shipped. Disheartened by his lost sense of individuality, David attempts suicide by falling from a skyscraper into the ocean. While underwater, David notices a figure resembling the Blue Fairy before Joe rescues him in an amphibious aircraft. Before David can explain, authorities capture Joe with an electromagnet. David and Teddy take control of the aircraft to see the Blue Fairy, which turns out to be a statue from an attraction on Coney Island. The two become trapped when the Wonder Wheel falls on their vehicle. Believing that the Blue Fairy is real, David repeatedly asks the statue to turn him into a real boy until his power source is depleted.\nTwo thousand years later, humanity is extinct and Manhattan is buried under glacial ice. Mechas have evolved into an advanced form, and a group known as the Specialists, interested in humanity, find and resurrect David and Teddy. They reconstruct the Swinton family home from David's memories before explaining, via an interactive version of the Blue Fairy, that he cannot become human. However, they recreate Monica through genetic material from the strand of hair that Teddy kept. This version of Monica can live for only one day and cannot be revived. David spends his happiest day with Monica, and as she falls asleep in the evening, Monica tells David that she has always loved him. David lies down next to her and closes his eyes.\n\n\n== Cast ==\n\n\n== Production ==\n\n\n=== Development ===\nStanley Kubrick began development on an adaptation of \"Super-Toys Last All Summer Long\" in the late 1970s, hiring the story's author, Brian Aldiss, to write a film treatment. In 1985, Kubrick asked Steven Spielberg to direct the film, with Kubrick producing. Warner Bros. agreed to co-finance A.I. and cover distribution duties. The film labored in development hell, and Aldiss was fired by Kubrick over creative differences in 1989. Bob Shaw briefly served as writer, leaving after six weeks due to Kubrick's demanding work schedule, and Ian Watson was hired as the new writer in March 1990. Aldiss later remarked, \"Not only did the bastard fire me, he hired my enemy [Watson] instead.\" Kubrick handed Watson Carlo Collodi's The Adventures of Pinocchio for inspiration, calling A.I. \"a picaresque robot version of Pinocchio\".\nThree weeks later, Watson gave Kubrick his first story treatment, and concluded his work on A.I. in May 1991 with another treatment of 90 pages. Gigolo Joe was originally conceived as a G.I. mecha, but Watson suggested changing him to a male prostitute. Kubrick joked, \"I guess we lost the kiddie market.\" Meanwhile, Kubrick dropped A.I. to work on a film adaptation of Wartime Lies, feeling computer animation was not advanced enough to create the David character. After the release of Spielberg's Jurassic Park, with its innovative CGI, it was announced in November 1993 that production of A.I. would begin in 1994. Dennis Muren and Ned Gorman, who worked on Jurassic Park, became visual effects supervisors, but Kubrick was displeased with their previsualization, and with the expense of hiring Industrial Light & Magic (ILM) and Stan Winston Studio.\n\nKubrick asked Sara Maitland to give the film mythic resonance. She recalls \"He never referred to the film as 'A.I.'; he always called it 'Pinocchio.'\" Kubrick's version ended the same way Spielberg's does, with advanced mechas reviving Monica, but only for a day.\n\n\n=== Pre-production ===\nIn early 1994, the film was in pre-production with Christopher \"Fangorn\" Baker as concept artist and Sara Maitland assisting on the story, which gave it \"a feminist fairy-tale focus\". Maitland said that Kubrick never referred to the film as A.I., but as Pinocchio. Chris Cunningham became the new visual effects supervisor. Some of his unproduced work for A.I. can be seen on the DVD The Work of Director Chris Cunningham.\nAside from considering computer animation, Kubrick also had Joseph Mazzello do a screen test for the lead role. Cunningham helped assemble a series of \"little robot-type humans\" for the David character. \"We tried to construct a little boy with a movable rubber face to see whether we could make it look appealing,\" producer Jan Harlan reflected. \"But it was a total failure, it looked awful.\" Hans Moravec was brought in as a technical consultant. Meanwhile, Kubrick and Harlan thought that A.I. would be closer to Steven Spielberg's sensibilities as director. Kubrick handed the position to Spielberg in 1995, but Spielberg chose to direct other projects and convinced Kubrick to remain as director. The film was put on hold due to Kubrick's commitment to Eyes Wide Shut (1999).\nAfter Kubrick's death in March 1999, Harlan and Christiane Kubrick approached Spielberg to take over the director's position. By November 1999, Spielberg was writing the screenplay based on Watson's 90-page story treatment. It was his first solo screenplay credit since Close Encounters of the Third Kind (1977). Pre-production was briefly halted during February 2000 because Spielberg pondered directing other projects, which were Harry Potter and the Philosopher's Stone, Minority Report and Memoirs of a Geisha. The following month, Spielberg announced that A.I. would be his next project, with Minority Report as a follow-up. When he decided to fast track A.I., Spielberg brought back Chris Baker as concept artist. Ian Watson reported that the final script was very faithful to Kubrick's vision; even the ending, which is often attributed to Spielberg, saying, \"The final 20 minutes are pretty close to what I wrote for Stanley, and what Stanley wanted, faithfully filmed by Spielberg without added schmaltz\".\n\n\n=== Filming and visual effects ===\nThe original start date was July 10, 2000, but filming was delayed until August. Aside from a couple of weeks of shooting on location in Oxbow Regional Park in Oregon, A.I. was shot entirely using sound stages at Warner Bros. Studios and the Spruce Goose Dome in Long Beach, California.\nSpielberg copied Kubrick's obsessively secretive approach to filmmaking by refusing to give the complete script to cast and crew, banning press from the set, and making actors sign confidentiality agreements. For instance, Jack Angel, who voiced Teddy, recorded his lines entirely out of context, only receiving direction to sound like Eeyore from Winnie the Pooh, except \"very wise and old and stoic\". However, Spielberg asked Angel to be on the set every day to make line alterations wherever he felt necessary. Social robotics expert Cynthia Breazeal served as technical consultant during production. Costume designer Bob Ringwood studied pedestrians on the Las Vegas Strip for his influence on the Rouge City extras.\nVisual effects, such as removing the visible rods controlling Teddy and removing Haley Joel Osment's breath, were provided in-houses by PDI/DreamWorks.\n\n\n=== Casting ===\nJulianne Moore and Gwyneth Paltrow were considered for the role of Monica Swinton before Frances O'Connor was cast. Jerry Seinfeld was originally considered to voice and play the Comedian Robot before Chris Rock was cast.\n\n\n=== Allusions ===\nA. O. Scott notes Spielberg's homages to Kubrick, \"sly references to A Clockwork Orange, The Shining and predominantly 2001: A Space Odyssey\" as well as Collodi's Pinocchio. The lines Dr. Know quotes are from W. B. Yeats's \"The Stolen Child\":\n\n\n== Soundtrack ==\n\nThe film's soundtrack album was released by Warner Sunset Records in 2001. The original score was composed and conducted by John Williams, performed by the Hollywood Studio Symphony and features singers Lara Fabian on two songs and Josh Groban on one. The film's score also had a limited release as an official \"For your consideration Academy Promo\", as well as a complete score issued by La-La Land Records in 2015. The band Ministry appears in the film playing the song \"What About Us?\", but the song does not appear on the official soundtrack album.\nWilliams called his score an \"homage a Kubrick.\" He includes echoes of Gyorgy Ligeti's choral music, which Kubrick used in 2001: A Space Odyssey. Per Kubrick's request, Williams included a quotation of Richard Strauss's Der Rosenkavalier in his score.\n\n\n== Release ==\n\n\n=== Marketing ===\nThe teaser trailer debuted on December 8, 2000, with the theatrical release of Proof of Life. Warner Bros. used an alternate reality game titled The Beast to promote the film. Over forty websites were created by Atomic Pictures in New York City (kept online at Cloudmakers.org), including the website for Cybertronics Corp. There were to be a series of video games for the Xbox video game console that followed the storyline of The Beast, but they went undeveloped. To avoid audiences mistaking A.I. for a family film, no action figures were created, although Hasbro released a talking Teddy following the film's release in June 2001.\nA.I. premiered at the Venice Film Festival in 2001.\n\n\n=== Home media ===\nA.I. Artificial Intelligence was released on VHS and DVD in the United States by DreamWorks Home Entertainment on March 5, 2002 in widescreen and fullscreen two-disc special editions featuring an extensive sixteen-part documentary detailing the film's development, production, visual effects, sound design and music. The bonuses also include interviews with Haley Joel Osment, Jude Law, Frances O'Connor, Steven Spielberg and John Williams, two teaser trailers for the film's original theatrical release, and an extensive photo gallery featuring production stills and Stanley Kubrick's original storyboards. It was released overseas by Warner Home Video.\nThe film was released on Blu-ray in Japan by Warner Home Video on December 22, 2010, followed shortly by a United States release by Paramount Home Entertainment (Paramount currently owns the pre-2010 DreamWorks catalog) on April 5, 2011. This Blu-ray features the film remastered in high-definition and incorporates all the bonus features previously included on the two-disc special-edition DVD.\n\n\n== Reception ==\n\n\n=== Box office ===\nThe film opened in 3,242 theaters in the United States and Canada on June 29, 2001, earning $29.35 million at #1 during its opening weekend. A.I went on to gross $78.62 million in the United States and Canada. Opening on 524 screens in Japan, A.I. grossed almost two billion Yen in its first five days, the biggest June opening in Japan at the time, and sold more tickets in its opening weekend than Star Wars: Episode I – The Phantom Menace, although it grossed slightly less. It went on to gross $78 million in Japan. It grossed $79 million in other countries, for a worldwide total of $235.93 million.\n\n\n=== Critical response ===\nOn Rotten Tomatoes, A.I. Artificial Intelligence holds an approval rating of 76% based on reviews from 201 critics, with an average rating of 6.60/10. The website's critical consensus reads: \"A curious, not always seamless, amalgamation of Kubrick's chilly bleakness and Spielberg's warm-hearted optimism. A.I. is, in a word, fascinating.\" On Metacritic, it has a weighted average score of 65 out of 100 based on reviews from 32 critics, which indicates \"generally favorable reviews\". Audiences surveyed by CinemaScore gave the film an average grade of \"C+\" on a scale of A+ to F.\nProducer Jan Harlan stated that Kubrick \"would have applauded\" the final film, while Kubrick's widow Christiane also enjoyed A.I. Brian Aldiss admired the film as well: \"I thought what an inventive, intriguing, ingenious, involving film this was. There are flaws in it and I suppose I might have a personal quibble but it's so long since I wrote it.\" Of the film's ending, he wondered how it might have been had Kubrick directed the film: \"That is one of the 'ifs' of film history—at least the ending indicates Spielberg adding some sugar to Kubrick's wine. The actual ending is overly sympathetic and moreover rather overtly engineered by a plot device that does not really bear credence. But it's a brilliant piece of film and of course it's a phenomenon because it contains the energies and talents of two brilliant filmmakers.\"\nA. O. Scott writes: \"Mr. Spielberg seems to be attempting the improbable feat of melding Kubrick's chilly, analytical style with his own warmer, needier sensibility. He tells the story slowly and films it with lucid, mesmerizing objectivity, creating a mood as layered, dissonant and strange as John Williams's unusually restrained, modernist score.\" He concludes: \"The very end somehow fuses the cathartic comfort of infantile wish fulfillment -- the dream that the first perfect love whose loss we experience as the fall from Eden might be restored -- with a feeling almost too terrible to acknowledge or to name. Refusing to cuddle us or lull us into easy sleep, Mr. Spielberg locates the unspoken moral of all our fairy tales. To be real is to be mortal; to be human is to love, to dream and to perish.\"\nRichard Corliss of Time magazine heavily praised Spielberg's direction, as well as the cast and visual effects.\nRoger Ebert of the Chicago Sun-Times gave the film three stars out of a possible four, saying that it is \"wonderful and maddening\". Ebert later gave the film a full four stars and added it to his \"Great Movies\" canon in 2011.\nLeonard Maltin, on the other hand, gives the film two stars out of four in his Movie Guide, writing, \"[The] intriguing story draws us in, thanks in part to Osment's exceptional performance, but takes several wrong turns; ultimately, it just doesn't work. Spielberg rewrote the adaptation Stanley Kubrick commissioned of the Brian Aldiss short story Super Toys Last All Summer Long; [the] result is a curious and uncomfortable hybrid of Kubrick and Spielberg sensibilities.\" However, Maltin called John Williams's music score \"striking\".\nJonathan Rosenbaum of the Chicago Reader compared A.I. to Solaris (1972), and praised both \"Kubrick for proposing that Spielberg direct the project and Spielberg for doing his utmost to respect Kubrick's intentions while making it a profoundly personal work\". In 2009, he described A.I. as \"a very great and deeply misunderstood film\", noting that Andrew Sarris, Stan Brakhage and James Naremore \"more or less\" agreed with this assessment.\nFilm critic Armond White of the New York Press praised the film, noting that \"each part of David's journey through carnal and sexual universes into the final eschatological devastation becomes as profoundly philosophical and contemplative as anything by cinema's most thoughtful, speculative artists – Borzage, Ozu, Demy, Tarkovsky.\"\nFilmmaker Billy Wilder hailed A.I. as \"the most underrated film of the past few years\". When British filmmaker Ken Russell saw the film, he wept during the ending.\nScreenwriter Ian Watson has speculated, \"Worldwide, A.I. was very successful (and the 4th-highest earner of the year) but it didn't do quite so well in America, because the film, so I'm told, was too poetical and intellectual in general for American tastes. Plus, quite a few critics in America misunderstood the film, thinking for instance that the Giacometti-style beings in the final 20 minutes were aliens (whereas they were robots of the future who had evolved themselves from the robots in the earlier part of the film) and also thinking that the final 20 minutes were a sentimental addition by Spielberg, whereas those scenes were exactly what I wrote for Stanley and exactly what he wanted, filmed faithfully by Spielberg.\"\nMick LaSalle of the San Francisco Chronicle gave a largely negative review. \"A.I. exhibits all its creators' bad traits and none of the good. So we end up with the structureless, meandering, slow-motion endlessness of Kubrick combined with the fuzzy, cuddly mindlessness of Spielberg.\" Dubbing it Spielberg's \"first boring movie\", LaSalle also believed that the robots at the end of the film were aliens, and compared Gigolo Joe to the \"useless\" Jar Jar Binks, yet praised Robin Williams for his portrayal of a futuristic Albert Einstein.\nPeter Travers of Rolling Stone magazine gave a mixed review, concluding, \"Spielberg cannot live up to Kubrick's darker side of the future\", but still put the film on his top ten list that year.\nDavid Denby in The New Yorker criticized A.I. for not adhering closely to his concept of the Pinocchio character.\nSpielberg responded to some of the criticisms of the film, stating that many of the \"so called sentimental\" elements of A.I., including the ending, were in fact Kubrick's, and the darker elements were his own. However, Sara Maitland, who worked on the project with Kubrick in the 1990s, said that Kubrick never started production on A.I. because he had a hard time making the ending work.\nJames Berardinelli found the film \"consistently involving, with moments of near-brilliance, but far from a masterpiece. In fact, as the long-awaited 'collaboration' of Kubrick and Spielberg, it ranks as something of a disappointment.\" Of the film's highly debated finale, he claimed, \"There is no doubt that the concluding 30 minutes are all Spielberg; the outstanding question is where Kubrick's vision left off and Spielberg's began.\"\nJohn Simon of the National Review described A.I. \"as an uneasy mix of trauma and treacle\".\nIn 2002, Spielberg told film critic Joe Leydon, \"People pretend to think they know Stanley Kubrick, and think they know me, when most of them don't know either of us... And what's really funny about that is, all the parts of A.I. that people assume were Stanley's were mine. And all the parts of A.I. that people accuse me of sweetening and softening and sentimentalizing were all Stanley's. The teddy bear was Stanley's. The whole last 20 minutes of the movie was completely Stanley's. The whole first 35, 40 minutes of the film—all the stuff in the house—was word for word, from Stanley's screenplay. This was Stanley's vision... Eighty percent of the critics got it all mixed up. But I could see why. Because, obviously, I've done a lot of movies where people have cried and have been sentimental. And I've been accused of sentimentalizing hard-core material. But in fact it was Stanley who did the sweetest parts of A.I., not me. I'm the guy who did the dark center of the movie, with the Flesh Fair and everything else. That's why he wanted me to make the movie in the first place. He said, 'This is much closer to your sensibilities than my own.'\" Spielberg said, \"While there was divisiveness when A.I. came out, I felt that I had achieved Stanley's wishes, or goals.\"\nOn re-watching the film many years after its release, BBC film critic Mark Kermode apologized to Spielberg in a January 2013 interview for \"getting it wrong\" on the film when he first viewed it in 2001. He came to believe that the film is Spielberg's \"enduring masterpiece\".\nIn July 2025, it was one of the films voted for the \"Readers' Choice\" edition of The New York Times' list of \"The 100 Best Movies of the 21st Century,\" finishing at number 258. That same month, it ranked number 61 on Rolling Stone's list of \"The 100 Best Movies of the 21st Century.\"\n\n\n=== Accolades ===\nVisual effects supervisors Dennis Muren, Stan Winston, Michael Lantieri and Scott Farrar were nominated for the Academy Award for Best Visual Effects, and John Williams was nominated for Best Original Music Score. Steven Spielberg, Jude Law and Williams received nominations at the 59th Golden Globe Awards. A.I. was successful at the Saturn Awards, winning five awards, including Best Science Fiction Film along with Best Writing for Spielberg and Best Performance by a Younger Actor for Osment.\n\nAmerican Film Institute nominated the film in AFI's 100 Years of Film Scores.\n\n\n== See also ==\nList of underwater science fiction works\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nHarlan, Jan; Struthers, Jane M. (2009). A.I. Artificial Intelligence: From Stanley Kubrick to Steven Spielberg: The Vision Behind the Film. London: Thames & Hudson. ISBN 978-0-500514894.\nRice, Julian (2017). Kubrick's Story: Spielberg's Film: A.I. Artificial Intelligence. Rowman & Littlefield. ISBN 978-1-442278189.\n\n\n== External links ==\n\nOfficial website at the Wayback Machine (archived 2008-05-26)\nOfficial Warner Bros. Site\nA.I. Artificial Intelligence at IMDb\nA.I. Artificial Intelligence at Box Office Mojo\nA.I. Artificial Intelligence at Rotten Tomatoes",
      "cleaned_text": "A.I. Artificial Intelligence (or simply A.I.) is a 2001 American science fiction drama film directed by Steven Spielberg. The screenplay by Spielberg and screen story by Ian Watson are loosely based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. Set in a futuristic society, the film stars Haley Joel Osment as David, a childlike android uniquely programmed with the ability to love. Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt star in supporting roles. Development of A.I. originally began after producer and director Stanley Kubrick acquired the rights to Aldiss's story in the early 1970s. Kubrick hired a series of writers, including Aldiss, Bob Shaw, Ian Watson and Sara Maitland, until the mid-1990s. The film languished in development hell for years, partly because Kubrick felt that computer-generated imagery was not advanced enough to create the David character, which he believed no child actor would convincingly portray. In 1995, Kubrick handed A.I. to Spielberg, but the film did not gain momentum until Kubrick died in 1999. Spielberg remained close to Watson's treatment for the screenplay and dedicated the film to Kubrick. A.I. Artificial Intelligence was released on June 29, 2001, by Warner Bros. Pictures in North America. It received generally positive reviews from critics and grossed $235.9 million against a budget of $90-100 million. It was also nominated for Best Visual Effects and Best Original Score (for John Williams) at the 74th Academy Awards. In a 2016 BBC poll of 177 critics around the world, A.I. Artificial Intelligence was voted the eighty-third greatest film since 2000. It has since been called one of Spielberg's best works and one of the greatest films of the 21st century, and of all time. In the 22nd century, rising sea levels from global warming have wiped out coastal cities and altered the world's climate. With the human population in decline, advanced nations have created humanoid robots called mechas to fulfill various roles in society. In Madison, New Jersey, David, an 11-year-old prototype mecha child capable of experiencing love, is given to Henry Swinton and his wife Monica, whose son Martin is in suspended animation after contracting a rare disease. Initially uncomfortable with David, Monica eventually warms to him and activates his imprinting protocol. Wanting her to love him in return, he befriends Teddy, Martin's old robotic teddy bear. After Martin is unexpectedly cured of his disease and brought home, he jealously goads David into cutting off a piece of Monica's hair. That night, David enters his adoptive parents' room, but as Monica turns over, the scissors accidentally poke her in the eye. While Henry attends to her wounds, Teddy picks up the lock of hair from the floor and places it in his pocket. During a pool party, one of Martin's friends pokes David with a knife, triggering his self-protection programming. David grabs Martin, causing both of them to fall into the pool. While Martin is rescued, David is accused of endangering others. Henry convinces Monica to return David to his creators for destruction. En route, she instead spares David by abandoning him in the woods full of scrap metal and obsolete mechas. Now accompanied solely by Teddy, David recalls The Adventures of Pinocchio and decides to find the Blue Fairy to become human, which he believes will regain Monica's love. David and Teddy are captured by the \"Flesh Fair\", a traveling circus-like event at which obsolete mechas are destroyed in front of jeering crowds. About to be destroyed himself, David pleads for his life, and the audience revolts and allows David to escape with Gigolo Joe, a prostitute mecha on the run after being framed for murder. David, Teddy and Joe go to the decadent resort town of Rouge City, where \"Dr. Know\", a holographic answer engine, directs them to the top of Rockefeller Center in the flooded ruins of New York City and provides fairy tale information that David interprets as suggesting that a Blue Fairy can help him. Above the ruins of New York, David meets Professor Hobby, his creator, who tells him that their meeting demonstrates David's ability to love and desire. David finds copies of himself, including female variants called \"Darlene\", ready to be shipped. Disheartened by his lost sense of individuality, David attempts suicide by falling from a skyscraper into the ocean. While underwater, David notices a figure resembling the Blue Fairy before Joe rescues him in an amphibious aircraft. Before David can explain, authorities capture Joe with an electromagnet. David and Teddy take control of the aircraft to see the Blue Fairy, which turns out to be a statue from an attraction on Coney Island. The two become trapped when the Wonder Wheel falls on their vehicle. Believing that the Blue Fairy is real, David repeatedly asks the statue to turn him into a real boy until his power source is depleted. Two thousand years later, humanity is extinct and Manhattan is buried under glacial ice. Mechas have evolved into an advanced form, and a group known as the Specialists, interested in humanity, find and resurrect David and Teddy. They reconstruct the Swinton family home from David's memories before explaining, via an interactive version of the Blue Fairy, that he cannot become human. However, they recreate Monica through genetic material from the strand of hair that Teddy kept. This version of Monica can live for only one day and cannot be revived. David spends his happiest day with Monica, and as she falls asleep in the evening, Monica tells David that she has always loved him. David lies down next to her and closes his eyes. Stanley Kubrick began development on an adaptation of \"Super-Toys Last All Summer Long\" in the late 1970s, hiring the story's author, Brian Aldiss, to write a film treatment. In 1985, Kubrick asked Steven Spielberg to direct the film, with Kubrick producing. Warner Bros. agreed to co-finance A.I. and cover distribution duties. The film labored in development hell, and Aldiss was fired by Kubrick over creative differences in 1989. Bob Shaw briefly served as writer, leaving after six weeks due to Kubrick's demanding work schedule, and Ian Watson was hired as the new writer in March 1990. Aldiss later remarked, \"Not only did the bastard fire me, he hired my enemy [Watson] instead.\" Kubrick handed Watson Carlo Collodi's The Adventures of Pinocchio for inspiration, calling A.I. \"a picaresque robot version of Pinocchio\". Three weeks later, Watson gave Kubrick his first story treatment, and concluded his work on A.I. in May 1991 with another treatment of 90 pages. Gigolo Joe was originally conceived as a G.I. mecha, but Watson suggested changing him to a male prostitute. Kubrick joked, \"I guess we lost the kiddie market.\" Meanwhile, Kubrick dropped A.I. to work on a film adaptation of Wartime Lies, feeling computer animation was not advanced enough to create the David character. After the release of Spielberg's Jurassic Park, with its innovative CGI, it was announced in November 1993 that production of A.I. would begin in 1994. Dennis Muren and Ned Gorman, who worked on Jurassic Park, became visual effects supervisors, but Kubrick was displeased with their previsualization, and with the expense of hiring Industrial Light & Magic (ILM) and Stan Winston Studio. Kubrick asked Sara Maitland to give the film mythic resonance. She recalls \"He never referred to the film as 'A.I.'; he always called it 'Pinocchio.'\" Kubrick's version ended the same way Spielberg's does, with advanced mechas reviving Monica, but only for a day. In early 1994, the film was in pre-production with Christopher \"Fangorn\" Baker as concept artist and Sara Maitland assisting on the story, which gave it \"a feminist fairy-tale focus\". Maitland said that Kubrick never referred to the film as A.I., but as Pinocchio. Chris Cunningham became the new visual effects supervisor. Some of his unproduced work for A.I. can be seen on the DVD The Work of Director Chris Cunningham. Aside from considering computer animation, Kubrick also had Joseph Mazzello do a screen test for the lead role. Cunningham helped assemble a series of \"little robot-type humans\" for the David character. \"We tried to construct a little boy with a movable rubber face to see whether we could make it look appealing,\" producer Jan Harlan reflected. \"But it was a total failure, it looked awful.\" Hans Moravec was brought in as a technical consultant. Meanwhile, Kubrick and Harlan thought that A.I. would be closer to Steven Spielberg's sensibilities as director. Kubrick handed the position to Spielberg in 1995, but Spielberg chose to direct other projects and convinced Kubrick to remain as director. The film was put on hold due to Kubrick's commitment to Eyes Wide Shut (1999). After Kubrick's death in March 1999, Harlan and Christiane Kubrick approached Spielberg to take over the director's position. By November 1999, Spielberg was writing the screenplay based on Watson's 90-page story treatment. It was his first solo screenplay credit since Close Encounters of the Third Kind (1977). Pre-production was briefly halted during February 2000 because Spielberg pondered directing other projects, which were Harry Potter and the Philosopher's Stone, Minority Report and Memoirs of a Geisha. The following month, Spielberg announced that A.I. would be his next project, with Minority Report as a follow-up. When he decided to fast track A.I., Spielberg brought back Chris Baker as concept artist. Ian Watson reported that the final script was very faithful to Kubrick's vision; even the ending, which is often attributed to Spielberg, saying, \"The final 20 minutes are pretty close to what I wrote for Stanley, and what Stanley wanted, faithfully filmed by Spielberg without added schmaltz\". The original start date was July 10, 2000, but filming was delayed until August. Aside from a couple of weeks of shooting on location in Oxbow Regional Park in Oregon, A.I. was shot entirely using sound stages at Warner Bros. Studios and the Spruce Goose Dome in Long Beach, California. Spielberg copied Kubrick's obsessively secretive approach to filmmaking by refusing to give the complete script to cast and crew, banning press from the set, and making actors sign confidentiality agreements. For instance, Jack Angel, who voiced Teddy, recorded his lines entirely out of context, only receiving direction to sound like Eeyore from Winnie the Pooh, except \"very wise and old and stoic\". However, Spielberg asked Angel to be on the set every day to make line alterations wherever he felt necessary. Social robotics expert Cynthia Breazeal served as technical consultant during production. Costume designer Bob Ringwood studied pedestrians on the Las Vegas Strip for his influence on the Rouge City extras. Visual effects, such as removing the visible rods controlling Teddy and removing Haley Joel Osment's breath, were provided in-houses by PDI/DreamWorks. Julianne Moore and Gwyneth Paltrow were considered for the role of Monica Swinton before Frances O'Connor was cast. Jerry Seinfeld was originally considered to voice and play the Comedian Robot before Chris Rock was cast. A. O. Scott notes Spielberg's homages to Kubrick, \"sly references to A Clockwork Orange, The Shining and predominantly 2001: A Space Odyssey\" as well as Collodi's Pinocchio. The lines Dr. Know quotes are from W. B. Yeats's \"The Stolen Child\": The film's soundtrack album was released by Warner Sunset Records in 2001. The original score was composed and conducted by John Williams, performed by the Hollywood Studio Symphony and features singers Lara Fabian on two songs and Josh Groban on one. The film's score also had a limited release as an official \"For your consideration Academy Promo\", as well as a complete score issued by La-La Land Records in 2015. The band Ministry appears in the film playing the song \"What About Us?\", but the song does not appear on the official soundtrack album. Williams called his score an \"homage a Kubrick.\" He includes echoes of Gyorgy Ligeti's choral music, which Kubrick used in 2001: A Space Odyssey. Per Kubrick's request, Williams included a quotation of Richard Strauss's Der Rosenkavalier in his score. The teaser trailer debuted on December 8, 2000, with the theatrical release of Proof of Life. Warner Bros. used an alternate reality game titled The Beast to promote the film. Over forty websites were created by Atomic Pictures in New York City (kept online at Cloudmakers.org), including the website for Cybertronics Corp. There were to be a series of video games for the Xbox video game console that followed the storyline of The Beast, but they went undeveloped. To avoid audiences mistaking A.I. for a family film, no action figures were created, although Hasbro released a talking Teddy following the film's release in June 2001. A.I. premiered at the Venice Film Festival in 2001. A.I. Artificial Intelligence was released on VHS and DVD in the United States by DreamWorks Home Entertainment on March 5, 2002 in widescreen and fullscreen two-disc special editions featuring an extensive sixteen-part documentary detailing the film's development, production, visual effects, sound design and music. The bonuses also include interviews with Haley Joel Osment, Jude Law, Frances O'Connor, Steven Spielberg and John Williams, two teaser trailers for the film's original theatrical release, and an extensive photo gallery featuring production stills and Stanley Kubrick's original storyboards. It was released overseas by Warner Home Video. The film was released on Blu-ray in Japan by Warner Home Video on December 22, 2010, followed shortly by a United States release by Paramount Home Entertainment (Paramount currently owns the pre-2010 DreamWorks catalog) on April 5, 2011. This Blu-ray features the film remastered in high-definition and incorporates all the bonus features previously included on the two-disc special-edition DVD. The film opened in 3,242 theaters in the United States and Canada on June 29, 2001, earning $29.35 million at #1 during its opening weekend. A.I went on to gross $78.62 million in the United States and Canada. Opening on 524 screens in Japan, A.I. grossed almost two billion Yen in its first five days, the biggest June opening in Japan at the time, and sold more tickets in its opening weekend than Star Wars: Episode I - The Phantom Menace, although it grossed slightly less. It went on to gross $78 million in Japan. It grossed $79 million in other countries, for a worldwide total of $235.93 million. On Rotten Tomatoes, A.I. Artificial Intelligence holds an approval rating of 76% based on reviews from 201 critics, with an average rating of 6.60/10. The website's critical consensus reads: \"A curious, not always seamless, amalgamation of Kubrick's chilly bleakness and Spielberg's warm-hearted optimism. A.I. is, in a word, fascinating.\" On Metacritic, it has a weighted average score of 65 out of 100 based on reviews from 32 critics, which indicates \"generally favorable reviews\". Audiences surveyed by CinemaScore gave the film an average grade of \"C+\" on a scale of A+ to F. Producer Jan Harlan stated that Kubrick \"would have applauded\" the final film, while Kubrick's widow Christiane also enjoyed A.I. Brian Aldiss admired the film as well: \"I thought what an inventive, intriguing, ingenious, involving film this was. There are flaws in it and I suppose I might have a personal quibble but it's so long since I wrote it.\" Of the film's ending, he wondered how it might have been had Kubrick directed the film: \"That is one of the 'ifs' of film history-at least the ending indicates Spielberg adding some sugar to Kubrick's wine. The actual ending is overly sympathetic and moreover rather overtly engineered by a plot device that does not really bear credence. But it's a brilliant piece of film and of course it's a phenomenon because it contains the energies and talents of two brilliant filmmakers.\" A. O. Scott writes: \"Mr. Spielberg seems to be attempting the improbable feat of melding Kubrick's chilly, analytical style with his own warmer, needier sensibility. He tells the story slowly and films it with lucid, mesmerizing objectivity, creating a mood as layered, dissonant and strange as John Williams's unusually restrained, modernist score.\" He concludes: \"The very end somehow fuses the cathartic comfort of infantile wish fulfillment -- the dream that the first perfect love whose loss we experience as the fall from Eden might be restored -- with a feeling almost too terrible to acknowledge or to name. Refusing to cuddle us or lull us into easy sleep, Mr. Spielberg locates the unspoken moral of all our fairy tales. To be real is to be mortal; to be human is to love, to dream and to perish.\" Richard Corliss of Time magazine heavily praised Spielberg's direction, as well as the cast and visual effects. Roger Ebert of the Chicago Sun-Times gave the film three stars out of a possible four, saying that it is \"wonderful and maddening\". Ebert later gave the film a full four stars and added it to his \"Great Movies\" canon in 2011. Leonard Maltin, on the other hand, gives the film two stars out of four in his Movie Guide, writing, \"[The] intriguing story draws us in, thanks in part to Osment's exceptional performance, but takes several wrong turns; ultimately, it just doesn't work. Spielberg rewrote the adaptation Stanley Kubrick commissioned of the Brian Aldiss short story Super Toys Last All Summer Long; [the] result is a curious and uncomfortable hybrid of Kubrick and Spielberg sensibilities.\" However, Maltin called John Williams's music score \"striking\". Jonathan Rosenbaum of the Chicago Reader compared A.I. to Solaris (1972), and praised both \"Kubrick for proposing that Spielberg direct the project and Spielberg for doing his utmost to respect Kubrick's intentions while making it a profoundly personal work\". In 2009, he described A.I. as \"a very great and deeply misunderstood film\", noting that Andrew Sarris, Stan Brakhage and James Naremore \"more or less\" agreed with this assessment. Film critic Armond White of the New York Press praised the film, noting that \"each part of David's journey through carnal and sexual universes into the final eschatological devastation becomes as profoundly philosophical and contemplative as anything by cinema's most thoughtful, speculative artists - Borzage, Ozu, Demy, Tarkovsky.\" Filmmaker Billy Wilder hailed A.I. as \"the most underrated film of the past few years\". When British filmmaker Ken Russell saw the film, he wept during the ending. Screenwriter Ian Watson has speculated, \"Worldwide, A.I. was very successful (and the 4th-highest earner of the year) but it didn't do quite so well in America, because the film, so I'm told, was too poetical and intellectual in general for American tastes. Plus, quite a few critics in America misunderstood the film, thinking for instance that the Giacometti-style beings in the final 20 minutes were aliens (whereas they were robots of the future who had evolved themselves from the robots in the earlier part of the film) and also thinking that the final 20 minutes were a sentimental addition by Spielberg, whereas those scenes were exactly what I wrote for Stanley and exactly what he wanted, filmed faithfully by Spielberg.\" Mick LaSalle of the San Francisco Chronicle gave a largely negative review. \"A.I. exhibits all its creators' bad traits and none of the good. So we end up with the structureless, meandering, slow-motion endlessness of Kubrick combined with the fuzzy, cuddly mindlessness of Spielberg.\" Dubbing it Spielberg's \"first boring movie\", LaSalle also believed that the robots at the end of the film were aliens, and compared Gigolo Joe to the \"useless\" Jar Jar Binks, yet praised Robin Williams for his portrayal of a futuristic Albert Einstein. Peter Travers of Rolling Stone magazine gave a mixed review, concluding, \"Spielberg cannot live up to Kubrick's darker side of the future\", but still put the film on his top ten list that year. David Denby in The New Yorker criticized A.I. for not adhering closely to his concept of the Pinocchio character. Spielberg responded to some of the criticisms of the film, stating that many of the \"so called sentimental\" elements of A.I., including the ending, were in fact Kubrick's, and the darker elements were his own. However, Sara Maitland, who worked on the project with Kubrick in the 1990s, said that Kubrick never started production on A.I. because he had a hard time making the ending work. James Berardinelli found the film \"consistently involving, with moments of near-brilliance, but far from a masterpiece. In fact, as the long-awaited 'collaboration' of Kubrick and Spielberg, it ranks as something of a disappointment.\" Of the film's highly debated finale, he claimed, \"There is no doubt that the concluding 30 minutes are all Spielberg; the outstanding question is where Kubrick's vision left off and Spielberg's began.\" John Simon of the National Review described A.I. \"as an uneasy mix of trauma and treacle\". In 2002, Spielberg told film critic Joe Leydon, \"People pretend to think they know Stanley Kubrick, and think they know me, when most of them don't know either of us... And what's really funny about that is, all the parts of A.I. that people assume were Stanley's were mine. And all the parts of A.I. that people accuse me of sweetening and softening and sentimentalizing were all Stanley's. The teddy bear was Stanley's. The whole last 20 minutes of the movie was completely Stanley's. The whole first 35, 40 minutes of the film-all the stuff in the house-was word for word, from Stanley's screenplay. This was Stanley's vision... Eighty percent of the critics got it all mixed up. But I could see why. Because, obviously, I've done a lot of movies where people have cried and have been sentimental. And I've been accused of sentimentalizing hard-core material. But in fact it was Stanley who did the sweetest parts of A.I., not me. I'm the guy who did the dark center of the movie, with the Flesh Fair and everything else. That's why he wanted me to make the movie in the first place. He said, 'This is much closer to your sensibilities than my own.'\" Spielberg said, \"While there was divisiveness when A.I. came out, I felt that I had achieved Stanley's wishes, or goals.\" On re-watching the film many years after its release, BBC film critic Mark Kermode apologized to Spielberg in a January 2013 interview for \"getting it wrong\" on the film when he first viewed it in 2001. He came to believe that the film is Spielberg's \"enduring masterpiece\". In July 2025, it was one of the films voted for the \"Readers' Choice\" edition of The New York Times' list of \"The 100 Best Movies of the 21st Century,\" finishing at number 258. That same month, it ranked number 61 on Rolling Stone's list of \"The 100 Best Movies of the 21st Century.\" Visual effects supervisors Dennis Muren, Stan Winston, Michael Lantieri and Scott Farrar were nominated for the Academy Award for Best Visual Effects, and John Williams was nominated for Best Original Music Score. Steven Spielberg, Jude Law and Williams received nominations at the 59th Golden Globe Awards. A.I. was successful at the Saturn Awards, winning five awards, including Best Science Fiction Film along with Best Writing for Spielberg and Best Performance by a Younger Actor for Osment. American Film Institute nominated the film in AFI's 100 Years of Film Scores.",
      "sentences": [
        "Artificial Intelligence (or simply A.I.)",
        "is a 2001 American science fiction drama film directed by Steven Spielberg.",
        "The screenplay by Spielberg and screen story by Ian Watson are loosely based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss.",
        "Set in a futuristic society, the film stars Haley Joel Osment as David, a childlike android uniquely programmed with the ability to love.",
        "Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt star in supporting roles.",
        "Development of A.I.",
        "originally began after producer and director Stanley Kubrick acquired the rights to Aldiss's story in the early 1970s.",
        "Kubrick hired a series of writers, including Aldiss, Bob Shaw, Ian Watson and Sara Maitland, until the mid-1990s.",
        "The film languished in development hell for years, partly because Kubrick felt that computer-generated imagery was not advanced enough to create the David character, which he believed no child actor would convincingly portray.",
        "In 1995, Kubrick handed A.I.",
        "to Spielberg, but the film did not gain momentum until Kubrick died in 1999.",
        "Spielberg remained close to Watson's treatment for the screenplay and dedicated the film to Kubrick.",
        "Artificial Intelligence was released on June 29, 2001, by Warner Bros. Pictures in North America.",
        "It received generally positive reviews from critics and grossed $235.9 million against a budget of $90-100 million.",
        "It was also nominated for Best Visual Effects and Best Original Score (for John Williams) at the 74th Academy Awards.",
        "In a 2016 BBC poll of 177 critics around the world, A.I.",
        "Artificial Intelligence was voted the eighty-third greatest film since 2000.",
        "It has since been called one of Spielberg's best works and one of the greatest films of the 21st century, and of all time.",
        "In the 22nd century, rising sea levels from global warming have wiped out coastal cities and altered the world's climate.",
        "With the human population in decline, advanced nations have created humanoid robots called mechas to fulfill various roles in society.",
        "In Madison, New Jersey, David, an 11-year-old prototype mecha child capable of experiencing love, is given to Henry Swinton and his wife Monica, whose son Martin is in suspended animation after contracting a rare disease.",
        "Initially uncomfortable with David, Monica eventually warms to him and activates his imprinting protocol.",
        "Wanting her to love him in return, he befriends Teddy, Martin's old robotic teddy bear.",
        "After Martin is unexpectedly cured of his disease and brought home, he jealously goads David into cutting off a piece of Monica's hair.",
        "That night, David enters his adoptive parents' room, but as Monica turns over, the scissors accidentally poke her in the eye.",
        "While Henry attends to her wounds, Teddy picks up the lock of hair from the floor and places it in his pocket.",
        "During a pool party, one of Martin's friends pokes David with a knife, triggering his self-protection programming.",
        "David grabs Martin, causing both of them to fall into the pool.",
        "While Martin is rescued, David is accused of endangering others.",
        "Henry convinces Monica to return David to his creators for destruction.",
        "En route, she instead spares David by abandoning him in the woods full of scrap metal and obsolete mechas.",
        "Now accompanied solely by Teddy, David recalls The Adventures of Pinocchio and decides to find the Blue Fairy to become human, which he believes will regain Monica's love.",
        "David and Teddy are captured by the \"Flesh Fair\", a traveling circus-like event at which obsolete mechas are destroyed in front of jeering crowds.",
        "About to be destroyed himself, David pleads for his life, and the audience revolts and allows David to escape with Gigolo Joe, a prostitute mecha on the run after being framed for murder.",
        "David, Teddy and Joe go to the decadent resort town of Rouge City, where \"Dr. Know\", a holographic answer engine, directs them to the top of Rockefeller Center in the flooded ruins of New York City and provides fairy tale information that David interprets as suggesting that a Blue Fairy can help him.",
        "Above the ruins of New York, David meets Professor Hobby, his creator, who tells him that their meeting demonstrates David's ability to love and desire.",
        "David finds copies of himself, including female variants called \"Darlene\", ready to be shipped.",
        "Disheartened by his lost sense of individuality, David attempts suicide by falling from a skyscraper into the ocean.",
        "While underwater, David notices a figure resembling the Blue Fairy before Joe rescues him in an amphibious aircraft.",
        "Before David can explain, authorities capture Joe with an electromagnet.",
        "David and Teddy take control of the aircraft to see the Blue Fairy, which turns out to be a statue from an attraction on Coney Island.",
        "The two become trapped when the Wonder Wheel falls on their vehicle.",
        "Believing that the Blue Fairy is real, David repeatedly asks the statue to turn him into a real boy until his power source is depleted.",
        "Two thousand years later, humanity is extinct and Manhattan is buried under glacial ice.",
        "Mechas have evolved into an advanced form, and a group known as the Specialists, interested in humanity, find and resurrect David and Teddy.",
        "They reconstruct the Swinton family home from David's memories before explaining, via an interactive version of the Blue Fairy, that he cannot become human.",
        "However, they recreate Monica through genetic material from the strand of hair that Teddy kept.",
        "This version of Monica can live for only one day and cannot be revived.",
        "David spends his happiest day with Monica, and as she falls asleep in the evening, Monica tells David that she has always loved him.",
        "David lies down next to her and closes his eyes.",
        "Stanley Kubrick began development on an adaptation of \"Super-Toys Last All Summer Long\" in the late 1970s, hiring the story's author, Brian Aldiss, to write a film treatment.",
        "In 1985, Kubrick asked Steven Spielberg to direct the film, with Kubrick producing.",
        "Warner Bros. agreed to co-finance A.I.",
        "and cover distribution duties.",
        "The film labored in development hell, and Aldiss was fired by Kubrick over creative differences in 1989.",
        "Bob Shaw briefly served as writer, leaving after six weeks due to Kubrick's demanding work schedule, and Ian Watson was hired as the new writer in March 1990.",
        "Aldiss later remarked, \"Not only did the bastard fire me, he hired my enemy [Watson] instead.\"",
        "Kubrick handed Watson Carlo Collodi's The Adventures of Pinocchio for inspiration, calling A.I.",
        "\"a picaresque robot version of Pinocchio\".",
        "Three weeks later, Watson gave Kubrick his first story treatment, and concluded his work on A.I.",
        "in May 1991 with another treatment of 90 pages.",
        "Gigolo Joe was originally conceived as a G.I.",
        "mecha, but Watson suggested changing him to a male prostitute.",
        "Kubrick joked, \"I guess we lost the kiddie market.\"",
        "Meanwhile, Kubrick dropped A.I.",
        "to work on a film adaptation of Wartime Lies, feeling computer animation was not advanced enough to create the David character.",
        "After the release of Spielberg's Jurassic Park, with its innovative CGI, it was announced in November 1993 that production of A.I.",
        "would begin in 1994.",
        "Dennis Muren and Ned Gorman, who worked on Jurassic Park, became visual effects supervisors, but Kubrick was displeased with their previsualization, and with the expense of hiring Industrial Light & Magic (ILM) and Stan Winston Studio.",
        "Kubrick asked Sara Maitland to give the film mythic resonance.",
        "She recalls \"He never referred to the film as 'A.I.",
        "'; he always called it 'Pinocchio.'\"",
        "Kubrick's version ended the same way Spielberg's does, with advanced mechas reviving Monica, but only for a day.",
        "In early 1994, the film was in pre-production with Christopher \"Fangorn\" Baker as concept artist and Sara Maitland assisting on the story, which gave it \"a feminist fairy-tale focus\".",
        "Maitland said that Kubrick never referred to the film as A.I., but as Pinocchio.",
        "Chris Cunningham became the new visual effects supervisor.",
        "Some of his unproduced work for A.I.",
        "can be seen on the DVD The Work of Director Chris Cunningham.",
        "Aside from considering computer animation, Kubrick also had Joseph Mazzello do a screen test for the lead role.",
        "Cunningham helped assemble a series of \"little robot-type humans\" for the David character.",
        "\"We tried to construct a little boy with a movable rubber face to see whether we could make it look appealing,\" producer Jan Harlan reflected.",
        "\"But it was a total failure, it looked awful.\"",
        "Hans Moravec was brought in as a technical consultant.",
        "Meanwhile, Kubrick and Harlan thought that A.I.",
        "would be closer to Steven Spielberg's sensibilities as director.",
        "Kubrick handed the position to Spielberg in 1995, but Spielberg chose to direct other projects and convinced Kubrick to remain as director.",
        "The film was put on hold due to Kubrick's commitment to Eyes Wide Shut (1999).",
        "After Kubrick's death in March 1999, Harlan and Christiane Kubrick approached Spielberg to take over the director's position.",
        "By November 1999, Spielberg was writing the screenplay based on Watson's 90-page story treatment.",
        "It was his first solo screenplay credit since Close Encounters of the Third Kind (1977).",
        "Pre-production was briefly halted during February 2000 because Spielberg pondered directing other projects, which were Harry Potter and the Philosopher's Stone, Minority Report and Memoirs of a Geisha.",
        "The following month, Spielberg announced that A.I.",
        "would be his next project, with Minority Report as a follow-up.",
        "When he decided to fast track A.I., Spielberg brought back Chris Baker as concept artist.",
        "Ian Watson reported that the final script was very faithful to Kubrick's vision; even the ending, which is often attributed to Spielberg, saying, \"The final 20 minutes are pretty close to what I wrote for Stanley, and what Stanley wanted, faithfully filmed by Spielberg without added schmaltz\".",
        "The original start date was July 10, 2000, but filming was delayed until August.",
        "Aside from a couple of weeks of shooting on location in Oxbow Regional Park in Oregon, A.I.",
        "was shot entirely using sound stages at Warner Bros. Studios and the Spruce Goose Dome in Long Beach, California.",
        "Spielberg copied Kubrick's obsessively secretive approach to filmmaking by refusing to give the complete script to cast and crew, banning press from the set, and making actors sign confidentiality agreements.",
        "For instance, Jack Angel, who voiced Teddy, recorded his lines entirely out of context, only receiving direction to sound like Eeyore from Winnie the Pooh, except \"very wise and old and stoic\".",
        "However, Spielberg asked Angel to be on the set every day to make line alterations wherever he felt necessary.",
        "Social robotics expert Cynthia Breazeal served as technical consultant during production.",
        "Costume designer Bob Ringwood studied pedestrians on the Las Vegas Strip for his influence on the Rouge City extras.",
        "Visual effects, such as removing the visible rods controlling Teddy and removing Haley Joel Osment's breath, were provided in-houses by PDI/DreamWorks.",
        "Julianne Moore and Gwyneth Paltrow were considered for the role of Monica Swinton before Frances O'Connor was cast.",
        "Jerry Seinfeld was originally considered to voice and play the Comedian Robot before Chris Rock was cast.",
        "A. O. Scott notes Spielberg's homages to Kubrick, \"sly references to A Clockwork Orange, The Shining and predominantly 2001: A Space Odyssey\" as well as Collodi's Pinocchio.",
        "The lines Dr. Know quotes are from W. B. Yeats's \"The Stolen Child\": The film's soundtrack album was released by Warner Sunset Records in 2001.",
        "The original score was composed and conducted by John Williams, performed by the Hollywood Studio Symphony and features singers Lara Fabian on two songs and Josh Groban on one.",
        "The film's score also had a limited release as an official \"For your consideration Academy Promo\", as well as a complete score issued by La-La Land Records in 2015.",
        "The band Ministry appears in the film playing the song \"What About Us?",
        "\", but the song does not appear on the official soundtrack album.",
        "Williams called his score an \"homage a Kubrick.\"",
        "He includes echoes of Gyorgy Ligeti's choral music, which Kubrick used in 2001: A Space Odyssey.",
        "Per Kubrick's request, Williams included a quotation of Richard Strauss's Der Rosenkavalier in his score.",
        "The teaser trailer debuted on December 8, 2000, with the theatrical release of Proof of Life.",
        "Warner Bros. used an alternate reality game titled The Beast to promote the film.",
        "Over forty websites were created by Atomic Pictures in New York City (kept online at Cloudmakers.org), including the website for Cybertronics Corp.",
        "There were to be a series of video games for the Xbox video game console that followed the storyline of The Beast, but they went undeveloped.",
        "To avoid audiences mistaking A.I.",
        "for a family film, no action figures were created, although Hasbro released a talking Teddy following the film's release in June 2001.",
        "premiered at the Venice Film Festival in 2001.",
        "Artificial Intelligence was released on VHS and DVD in the United States by DreamWorks Home Entertainment on March 5, 2002 in widescreen and fullscreen two-disc special editions featuring an extensive sixteen-part documentary detailing the film's development, production, visual effects, sound design and music.",
        "The bonuses also include interviews with Haley Joel Osment, Jude Law, Frances O'Connor, Steven Spielberg and John Williams, two teaser trailers for the film's original theatrical release, and an extensive photo gallery featuring production stills and Stanley Kubrick's original storyboards.",
        "It was released overseas by Warner Home Video.",
        "The film was released on Blu-ray in Japan by Warner Home Video on December 22, 2010, followed shortly by a United States release by Paramount Home Entertainment (Paramount currently owns the pre-2010 DreamWorks catalog) on April 5, 2011.",
        "This Blu-ray features the film remastered in high-definition and incorporates all the bonus features previously included on the two-disc special-edition DVD.",
        "The film opened in 3,242 theaters in the United States and Canada on June 29, 2001, earning $29.35 million at #1 during its opening weekend.",
        "A.I went on to gross $78.62 million in the United States and Canada.",
        "Opening on 524 screens in Japan, A.I.",
        "grossed almost two billion Yen in its first five days, the biggest June opening in Japan at the time, and sold more tickets in its opening weekend than Star Wars: Episode I - The Phantom Menace, although it grossed slightly less.",
        "It went on to gross $78 million in Japan.",
        "It grossed $79 million in other countries, for a worldwide total of $235.93 million.",
        "On Rotten Tomatoes, A.I.",
        "Artificial Intelligence holds an approval rating of 76% based on reviews from 201 critics, with an average rating of 6.60/10.",
        "The website's critical consensus reads: \"A curious, not always seamless, amalgamation of Kubrick's chilly bleakness and Spielberg's warm-hearted optimism.",
        "is, in a word, fascinating.\"",
        "On Metacritic, it has a weighted average score of 65 out of 100 based on reviews from 32 critics, which indicates \"generally favorable reviews\".",
        "Audiences surveyed by CinemaScore gave the film an average grade of \"C+\" on a scale of A+ to F. Producer Jan Harlan stated that Kubrick \"would have applauded\" the final film, while Kubrick's widow Christiane also enjoyed A.I.",
        "Brian Aldiss admired the film as well: \"I thought what an inventive, intriguing, ingenious, involving film this was.",
        "There are flaws in it and I suppose I might have a personal quibble but it's so long since I wrote it.\"",
        "Of the film's ending, he wondered how it might have been had Kubrick directed the film: \"That is one of the 'ifs' of film history-at least the ending indicates Spielberg adding some sugar to Kubrick's wine.",
        "The actual ending is overly sympathetic and moreover rather overtly engineered by a plot device that does not really bear credence.",
        "But it's a brilliant piece of film and of course it's a phenomenon because it contains the energies and talents of two brilliant filmmakers.\"",
        "A. O. Scott writes: \"Mr. Spielberg seems to be attempting the improbable feat of melding Kubrick's chilly, analytical style with his own warmer, needier sensibility.",
        "He tells the story slowly and films it with lucid, mesmerizing objectivity, creating a mood as layered, dissonant and strange as John Williams's unusually restrained, modernist score.\"",
        "He concludes: \"The very end somehow fuses the cathartic comfort of infantile wish fulfillment -- the dream that the first perfect love whose loss we experience as the fall from Eden might be restored -- with a feeling almost too terrible to acknowledge or to name.",
        "Refusing to cuddle us or lull us into easy sleep, Mr. Spielberg locates the unspoken moral of all our fairy tales.",
        "To be real is to be mortal; to be human is to love, to dream and to perish.\"",
        "Richard Corliss of Time magazine heavily praised Spielberg's direction, as well as the cast and visual effects.",
        "Roger Ebert of the Chicago Sun-Times gave the film three stars out of a possible four, saying that it is \"wonderful and maddening\".",
        "Ebert later gave the film a full four stars and added it to his \"Great Movies\" canon in 2011.",
        "Leonard Maltin, on the other hand, gives the film two stars out of four in his Movie Guide, writing, \"[The] intriguing story draws us in, thanks in part to Osment's exceptional performance, but takes several wrong turns; ultimately, it just doesn't work.",
        "Spielberg rewrote the adaptation Stanley Kubrick commissioned of the Brian Aldiss short story Super Toys Last All Summer Long; [the] result is a curious and uncomfortable hybrid of Kubrick and Spielberg sensibilities.\"",
        "However, Maltin called John Williams's music score \"striking\".",
        "Jonathan Rosenbaum of the Chicago Reader compared A.I.",
        "to Solaris (1972), and praised both \"Kubrick for proposing that Spielberg direct the project and Spielberg for doing his utmost to respect Kubrick's intentions while making it a profoundly personal work\".",
        "In 2009, he described A.I.",
        "as \"a very great and deeply misunderstood film\", noting that Andrew Sarris, Stan Brakhage and James Naremore \"more or less\" agreed with this assessment.",
        "Film critic Armond White of the New York Press praised the film, noting that \"each part of David's journey through carnal and sexual universes into the final eschatological devastation becomes as profoundly philosophical and contemplative as anything by cinema's most thoughtful, speculative artists - Borzage, Ozu, Demy, Tarkovsky.\"",
        "Filmmaker Billy Wilder hailed A.I.",
        "as \"the most underrated film of the past few years\".",
        "When British filmmaker Ken Russell saw the film, he wept during the ending.",
        "Screenwriter Ian Watson has speculated, \"Worldwide, A.I.",
        "was very successful (and the 4th-highest earner of the year) but it didn't do quite so well in America, because the film, so I'm told, was too poetical and intellectual in general for American tastes.",
        "Plus, quite a few critics in America misunderstood the film, thinking for instance that the Giacometti-style beings in the final 20 minutes were aliens (whereas they were robots of the future who had evolved themselves from the robots in the earlier part of the film) and also thinking that the final 20 minutes were a sentimental addition by Spielberg, whereas those scenes were exactly what I wrote for Stanley and exactly what he wanted, filmed faithfully by Spielberg.\"",
        "Mick LaSalle of the San Francisco Chronicle gave a largely negative review.",
        "exhibits all its creators' bad traits and none of the good.",
        "So we end up with the structureless, meandering, slow-motion endlessness of Kubrick combined with the fuzzy, cuddly mindlessness of Spielberg.\"",
        "Dubbing it Spielberg's \"first boring movie\", LaSalle also believed that the robots at the end of the film were aliens, and compared Gigolo Joe to the \"useless\" Jar Jar Binks, yet praised Robin Williams for his portrayal of a futuristic Albert Einstein.",
        "Peter Travers of Rolling Stone magazine gave a mixed review, concluding, \"Spielberg cannot live up to Kubrick's darker side of the future\", but still put the film on his top ten list that year.",
        "David Denby in The New Yorker criticized A.I.",
        "for not adhering closely to his concept of the Pinocchio character.",
        "Spielberg responded to some of the criticisms of the film, stating that many of the \"so called sentimental\" elements of A.I., including the ending, were in fact Kubrick's, and the darker elements were his own.",
        "However, Sara Maitland, who worked on the project with Kubrick in the 1990s, said that Kubrick never started production on A.I.",
        "because he had a hard time making the ending work.",
        "James Berardinelli found the film \"consistently involving, with moments of near-brilliance, but far from a masterpiece.",
        "In fact, as the long-awaited 'collaboration' of Kubrick and Spielberg, it ranks as something of a disappointment.\"",
        "Of the film's highly debated finale, he claimed, \"There is no doubt that the concluding 30 minutes are all Spielberg; the outstanding question is where Kubrick's vision left off and Spielberg's began.\"",
        "John Simon of the National Review described A.I.",
        "\"as an uneasy mix of trauma and treacle\".",
        "In 2002, Spielberg told film critic Joe Leydon, \"People pretend to think they know Stanley Kubrick, and think they know me, when most of them don't know either of us... And what's really funny about that is, all the parts of A.I.",
        "that people assume were Stanley's were mine.",
        "And all the parts of A.I.",
        "that people accuse me of sweetening and softening and sentimentalizing were all Stanley's.",
        "The teddy bear was Stanley's.",
        "The whole last 20 minutes of the movie was completely Stanley's.",
        "The whole first 35, 40 minutes of the film-all the stuff in the house-was word for word, from Stanley's screenplay.",
        "This was Stanley's vision... Eighty percent of the critics got it all mixed up.",
        "But I could see why.",
        "Because, obviously, I've done a lot of movies where people have cried and have been sentimental.",
        "And I've been accused of sentimentalizing hard-core material.",
        "But in fact it was Stanley who did the sweetest parts of A.I., not me.",
        "I'm the guy who did the dark center of the movie, with the Flesh Fair and everything else.",
        "That's why he wanted me to make the movie in the first place.",
        "He said, 'This is much closer to your sensibilities than my own.'\"",
        "Spielberg said, \"While there was divisiveness when A.I.",
        "came out, I felt that I had achieved Stanley's wishes, or goals.\"",
        "On re-watching the film many years after its release, BBC film critic Mark Kermode apologized to Spielberg in a January 2013 interview for \"getting it wrong\" on the film when he first viewed it in 2001.",
        "He came to believe that the film is Spielberg's \"enduring masterpiece\".",
        "In July 2025, it was one of the films voted for the \"Readers' Choice\" edition of The New York Times' list of \"The 100 Best Movies of the 21st Century,\" finishing at number 258.",
        "That same month, it ranked number 61 on Rolling Stone's list of \"The 100 Best Movies of the 21st Century.\"",
        "Visual effects supervisors Dennis Muren, Stan Winston, Michael Lantieri and Scott Farrar were nominated for the Academy Award for Best Visual Effects, and John Williams was nominated for Best Original Music Score.",
        "Steven Spielberg, Jude Law and Williams received nominations at the 59th Golden Globe Awards.",
        "was successful at the Saturn Awards, winning five awards, including Best Science Fiction Film along with Best Writing for Spielberg and Best Performance by a Younger Actor for Osment.",
        "American Film Institute nominated the film in AFI's 100 Years of Film Scores."
      ],
      "metadata": {
        "title": "A.I. Artificial Intelligence",
        "url": "https://en.wikipedia.org/wiki/A.I._Artificial_Intelligence",
        "word_count": 3837,
        "char_count": 23565,
        "sentence_count": 206,
        "scraped_at": "2025-08-09T14:46:47.069504",
        "language": "en",
        "processing_time": 0.005594015121459961,
        "source_hash": "4ba7e58534b42eba9d89ade4e9fa5162"
      }
    },
    {
      "title": "History of artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/History_of_artificial_intelligence",
      "raw_text": "The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain.\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College in 1956. Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars with the hope of making this vision come true.\nEventually, it became obvious that researchers had grossly underestimated the difficulty of this feat. In 1974, criticism from James Lighthill and pressure from the U.S.A. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success of expert systems  reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an \"AI winter\"). Nevertheless, research and funding continued to grow under other names.\nIn the early 2000s, machine learning was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications, amongst other use cases.\nInvestment in AI boomed in the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of large language models (LLMs) like ChatGPT. These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks and ethical implications of advanced AI have also emerged, causing debate about the future of AI and its impact on society.\n\n\n== Precursors ==\n\n\n=== Mythical, fictional, and speculative precursors ===\n\n\n==== Myth and legend ====\nIn Greek mythology, Talos was a creature made of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily. According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos. In the Argonautica, Jason and the Argonauts defeated Talos by removing a plug near his foot, causing the vital ichor to flow out from his body and rendering him lifeless.\nPygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved.\n\n\n==== Medieval legends of artificial beings ====\n\nIn Of the Nature of Things, the Swiss alchemist Paracelsus describes a procedure that he claims can fabricate an \"artificial man\". By placing the \"sperm of a man\" in horse dung, and feeding it the \"Arcanum of Mans blood\" after 40 days, the concoction will become a living infant.\nThe earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century. During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God's names on it, into the mouth of the clay figure. Unlike legendary automata like Brazen Heads, a Golem was unable to speak.\nTakwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals.\nIn Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.\n\n\n==== Modern fiction ====\n\nBy the 19th century, ideas about artificial men and thinking machines became a popular theme in fiction. Notable works like Mary Shelley's Frankenstein  and Karel Čapek's R.U.R. (Rossum's Universal Robots)\nexplored the concept of artificial life. Speculative essays, such as Samuel Butler's \"Darwin among the Machines\", and Edgar Allan Poe's \"Maelzel's Chess Player\" reflected society's growing interest in machines with artificial intelligence. AI remains a common topic in science fiction today.\n\n\n==== Automata ====\n\nRealistic humanoid automata were built by craftsman from many civilizations, including Yan Shi, Hero of Alexandria, Al-Jazari, Haroun al-Rashid, Jacques de Vaucanson, Leonardo Torres y Quevedo, Pierre Jaquet-Droz and Wolfgang von Kempelen.\nThe oldest known automata were the sacred statues of ancient Egypt and Greece. The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion—Hermes Trismegistus wrote that \"by discovering the true nature of the gods, man has been able to reproduce it\". English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.\nDuring the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-Protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard. These legends were similar to the Norse myth of the Head of Mímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War. Odin is said to have \"embalmed\" the head with herbs and spoke incantations over it such that Mímir's head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.\n\n\n=== Formal reasoning ===\nArtificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or \"formal\"—reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction by the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism), Euclid (whose Elements was a model of formal reasoning), al-Khwārizmī (who developed algebra and gave his name to the word algorithm) and European scholastic philosophers such as William of Ockham and Duns Scotus.\nSpanish philosopher Ramon Llull (1232–1315) developed several logical machines devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.\n\nIn the 17th century, Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry. Hobbes famously wrote in Leviathan: \"For reason ... is nothing but reckoning, that is adding and subtracting\". Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that \"there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate.\" These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.\nThe study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift. Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: \"can all of mathematical reasoning be formalized?\" His question was answered by Gödel's incompleteness proof, Turing's machine and Church's Lambda calculus.\n\nTheir answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction. The key insight was the Turing machine—a simple theoretical construct that captured the essence of abstract symbol manipulation. This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.\n\n\n=== Computer science ===\n\nCalculating machines were designed or built in antiquity and throughout history by many people, including \nGottfried Leibniz,\nJoseph Marie Jacquard, \nCharles Babbage,\nPercy Ludgate,\nLeonardo Torres Quevedo,\nVannevar Bush,\nand others. Ada Lovelace speculated that Babbage's machine was \"a thinking or ... reasoning machine\", but warned \"It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers\" of the machine.\nThe first modern computers were the massive machines of the Second World War (such as Konrad Zuse's Z3, Alan Turing's Heath Robinson and Colossus, Atanasoff and Berry's ABC and ENIAC at the University of Pennsylvania). ENIAC was based on the theoretical foundation laid by Alan Turing and developed by John von Neumann, and proved to be the most influential.\n\n\n== Birth of artificial intelligence (1941-56) ==\n\nThe earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an \"electronic brain\".\nIn the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research. Alan Turing was among the first people to seriously investigate the theoretical possibility of \"machine intelligence\". The field of \"artificial intelligence research\" was founded as an academic discipline in 1956.\n\n\n=== Turing Test ===\n\nIn 1950 Turing published a landmark paper \"Computing Machinery and Intelligence\", in which he speculated about the possibility of creating machines that think. In the paper, he noted that \"thinking\" is difficult to define and devised his famous Turing Test: If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was \"thinking\". This simplified version of the problem allowed Turing to argue convincingly that a \"thinking machine\" was at least plausible and the paper answered all the most common objections to the proposition. The Turing Test was the first serious proposal in the philosophy of artificial intelligence.\n\n\n=== Neuroscience and Hebbian theory ===\nDonald Hebb was a Canadian psychologist whose work laid the foundation for modern neuroscience, particularly in understanding learning, memory, and neural plasticity. His most influential book, The Organization of Behavior (1949), introduced the concept of Hebbian learning, often summarized as \"cells that fire together wire together.\"\n\nHebb began formulating the foundational ideas for this book in the early 1940s, particularly during his time at the Yerkes Laboratories of Primate Biology from 1942 to 1947. He made extensive notes between June 1944 and March 1945 and sent a complete draft to his mentor Karl Lashley in 1946. The manuscript for The Organization of Behavior wasn’t published until 1949. The delay was due to various factors, including World War II and shifts in academic focus. By the time it was published, several of his peers had already published related ideas, making Hebb’s work seem less groundbreaking at first glance. However, his synthesis of psychological and neurophysiological principles became a cornerstone of neuroscience and machine learning.\n\n\n=== Artificial neural networks ===\nWalter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network. The paper was influenced by Turing's paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function. One of the students inspired by Pitts and McCulloch was Marvin Minsky who was a 24-year-old graduate student at the time. In 1951 Minsky and Dean Edmonds built the first neural net machine, the SNARC. Minsky would later become one of the most important leaders and innovators in AI.\n\n\n=== Cybernetic robots ===\nExperimental robots such as W. Grey Walter's turtles and the Johns Hopkins Beast, were built in the 1950s. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.\n\n\n=== Game AI ===\nIn 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. Arthur Samuel's checkers program, the subject of his 1959 paper \"Some Studies in Machine Learning Using the Game of Checkers\", eventually achieved sufficient skill to challenge a respectable amateur. Samuel's program was among the first uses of what would later be called machine learning. Game AI would continue to be used as a measure of progress in AI throughout its history.\n\n\n=== Symbolic reasoning and the Logic Theorist ===\n\nWhen access to digital computers became possible in the mid-fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.\nIn 1955, Allen Newell and future Nobel Laureate Herbert A. Simon created the \"Logic Theorist\", with help from J. C. Shaw. The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some. Simon said that they had \"solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.\" The symbolic reasoning paradigm they introduced would dominate AI research and funding until the middle 90s, as well as inspire the cognitive revolution.\n\n\n=== Dartmouth Workshop ===\n\nThe Dartmouth workshop of 1956 was a pivotal event that marked the formal inception of AI as an academic discipline. It was organized by Marvin Minsky and John McCarthy, with the support of two senior scientists Claude Shannon and Nathan Rochester of IBM. The proposal for the conference stated they intended to test the assertion that \"every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it\". The term \"Artificial Intelligence\" was introduced by John McCarthy at the workshop. \nThe participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research. At the workshop Newell and Simon debuted the \"Logic Theorist\". The workshop was the moment that AI gained its name, its mission, its first major success and its key players, and is widely considered the birth of AI.\n\n\n=== Cognitive revolution ===\n\nIn the autumn of 1956, Newell and Simon also presented the Logic Theorist at a meeting of the Special Interest Group in Information Theory at the Massachusetts Institute of Technology (MIT). At the same meeting, Noam Chomsky discussed his generative grammar, and George Miller described his landmark paper \"The Magical Number Seven, Plus or Minus Two\". Miller wrote \"I left the symposium with a conviction, more intuitive than rational, that experimental psychology, theoretical linguistics, and the computer simulation of cognitive processes were all pieces from a larger whole.\"\nThis meeting was the beginning of the \"cognitive revolution\"—an interdisciplinary paradigm shift in psychology, philosophy, computer science and neuroscience. It inspired the creation of the sub-fields of symbolic artificial intelligence, generative linguistics, cognitive science, cognitive psychology, cognitive neuroscience and the philosophical schools of computationalism and functionalism. All these fields used related tools to model the mind and results discovered in one field were relevant to the others.\nThe cognitive approach allowed researchers to consider \"mental objects\" like thoughts, plans, goals, facts or memories, often analyzed using high level symbols in functional networks. These objects had been forbidden as \"unobservable\" by earlier paradigms such as behaviorism. Symbolic mental objects would become the major focus of AI research and funding for the next several decades.\n\n\n== Early successes (1956-1974) ==\nThe programs developed in the years after the Dartmouth Workshop were, to most people, simply \"astonishing\": computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such \"intelligent\" behavior by machines was possible at all. Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years. Government agencies like the Defense Advanced Research Projects Agency (DARPA, then known as \"ARPA\") poured money into the field. Artificial Intelligence laboratories were set up at a number of British and US universities in the latter 1950s and early 1960s.\n\n\n=== Approaches ===\nThere were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:\n\n\n==== Reasoning, planning and problem solving as search ====\nMany early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end. The principal difficulty was that, for many problems, the number of possible paths through the \"maze\" was astronomical (a situation known as a \"combinatorial explosion\"). Researchers would reduce the search space by using heuristics that would eliminate paths that were unlikely to lead to a solution.\nNewell and Simon tried to capture a general version of this algorithm in a program called the \"General Problem Solver\". Other \"searching\" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958) and Symbolic Automatic Integrator (SAINT), written by Minsky's student James Slagle in 1961. Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of the robot Shakey.\n\n\n==== Natural language ====\n\nAn important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.\nA semantic net represents concepts (e.g. \"house\", \"door\") as nodes, and relations among concepts as links between the nodes (e.g. \"has-a\"). The first AI program to use a semantic net was written by Ross Quillian and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.\nJoseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a computer program (see ELIZA effect). But in fact, ELIZA simply gave a canned response or repeated back what was said to it, rephrasing its response with a few grammar rules. ELIZA was the first chatbot.\n\n\n==== Micro-worlds ====\nIn the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds. They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a \"blocks world,\" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.\nThis paradigm led to innovative work in machine vision by Gerald Sussman, Adolfo Guzman, David Waltz (who invented \"constraint propagation\"), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. Terry Winograd's SHRDLU could communicate in ordinary English sentences about the micro-world, plan operations and execute them.\n\n\n==== Perceptrons and early neural networks ====\n\nIn the 1960s funding was primarily directed towards laboratories researching symbolic AI, however several people still pursued research in neural networks.\n\nThe perceptron, a single-layer neural network was introduced in 1958 by Frank Rosenblatt (who had been a schoolmate of Marvin Minsky at the Bronx High School of Science). Like most AI researchers, he was optimistic about their power, predicting that a perceptron \"may eventually be able to learn, make decisions, and translate languages.\" Rosenblatt was primarily funded by Office of Naval Research.\nBernard Widrow and his student Ted Hoff built ADALINE (1960) and MADALINE (1962), which had up to 1000 adjustable weights. A group at Stanford Research Institute led by Charles A. Rosen and Alfred E. (Ted) Brain built two neural network machines named MINOS I (1960) and II (1963), mainly funded by U.S. Army Signal Corps. MINOS II had 6600 adjustable weights, and was controlled with an SDS 910 computer in a configuration named MINOS III (1968), which could classify symbols on army maps, and recognize hand-printed characters on Fortran coding sheets. Most of neural network research during this early period involved building and using bespoke hardware, rather than simulation on digital computers.\nHowever, partly due to lack of results and partly due to competition from symbolic AI research, the MINOS project ran out of funding in 1966. Rosenblatt failed to secure continued funding in the 1960s. In 1969, research came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Rosenblatt's predictions had been grossly exaggerated. The effect of the book was that virtually no research was funded in connectionism for 10 years. The competition for government funding ended with the victory of symbolic AI approaches over neural networks.\nMinsky (who had worked on SNARC) became a staunch objector to pure connectionist AI. Widrow (who had worked on ADALINE) turned to adaptive signal processing. The SRI group (which worked on MINOS) turned to symbolic AI and robotics.\nThe main problem was the inability to train multilayered networks (versions of backpropagation had already been used in other fields but it was unknown to these researchers). The AI community became aware of backpropogation in the 80s, and, in the 21st century, neural networks would become enormously successful, fulfilling all of Rosenblatt's optimistic predictions. Rosenblatt did not live to see this, however, as he died in a boating accident in 1971.\n\n\n=== Optimism ===\nThe first generation of AI researchers made these predictions about their work:\n\n1958, H. A. Simon and Allen Newell: \"within ten years a digital computer will be the world's chess champion\" and \"within ten years a digital computer will discover and prove an important new mathematical theorem.\"\n1965, H. A. Simon: \"machines will be capable, within twenty years, of doing any work a man can do.\"\n1967, Marvin Minsky: \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved.\"\n1970, Marvin Minsky (in Life magazine): \"In from three to eight years we will have a machine with the general intelligence of an average human being.\"\n\n\n=== Financing ===\nIn June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (ARPA, later known as DARPA). The money was used to fund project MAC which subsumed the \"AI Group\" founded by Minsky and McCarthy five years earlier. DARPA continued to provide $3 million each year until the 70s. DARPA made similar grants to Newell and Simon's program at Carnegie Mellon University and to Stanford University's AI Lab, founded by John McCarthy in 1963. Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965. These four institutions would continue to be the main centers of AI research and funding in academia for many years.\nThe money was given with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should \"fund people, not projects!\" and allowed researchers to pursue whatever directions might interest them.  This created a freewheeling atmosphere at MIT that gave birth to the hacker culture, but this \"hands off\" approach did not last.\n\n\n== First AI Winter (1974–1980) ==\nIn the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised public expectations impossibly high, and when the promised results failed to materialize, funding targeted at AI was severely reduced. The lack of success indicated the techniques being used by AI researchers at the time were insufficient to achieve their goals.\nThese setbacks did not affect the growth and progress of the field, however. The funding cuts only impacted a handful of major laboratories and the critiques were largely ignored. General public interest in the field continued to grow, the number of researchers increased dramatically, and new ideas were explored in logic programming, commonsense reasoning and many other areas. Historian Thomas Haigh argued in 2023 that there was no winter, and AI researcher Nils Nilsson described this period as the most \"exciting\" time to work in AI.\n\n\n=== Problems ===\nIn the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, \"toys\". AI researchers had begun to run into several limits that would be only conquered decades later, and others that still stymie the field in the 2020s:\n\nLimited computer power: There was not enough memory or processing speed to accomplish anything truly useful. For example: Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only 20 words, because that was all that would fit in memory. Hans Moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence. He suggested an analogy: artificial intelligence requires computer power in the same way that aircraft require horsepower. Below a certain threshold, it's impossible, but, as power increases, eventually it could become easy. \"With enough horsepower,\" he wrote, \"anything will fly\".\nIntractability and the combinatorial explosion: In 1972 Richard Karp (building on Stephen Cook's 1971 theorem) showed there are many problems that can only be solved in exponential time. Finding optimal solutions to these problems requires extraordinary amounts of computer time, except when the problems are trivial. This limitation applied to all symbolic AI programs that used search trees and meant that many of the \"toy\" solutions used by AI would never scale to useful systems.\nMoravec's paradox: Early AI research had been very successful at getting computers to do \"intelligent\" tasks like proving theorems, solving geometry problems and playing chess. Their success at these intelligent tasks convinced them that the problem of intelligent behavior had been largely solved. However, they utterly failed to make progress on \"unintelligent\" tasks like recognizing a face or crossing a room without bumping into anything. By the 1980s, researchers would realize that symbolic reasoning was utterly unsuited for these perceptual and sensorimotor tasks and that there were limits to this approach.\nThe breadth of commonsense knowledge: Many important artificial intelligence applications like vision or natural language require enormous amounts of information about the world: the program needs to have some idea of what it might be looking at or what it is talking about. This requires that the program know most of the same things about the world that a child does. Researchers soon discovered that this was a vast amount of information with billions of atomic facts. No one in 1970 could build a database large enough and no one knew how a program might learn so much information.\nRepresenting commonsense reasoning: A number of related problems appeared when researchers tried to represent commonsense reasoning using formal logic or symbols.  Descriptions of very ordinary deductions tended to get longer and longer the more one worked on them, as more and more exceptions, clarifications and distinctions were required. However, when people thought about ordinary concepts they did not rely on precise definitions, rather they seemed to make hundreds of imprecise assumptions, correcting them when necessary using their entire body of commonsense knowledge. Gerald Sussman observed that \"using precise language to describe essentially imprecise concepts doesn't make them any more precise.\"\n\n\n=== Decrease in funding ===\n\nThe agencies which funded AI research, such as the British government, DARPA and the National Research Council (NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected AI research. The pattern began in 1966 when the Automatic Language Processing Advisory Committee (ALPAC) report criticized machine translation efforts. After spending $20 million, the NRC ended all support. In 1973, the Lighthill report on the state of AI research in the UK criticized the failure of AI to achieve its \"grandiose objectives\" and led to the dismantling of AI research in that country. (The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.) DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of $3 million.\nHans Moravec blamed the crisis on the unrealistic predictions of his colleagues. \"Many researchers were caught up in a web of increasing exaggeration.\" However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund \"mission-oriented direct research, rather than basic undirected research\". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA, which instead directed money at specific projects with clear objectives, such as autonomous tanks and battle management systems.\nThe major laboratories (MIT, Stanford, CMU and Edinburgh) had been receiving generous support from their governments, and when it was withdrawn, these were the only places that were seriously impacted by the budget cuts. The thousands of researchers outside these institutions and the many more thousands that were joining the field were unaffected.\n\n\n=== Philosophical and ethical critiques ===\n\nSeveral philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could. Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little \"symbol processing\" and a great deal of embodied, instinctive, unconscious \"know how\". John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to \"understand\" the symbols that it uses (a quality called \"intentionality\"). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as \"thinking\".\nThese critiques were not taken seriously by AI researchers. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference \"know how\" or \"intentionality\" made to an actual computer program. MIT's Minsky said of Dreyfus and Searle \"they misunderstand, and should be ignored.\" Dreyfus, who also taught at MIT, was given a cold shoulder: he later said that AI researchers \"dared not be seen having lunch with me.\" Joseph Weizenbaum, the author of ELIZA, was also an outspoken critic of Dreyfus' positions, but he \"deliberately made it plain that [his AI colleagues' treatment of Dreyfus] was not the way to treat a human being,\" and was unprofessional and childish.\nWeizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a \"computer program which can conduct psychotherapeutic dialogue\" based on ELIZA. Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.\n\n\n=== Logic at Stanford, CMU and Edinburgh ===\nLogic was introduced into AI research as early as 1958, by John McCarthy in his Advice Taker proposal. In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm. However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems. A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel who created the successful logic programming language Prolog. Prolog uses a subset of logic (Horn clauses, closely related to \"rules\" and \"production rules\") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.\nCritics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof. McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems—not machines that think as people do.\n\n\n=== MIT's \"anti-logic\" approach ===\nAmong the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like \"story understanding\" and \"object recognition\" that required a machine to think like a person. In order to use ordinary concepts like \"chair\" or \"restaurant\" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. MIT chose instead to focus on writing programs that solved a given task without using high-level abstract definitions or general theories of cognition, and measured performance by iterative testing, rather than arguments from first principles. Schank described their \"anti-logic\" approaches as scruffy, as opposed to the neat paradigm used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.\nIn 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on (none of which are true for all birds). Minsky associated these assumptions with the general category and they could be inherited by the frames for subcategories and individuals, or over-ridden as necessary. He called these structures frames. Schank used a version of frames he called \"scripts\" to successfully answer questions about short stories in English. Frames would eventually be widely used in software engineering under the name object-oriented programming.\nThe logicians rose to the challenge. Pat Hayes claimed that \"most of 'frames' is just a new syntax for parts of first-order logic.\" But he noted that \"there are one or two apparently minor details which give a lot of trouble, however, especially defaults\".\nRay Reiter admitted that \"conventional logics, such as first-order\nlogic, lack the expressive power to adequately represent the knowledge required for reasoning by default\". He proposed augmenting first-order logic with a closed world assumption that a conclusion holds (by default) if its contrary cannot be shown. He showed how such an assumption corresponds to the common sense assumption made in reasoning with frames. He also showed that it has its \"procedural equivalent\" as negation as failure in Prolog. The closed world assumption, as formulated by Reiter, \"is not a first-order notion. (It is a meta notion.)\" However, Keith Clark showed that negation as finite failure can be understood as reasoning implicitly with definitions in first-order logic including a unique name assumption that different terms denote different individuals.\nDuring the late 1970s and throughout the 1980s, a variety of logics and extensions of first-order logic were developed both for negation as failure in logic programming and for default reasoning more generally. Collectively, these logics have become known as non-monotonic logics.\n\n\n== Boom (1980–1987) ==\nIn the 1980s, a form of AI program called \"expert systems\" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. Governments provided substantial funding, such as Japan's fifth generation computer project and the U.S. Strategic Computing Initiative. \"Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars in 1988.\"\n\n\n=== Expert systems become widely used ===\nAn expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts.\nThe earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings. MYCIN, developed in 1972, diagnosed infectious blood diseases. They demonstrated the feasibility of the approach.\nExpert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem) and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.\nIn 1980, an expert system called R1 was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.\n\n\n=== Government funding increases ===\nIn 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. Much to the chagrin of scruffies, they initially chose Prolog as the primary computer language for the project.\nOther countries responded with new programs of their own. The UK began the £350 million Alvey project. A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or \"MCC\") to fund large scale projects in AI and information technology. DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.\n\n\n=== Knowledge revolution ===\nThe power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. \"AI researchers were beginning to suspect—reluctantly, for it violated the scientific canon of parsimony—that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,\" writes Pamela McCorduck. \"[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay\". Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s. It was hoped that vast databases would solve the commonsense knowledge problem and provide the support that commonsense reasoning required.\nIn the 1980s some researchers attempted to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started a database called Cyc, argued that there is no shortcut ― the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand.\n\n\n== New directions in the 1980s ==\nAlthough symbolic knowledge representation and logical reasoning produced useful applications in the 80s and received massive amounts of funding, it was still unable to solve problems in perception, robotics, learning and common sense. A small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches, such as \"connectionism\", robotics, \"soft\" computing and reinforcement learning. Nils Nilsson called these approaches \"sub-symbolic\".\n\n\n=== Revival of neural networks: \"connectionism\" ===\n\nIn 1982, physicist John Hopfield was able to prove that a form of neural network (now called a \"Hopfield net\") could learn and process information, and provably converges after enough time under any fixed condition. It was a breakthrough, as it was previously thought that nonlinear networks would, in general, evolve chaotically. Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called \"backpropagation\". These two developments helped to revive the exploration of artificial neural networks.\nNeural networks, along with several other similar models, received widespread attention after the 1986 publication of the Parallel Distributed Processing, a two volume collection of papers edited by Rumelhart and psychologist James McClelland. The new field was christened \"connectionism\" and there was a considerable debate between advocates of symbolic AI and the \"connectionists\". Hinton called symbols the \"luminous aether of AI\" – that is, an unworkable and misleading model of intelligence. This was a direct attack on the principles that inspired the cognitive revolution.\nNeural networks started to advance state of the art in some specialist areas such as protein structure prediction. Following pioneering work from Terry Sejnowski, cascading multilayer perceptrons such as PhD and PsiPred reached near-theoretical maximum accuracy in predicting secondary structure.\nIn 1990, Yann LeCun at Bell Labs used convolutional neural networks to recognize handwritten digits. The system was used widely in 90s, reading zip codes and personal checks. This was the first genuinely useful application of neural networks.\n\n\n=== Robotics and embodied reason ===\n\nRodney Brooks, Hans Moravec and others argued that, in order to show real intelligence, a machine needs to have a body — it needs to perceive, move, survive and deal with the world. Sensorimotor skills are essential to higher level skills such as commonsense reasoning. They can't be efficiently implemented using abstract symbolic reasoning, so AI should solve the problems of perception, mobility, manipulation and survival without using symbolic representation at all. These robotics researchers advocated building intelligence \"from the bottom up\".\nA precursor to this idea was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)\nIn his 1990 paper \"Elephants Don't Play Chess,\" robotics researcher Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since \"the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough.\"\nIn the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the \"embodied mind thesis\".\n\n\n=== Soft computing and probabilistic reasoning ===\nSoft computing uses methods that work with incomplete and imprecise information. They do not attempt to give precise, logical answers, but give results that are only \"probably\" correct. This allowed them to solve problems that precise symbolic methods could not handle. Press accounts often claimed these tools could \"think like a human\".\nJudea Pearl's Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, an influential 1988 book brought probability and decision theory into AI. Fuzzy logic, developed by Lofti Zadeh in the 60s, began to be more widely used in AI and robotics. Evolutionary computation and artificial neural networks also handle imprecise information, and are classified  as \"soft\". In the 90s and early 2000s many other soft computing tools were developed and put into use, including Bayesian networks, hidden Markov models, information theory and stochastic modeling. These tools in turn depended on advanced mathematical techniques such as classical optimization. For a time in the 1990s and early 2000s, these soft tools were studied by a subfield of AI called \"computational intelligence\".\n\n\n=== Reinforcement learning ===\nReinforcement learning gives an agent a reward every time it performs a desired action well, and may give negative rewards (or \"punishments\") when it performs poorly. It was described in the first half of the twentieth century by psychologists using animal models, such as Thorndike, Pavlov and Skinner. In the 1950s, Alan Turing and Arthur Samuel foresaw the role of reinforcement learning in AI.\nA successful and influential research program was led by Richard Sutton and Andrew Barto beginning 1972. Their collaboration revolutionized the study of reinforcement learning and decision making over the four decades. In 1988, Sutton described machine learning in terms of decision theory (i.e., the Markov decision process). This gave the subject a solid theoretical foundation and access to a large body of theoretical results developed in the field of operations research.\nAlso in 1988, Sutton and Barto developed the \"temporal difference\" (TD) learning algorithm, where the agent is rewarded only when its predictions about the future show improvement. It significantly outperformed previous algorithms. TD-learning was used by Gerald Tesauro in 1992 in the program TD-Gammon, which played backgammon as well as the best human players. The program learned the game by playing against itself with zero prior knowledge. In an interesting case of interdisciplinary convergence, neurologists discovered in 1997 that the dopamine reward system in brains also uses a version of the TD-learning algorithm. TD learning would be become highly influential in the 21st century, used in both AlphaGo and AlphaZero.\n\n\n== Second AI winter (1990s) ==\nThe business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. As dozens of companies failed, the perception in the business world was that the technology was not viable. The damage to AI's reputation would last into the 21st century. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of \"artificial intelligence\".\nOver the next 20 years, AI consistently delivered working solutions to specific isolated problems. By the late 1990s, it was being used throughout the technology industry, although somewhat behind the scenes. The success was due to increasing computer power, by collaboration with other fields (such as mathematical optimization and statistics) and using the highest standards of scientific accountability.                                                                            By 2000, AI had achieved some of its oldest goals. The field was both more cautious and more successful than it had ever been.\n\n\n=== AI winter ===\nThe term \"AI winter\" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow. Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.\nThe first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.\nEventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, and they were \"brittle\" (i.e., they could make grotesque mistakes when given unusual inputs). Expert systems proved useful, but only in a few special contexts.\nIn the late 1980s, the Strategic Computing Initiative cut funding to AI \"deeply and brutally\". New leadership at DARPA had decided that AI was not \"the next wave\" and directed funds towards projects that seemed more likely to produce immediate results.\nBy 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like \"carry on a casual conversation\" would not be accomplished for another 30 years. As with other AI projects, expectations had run much higher than what was actually possible.\nOver 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI. In 1994, HP Newquist stated in The Brain Makers that \"The immediate future of artificial intelligence—in its commercial form—seems to rest in part on the continued success of neural networks.\"\n\n\n=== AI behind the scenes ===\nIn the 1990s, algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems and their solutions proved to be useful throughout the technology industry, such as data mining, industrial robotics, logistics, speech recognition, banking software, medical diagnosis and Google's search engine.\nThe field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science. Nick Bostrom explains: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nMany researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, \"cognitive systems\" or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding. In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: \"Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\"\n\n\n=== Mathematical rigor, greater collaboration and a narrow focus ===\nAI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past. Most of the new directions in AI relied heavily on mathematical models, including artificial neural networks, probabilistic reasoning, soft computing and reinforcement learning. In the 90s and 2000s, many other highly mathematical tools were adapted for AI. These tools were applied to machine learning, perception and mobility.\nThere was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like statistics, mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous \"scientific\" discipline.\nAnother key reason for the success in the 90s was that AI researchers focussed on specific problems with verifiable solutions (an approach later derided as narrow AI). This provided useful tools in the present, rather than speculation about the future.\n\n\n=== Intelligent agents ===\nA new paradigm called \"intelligent agents\" became widely accepted during the 1990s. Although earlier researchers had proposed modular \"divide and conquer\" approaches to AI, the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI. When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.\nAn intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are \"intelligent agents\", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as \"the study of intelligent agents\". This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence.\nThe paradigm gave researchers license to study isolated problems and to disagree about methods, but still retain hope that their work could be combined into an agent architecture that would be capable of general intelligence.\n\n\n=== Milestones and Moore's law ===\nOn May 11, 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov. In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an urban environment while responding to traffic hazards and adhering to traffic laws.\nThese successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computers by the 90s. In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951. This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years. The fundamental problem of \"raw computer power\" was slowly being overcome.\n\n\n=== Arts and literature influences from AI ===\nElectronic literature experiments such as The Impermanence Agent (1998 -2002) and digital art such as Agent Ruby, used  AI in their art and literature, \"laying bare the bias accompanying forms of technology that feign objectivity.\"\n\n\n== Big data, deep learning, AGI (2005–2017) ==\nIn the first decades of the 21st century, access to large amounts of data (known as \"big data\"), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. A turning point was the success of deep learning around 2012 which improved the performance of machine learning on many tasks, including image and video processing, text analysis, and speech recognition. Investment in AI increased along with its capabilities, and by 2016, the market for AI-related products, hardware, and software reached more than $8 billion, and the New York Times reported that interest in AI had reached a \"frenzy\".\nIn 2002, Ben Goertzel and others became concerned that AI had largely abandoned its original goal of producing versatile, fully intelligent machines, and argued in favor of more direct research into artificial general intelligence. By the mid-2010s several companies and institutions had been founded to pursue Artificial General Intelligence (AGI), such as OpenAI and Google's DeepMind. During the same period, new insights into superintelligence raised concerns that AI was an existential threat. The risks and unintended consequences of AI technology became an area of serious academic research after 2016.\n\n\n=== Big data and big machines ===\n\nThe success of machine learning in the 2000s depended on the availability of vast amounts of training data and faster computers. Russell and Norvig wrote that the \"improvement in performance obtained by increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be made by tweaking the algorithm.\" Geoffrey Hinton recalled that back in the 90s, the problem was that \"our labeled datasets were thousands of times too small. [And] our computers were millions of times too slow.\" This was no longer true by 2010.\nThe most useful data in the 2000s came from curated, labeled data sets created specifically for machine learning and AI. In 2007, a group at UMass Amherst released Labeled Faces in the Wild, an annotated set of images of faces that was widely used to train and test face recognition systems for the next several decades. Fei-Fei Li developed ImageNet, a database of three million images captioned by volunteers using the Amazon Mechanical Turk. Released in 2009, it was a useful body of training data and a benchmark for testing for the next generation of image processing systems. Google released word2vec in 2013 as an open source resource. It used large amounts of data text scraped from the internet and word embedding to create a numeric vector to represent each word. Users were surprised at how well it was able to capture word meanings, for example, ordinary vector addition would give equivalences like China + River = Yangtze, London-England+France = Paris. This database in particular would be essential for the development of large language models in the late 2010s.\nThe explosive growth of the internet gave machine learning programs access to billions of pages of text and images that could be scraped. And, for specific problems, large privately held databases contained the relevant data. McKinsey Global Institute reported that \"by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data\". This collection of information was known in the 2000s as big data.\nIn a Jeopardy! exhibition match in February 2011, IBM's question answering system Watson defeated the two best Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. Watson's expertise would have been impossible without the information available on the internet.\n\n\n=== Deep learning ===\n\nIn 2012, AlexNet, a deep learning model, developed by Alex Krizhevsky, won the ImageNet Large Scale Visual Recognition Challenge, with significantly fewer errors than the second-place winner. Krizhevsky worked with Geoffrey Hinton at the University of Toronto. This was a turning point in machine learning: over the next few years dozens of other approaches to image recognition were abandoned in favor of deep learning.\nDeep learning uses a multi-layer perceptron. Although this architecture has been known since the 60s, getting it to work requires powerful hardware and large amounts of training data. Before these became \navailable, improving performance of image processing systems required hand-crafted ad hoc features that were difficult to implement. Deep learning was simpler and more general.\nDeep learning was applied to dozens of problems over the next few years (such as speech recognition, machine translation, medical diagnosis, and game playing). In every case it showed enormous gains in performance. Investment and interest in AI boomed as a result.\n\n\n=== The alignment problem ===\nIt became fashionable in the 2000s to begin talking about the future of AI again and several popular books considered the possibility of superintelligent machines and what they might mean for human society. Some of this was optimistic (such as Ray Kurzweil's The Singularity is Near), but others warned that a sufficiently powerful AI was existential threat to humanity, such as Nick Bostrom and Eliezer Yudkowsky. The topic became widely covered in the press and many leading intellectuals and politicians commented on the issue.\nAI programs in the 21st century are defined by their goals – the specific measures that they are designed to optimize. Nick Bostrom's influential 2014 book Superintelligence argued that, if one isn't careful about defining these goals, the machine may cause harm to humanity in the process of achieving a goal. Stuart J. Russell used the example of an intelligent robot that kills its owner to prevent it from being unplugged, reasoning \"you can't fetch the coffee if you're dead\". (This problem is known by the technical term \"instrumental convergence\".) The solution is to align the machine's goal function with the goals of its owner and humanity in general. Thus, the problem of mitigating the risks and unintended consequences of AI became known as \"the value alignment problem\" or AI alignment.\nAt the same time, machine learning systems had begun to have disturbing unintended consequences. Cathy O'Neil explained how statistical algorithms had been among the causes of the 2008 economic crash, Julia Angwin of ProPublica argued that the COMPAS system used by the criminal justice system exhibited racial bias under some measures, others showed that many machine learning systems exhibited some form of racial bias, and there were many other examples of dangerous outcomes that had resulted from machine learning systems.\nIn 2016, the election of Donald Trump and the controversy over the COMPAS system illuminated several problems with the current technological infrastructure, including misinformation, social media algorithms designed to maximize engagement, the misuse of personal data and the trustworthiness of predictive models. Issues of fairness and unintended consequences became significantly more popular at AI conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The value alignment problem became a serious field of academic study.\n\n\n=== Artificial general intelligence research ===\nIn the early 2000s, several researchers became concerned that mainstream AI was too focused on \"measurable performance in specific applications\" (known as \"narrow AI\") and had abandoned AI's original goal of creating versatile, fully intelligent machines. An early critic was Nils Nilsson in 1995, and similar opinions were published by AI elder statesmen John McCarthy, Marvin Minsky, and Patrick Winston in 2007–2009. Minsky organized a symposium on \"human-level AI\" in 2004. Ben Goertzel adopted the term \"artificial general intelligence\" for the new sub-field, founding a journal and holding conferences beginning in 2008. The new field grew rapidly, buoyed by the continuing success of artificial neural networks and the hope that it was the key to AGI.\nSeveral competing companies, laboratories and foundations were founded to develop AGI in the 2010s. DeepMind was founded in 2010 by three English scientists, Demis Hassabis, Shane Legg and Mustafa Suleyman, with funding from Peter Thiel and later Elon Musk. The founders and financiers were deeply concerned about AI safety and the existential risk of AI. DeepMind's founders had a personal connection with Yudkowsky and Musk was among those who was actively raising the alarm. Hassabis was both worried about the dangers of AGI and optimistic about its power; he hoped they could \"solve AI, then solve everything else.\" The New York Times wrote in 2023 \"At the heart of this competition is a brain-stretching paradox. The people who say they are most worried about AI are among the most determined to create it and enjoy its riches. They have justified their ambition with their strong belief that they alone can keep AI from endangering Earth.\"\nIn 2012, Geoffrey Hinton (who been leading neural network research since the 80s) was approached by Baidu, which wanted to hire him and all his students for an enormous sum. Hinton decided to hold an auction and, at a Lake Tahoe AI conference, they sold themselves to Google for a price of $44 million. Hassabis took notice and sold DeepMind to Google in 2014, on the condition that it would not accept military contracts and would be overseen by an ethics board.\nLarry Page of Google, unlike Musk and Hassabis, was an optimist about the future of AI. Musk and Paige became embroiled in an argument about the risk of AGI at Musk's 2015 birthday party. They had been friends for decades but stopped speaking to each other shortly afterwards. Musk attended the one and only meeting of the DeepMind's ethics board, where it became clear that Google was uninterested in mitigating the harm of AGI. Frustrated by his lack of influence he founded OpenAI in 2015, enlisting Sam Altman to run it and hiring top scientists. OpenAI began as a non-profit, \"free from the economic incentives that were driving Google and other corporations.\" Musk became frustrated again and left the company in 2018. OpenAI turned to Microsoft for continued financial support and Altman and OpenAI formed a for-profit version of the company with more than $1 billion in financing.\nIn 2021, Dario Amodei  and 14 other scientists left OpenAI over concerns that the company was putting profits above safety. They formed Anthropic, which soon had $6 billion in financing from Microsoft and Google.\n\n\n== Large language models, AI boom (2017–present) ==\n\nThe AI boom started with the initial development of key architectures and algorithms such as the transformer architecture in 2017, leading to the scaling and development of large language models exhibiting human-like traits of knowledge, attention and creativity. The new AI era began since 2020, with the public release of scaled large language models (LLMs) such as ChatGPT.\n\n\n=== Transformer architecture and large language models ===\n\nIn 2017, the transformer architecture was proposed by Google researchers. It exploits an attention mechanism and became widely used in large language models.\nLarge language models, based on the transformer, were developed by AGI companies: OpenAI released GPT-3 in 2020, and DeepMind released Gato in 2022. These are foundation models: they are trained on vast quantities of unlabeled data and can be adapted to a wide range of downstream tasks.\nThese models can discuss a huge number of topics and display general knowledge. The question naturally arises: are these models an example of artificial general intelligence? Bill Gates was skeptical of the new technology and the hype that surrounded AGI. However, Altman presented him with a live demo of ChatGPT4 passing an advanced biology test. Gates was convinced. In 2023, Microsoft Research tested the model with a large variety of tasks, and concluded that \"it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system\".\nIn 2024, OpenAI o3, a type of advanced reasoning model developed by OpenAI was announced. On the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) benchmark developed by François Chollet in 2019, the model achieved an unofficial score of 87.5% on the semi-private test, surpassing the typical human score of 84%. The benchmark is supposed to be a necessary, but not sufficient test for AGI. Speaking of the benchmark, Chollet has said \"You’ll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.\"\n\n\n=== AI boom ===\n\nInvestment in AI grew exponentially after 2020, with venture capital funding for generative AI companies increasing dramatically. Total AI investments rose from $18 billion in 2014 to $119 billion in 2021, with generative AI accounting for approximately 30% of investments by 2023. According to metrics from 2017 to 2021, the United States outranked the rest of the world in terms of venture capital funding, number of startups, and AI patents granted. The commercial AI scene became dominated by American Big Tech companies, whose investments in this area surpassed those from U.S.-based venture capitalists. OpenAI's valuation reached $86 billion by early 2024, while NVIDIA's market capitalization surpassed $3.3 trillion by mid-2024, making it the world's largest company by market capitalization as the demand for AI-capable GPUs surged.\n15.ai, launched in March 2020 by an anonymous MIT researcher, was one of the earliest examples of generative AI gaining widespread public attention during the initial stages of the AI boom. The free web application demonstrated the ability to clone character voices using neural networks with minimal training data, requiring as little as 15 seconds of audio to reproduce a voice—a capability later corroborated by OpenAI in 2024. The service went viral on social media platforms in early 2021, allowing users to generate speech for characters from popular media franchises, and became particularly notable for its pioneering role in popularizing AI voice synthesis for creative content and memes.\n\n\n=== Advent of AI for public use ===\nChatGPT was launched on November 30, 2022, marking a pivotal moment in artificial intelligence's public adoption. Within days of its release it went viral, gaining over 100 million users in two months and becoming the fastest-growing consumer software application in history. The chatbot's ability to engage in human-like conversations, write code, and generate creative content captured public imagination and led to rapid adoption across various sectors including education, business, and research. ChatGPT's success prompted unprecedented responses from major technology companies—Google declared a \"code red\" and rapidly launched Gemini (formerly known as Google Bard), while Microsoft incorporated the technology into Bing Chat.\nThe rapid adoption of these AI technologies sparked intense debate about their implications. Notable AI researchers and industry leaders voiced both optimism and concern about the accelerating pace of development. In March 2023, over 20,000 signatories, including computer scientist Yoshua Bengio, Elon Musk, and Apple co-founder Steve Wozniak, signed an open letter calling for a pause in advanced AI development, citing \"profound risks to society and humanity.\" However, other prominent researchers like Juergen Schmidhuber took a more optimistic view, emphasizing that the majority of AI research aims to make \"human lives longer and healthier and easier.\"\nBy mid-2024, however, the financial sector began to scrutinize AI companies more closely, particularly questioning their capacity to produce a return on investment commensurate with their massive valuations. Some prominent investors raised concerns about market expectations becoming disconnected from fundamental business realities. Jeremy Grantham, co-founder of GMO LLC, warned investors to \"be quite careful\" and drew parallels to previous technology-driven market bubbles. Similarly, Jeffrey Gundlach, CEO of DoubleLine Capital, explicitly compared the AI boom to the dot-com bubble of the late 1990s, suggesting that investor enthusiasm might be outpacing realistic near-term capabilities and revenue potential. These concerns were amplified by the substantial market capitalizations of AI-focused companies, many of which had yet to demonstrate sustainable profitability models.\nIn March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus. The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google. In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.\n\n\n=== 2024 Nobel Prizes ===\nIn 2024, the Royal Swedish Academy of Sciences awarded Nobel Prizes in recognition of groundbreaking contributions to artificial intelligence. The recipients included:\n\nIn physics: John Hopfield for his work on physics-inspired Hopfield networks, and Geoffrey Hinton for foundational contributions to Boltzmann machines and deep learning.\nIn chemistry: David Baker, Demis Hassabis, and John Jumper for their advancements in protein folding predictions. See AlphaFold.\n\n\n=== Further Study and development of AI ===\nIn January 2025, OpenAI announced a new AI, ChatGPT-Gov, which would be specifically designed for US government agencies to use securely. Open AI said that agencies could utilize ChatGPT Gov on a Microsoft Azure cloud or Azure Government cloud, \"on top of Microsoft’s Azure’s OpenAI Service.\" OpenAI's announcement stated that \"Self-hosting ChatGPT Gov enables agencies to more easily manage their own security, privacy, and compliance requirements, such as stringent cybersecurity frameworks (IL5, CJIS, ITAR, FedRAMP High). Additionally, we believe this infrastructure will expedite internal authorization of OpenAI’s tools for the handling of non-public sensitive data.\"\n\n\n=== National policies ===\nCountries have invested in policies and funding to deploy autonomous robots in an attempt to address labor shortages and enhancing efficiency, while also implementing regulatory frameworks for ethical and safe development.\n\n\n==== China ====\nIn 2025, China invested approximately 730 billion yuan (roughly $100 billion USD) to advance AI and robotics in smart manufacturing and healthcare. The \"14th Five-Year Plan\" (2021–2025) prioritized service robots, with AI systems enabling robots to perform complex tasks like assisting in surgeries or automating factory assembly lines. Some funding also supported defense applications, such as autonomous drones. Starting in September 2025, China mandated labeling of AI-generated content to ensure transparency and public trust in these technologies.\n\n\n==== United States ====\nIn January 2025, Stargate LLC was formed as a joint venture of OpenAI, SoftBank, Oracle, and MGX, who announced plans to invest US$500 billion in AI infrastructure in the United States by 2029. Stargate LLC started with a US$100 billion investment in re-industrialization and national security capabilities. The venture was formally announced by U.S. President Donald Trump on January 21, 2025, with SoftBank CEO Masayoshi Son appointed as chairman.\nThe U.S. government allocated approximately $2 billion to integrate AI and robotics in manufacturing and logistics. State governments supplemented this with funding for service robots, such as those deployed in warehouses to fulfill verbal commands for inventory management or in eldercare facilities to respond to residents' requests for assistance. Some funds were directed to defense, including Lethal autonomous weapon and Military robot. In January 2025, Executive Order 14179 established an \"AI Action Plan\" to accelerate innovation and deployment of these technologies.\n\n\n== See also ==\nHistory of artificial neural networks\nHistory of knowledge representation and reasoning\nHistory of natural language processing\nOutline of artificial intelligence\nProgress in artificial intelligence\nTimeline of artificial intelligence\nTimeline of machine learning\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Works cited ==\nBonner A (2007), The Art and Logic of Ramón Llull: A User's Guide, Brill, ISBN 978-9004163256\nBonner A (1985). \"Llull's Influence: The History of Lullism\". Doctor Illuminatus. A Ramon Llull Reader. Princeton University Press.\nBrooks R (2002), Flesh and Machines, Pantheon Books\nBubeck S, Chandrasekaran V, Eldan R, Gehrke J, Horvitz E, Kamar E, Lee P, Lee YT, Li Y, Lundberg S, Nori H, Palangi H, Ribeiro MT, Zhang Y (22 March 2023). \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\". arXiv:2303.12712 [cs.CL].\nCarreras y Artau T (2018) [1939], Historia de la filosofía española. Filosofía cristiana de los siglos XIII al XV (in Spanish), vol. 1, Madrid: Forgotten Books, ISBN 9781390433708\nButler EM (1979) [1948]. The myth of the magus. London: Cambridge University Press. ISBN 0-521-22564-7. OCLC 5063114.\nClark S (21 December 2023). \"The Era of AI: 2023's Landmark Year\". CMSWire.com. Retrieved 28 January 2024.\nCopeland J (1999). \"A Brief History of Computing\". AlanTuring.net.\nCave S, Dihal K (2019). \"Hopes and fears for intelligent machines in fiction and reality\". Nature Machine Intelligence. 1 (2): 74–78. doi:10.1038/s42256-019-0020-9. ISSN 2522-5839. S2CID 150700981.\nCave S, Dihal K, Dillon S (2020). AI Narratives: A History of Imaginative Thinking about Intelligent Machines. Oxford University Press. ISBN 978-0-19-884666-6. Retrieved 2 May 2023.\nChristian B (2020). The Alignment Problem: Machine learning and human values. W. W. Norton & Company. ISBN 978-0-393-86833-3. OCLC 1233266753.\nClark K (1977). \"Negation as Failure\". Logic and Data Bases. Boston, MA: Springer US. pp. 293–322. doi:10.1007/978-1-4684-3384-5_11. ISBN 978-1-4684-3386-9.\nGates B (21 December 2023). \"This year signaled the start of a new era\". www.linkedin.com. Retrieved 28 January 2024.\nGoethe JW (1890). Faust; a tragedy. Translated, in the original metres ... by Bayard Taylor. Authorised ed., published by special arrangement with Mrs. Bayard Taylor. With a biographical introd. London Ward, Lock.\nHart PE, Nilsson NJ, Perrault R, Mitchell T, Kulikowski CA, Leake DB (15 March 2003). \"In Memoriam: Charles Rosen, Norman Nielsen, and Saul Amarel\". AI Magazine. 24 (1): 6. doi:10.1609/aimag.v24i1.1683. ISSN 2371-9621.\nHayes P (1981). \"The logic of frames\". In Kaufmann M (ed.). Readings in artificial intelligence. pp. 451–458.\n\"GOLEM\", The Jewish Encyclopedia, retrieved 15 March 2020\nHollander LM (1991) [1964]. Heimskringla; history of the kings of Norway. Austin: Published for the American-Scandinavian Foundation by the University of Texas Press. ISBN 0-292-73061-6. OCLC 638953.\nKressel M (1 October 2015). \"36 Days of Judaic Myth: Day 24, The Golem of Prague 2015\". Matthew Kressel. Retrieved 15 March 2020.\nLeCun Y, Bengio Y, Hinton G (2015). \"Deep learning\" (PDF). Nature. 521 (7553): 436–444. Bibcode:2015Natur.521..436L. doi:10.1038/nature14539. PMID 26017442. S2CID 3074096.\nLee A (23 January 2024). \"UT Designates 2024 'The Year of AI'\". UT News. Retrieved 28 January 2024.\nLinden SJ (2003). The alchemy reader : from Hermes Trismegistus to Isaac Newton. New York: Cambridge University Press. pp. Ch. 18. ISBN 0-521-79234-7. OCLC 51210362.\nLohr S (17 October 2016), \"IBM Is Counting on Its Bet on Watson, and Paying Big Money for It\", New York Times\nMarkoff J (16 February 2011). \"On 'Jeopardy!' Watson Win Is All but Trivial\". The New York Times.\nMarr B (20 March 2023). \"Beyond The Hype: What You Really Need To Know About AI In 2023\". Forbes. Retrieved 27 January 2024.\nMcCarthy J (1988). \"Review of The Question of Artificial Intelligence\". Annals of the History of Computing. 10 (3): 224–229., collected in McCarthy J (1996). \"10. Review of The Question of Artificial Intelligence\". Defending AI Research: A Collection of Essays and Reviews. CSLI.\nMcCulloch WS, Pitts W (1 December 1943). \"A logical calculus of the ideas immanent in nervous activity\". Bulletin of Mathematical Biophysics. 5 (4): 115–133. doi:10.1007/BF02478259. ISSN 1522-9602.\n\"Big data: The next frontier for innovation, competition, and productivity\". McKinsey.com. 1 May 2011.\nMetz C, Weise K, Grant N, Isaac M (3 December 2023). \"Ego, Fear and Money: How the A.I. Fuse Was Lit\". The New York Times.\nMiller G (2003). \"The cognitive revolution: a historical perspective\" (PDF). Trends in Cognitive Sciences. 7 (3): 141–144. doi:10.1016/s1364-6613(03)00029-9. PMID 12639696.\nMoravec H (18 May 2000). Robot: Mere Machine to Transcendent Mind. Oxford University Press. ISBN 9780195136302.\nMorford M (2007). Classical mythology. Oxford: Oxford University Press. p. 184. ISBN 978-0-19-085164-4. OCLC 1102437035.\nMurgia M (23 July 2023). \"Transformers: the Google scientists who pioneered an AI revolution\". www.ft.com. Retrieved 10 December 2023.\nO'Neill C (6 September 2016). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Crown. ISBN 978-0553418811.\nNielson DL (1 January 2005). \"Chapter 4: The Life and Times of a Successful SRI Laboratory: Artificial Intelligence and Robotics\" (PDF). A HERITAGE OF INNOVATION SRI's First Half Century (1st ed.). SRI International. ISBN 978-0-9745208-0-3.\nNilsson NJ (1984). \"The SRI Artificial Intelligence Center: A Brief History\" (PDF). Artificial Intelligence Center, SRI International. Archived from the original (PDF) on 10 August 2022.\nOlazaran Rodriguez JM (1991). A historical sociology of neural network research] (PDF) (Thesis). University of Edinburgh. Archived from the original (PDF) on 11 November 2022. See especially Chapter 2 and 3.\nPiccinini G (1 August 2004). \"The First Computational Theory of Mind and Brain: A Close Look at Mcculloch and Pitts's \"Logical Calculus of Ideas Immanent in Nervous Activity\"\". Synthese. 141 (2): 175–215. doi:10.1023/B:SYNT.0000043018.52445.3e. ISSN 1573-0964. S2CID 10442035.\nPorterfield A (2006). The Protestant Experience in America. American religious experience. Greenwood Press. p. 136. ISBN 978-0-313-32801-5. Retrieved 15 May 2023.\nReiter R (1978). \"On reasoning by default\". American Journal of Computational Linguistics: 29–37.\nRhodios A (2007). The Argonautika : Expanded Edition. University of California Press. p. 355. ISBN 978-0-520-93439-9. OCLC 811491744.\nRose A (April 1946). \"Lightning Strikes Mathematics\". Popular Science: 83–86. Retrieved 15 April 2012.\nRosen CA, Nilsson NJ, Adams MB (8 January 1965). \"A research and development program in applications of intelligent automata to reconnaissance-phase I. (Proposal for Research SRI No. ESU 65-1)\" (PDF). Stanford Research Institute. Archived from the original (PDF) on 16 March 2006.\nRosenblatt F (1962), Principles of neurodynamics: Perceptrons and the theory of brain mechanisms, vol. 55, Washington DC: Spartan books\nRussell SJ (2020). Human compatible: Artificial intelligence and the problem of control. Penguin Random House. ISBN 9780525558637. OCLC 1113410915.\nSchaeffer J (1997). One Jump Ahead:: Challenging Human Supremacy in Checkers. Springer. ISBN 978-0-387-76575-4.\nSchmidhuber J (2022). \"Annotated History of Modern AI and Deep Learning\".\nSchultz W, Dayan P, Montague PR (14 March 1997). \"A Neural Substrate of Prediction and Reward\". Science. 275 (5306): 1593–1599. doi:10.1126/science.275.5306.1593. PMID 9054347.\nSejnowski TJ (23 October 2018). The Deep Learning Revolution (1st ed.). Cambridge, Massachusetts London, England: The MIT Press. pp. 93–94. ISBN 978-0-262-03803-4.\n\"Sanhedrin 65b\". www.sefaria.org. Retrieved 15 March 2020.\nWidrow B, Lehr M (September 1990). \"30 years of adaptive neural networks: perceptron, Madaline, and backpropagation\". Proceedings of the IEEE. 78 (9): 1415–1442. doi:10.1109/5.58323. S2CID 195704643.\nBerlinski D (2000), The Advent of the Algorithm, Harcourt Books, ISBN 978-0-15-601391-8, OCLC 46890682.\nBrooks RA (1990). \"Elephants Don't Play Chess\" (PDF). Robotics and Autonomous Systems. 6 (1–2): 3–15. doi:10.1016/S0921-8890(05)80025-9.\nBuchanan BG (Winter 2005), \"A (Very) Brief History of Artificial Intelligence\" (PDF), AI Magazine, pp. 53–60, archived from the original (PDF) on 26 September 2007, retrieved 30 August 2007.\nButler S (13 June 1863), \"Darwin Among the Machines\", The Press, Christchurch, New Zealand, retrieved 10 October 2008.\nByrne JG (8 December 2012). \"The John Gabriel Byrne Computer Science Collection\" (PDF). Archived from the original on 16 April 2019. Retrieved 8 August 2019.\n\"AI set to exceed human brain power\", CNN.com, 26 July 2006, retrieved 16 October 2007.\nColby KM, Watt JB, Gilbert JP (1966), \"A Computer Method of Psychotherapy: Preliminary Communication\", The Journal of Nervous and Mental Disease, vol. 142, no. 2, pp. 148–152, doi:10.1097/00005053-196602000-00005, PMID 5936301, S2CID 36947398.\nColby KM (September 1974), Ten Criticisms of Parry (PDF), Stanford Artificial Intelligence Laboratory, REPORT NO. STAN-CS-74-457, retrieved 17 June 2018.\nCouturat L (1901), La Logique de Leibniz\nCopeland J (2000), Micro-World AI, retrieved 8 October 2008.\nCopeland J( (2004). The Essential Turing: the ideas that gave birth to the computer age. Oxford: Clarendon Press. ISBN 0-19-825079-7..\nCordeschi R (2002), The Discovery of the Artificial, Dordrecht: Kluwer..\nCrevier D (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3.\nDarrach B (20 November 1970), \"Meet Shaky, the First Electronic Person\", Life Magazine, pp. 58–68.\nDoyle J (1983), \"What is rational psychology? Toward a modern mental philosophy\", AI Magazine, vol. 4, no. 3, pp. 50–53.\nDreyfus H (1965), Alchemy and AI, RAND Corporation Memo.\nDreyfus H (1972), What Computers Can't Do, New York: MIT Press, ISBN 978-0-06-090613-9, OCLC 5056816.\nDreyfus H, Dreyfus S (1986). Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer. Oxford, UK: Blackwell. ISBN 978-0-02-908060-3. Retrieved 22 August 2020.\nThe Economist (7 June 2007), \"Are You Talking to Me?\", The Economist, retrieved 16 October 2008.\nFeigenbaum EA, McCorduck P (1983), The Fifth Generation: Artificial Intelligence and Japan's Computer Challenge to the World, Michael Joseph, ISBN 978-0-7181-2401-4.\nHaigh T (December 2023). \"There Was No 'First AI Winter'\". Communications of the ACM. 66 (12): 35–39. doi:10.1145/3625833. ISSN 0001-0782..\nHaugeland J (1985). Artificial Intelligence: The Very Idea. Cambridge, Mass.: MIT Press. ISBN 978-0-262-08153-5.\nHawkins J, Blakeslee S (2004), On Intelligence, New York, NY: Owl Books, ISBN 978-0-8050-7853-4, OCLC 61273290.\nHebb D (2002) [1949], The Organization of Behavior, New York: Wiley, ISBN 978-0-8058-4300-2, OCLC 48871099.\nHewitt C, Bishop P, Steiger R (1973), A Universal Modular Actor Formalism for Artificial Intelligence (PDF), IJCAI, archived from the original (PDF) on 29 December 2009\nHobbes T (1651), Leviathan.\nHofstadter D (1999) [1979], Gödel, Escher, Bach: an Eternal Golden Braid, Basic Books, ISBN 978-0-465-02656-2, OCLC 225590743.\nHowe J (November 1994), Artificial Intelligence at Edinburgh University: a Perspective, retrieved 30 August 2007.\nKahneman D, Slovic D, Tversky A (1982). \"Judgment under uncertainty: Heuristics and biases\". Science. 185 (4157). New York: Cambridge University Press: 1124–1131. Bibcode:1974Sci...185.1124T. doi:10.1126/science.185.4157.1124. ISBN 978-0-521-28414-1. PMID 17835457. S2CID 143452957.\nKaplan A, Haenlein M (2018), \"Siri, Siri in my Hand, who's the Fairest in the Land? On the Interpretations, Illustrations and Implications of Artificial Intelligence\", Business Horizons, 62: 15–25, doi:10.1016/j.bushor.2018.08.004, S2CID 158433736.\nKolata G (1982), \"How can computers get common sense?\", Science, 217 (4566): 1237–1238, Bibcode:1982Sci...217.1237K, doi:10.1126/science.217.4566.1237, PMID 17837639.\nKurzweil R (2005), The Singularity is Near, Viking Press, ISBN 978-0-14-303788-0, OCLC 71826177.\nLakoff G (1987), Women, Fire, and Dangerous Things: What Categories Reveal About the Mind, University of Chicago Press., ISBN 978-0-226-46804-4.\nLakoff G, Johnson M (1999). Philosophy in the flesh: The embodied mind and its challenge to western thought. Basic Books. ISBN 978-0-465-05674-3.\nLenat D, Guha RV (1989), Building Large Knowledge-Based Systems, Addison-Wesley, ISBN 978-0-201-51752-1, OCLC 19981533.\nLevitt GM (2000), The Turk, Chess Automaton, Jefferson, N.C.: McFarland, ISBN 978-0-7864-0778-1.\nLighthill PS (1973), \"Artificial Intelligence: A General Survey\", Artificial Intelligence: a paper symposium, Science Research Council\nLucas J (1961), \"Minds, Machines and Gödel\", Philosophy, 36 (XXXVI): 112–127, doi:10.1017/S0031819100057983, S2CID 55408480\nLuger G, Stubblefield W (2004). Artificial Intelligence: Structures and Strategies for Complex Problem Solving (5th ed.). Benjamin/Cummings. ISBN 978-0-8053-4780-7. Retrieved 17 December 2019.\nMaker MH (2006), AI@50: AI Past, Present, Future, Dartmouth College, archived from the original on 8 October 2008, retrieved 16 October 2008\nMarkoff J (14 October 2005), \"Behind Artificial Intelligence, a Squadron of Bright Real People\", The New York Times, retrieved 16 October 2008\nMcCarthy J, Minsky M, Rochester N, Shannon C (31 August 1955), A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, archived from the original on 30 September 2008, retrieved 16 October 2008\nMcCarthy J, Hayes PJ (1969), \"Some philosophical problems from the standpoint of artificial intelligence\", in Meltzer BJ, Mitchie D (eds.), Machine Intelligence 4, Edinburgh University Press, pp. 463–502, retrieved 16 October 2008\nMcCarthy J (1974). \"Review of Lighthill report\".\nMcCorduck P (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 978-1-56881-205-2, OCLC 52197627.\nMcCullough WS, Pitts W (1943), \"A logical calculus of the ideas immanent in nervous activity\", Bulletin of Mathematical Biophysics, 5 (4): 115–127, doi:10.1007/BF02478259\nMenabrea LF, Lovelace A (1843), \"Sketch of the Analytical Engine Invented by Charles Babbage\", Scientific Memoirs, 3, retrieved 29 August 2008 With notes upon the Memoir by the Translator\nMinsky M (1967), Computation: Finite and Infinite Machines, Englewood Cliffs, N.J.: Prentice-Hall\nMinsky M, Papert S (1969), Perceptrons: An Introduction to Computational Geometry, The MIT Press, ISBN 978-0-262-63111-2, OCLC 16924756\nMinsky M (1974), A Framework for Representing Knowledge, archived from the original on 7 January 2021, retrieved 16 October 2008\nMinsky M (1986), The Society of Mind, Simon and Schuster, ISBN 978-0-671-65713-0, OCLC 223353010\nMinsky M (2001), It's 2001. Where Is HAL?, Dr. Dobb's Technetcast, retrieved 8 August 2009\nMoor J, ed. (2003), The Turing Test: The Elusive Standard of Artificial Intelligence, Dordrecht: Kluwer Academic Publishers, ISBN 978-1-4020-1205-1\nMoravec H (1976), The Role of Raw Power in Intelligence, archived from the original on 3 March 2016, retrieved 16 October 2008\nMoravec H (1988), Mind Children, Harvard University Press, ISBN 978-0-674-57618-6, OCLC 245755104\nMulvihill M (17 October 2012). \"1907: was the first portable computer design Irish?\". Ingenious Ireland.\nNeedham J (1986). Science and Civilization in China: Volume 2. Taipei: Caves Books Ltd.\nNewell A, Simon HA (1995) [1963], \"GPS: A Program that Simulates Human Thought\", in Feigenbaum E, Feldman J (eds.), Computers and Thought, New York: McGraw-Hill, ISBN 978-0-262-56092-4, OCLC 246968117\nNewquist HP (1994), The Brain Makers: Genius, Ego, And Greed in the Quest For Machines That Think, New York: Macmillan/SAMS, ISBN 978-0-9885937-1-8, OCLC 313139906\nNRC (1999), \"Developments in Artificial Intelligence\", Funding a Revolution: Government Support for Computing Research, National Academy Press, ISBN 978-0-309-06278-7, OCLC 246584055\nNick M (2005), Al Jazari: The Ingenious 13th Century Muslim Mechanic, Al Shindagah, retrieved 16 October 2008\nNilsson N (30 October 2009). The Quest for Artificial Intelligence. Cambridge University Press. ISBN 978-0-52-112293-1.\nO'Connor KM (1994), The alchemical creation of life (takwin) and other concepts of Genesis in medieval Islam, University of Pennsylvania, pp. 1–435, retrieved 10 January 2007\nOlsen S (10 May 2004), Newsmaker: Google's man behind the curtain, CNET, retrieved 17 October 2008\nOlsen S (18 August 2006), Spying an intelligent search engine, CNET, retrieved 17 October 2008\nPearl J (1988), Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, San Mateo, California: Morgan Kaufmann, ISBN 978-1-55860-479-7, OCLC 249625842\nPoole D, Mackworth A, Goebel R (1998), Computational Intelligence: A Logical Approach, Oxford University Press., ISBN 978-0-19-510270-3\nPollack A (11 October 1984). \"Technology, Fuzzy Logic For Computers\". The New York Times.\nPollack A (2 April 1989). \"Fuzzy Computer Theory: How to Mimic the Mind?\". The New York Times.\nQuevedo LT (1914), \"Revista de la Academia de Ciencias Exacta\", Ensayos sobre Automática – Su definicion. Extension teórica de sus aplicaciones, vol. 12, pp. 391–418\nQuevedo LT (1915), \"Revue Génerale des Sciences Pures et Appliquées\", Essais sur l'Automatique - Sa définition. Etendue théorique de ses applications, vol. 2, pp. 601–611\nRandall B (1982), \"From Analytical Engine to Electronic Digital Computer: The Contributions of Ludgate, Torres, and Bush\", fano.co.uk, retrieved 29 October 2018\nRussell SJ, Norvig P (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2\nRussell SJ, Norvig P (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-13-461099-3. LCCN 20190474.\nSamuel AL (July 1959), \"Some studies in machine learning using the game of checkers\", IBM Journal of Research and Development, 3 (3): 210–219, CiteSeerX 10.1.1.368.2254, doi:10.1147/rd.33.0210, S2CID 2126705, archived from the original on 3 March 2016, retrieved 20 August 2007\nSaygin AP, Cicekli I, Akman V (2000), \"Turing Test: 50 Years Later\" (PDF), Minds and Machines, 10 (4): 463–518, doi:10.1023/A:1011288000451, hdl:11693/24987, S2CID 990084, archived from the original (PDF) on 9 April 2011, retrieved 7 January 2004 Reprinted in Moor (2003, pp. 23–78)\nSearle J (1980), \"Minds, Brains and Programs\", Behavioral and Brain Sciences, 3 (3): 417–457, doi:10.1017/S0140525X00005756, archived from the original on 10 December 2007, retrieved 13 May 2009\nSimon HA, Newell A (1958), \"Heuristic Problem Solving: The Next Advance in Operations Research\", Operations Research, 6: 1–10, doi:10.1287/opre.6.1.1\nSimon HA (1965), The Shape of Automation for Men and Management, New York: Harper & Row\nSkillings J (2006), Newsmaker: Getting machines to think like us, CNET, retrieved 8 October 2008\nTascarella P (14 August 2006), \"Robotics firms find fundraising struggle, with venture capital shy\", Pittsburgh Business Times, retrieved 15 March 2016\nTuring A (1936–1937), \"On Computable Numbers, with an Application to the Entscheidungsproblem\", Proceedings of the London Mathematical Society, 2, 42 (42): 230–265, doi:10.1112/plms/s2-42.1.230, S2CID 73712, retrieved 8 October 2008\nTuring A (October 1950). \"Computing Machinery and Intelligence\". Mind. 59 (236): 433–460. doi:10.1093/mind/LIX.236.433. ISSN 1460-2113. JSTOR 2251299. S2CID 14636783.\nTurkle S (1984). The second self: computers and the human spirit. Simon and Schuster. ISBN 978-0-671-46848-4. OCLC 895659909.\nWason PC, Shapiro D (1966). \"Reasoning\". In Foss, B. M. (ed.). New horizons in psychology. Harmondsworth: Penguin. Retrieved 18 November 2019.\nWeizenbaum J (1976), Computer Power and Human Reason, W.H. Freeman & Company, ISBN 978-0-14-022535-8, OCLC 10952283",
      "cleaned_text": "The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain. The field of AI research was founded at a workshop held on the campus of Dartmouth College in 1956. Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars with the hope of making this vision come true. Eventually, it became obvious that researchers had grossly underestimated the difficulty of this feat. In 1974, criticism from James Lighthill and pressure from the U.S.A. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success of expert systems reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an \"AI winter\"). Nevertheless, research and funding continued to grow under other names. In the early 2000s, machine learning was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications, amongst other use cases. Investment in AI boomed in the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of large language models (LLMs) like ChatGPT. These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks and ethical implications of advanced AI have also emerged, causing debate about the future of AI and its impact on society. In Greek mythology, Talos was a creature made of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily. According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos. In the Argonautica, Jason and the Argonauts defeated Talos by removing a plug near his foot, causing the vital ichor to flow out from his body and rendering him lifeless. Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved. In Of the Nature of Things, the Swiss alchemist Paracelsus describes a procedure that he claims can fabricate an \"artificial man\". By placing the \"sperm of a man\" in horse dung, and feeding it the \"Arcanum of Mans blood\" after 40 days, the concoction will become a living infant. The earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century. During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God's names on it, into the mouth of the clay figure. Unlike legendary automata like Brazen Heads, a Golem was unable to speak. Takwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals. In Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies. By the 19th century, ideas about artificial men and thinking machines became a popular theme in fiction. Notable works like Mary Shelley's Frankenstein and Karel Čapek's R.U.R. (Rossum's Universal Robots) explored the concept of artificial life. Speculative essays, such as Samuel Butler's \"Darwin among the Machines\", and Edgar Allan Poe's \"Maelzel's Chess Player\" reflected society's growing interest in machines with artificial intelligence. AI remains a common topic in science fiction today. Realistic humanoid automata were built by craftsman from many civilizations, including Yan Shi, Hero of Alexandria, Al-Jazari, Haroun al-Rashid, Jacques de Vaucanson, Leonardo Torres y Quevedo, Pierre Jaquet-Droz and Wolfgang von Kempelen. The oldest known automata were the sacred statues of ancient Egypt and Greece. The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion-Hermes Trismegistus wrote that \"by discovering the true nature of the gods, man has been able to reproduce it\". English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues. During the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-Protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard. These legends were similar to the Norse myth of the Head of Mímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War. Odin is said to have \"embalmed\" the head with herbs and spoke incantations over it such that Mímir's head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel. Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical-or \"formal\"-reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction by the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism), Euclid (whose Elements was a model of formal reasoning), al-Khwārizmī (who developed algebra and gave his name to the word algorithm) and European scholastic philosophers such as William of Ockham and Duns Scotus. Spanish philosopher Ramon Llull (1232-1315) developed several logical machines devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas. In the 17th century, Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry. Hobbes famously wrote in Leviathan: \"For reason ... is nothing but reckoning, that is adding and subtracting\". Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that \"there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate.\" These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research. The study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift. Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: \"can all of mathematical reasoning be formalized?\" His question was answered by Gödel's incompleteness proof, Turing's machine and Church's Lambda calculus. Their answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction. The key insight was the Turing machine-a simple theoretical construct that captured the essence of abstract symbol manipulation. This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines. Calculating machines were designed or built in antiquity and throughout history by many people, including Gottfried Leibniz, Joseph Marie Jacquard, Charles Babbage, Percy Ludgate, Leonardo Torres Quevedo, Vannevar Bush, and others. Ada Lovelace speculated that Babbage's machine was \"a thinking or ... reasoning machine\", but warned \"It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers\" of the machine. The first modern computers were the massive machines of the Second World War (such as Konrad Zuse's Z3, Alan Turing's Heath Robinson and Colossus, Atanasoff and Berry's ABC and ENIAC at the University of Pennsylvania). ENIAC was based on the theoretical foundation laid by Alan Turing and developed by John von Neumann, and proved to be the most influential. The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an \"electronic brain\". In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research. Alan Turing was among the first people to seriously investigate the theoretical possibility of \"machine intelligence\". The field of \"artificial intelligence research\" was founded as an academic discipline in 1956. In 1950 Turing published a landmark paper \"Computing Machinery and Intelligence\", in which he speculated about the possibility of creating machines that think. In the paper, he noted that \"thinking\" is difficult to define and devised his famous Turing Test: If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was \"thinking\". This simplified version of the problem allowed Turing to argue convincingly that a \"thinking machine\" was at least plausible and the paper answered all the most common objections to the proposition. The Turing Test was the first serious proposal in the philosophy of artificial intelligence. Donald Hebb was a Canadian psychologist whose work laid the foundation for modern neuroscience, particularly in understanding learning, memory, and neural plasticity. His most influential book, The Organization of Behavior (1949), introduced the concept of Hebbian learning, often summarized as \"cells that fire together wire together.\" Hebb began formulating the foundational ideas for this book in the early 1940s, particularly during his time at the Yerkes Laboratories of Primate Biology from 1942 to 1947. He made extensive notes between June 1944 and March 1945 and sent a complete draft to his mentor Karl Lashley in 1946. The manuscript for The Organization of Behavior wasn’t published until 1949. The delay was due to various factors, including World War II and shifts in academic focus. By the time it was published, several of his peers had already published related ideas, making Hebb’s work seem less groundbreaking at first glance. However, his synthesis of psychological and neurophysiological principles became a cornerstone of neuroscience and machine learning. Walter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network. The paper was influenced by Turing's paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function. One of the students inspired by Pitts and McCulloch was Marvin Minsky who was a 24-year-old graduate student at the time. In 1951 Minsky and Dean Edmonds built the first neural net machine, the SNARC. Minsky would later become one of the most important leaders and innovators in AI. Experimental robots such as W. Grey Walter's turtles and the Johns Hopkins Beast, were built in the 1950s. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry. In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. Arthur Samuel's checkers program, the subject of his 1959 paper \"Some Studies in Machine Learning Using the Game of Checkers\", eventually achieved sufficient skill to challenge a respectable amateur. Samuel's program was among the first uses of what would later be called machine learning. Game AI would continue to be used as a measure of progress in AI throughout its history. When access to digital computers became possible in the mid-fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines. In 1955, Allen Newell and future Nobel Laureate Herbert A. Simon created the \"Logic Theorist\", with help from J. C. Shaw. The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some. Simon said that they had \"solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.\" The symbolic reasoning paradigm they introduced would dominate AI research and funding until the middle 90s, as well as inspire the cognitive revolution. The Dartmouth workshop of 1956 was a pivotal event that marked the formal inception of AI as an academic discipline. It was organized by Marvin Minsky and John McCarthy, with the support of two senior scientists Claude Shannon and Nathan Rochester of IBM. The proposal for the conference stated they intended to test the assertion that \"every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it\". The term \"Artificial Intelligence\" was introduced by John McCarthy at the workshop. The participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research. At the workshop Newell and Simon debuted the \"Logic Theorist\". The workshop was the moment that AI gained its name, its mission, its first major success and its key players, and is widely considered the birth of AI. In the autumn of 1956, Newell and Simon also presented the Logic Theorist at a meeting of the Special Interest Group in Information Theory at the Massachusetts Institute of Technology (MIT). At the same meeting, Noam Chomsky discussed his generative grammar, and George Miller described his landmark paper \"The Magical Number Seven, Plus or Minus Two\". Miller wrote \"I left the symposium with a conviction, more intuitive than rational, that experimental psychology, theoretical linguistics, and the computer simulation of cognitive processes were all pieces from a larger whole.\" This meeting was the beginning of the \"cognitive revolution\"-an interdisciplinary paradigm shift in psychology, philosophy, computer science and neuroscience. It inspired the creation of the sub-fields of symbolic artificial intelligence, generative linguistics, cognitive science, cognitive psychology, cognitive neuroscience and the philosophical schools of computationalism and functionalism. All these fields used related tools to model the mind and results discovered in one field were relevant to the others. The cognitive approach allowed researchers to consider \"mental objects\" like thoughts, plans, goals, facts or memories, often analyzed using high level symbols in functional networks. These objects had been forbidden as \"unobservable\" by earlier paradigms such as behaviorism. Symbolic mental objects would become the major focus of AI research and funding for the next several decades. The programs developed in the years after the Dartmouth Workshop were, to most people, simply \"astonishing\": computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such \"intelligent\" behavior by machines was possible at all. Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years. Government agencies like the Defense Advanced Research Projects Agency (DARPA, then known as \"ARPA\") poured money into the field. Artificial Intelligence laboratories were set up at a number of British and US universities in the latter 1950s and early 1960s. There were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these: Many early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end. The principal difficulty was that, for many problems, the number of possible paths through the \"maze\" was astronomical (a situation known as a \"combinatorial explosion\"). Researchers would reduce the search space by using heuristics that would eliminate paths that were unlikely to lead to a solution. Newell and Simon tried to capture a general version of this algorithm in a program called the \"General Problem Solver\". Other \"searching\" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958) and Symbolic Automatic Integrator (SAINT), written by Minsky's student James Slagle in 1961. Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of the robot Shakey. An important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems. A semantic net represents concepts (e.g. \"house\", \"door\") as nodes, and relations among concepts as links between the nodes (e.g. \"has-a\"). The first AI program to use a semantic net was written by Ross Quillian and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory. Joseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a computer program (see ELIZA effect). But in fact, ELIZA simply gave a canned response or repeated back what was said to it, rephrasing its response with a few grammar rules. ELIZA was the first chatbot. In the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds. They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a \"blocks world,\" which consists of colored blocks of various shapes and sizes arrayed on a flat surface. This paradigm led to innovative work in machine vision by Gerald Sussman, Adolfo Guzman, David Waltz (who invented \"constraint propagation\"), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. Terry Winograd's SHRDLU could communicate in ordinary English sentences about the micro-world, plan operations and execute them. In the 1960s funding was primarily directed towards laboratories researching symbolic AI, however several people still pursued research in neural networks. The perceptron, a single-layer neural network was introduced in 1958 by Frank Rosenblatt (who had been a schoolmate of Marvin Minsky at the Bronx High School of Science). Like most AI researchers, he was optimistic about their power, predicting that a perceptron \"may eventually be able to learn, make decisions, and translate languages.\" Rosenblatt was primarily funded by Office of Naval Research. Bernard Widrow and his student Ted Hoff built ADALINE (1960) and MADALINE (1962), which had up to 1000 adjustable weights. A group at Stanford Research Institute led by Charles A. Rosen and Alfred E. (Ted) Brain built two neural network machines named MINOS I (1960) and II (1963), mainly funded by U.S. Army Signal Corps. MINOS II had 6600 adjustable weights, and was controlled with an SDS 910 computer in a configuration named MINOS III (1968), which could classify symbols on army maps, and recognize hand-printed characters on Fortran coding sheets. Most of neural network research during this early period involved building and using bespoke hardware, rather than simulation on digital computers. However, partly due to lack of results and partly due to competition from symbolic AI research, the MINOS project ran out of funding in 1966. Rosenblatt failed to secure continued funding in the 1960s. In 1969, research came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Rosenblatt's predictions had been grossly exaggerated. The effect of the book was that virtually no research was funded in connectionism for 10 years. The competition for government funding ended with the victory of symbolic AI approaches over neural networks. Minsky (who had worked on SNARC) became a staunch objector to pure connectionist AI. Widrow (who had worked on ADALINE) turned to adaptive signal processing. The SRI group (which worked on MINOS) turned to symbolic AI and robotics. The main problem was the inability to train multilayered networks (versions of backpropagation had already been used in other fields but it was unknown to these researchers). The AI community became aware of backpropogation in the 80s, and, in the 21st century, neural networks would become enormously successful, fulfilling all of Rosenblatt's optimistic predictions. Rosenblatt did not live to see this, however, as he died in a boating accident in 1971. The first generation of AI researchers made these predictions about their work: 1958, H. A. Simon and Allen Newell: \"within ten years a digital computer will be the world's chess champion\" and \"within ten years a digital computer will discover and prove an important new mathematical theorem.\" 1965, H. A. Simon: \"machines will be capable, within twenty years, of doing any work a man can do.\" 1967, Marvin Minsky: \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved.\" 1970, Marvin Minsky (in Life magazine): \"In from three to eight years we will have a machine with the general intelligence of an average human being.\" In June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (ARPA, later known as DARPA). The money was used to fund project MAC which subsumed the \"AI Group\" founded by Minsky and McCarthy five years earlier. DARPA continued to provide $3 million each year until the 70s. DARPA made similar grants to Newell and Simon's program at Carnegie Mellon University and to Stanford University's AI Lab, founded by John McCarthy in 1963. Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965. These four institutions would continue to be the main centers of AI research and funding in academia for many years. The money was given with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should \"fund people, not projects!\" and allowed researchers to pursue whatever directions might interest them. This created a freewheeling atmosphere at MIT that gave birth to the hacker culture, but this \"hands off\" approach did not last. In the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised public expectations impossibly high, and when the promised results failed to materialize, funding targeted at AI was severely reduced. The lack of success indicated the techniques being used by AI researchers at the time were insufficient to achieve their goals. These setbacks did not affect the growth and progress of the field, however. The funding cuts only impacted a handful of major laboratories and the critiques were largely ignored. General public interest in the field continued to grow, the number of researchers increased dramatically, and new ideas were explored in logic programming, commonsense reasoning and many other areas. Historian Thomas Haigh argued in 2023 that there was no winter, and AI researcher Nils Nilsson described this period as the most \"exciting\" time to work in AI. In the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, \"toys\". AI researchers had begun to run into several limits that would be only conquered decades later, and others that still stymie the field in the 2020s: Limited computer power: There was not enough memory or processing speed to accomplish anything truly useful. For example: Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only 20 words, because that was all that would fit in memory. Hans Moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence. He suggested an analogy: artificial intelligence requires computer power in the same way that aircraft require horsepower. Below a certain threshold, it's impossible, but, as power increases, eventually it could become easy. \"With enough horsepower,\" he wrote, \"anything will fly\". Intractability and the combinatorial explosion: In 1972 Richard Karp (building on Stephen Cook's 1971 theorem) showed there are many problems that can only be solved in exponential time. Finding optimal solutions to these problems requires extraordinary amounts of computer time, except when the problems are trivial. This limitation applied to all symbolic AI programs that used search trees and meant that many of the \"toy\" solutions used by AI would never scale to useful systems. Moravec's paradox: Early AI research had been very successful at getting computers to do \"intelligent\" tasks like proving theorems, solving geometry problems and playing chess. Their success at these intelligent tasks convinced them that the problem of intelligent behavior had been largely solved. However, they utterly failed to make progress on \"unintelligent\" tasks like recognizing a face or crossing a room without bumping into anything. By the 1980s, researchers would realize that symbolic reasoning was utterly unsuited for these perceptual and sensorimotor tasks and that there were limits to this approach. The breadth of commonsense knowledge: Many important artificial intelligence applications like vision or natural language require enormous amounts of information about the world: the program needs to have some idea of what it might be looking at or what it is talking about. This requires that the program know most of the same things about the world that a child does. Researchers soon discovered that this was a vast amount of information with billions of atomic facts. No one in 1970 could build a database large enough and no one knew how a program might learn so much information. Representing commonsense reasoning: A number of related problems appeared when researchers tried to represent commonsense reasoning using formal logic or symbols. Descriptions of very ordinary deductions tended to get longer and longer the more one worked on them, as more and more exceptions, clarifications and distinctions were required. However, when people thought about ordinary concepts they did not rely on precise definitions, rather they seemed to make hundreds of imprecise assumptions, correcting them when necessary using their entire body of commonsense knowledge. Gerald Sussman observed that \"using precise language to describe essentially imprecise concepts doesn't make them any more precise.\" The agencies which funded AI research, such as the British government, DARPA and the National Research Council (NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected AI research. The pattern began in 1966 when the Automatic Language Processing Advisory Committee (ALPAC) report criticized machine translation efforts. After spending $20 million, the NRC ended all support. In 1973, the Lighthill report on the state of AI research in the UK criticized the failure of AI to achieve its \"grandiose objectives\" and led to the dismantling of AI research in that country. (The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.) DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of $3 million. Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues. \"Many researchers were caught up in a web of increasing exaggeration.\" However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund \"mission-oriented direct research, rather than basic undirected research\". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA, which instead directed money at specific projects with clear objectives, such as autonomous tanks and battle management systems. The major laboratories (MIT, Stanford, CMU and Edinburgh) had been receiving generous support from their governments, and when it was withdrawn, these were the only places that were seriously impacted by the budget cuts. The thousands of researchers outside these institutions and the many more thousands that were joining the field were unaffected. Several philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could. Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little \"symbol processing\" and a great deal of embodied, instinctive, unconscious \"know how\". John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to \"understand\" the symbols that it uses (a quality called \"intentionality\"). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as \"thinking\". These critiques were not taken seriously by AI researchers. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference \"know how\" or \"intentionality\" made to an actual computer program. MIT's Minsky said of Dreyfus and Searle \"they misunderstand, and should be ignored.\" Dreyfus, who also taught at MIT, was given a cold shoulder: he later said that AI researchers \"dared not be seen having lunch with me.\" Joseph Weizenbaum, the author of ELIZA, was also an outspoken critic of Dreyfus' positions, but he \"deliberately made it plain that [his AI colleagues' treatment of Dreyfus] was not the way to treat a human being,\" and was unprofessional and childish. Weizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a \"computer program which can conduct psychotherapeutic dialogue\" based on ELIZA. Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life. Logic was introduced into AI research as early as 1958, by John McCarthy in his Advice Taker proposal. In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm. However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems. A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel who created the successful logic programming language Prolog. Prolog uses a subset of logic (Horn clauses, closely related to \"rules\" and \"production rules\") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition. Critics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof. McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems-not machines that think as people do. Among the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like \"story understanding\" and \"object recognition\" that required a machine to think like a person. In order to use ordinary concepts like \"chair\" or \"restaurant\" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. MIT chose instead to focus on writing programs that solved a given task without using high-level abstract definitions or general theories of cognition, and measured performance by iterative testing, rather than arguments from first principles. Schank described their \"anti-logic\" approaches as scruffy, as opposed to the neat paradigm used by McCarthy, Kowalski, Feigenbaum, Newell and Simon. In 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on (none of which are true for all birds). Minsky associated these assumptions with the general category and they could be inherited by the frames for subcategories and individuals, or over-ridden as necessary. He called these structures frames. Schank used a version of frames he called \"scripts\" to successfully answer questions about short stories in English. Frames would eventually be widely used in software engineering under the name object-oriented programming. The logicians rose to the challenge. Pat Hayes claimed that \"most of 'frames' is just a new syntax for parts of first-order logic.\" But he noted that \"there are one or two apparently minor details which give a lot of trouble, however, especially defaults\". Ray Reiter admitted that \"conventional logics, such as first-order logic, lack the expressive power to adequately represent the knowledge required for reasoning by default\". He proposed augmenting first-order logic with a closed world assumption that a conclusion holds (by default) if its contrary cannot be shown. He showed how such an assumption corresponds to the common sense assumption made in reasoning with frames. He also showed that it has its \"procedural equivalent\" as negation as failure in Prolog. The closed world assumption, as formulated by Reiter, \"is not a first-order notion. (It is a meta notion.)\" However, Keith Clark showed that negation as finite failure can be understood as reasoning implicitly with definitions in first-order logic including a unique name assumption that different terms denote different individuals. During the late 1970s and throughout the 1980s, a variety of logics and extensions of first-order logic were developed both for negation as failure in logic programming and for default reasoning more generally. Collectively, these logics have become known as non-monotonic logics. In the 1980s, a form of AI program called \"expert systems\" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. Governments provided substantial funding, such as Japan's fifth generation computer project and the U.S. Strategic Computing Initiative. \"Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars in 1988.\" An expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts. The earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings. MYCIN, developed in 1972, diagnosed infectious blood diseases. They demonstrated the feasibility of the approach. Expert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem) and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point. In 1980, an expert system called R1 was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion. In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. Much to the chagrin of scruffies, they initially chose Prolog as the primary computer language for the project. Other countries responded with new programs of their own. The UK began the £350 million Alvey project. A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or \"MCC\") to fund large scale projects in AI and information technology. DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988. The power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. \"AI researchers were beginning to suspect-reluctantly, for it violated the scientific canon of parsimony-that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,\" writes Pamela McCorduck. \"[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay\". Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s. It was hoped that vast databases would solve the commonsense knowledge problem and provide the support that commonsense reasoning required. In the 1980s some researchers attempted to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started a database called Cyc, argued that there is no shortcut ― the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand. Although symbolic knowledge representation and logical reasoning produced useful applications in the 80s and received massive amounts of funding, it was still unable to solve problems in perception, robotics, learning and common sense. A small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches, such as \"connectionism\", robotics, \"soft\" computing and reinforcement learning. Nils Nilsson called these approaches \"sub-symbolic\". In 1982, physicist John Hopfield was able to prove that a form of neural network (now called a \"Hopfield net\") could learn and process information, and provably converges after enough time under any fixed condition. It was a breakthrough, as it was previously thought that nonlinear networks would, in general, evolve chaotically. Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called \"backpropagation\". These two developments helped to revive the exploration of artificial neural networks. Neural networks, along with several other similar models, received widespread attention after the 1986 publication of the Parallel Distributed Processing, a two volume collection of papers edited by Rumelhart and psychologist James McClelland. The new field was christened \"connectionism\" and there was a considerable debate between advocates of symbolic AI and the \"connectionists\". Hinton called symbols the \"luminous aether of AI\" - that is, an unworkable and misleading model of intelligence. This was a direct attack on the principles that inspired the cognitive revolution. Neural networks started to advance state of the art in some specialist areas such as protein structure prediction. Following pioneering work from Terry Sejnowski, cascading multilayer perceptrons such as PhD and PsiPred reached near-theoretical maximum accuracy in predicting secondary structure. In 1990, Yann LeCun at Bell Labs used convolutional neural networks to recognize handwritten digits. The system was used widely in 90s, reading zip codes and personal checks. This was the first genuinely useful application of neural networks. Rodney Brooks, Hans Moravec and others argued that, in order to show real intelligence, a machine needs to have a body - it needs to perceive, move, survive and deal with the world. Sensorimotor skills are essential to higher level skills such as commonsense reasoning. They can't be efficiently implemented using abstract symbolic reasoning, so AI should solve the problems of perception, mobility, manipulation and survival without using symbolic representation at all. These robotics researchers advocated building intelligence \"from the bottom up\". A precursor to this idea was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.) In his 1990 paper \"Elephants Don't Play Chess,\" robotics researcher Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since \"the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough.\" In the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the \"embodied mind thesis\". Soft computing uses methods that work with incomplete and imprecise information. They do not attempt to give precise, logical answers, but give results that are only \"probably\" correct. This allowed them to solve problems that precise symbolic methods could not handle. Press accounts often claimed these tools could \"think like a human\". Judea Pearl's Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, an influential 1988 book brought probability and decision theory into AI. Fuzzy logic, developed by Lofti Zadeh in the 60s, began to be more widely used in AI and robotics. Evolutionary computation and artificial neural networks also handle imprecise information, and are classified as \"soft\". In the 90s and early 2000s many other soft computing tools were developed and put into use, including Bayesian networks, hidden Markov models, information theory and stochastic modeling. These tools in turn depended on advanced mathematical techniques such as classical optimization. For a time in the 1990s and early 2000s, these soft tools were studied by a subfield of AI called \"computational intelligence\". Reinforcement learning gives an agent a reward every time it performs a desired action well, and may give negative rewards (or \"punishments\") when it performs poorly. It was described in the first half of the twentieth century by psychologists using animal models, such as Thorndike, Pavlov and Skinner. In the 1950s, Alan Turing and Arthur Samuel foresaw the role of reinforcement learning in AI. A successful and influential research program was led by Richard Sutton and Andrew Barto beginning 1972. Their collaboration revolutionized the study of reinforcement learning and decision making over the four decades. In 1988, Sutton described machine learning in terms of decision theory (i.e., the Markov decision process). This gave the subject a solid theoretical foundation and access to a large body of theoretical results developed in the field of operations research. Also in 1988, Sutton and Barto developed the \"temporal difference\" (TD) learning algorithm, where the agent is rewarded only when its predictions about the future show improvement. It significantly outperformed previous algorithms. TD-learning was used by Gerald Tesauro in 1992 in the program TD-Gammon, which played backgammon as well as the best human players. The program learned the game by playing against itself with zero prior knowledge. In an interesting case of interdisciplinary convergence, neurologists discovered in 1997 that the dopamine reward system in brains also uses a version of the TD-learning algorithm. TD learning would be become highly influential in the 21st century, used in both AlphaGo and AlphaZero. The business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. As dozens of companies failed, the perception in the business world was that the technology was not viable. The damage to AI's reputation would last into the 21st century. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of \"artificial intelligence\". Over the next 20 years, AI consistently delivered working solutions to specific isolated problems. By the late 1990s, it was being used throughout the technology industry, although somewhat behind the scenes. The success was due to increasing computer power, by collaboration with other fields (such as mathematical optimization and statistics) and using the highest standards of scientific accountability. By 2000, AI had achieved some of its oldest goals. The field was both more cautious and more successful than it had ever been. The term \"AI winter\" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow. Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks. The first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight. Eventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, and they were \"brittle\" (i.e., they could make grotesque mistakes when given unusual inputs). Expert systems proved useful, but only in a few special contexts. In the late 1980s, the Strategic Computing Initiative cut funding to AI \"deeply and brutally\". New leadership at DARPA had decided that AI was not \"the next wave\" and directed funds towards projects that seemed more likely to produce immediate results. By 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like \"carry on a casual conversation\" would not be accomplished for another 30 years. As with other AI projects, expectations had run much higher than what was actually possible. Over 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI. In 1994, HP Newquist stated in The Brain Makers that \"The immediate future of artificial intelligence-in its commercial form-seems to rest in part on the continued success of neural networks.\" In the 1990s, algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems and their solutions proved to be useful throughout the technology industry, such as data mining, industrial robotics, logistics, speech recognition, banking software, medical diagnosis and Google's search engine. The field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science. Nick Bostrom explains: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, \"cognitive systems\" or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding. In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: \"Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\" AI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past. Most of the new directions in AI relied heavily on mathematical models, including artificial neural networks, probabilistic reasoning, soft computing and reinforcement learning. In the 90s and 2000s, many other highly mathematical tools were adapted for AI. These tools were applied to machine learning, perception and mobility. There was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like statistics, mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous \"scientific\" discipline. Another key reason for the success in the 90s was that AI researchers focussed on specific problems with verifiable solutions (an approach later derided as narrow AI). This provided useful tools in the present, rather than speculation about the future. A new paradigm called \"intelligent agents\" became widely accepted during the 1990s. Although earlier researchers had proposed modular \"divide and conquer\" approaches to AI, the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI. When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete. An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are \"intelligent agents\", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as \"the study of intelligent agents\". This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence. The paradigm gave researchers license to study isolated problems and to disagree about methods, but still retain hope that their work could be combined into an agent architecture that would be capable of general intelligence. On May 11, 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov. In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an urban environment while responding to traffic hazards and adhering to traffic laws. These successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computers by the 90s. In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951. This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years. The fundamental problem of \"raw computer power\" was slowly being overcome. Electronic literature experiments such as The Impermanence Agent (1998 -2002) and digital art such as Agent Ruby, used AI in their art and literature, \"laying bare the bias accompanying forms of technology that feign objectivity.\" In the first decades of the 21st century, access to large amounts of data (known as \"big data\"), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. A turning point was the success of deep learning around 2012 which improved the performance of machine learning on many tasks, including image and video processing, text analysis, and speech recognition. Investment in AI increased along with its capabilities, and by 2016, the market for AI-related products, hardware, and software reached more than $8 billion, and the New York Times reported that interest in AI had reached a \"frenzy\". In 2002, Ben Goertzel and others became concerned that AI had largely abandoned its original goal of producing versatile, fully intelligent machines, and argued in favor of more direct research into artificial general intelligence. By the mid-2010s several companies and institutions had been founded to pursue Artificial General Intelligence (AGI), such as OpenAI and Google's DeepMind. During the same period, new insights into superintelligence raised concerns that AI was an existential threat. The risks and unintended consequences of AI technology became an area of serious academic research after 2016. The success of machine learning in the 2000s depended on the availability of vast amounts of training data and faster computers. Russell and Norvig wrote that the \"improvement in performance obtained by increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be made by tweaking the algorithm.\" Geoffrey Hinton recalled that back in the 90s, the problem was that \"our labeled datasets were thousands of times too small. [And] our computers were millions of times too slow.\" This was no longer true by 2010. The most useful data in the 2000s came from curated, labeled data sets created specifically for machine learning and AI. In 2007, a group at UMass Amherst released Labeled Faces in the Wild, an annotated set of images of faces that was widely used to train and test face recognition systems for the next several decades. Fei-Fei Li developed ImageNet, a database of three million images captioned by volunteers using the Amazon Mechanical Turk. Released in 2009, it was a useful body of training data and a benchmark for testing for the next generation of image processing systems. Google released word2vec in 2013 as an open source resource. It used large amounts of data text scraped from the internet and word embedding to create a numeric vector to represent each word. Users were surprised at how well it was able to capture word meanings, for example, ordinary vector addition would give equivalences like China + River = Yangtze, London-England+France = Paris. This database in particular would be essential for the development of large language models in the late 2010s. The explosive growth of the internet gave machine learning programs access to billions of pages of text and images that could be scraped. And, for specific problems, large privately held databases contained the relevant data. McKinsey Global Institute reported that \"by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data\". This collection of information was known in the 2000s as big data. In a Jeopardy! exhibition match in February 2011, IBM's question answering system Watson defeated the two best Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. Watson's expertise would have been impossible without the information available on the internet. In 2012, AlexNet, a deep learning model, developed by Alex Krizhevsky, won the ImageNet Large Scale Visual Recognition Challenge, with significantly fewer errors than the second-place winner. Krizhevsky worked with Geoffrey Hinton at the University of Toronto. This was a turning point in machine learning: over the next few years dozens of other approaches to image recognition were abandoned in favor of deep learning. Deep learning uses a multi-layer perceptron. Although this architecture has been known since the 60s, getting it to work requires powerful hardware and large amounts of training data. Before these became available, improving performance of image processing systems required hand-crafted ad hoc features that were difficult to implement. Deep learning was simpler and more general. Deep learning was applied to dozens of problems over the next few years (such as speech recognition, machine translation, medical diagnosis, and game playing). In every case it showed enormous gains in performance. Investment and interest in AI boomed as a result. It became fashionable in the 2000s to begin talking about the future of AI again and several popular books considered the possibility of superintelligent machines and what they might mean for human society. Some of this was optimistic (such as Ray Kurzweil's The Singularity is Near), but others warned that a sufficiently powerful AI was existential threat to humanity, such as Nick Bostrom and Eliezer Yudkowsky. The topic became widely covered in the press and many leading intellectuals and politicians commented on the issue. AI programs in the 21st century are defined by their goals - the specific measures that they are designed to optimize. Nick Bostrom's influential 2014 book Superintelligence argued that, if one isn't careful about defining these goals, the machine may cause harm to humanity in the process of achieving a goal. Stuart J. Russell used the example of an intelligent robot that kills its owner to prevent it from being unplugged, reasoning \"you can't fetch the coffee if you're dead\". (This problem is known by the technical term \"instrumental convergence\".) The solution is to align the machine's goal function with the goals of its owner and humanity in general. Thus, the problem of mitigating the risks and unintended consequences of AI became known as \"the value alignment problem\" or AI alignment. At the same time, machine learning systems had begun to have disturbing unintended consequences. Cathy O'Neil explained how statistical algorithms had been among the causes of the 2008 economic crash, Julia Angwin of ProPublica argued that the COMPAS system used by the criminal justice system exhibited racial bias under some measures, others showed that many machine learning systems exhibited some form of racial bias, and there were many other examples of dangerous outcomes that had resulted from machine learning systems. In 2016, the election of Donald Trump and the controversy over the COMPAS system illuminated several problems with the current technological infrastructure, including misinformation, social media algorithms designed to maximize engagement, the misuse of personal data and the trustworthiness of predictive models. Issues of fairness and unintended consequences became significantly more popular at AI conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The value alignment problem became a serious field of academic study. In the early 2000s, several researchers became concerned that mainstream AI was too focused on \"measurable performance in specific applications\" (known as \"narrow AI\") and had abandoned AI's original goal of creating versatile, fully intelligent machines. An early critic was Nils Nilsson in 1995, and similar opinions were published by AI elder statesmen John McCarthy, Marvin Minsky, and Patrick Winston in 2007-2009. Minsky organized a symposium on \"human-level AI\" in 2004. Ben Goertzel adopted the term \"artificial general intelligence\" for the new sub-field, founding a journal and holding conferences beginning in 2008. The new field grew rapidly, buoyed by the continuing success of artificial neural networks and the hope that it was the key to AGI. Several competing companies, laboratories and foundations were founded to develop AGI in the 2010s. DeepMind was founded in 2010 by three English scientists, Demis Hassabis, Shane Legg and Mustafa Suleyman, with funding from Peter Thiel and later Elon Musk. The founders and financiers were deeply concerned about AI safety and the existential risk of AI. DeepMind's founders had a personal connection with Yudkowsky and Musk was among those who was actively raising the alarm. Hassabis was both worried about the dangers of AGI and optimistic about its power; he hoped they could \"solve AI, then solve everything else.\" The New York Times wrote in 2023 \"At the heart of this competition is a brain-stretching paradox. The people who say they are most worried about AI are among the most determined to create it and enjoy its riches. They have justified their ambition with their strong belief that they alone can keep AI from endangering Earth.\" In 2012, Geoffrey Hinton (who been leading neural network research since the 80s) was approached by Baidu, which wanted to hire him and all his students for an enormous sum. Hinton decided to hold an auction and, at a Lake Tahoe AI conference, they sold themselves to Google for a price of $44 million. Hassabis took notice and sold DeepMind to Google in 2014, on the condition that it would not accept military contracts and would be overseen by an ethics board. Larry Page of Google, unlike Musk and Hassabis, was an optimist about the future of AI. Musk and Paige became embroiled in an argument about the risk of AGI at Musk's 2015 birthday party. They had been friends for decades but stopped speaking to each other shortly afterwards. Musk attended the one and only meeting of the DeepMind's ethics board, where it became clear that Google was uninterested in mitigating the harm of AGI. Frustrated by his lack of influence he founded OpenAI in 2015, enlisting Sam Altman to run it and hiring top scientists. OpenAI began as a non-profit, \"free from the economic incentives that were driving Google and other corporations.\" Musk became frustrated again and left the company in 2018. OpenAI turned to Microsoft for continued financial support and Altman and OpenAI formed a for-profit version of the company with more than $1 billion in financing. In 2021, Dario Amodei and 14 other scientists left OpenAI over concerns that the company was putting profits above safety. They formed Anthropic, which soon had $6 billion in financing from Microsoft and Google. The AI boom started with the initial development of key architectures and algorithms such as the transformer architecture in 2017, leading to the scaling and development of large language models exhibiting human-like traits of knowledge, attention and creativity. The new AI era began since 2020, with the public release of scaled large language models (LLMs) such as ChatGPT. In 2017, the transformer architecture was proposed by Google researchers. It exploits an attention mechanism and became widely used in large language models. Large language models, based on the transformer, were developed by AGI companies: OpenAI released GPT-3 in 2020, and DeepMind released Gato in 2022. These are foundation models: they are trained on vast quantities of unlabeled data and can be adapted to a wide range of downstream tasks. These models can discuss a huge number of topics and display general knowledge. The question naturally arises: are these models an example of artificial general intelligence? Bill Gates was skeptical of the new technology and the hype that surrounded AGI. However, Altman presented him with a live demo of ChatGPT4 passing an advanced biology test. Gates was convinced. In 2023, Microsoft Research tested the model with a large variety of tasks, and concluded that \"it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system\". In 2024, OpenAI o3, a type of advanced reasoning model developed by OpenAI was announced. On the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) benchmark developed by François Chollet in 2019, the model achieved an unofficial score of 87.5% on the semi-private test, surpassing the typical human score of 84%. The benchmark is supposed to be a necessary, but not sufficient test for AGI. Speaking of the benchmark, Chollet has said \"You’ll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.\" Investment in AI grew exponentially after 2020, with venture capital funding for generative AI companies increasing dramatically. Total AI investments rose from $18 billion in 2014 to $119 billion in 2021, with generative AI accounting for approximately 30% of investments by 2023. According to metrics from 2017 to 2021, the United States outranked the rest of the world in terms of venture capital funding, number of startups, and AI patents granted. The commercial AI scene became dominated by American Big Tech companies, whose investments in this area surpassed those from U.S.-based venture capitalists. OpenAI's valuation reached $86 billion by early 2024, while NVIDIA's market capitalization surpassed $3.3 trillion by mid-2024, making it the world's largest company by market capitalization as the demand for AI-capable GPUs surged. 15.ai, launched in March 2020 by an anonymous MIT researcher, was one of the earliest examples of generative AI gaining widespread public attention during the initial stages of the AI boom. The free web application demonstrated the ability to clone character voices using neural networks with minimal training data, requiring as little as 15 seconds of audio to reproduce a voice-a capability later corroborated by OpenAI in 2024. The service went viral on social media platforms in early 2021, allowing users to generate speech for characters from popular media franchises, and became particularly notable for its pioneering role in popularizing AI voice synthesis for creative content and memes. ChatGPT was launched on November 30, 2022, marking a pivotal moment in artificial intelligence's public adoption. Within days of its release it went viral, gaining over 100 million users in two months and becoming the fastest-growing consumer software application in history. The chatbot's ability to engage in human-like conversations, write code, and generate creative content captured public imagination and led to rapid adoption across various sectors including education, business, and research. ChatGPT's success prompted unprecedented responses from major technology companies-Google declared a \"code red\" and rapidly launched Gemini (formerly known as Google Bard), while Microsoft incorporated the technology into Bing Chat. The rapid adoption of these AI technologies sparked intense debate about their implications. Notable AI researchers and industry leaders voiced both optimism and concern about the accelerating pace of development. In March 2023, over 20,000 signatories, including computer scientist Yoshua Bengio, Elon Musk, and Apple co-founder Steve Wozniak, signed an open letter calling for a pause in advanced AI development, citing \"profound risks to society and humanity.\" However, other prominent researchers like Juergen Schmidhuber took a more optimistic view, emphasizing that the majority of AI research aims to make \"human lives longer and healthier and easier.\" By mid-2024, however, the financial sector began to scrutinize AI companies more closely, particularly questioning their capacity to produce a return on investment commensurate with their massive valuations. Some prominent investors raised concerns about market expectations becoming disconnected from fundamental business realities. Jeremy Grantham, co-founder of GMO LLC, warned investors to \"be quite careful\" and drew parallels to previous technology-driven market bubbles. Similarly, Jeffrey Gundlach, CEO of DoubleLine Capital, explicitly compared the AI boom to the dot-com bubble of the late 1990s, suggesting that investor enthusiasm might be outpacing realistic near-term capabilities and revenue potential. These concerns were amplified by the substantial market capitalizations of AI-focused companies, many of which had yet to demonstrate sustainable profitability models. In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus. The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google. In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis. In 2024, the Royal Swedish Academy of Sciences awarded Nobel Prizes in recognition of groundbreaking contributions to artificial intelligence. The recipients included: In physics: John Hopfield for his work on physics-inspired Hopfield networks, and Geoffrey Hinton for foundational contributions to Boltzmann machines and deep learning. In chemistry: David Baker, Demis Hassabis, and John Jumper for their advancements in protein folding predictions. See AlphaFold. In January 2025, OpenAI announced a new AI, ChatGPT-Gov, which would be specifically designed for US government agencies to use securely. Open AI said that agencies could utilize ChatGPT Gov on a Microsoft Azure cloud or Azure Government cloud, \"on top of Microsoft’s Azure’s OpenAI Service.\" OpenAI's announcement stated that \"Self-hosting ChatGPT Gov enables agencies to more easily manage their own security, privacy, and compliance requirements, such as stringent cybersecurity frameworks (IL5, CJIS, ITAR, FedRAMP High). Additionally, we believe this infrastructure will expedite internal authorization of OpenAI’s tools for the handling of non-public sensitive data.\" Countries have invested in policies and funding to deploy autonomous robots in an attempt to address labor shortages and enhancing efficiency, while also implementing regulatory frameworks for ethical and safe development. In 2025, China invested approximately 730 billion yuan (roughly $100 billion USD) to advance AI and robotics in smart manufacturing and healthcare. The \"14th Five-Year Plan\" (2021-2025) prioritized service robots, with AI systems enabling robots to perform complex tasks like assisting in surgeries or automating factory assembly lines. Some funding also supported defense applications, such as autonomous drones. Starting in September 2025, China mandated labeling of AI-generated content to ensure transparency and public trust in these technologies. In January 2025, Stargate LLC was formed as a joint venture of OpenAI, SoftBank, Oracle, and MGX, who announced plans to invest US$500 billion in AI infrastructure in the United States by 2029. Stargate LLC started with a US$100 billion investment in re-industrialization and national security capabilities. The venture was formally announced by U.S. President Donald Trump on January 21, 2025, with SoftBank CEO Masayoshi Son appointed as chairman. The U.S. government allocated approximately $2 billion to integrate AI and robotics in manufacturing and logistics. State governments supplemented this with funding for service robots, such as those deployed in warehouses to fulfill verbal commands for inventory management or in eldercare facilities to respond to residents' requests for assistance. Some funds were directed to defense, including Lethal autonomous weapon and Military robot. In January 2025, Executive Order 14179 established an \"AI Action Plan\" to accelerate innovation and deployment of these technologies.",
      "sentences": [
        "The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen.",
        "The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on abstract mathematical reasoning.",
        "This device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain.",
        "The field of AI research was founded at a workshop held on the campus of Dartmouth College in 1956.",
        "Attendees of the workshop became the leaders of AI research for decades.",
        "Many of them predicted that machines as intelligent as humans would exist within a generation.",
        "The U.S. government provided millions of dollars with the hope of making this vision come true.",
        "Eventually, it became obvious that researchers had grossly underestimated the difficulty of this feat.",
        "In 1974, criticism from James Lighthill and pressure from the U.S.A. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence.",
        "Seven years later, a visionary initiative by the Japanese Government and the success of expert systems reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise.",
        "However, investors' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an \"AI winter\").",
        "Nevertheless, research and funding continued to grow under other names.",
        "In the early 2000s, machine learning was applied to a wide range of problems in academia and industry.",
        "The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods.",
        "Soon after, deep learning proved to be a breakthrough technology, eclipsing all other methods.",
        "The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications, amongst other use cases.",
        "Investment in AI boomed in the 2020s.",
        "The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of large language models (LLMs) like ChatGPT.",
        "These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI.",
        "However, concerns about the potential risks and ethical implications of advanced AI have also emerged, causing debate about the future of AI and its impact on society.",
        "In Greek mythology, Talos was a creature made of bronze who acted as guardian for the island of Crete.",
        "He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily.",
        "According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos.",
        "In the Argonautica, Jason and the Argonauts defeated Talos by removing a plug near his foot, causing the vital ichor to flow out from his body and rendering him lifeless.",
        "Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses.",
        "In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves.",
        "Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved.",
        "In Of the Nature of Things, the Swiss alchemist Paracelsus describes a procedure that he claims can fabricate an \"artificial man\".",
        "By placing the \"sperm of a man\" in horse dung, and feeding it the \"Arcanum of Mans blood\" after 40 days, the concoction will become a living infant.",
        "The earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century.",
        "During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God's names on it, into the mouth of the clay figure.",
        "Unlike legendary automata like Brazen Heads, a Golem was unable to speak.",
        "Takwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan.",
        "Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals.",
        "In Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body.",
        "Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.",
        "By the 19th century, ideas about artificial men and thinking machines became a popular theme in fiction.",
        "Notable works like Mary Shelley's Frankenstein and Karel Čapek's R.U.R.",
        "(Rossum's Universal Robots) explored the concept of artificial life.",
        "Speculative essays, such as Samuel Butler's \"Darwin among the Machines\", and Edgar Allan Poe's \"Maelzel's Chess Player\" reflected society's growing interest in machines with artificial intelligence.",
        "AI remains a common topic in science fiction today.",
        "Realistic humanoid automata were built by craftsman from many civilizations, including Yan Shi, Hero of Alexandria, Al-Jazari, Haroun al-Rashid, Jacques de Vaucanson, Leonardo Torres y Quevedo, Pierre Jaquet-Droz and Wolfgang von Kempelen.",
        "The oldest known automata were the sacred statues of ancient Egypt and Greece.",
        "The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion-Hermes Trismegistus wrote that \"by discovering the true nature of the gods, man has been able to reproduce it\".",
        "English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.",
        "During the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them.",
        "The late medieval alchemist and proto-Protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard.",
        "These legends were similar to the Norse myth of the Head of Mímir.",
        "According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War.",
        "Odin is said to have \"embalmed\" the head with herbs and spoke incantations over it such that Mímir's head remained able to speak wisdom to Odin.",
        "Odin then kept the head near him for counsel.",
        "Artificial intelligence is based on the assumption that the process of human thought can be mechanized.",
        "The study of mechanical-or \"formal\"-reasoning has a long history.",
        "Chinese, Indian and Greek philosophers all developed structured methods of formal deduction by the first millennium BCE.",
        "Spanish philosopher Ramon Llull (1232-1315) developed several logical machines devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge.",
        "Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.",
        "In the 17th century, Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry.",
        "Hobbes famously wrote in Leviathan: \"For reason ... is nothing but reckoning, that is adding and subtracting\".",
        "Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that \"there would be no more need of disputation between two philosophers than between two accountants.",
        "For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate.\"",
        "These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.",
        "The study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible.",
        "The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift.",
        "Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913.",
        "Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: \"can all of mathematical reasoning be formalized?\"",
        "His question was answered by Gödel's incompleteness proof, Turing's machine and Church's Lambda calculus.",
        "Their answer was surprising in two ways.",
        "First, they proved that there were, in fact, limits to what mathematical logic could accomplish.",
        "But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized.",
        "The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction.",
        "The key insight was the Turing machine-a simple theoretical construct that captured the essence of abstract symbol manipulation.",
        "This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.",
        "Calculating machines were designed or built in antiquity and throughout history by many people, including Gottfried Leibniz, Joseph Marie Jacquard, Charles Babbage, Percy Ludgate, Leonardo Torres Quevedo, Vannevar Bush, and others.",
        "Ada Lovelace speculated that Babbage's machine was \"a thinking or ... reasoning machine\", but warned \"It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers\" of the machine.",
        "The first modern computers were the massive machines of the Second World War (such as Konrad Zuse's Z3, Alan Turing's Heath Robinson and Colossus, Atanasoff and Berry's ABC and ENIAC at the University of Pennsylvania).",
        "ENIAC was based on the theoretical foundation laid by Alan Turing and developed by John von Neumann, and proved to be the most influential.",
        "The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s.",
        "Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses.",
        "Norbert Wiener's cybernetics described control and stability in electrical networks.",
        "Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals).",
        "Alan Turing's theory of computation showed that any form of computation could be described digitally.",
        "The close relationship between these ideas suggested that it might be possible to construct an \"electronic brain\".",
        "In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research.",
        "Alan Turing was among the first people to seriously investigate the theoretical possibility of \"machine intelligence\".",
        "The field of \"artificial intelligence research\" was founded as an academic discipline in 1956.",
        "In 1950 Turing published a landmark paper \"Computing Machinery and Intelligence\", in which he speculated about the possibility of creating machines that think.",
        "In the paper, he noted that \"thinking\" is difficult to define and devised his famous Turing Test: If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was \"thinking\".",
        "This simplified version of the problem allowed Turing to argue convincingly that a \"thinking machine\" was at least plausible and the paper answered all the most common objections to the proposition.",
        "The Turing Test was the first serious proposal in the philosophy of artificial intelligence.",
        "Donald Hebb was a Canadian psychologist whose work laid the foundation for modern neuroscience, particularly in understanding learning, memory, and neural plasticity.",
        "His most influential book, The Organization of Behavior (1949), introduced the concept of Hebbian learning, often summarized as \"cells that fire together wire together.\"",
        "Hebb began formulating the foundational ideas for this book in the early 1940s, particularly during his time at the Yerkes Laboratories of Primate Biology from 1942 to 1947.",
        "He made extensive notes between June 1944 and March 1945 and sent a complete draft to his mentor Karl Lashley in 1946.",
        "The manuscript for The Organization of Behavior wasn’t published until 1949.",
        "The delay was due to various factors, including World War II and shifts in academic focus.",
        "By the time it was published, several of his peers had already published related ideas, making Hebb’s work seem less groundbreaking at first glance.",
        "However, his synthesis of psychological and neurophysiological principles became a cornerstone of neuroscience and machine learning.",
        "Walter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943.",
        "They were the first to describe what later researchers would call a neural network.",
        "The paper was influenced by Turing's paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function.",
        "One of the students inspired by Pitts and McCulloch was Marvin Minsky who was a 24-year-old graduate student at the time.",
        "In 1951 Minsky and Dean Edmonds built the first neural net machine, the SNARC.",
        "Minsky would later become one of the most important leaders and innovators in AI.",
        "Experimental robots such as W. Grey Walter's turtles and the Johns Hopkins Beast, were built in the 1950s.",
        "These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.",
        "In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess.",
        "Arthur Samuel's checkers program, the subject of his 1959 paper \"Some Studies in Machine Learning Using the Game of Checkers\", eventually achieved sufficient skill to challenge a respectable amateur.",
        "Samuel's program was among the first uses of what would later be called machine learning.",
        "Game AI would continue to be used as a measure of progress in AI throughout its history.",
        "When access to digital computers became possible in the mid-fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought.",
        "This was a new approach to creating thinking machines.",
        "In 1955, Allen Newell and future Nobel Laureate Herbert A. Simon created the \"Logic Theorist\", with help from J. C. Shaw.",
        "The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some.",
        "Simon said that they had \"solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.\"",
        "The symbolic reasoning paradigm they introduced would dominate AI research and funding until the middle 90s, as well as inspire the cognitive revolution.",
        "The Dartmouth workshop of 1956 was a pivotal event that marked the formal inception of AI as an academic discipline.",
        "It was organized by Marvin Minsky and John McCarthy, with the support of two senior scientists Claude Shannon and Nathan Rochester of IBM.",
        "The proposal for the conference stated they intended to test the assertion that \"every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it\".",
        "The term \"Artificial Intelligence\" was introduced by John McCarthy at the workshop.",
        "The participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research.",
        "At the workshop Newell and Simon debuted the \"Logic Theorist\".",
        "The workshop was the moment that AI gained its name, its mission, its first major success and its key players, and is widely considered the birth of AI.",
        "In the autumn of 1956, Newell and Simon also presented the Logic Theorist at a meeting of the Special Interest Group in Information Theory at the Massachusetts Institute of Technology (MIT).",
        "At the same meeting, Noam Chomsky discussed his generative grammar, and George Miller described his landmark paper \"The Magical Number Seven, Plus or Minus Two\".",
        "Miller wrote \"I left the symposium with a conviction, more intuitive than rational, that experimental psychology, theoretical linguistics, and the computer simulation of cognitive processes were all pieces from a larger whole.\"",
        "This meeting was the beginning of the \"cognitive revolution\"-an interdisciplinary paradigm shift in psychology, philosophy, computer science and neuroscience.",
        "It inspired the creation of the sub-fields of symbolic artificial intelligence, generative linguistics, cognitive science, cognitive psychology, cognitive neuroscience and the philosophical schools of computationalism and functionalism.",
        "All these fields used related tools to model the mind and results discovered in one field were relevant to the others.",
        "The cognitive approach allowed researchers to consider \"mental objects\" like thoughts, plans, goals, facts or memories, often analyzed using high level symbols in functional networks.",
        "These objects had been forbidden as \"unobservable\" by earlier paradigms such as behaviorism.",
        "Symbolic mental objects would become the major focus of AI research and funding for the next several decades.",
        "The programs developed in the years after the Dartmouth Workshop were, to most people, simply \"astonishing\": computers were solving algebra word problems, proving theorems in geometry and learning to speak English.",
        "Few at the time would have believed that such \"intelligent\" behavior by machines was possible at all.",
        "Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years.",
        "Government agencies like the Defense Advanced Research Projects Agency (DARPA, then known as \"ARPA\") poured money into the field.",
        "Artificial Intelligence laboratories were set up at a number of British and US universities in the latter 1950s and early 1960s.",
        "There were many successful programs and new directions in the late 50s and 1960s.",
        "Among the most influential were these: Many early AI programs used the same basic algorithm.",
        "To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end.",
        "The principal difficulty was that, for many problems, the number of possible paths through the \"maze\" was astronomical (a situation known as a \"combinatorial explosion\").",
        "Researchers would reduce the search space by using heuristics that would eliminate paths that were unlikely to lead to a solution.",
        "Newell and Simon tried to capture a general version of this algorithm in a program called the \"General Problem Solver\".",
        "Other \"searching\" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958) and Symbolic Automatic Integrator (SAINT), written by Minsky's student James Slagle in 1961.",
        "Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of the robot Shakey.",
        "An important goal of AI research is to allow computers to communicate in natural languages like English.",
        "An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.",
        "A semantic net represents concepts (e.g.",
        "\"house\", \"door\") as nodes, and relations among concepts as links between the nodes (e.g.",
        "The first AI program to use a semantic net was written by Ross Quillian and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.",
        "Joseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a computer program (see ELIZA effect).",
        "But in fact, ELIZA simply gave a canned response or repeated back what was said to it, rephrasing its response with a few grammar rules.",
        "ELIZA was the first chatbot.",
        "In the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds.",
        "They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies.",
        "Much of the research focused on a \"blocks world,\" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.",
        "This paradigm led to innovative work in machine vision by Gerald Sussman, Adolfo Guzman, David Waltz (who invented \"constraint propagation\"), and especially Patrick Winston.",
        "At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life.",
        "Terry Winograd's SHRDLU could communicate in ordinary English sentences about the micro-world, plan operations and execute them.",
        "In the 1960s funding was primarily directed towards laboratories researching symbolic AI, however several people still pursued research in neural networks.",
        "The perceptron, a single-layer neural network was introduced in 1958 by Frank Rosenblatt (who had been a schoolmate of Marvin Minsky at the Bronx High School of Science).",
        "Like most AI researchers, he was optimistic about their power, predicting that a perceptron \"may eventually be able to learn, make decisions, and translate languages.\"",
        "Rosenblatt was primarily funded by Office of Naval Research.",
        "Bernard Widrow and his student Ted Hoff built ADALINE (1960) and MADALINE (1962), which had up to 1000 adjustable weights.",
        "MINOS II had 6600 adjustable weights, and was controlled with an SDS 910 computer in a configuration named MINOS III (1968), which could classify symbols on army maps, and recognize hand-printed characters on Fortran coding sheets.",
        "Most of neural network research during this early period involved building and using bespoke hardware, rather than simulation on digital computers.",
        "However, partly due to lack of results and partly due to competition from symbolic AI research, the MINOS project ran out of funding in 1966.",
        "Rosenblatt failed to secure continued funding in the 1960s.",
        "In 1969, research came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons.",
        "It suggested that there were severe limitations to what perceptrons could do and that Rosenblatt's predictions had been grossly exaggerated.",
        "The effect of the book was that virtually no research was funded in connectionism for 10 years.",
        "The competition for government funding ended with the victory of symbolic AI approaches over neural networks.",
        "Minsky (who had worked on SNARC) became a staunch objector to pure connectionist AI.",
        "Widrow (who had worked on ADALINE) turned to adaptive signal processing.",
        "The SRI group (which worked on MINOS) turned to symbolic AI and robotics.",
        "The main problem was the inability to train multilayered networks (versions of backpropagation had already been used in other fields but it was unknown to these researchers).",
        "The AI community became aware of backpropogation in the 80s, and, in the 21st century, neural networks would become enormously successful, fulfilling all of Rosenblatt's optimistic predictions.",
        "Rosenblatt did not live to see this, however, as he died in a boating accident in 1971.",
        "The first generation of AI researchers made these predictions about their work: 1958, H. A. Simon and Allen Newell: \"within ten years a digital computer will be the world's chess champion\" and \"within ten years a digital computer will discover and prove an important new mathematical theorem.\"",
        "1965, H. A. Simon: \"machines will be capable, within twenty years, of doing any work a man can do.\"",
        "1967, Marvin Minsky: \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved.\"",
        "1970, Marvin Minsky (in Life magazine): \"In from three to eight years we will have a machine with the general intelligence of an average human being.\"",
        "In June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (ARPA, later known as DARPA).",
        "The money was used to fund project MAC which subsumed the \"AI Group\" founded by Minsky and McCarthy five years earlier.",
        "DARPA continued to provide $3 million each year until the 70s.",
        "DARPA made similar grants to Newell and Simon's program at Carnegie Mellon University and to Stanford University's AI Lab, founded by John McCarthy in 1963.",
        "Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965.",
        "These four institutions would continue to be the main centers of AI research and funding in academia for many years.",
        "The money was given with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should \"fund people, not projects!\"",
        "and allowed researchers to pursue whatever directions might interest them.",
        "This created a freewheeling atmosphere at MIT that gave birth to the hacker culture, but this \"hands off\" approach did not last.",
        "In the 1970s, AI was subject to critiques and financial setbacks.",
        "AI researchers had failed to appreciate the difficulty of the problems they faced.",
        "Their tremendous optimism had raised public expectations impossibly high, and when the promised results failed to materialize, funding targeted at AI was severely reduced.",
        "The lack of success indicated the techniques being used by AI researchers at the time were insufficient to achieve their goals.",
        "These setbacks did not affect the growth and progress of the field, however.",
        "The funding cuts only impacted a handful of major laboratories and the critiques were largely ignored.",
        "General public interest in the field continued to grow, the number of researchers increased dramatically, and new ideas were explored in logic programming, commonsense reasoning and many other areas.",
        "Historian Thomas Haigh argued in 2023 that there was no winter, and AI researcher Nils Nilsson described this period as the most \"exciting\" time to work in AI.",
        "In the early seventies, the capabilities of AI programs were limited.",
        "Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, \"toys\".",
        "AI researchers had begun to run into several limits that would be only conquered decades later, and others that still stymie the field in the 2020s: Limited computer power: There was not enough memory or processing speed to accomplish anything truly useful.",
        "For example: Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only 20 words, because that was all that would fit in memory.",
        "Hans Moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence.",
        "He suggested an analogy: artificial intelligence requires computer power in the same way that aircraft require horsepower.",
        "Below a certain threshold, it's impossible, but, as power increases, eventually it could become easy.",
        "\"With enough horsepower,\" he wrote, \"anything will fly\".",
        "Intractability and the combinatorial explosion: In 1972 Richard Karp (building on Stephen Cook's 1971 theorem) showed there are many problems that can only be solved in exponential time.",
        "Finding optimal solutions to these problems requires extraordinary amounts of computer time, except when the problems are trivial.",
        "This limitation applied to all symbolic AI programs that used search trees and meant that many of the \"toy\" solutions used by AI would never scale to useful systems.",
        "Moravec's paradox: Early AI research had been very successful at getting computers to do \"intelligent\" tasks like proving theorems, solving geometry problems and playing chess.",
        "Their success at these intelligent tasks convinced them that the problem of intelligent behavior had been largely solved.",
        "However, they utterly failed to make progress on \"unintelligent\" tasks like recognizing a face or crossing a room without bumping into anything.",
        "By the 1980s, researchers would realize that symbolic reasoning was utterly unsuited for these perceptual and sensorimotor tasks and that there were limits to this approach.",
        "The breadth of commonsense knowledge: Many important artificial intelligence applications like vision or natural language require enormous amounts of information about the world: the program needs to have some idea of what it might be looking at or what it is talking about.",
        "This requires that the program know most of the same things about the world that a child does.",
        "Researchers soon discovered that this was a vast amount of information with billions of atomic facts.",
        "No one in 1970 could build a database large enough and no one knew how a program might learn so much information.",
        "Representing commonsense reasoning: A number of related problems appeared when researchers tried to represent commonsense reasoning using formal logic or symbols.",
        "Descriptions of very ordinary deductions tended to get longer and longer the more one worked on them, as more and more exceptions, clarifications and distinctions were required.",
        "However, when people thought about ordinary concepts they did not rely on precise definitions, rather they seemed to make hundreds of imprecise assumptions, correcting them when necessary using their entire body of commonsense knowledge.",
        "Gerald Sussman observed that \"using precise language to describe essentially imprecise concepts doesn't make them any more precise.\"",
        "The agencies which funded AI research, such as the British government, DARPA and the National Research Council (NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected AI research.",
        "The pattern began in 1966 when the Automatic Language Processing Advisory Committee (ALPAC) report criticized machine translation efforts.",
        "After spending $20 million, the NRC ended all support.",
        "In 1973, the Lighthill report on the state of AI research in the UK criticized the failure of AI to achieve its \"grandiose objectives\" and led to the dismantling of AI research in that country.",
        "(The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.)",
        "DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of $3 million.",
        "Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues.",
        "\"Many researchers were caught up in a web of increasing exaggeration.\"",
        "However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund \"mission-oriented direct research, rather than basic undirected research\".",
        "Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA, which instead directed money at specific projects with clear objectives, such as autonomous tanks and battle management systems.",
        "The major laboratories (MIT, Stanford, CMU and Edinburgh) had been receiving generous support from their governments, and when it was withdrawn, these were the only places that were seriously impacted by the budget cuts.",
        "The thousands of researchers outside these institutions and the many more thousands that were joining the field were unaffected.",
        "Several philosophers had strong objections to the claims being made by AI researchers.",
        "One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could.",
        "Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little \"symbol processing\" and a great deal of embodied, instinctive, unconscious \"know how\".",
        "John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to \"understand\" the symbols that it uses (a quality called \"intentionality\").",
        "If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as \"thinking\".",
        "These critiques were not taken seriously by AI researchers.",
        "Problems like intractability and commonsense knowledge seemed much more immediate and serious.",
        "It was unclear what difference \"know how\" or \"intentionality\" made to an actual computer program.",
        "MIT's Minsky said of Dreyfus and Searle \"they misunderstand, and should be ignored.\"",
        "Dreyfus, who also taught at MIT, was given a cold shoulder: he later said that AI researchers \"dared not be seen having lunch with me.\"",
        "Joseph Weizenbaum, the author of ELIZA, was also an outspoken critic of Dreyfus' positions, but he \"deliberately made it plain that [his AI colleagues' treatment of Dreyfus] was not the way to treat a human being,\" and was unprofessional and childish.",
        "Weizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a \"computer program which can conduct psychotherapeutic dialogue\" based on ELIZA.",
        "Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool.",
        "A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program.",
        "In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.",
        "Logic was introduced into AI research as early as 1958, by John McCarthy in his Advice Taker proposal.",
        "In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm.",
        "However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems.",
        "A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel who created the successful logic programming language Prolog.",
        "Prolog uses a subset of logic (Horn clauses, closely related to \"rules\" and \"production rules\") that permit tractable computation.",
        "Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.",
        "Critics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems.",
        "Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof.",
        "McCarthy responded that what people do is irrelevant.",
        "He argued that what is really needed are machines that can solve problems-not machines that think as people do.",
        "Among the critics of McCarthy's approach were his colleagues across the country at MIT.",
        "Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like \"story understanding\" and \"object recognition\" that required a machine to think like a person.",
        "In order to use ordinary concepts like \"chair\" or \"restaurant\" they had to make all the same illogical assumptions that people normally made.",
        "Unfortunately, imprecise concepts like these are hard to represent in logic.",
        "MIT chose instead to focus on writing programs that solved a given task without using high-level abstract definitions or general theories of cognition, and measured performance by iterative testing, rather than arguments from first principles.",
        "Schank described their \"anti-logic\" approaches as scruffy, as opposed to the neat paradigm used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.",
        "In 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something.",
        "For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on (none of which are true for all birds).",
        "Minsky associated these assumptions with the general category and they could be inherited by the frames for subcategories and individuals, or over-ridden as necessary.",
        "He called these structures frames.",
        "Schank used a version of frames he called \"scripts\" to successfully answer questions about short stories in English.",
        "Frames would eventually be widely used in software engineering under the name object-oriented programming.",
        "The logicians rose to the challenge.",
        "Pat Hayes claimed that \"most of 'frames' is just a new syntax for parts of first-order logic.\"",
        "But he noted that \"there are one or two apparently minor details which give a lot of trouble, however, especially defaults\".",
        "Ray Reiter admitted that \"conventional logics, such as first-order logic, lack the expressive power to adequately represent the knowledge required for reasoning by default\".",
        "He proposed augmenting first-order logic with a closed world assumption that a conclusion holds (by default) if its contrary cannot be shown.",
        "He showed how such an assumption corresponds to the common sense assumption made in reasoning with frames.",
        "He also showed that it has its \"procedural equivalent\" as negation as failure in Prolog.",
        "The closed world assumption, as formulated by Reiter, \"is not a first-order notion.",
        "(It is a meta notion.)\"",
        "However, Keith Clark showed that negation as finite failure can be understood as reasoning implicitly with definitions in first-order logic including a unique name assumption that different terms denote different individuals.",
        "During the late 1970s and throughout the 1980s, a variety of logics and extensions of first-order logic were developed both for negation as failure in logic programming and for default reasoning more generally.",
        "Collectively, these logics have become known as non-monotonic logics.",
        "In the 1980s, a form of AI program called \"expert systems\" was adopted by corporations around the world and knowledge became the focus of mainstream AI research.",
        "Governments provided substantial funding, such as Japan's fifth generation computer project and the U.S. Strategic Computing Initiative.",
        "\"Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars in 1988.\"",
        "An expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts.",
        "The earliest examples were developed by Edward Feigenbaum and his students.",
        "Dendral, begun in 1965, identified compounds from spectrometer readings.",
        "MYCIN, developed in 1972, diagnosed infectious blood diseases.",
        "They demonstrated the feasibility of the approach.",
        "Expert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem) and their simple design made it relatively easy for programs to be built and then modified once they were in place.",
        "All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.",
        "In 1980, an expert system called R1 was completed at CMU for the Digital Equipment Corporation.",
        "It was an enormous success: it was saving the company 40 million dollars annually by 1986.",
        "Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments.",
        "An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.",
        "In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project.",
        "Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings.",
        "Much to the chagrin of scruffies, they initially chose Prolog as the primary computer language for the project.",
        "Other countries responded with new programs of their own.",
        "The UK began the £350 million Alvey project.",
        "A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or \"MCC\") to fund large scale projects in AI and information technology.",
        "DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.",
        "The power of expert systems came from the expert knowledge they contained.",
        "They were part of a new direction in AI research that had been gaining ground throughout the 70s.",
        "\"AI researchers were beginning to suspect-reluctantly, for it violated the scientific canon of parsimony-that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,\" writes Pamela McCorduck.",
        "\"[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay\".",
        "Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s.",
        "It was hoped that vast databases would solve the commonsense knowledge problem and provide the support that commonsense reasoning required.",
        "In the 1980s some researchers attempted to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows.",
        "Douglas Lenat, who started a database called Cyc, argued that there is no shortcut ― the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand.",
        "Although symbolic knowledge representation and logical reasoning produced useful applications in the 80s and received massive amounts of funding, it was still unable to solve problems in perception, robotics, learning and common sense.",
        "A small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches, such as \"connectionism\", robotics, \"soft\" computing and reinforcement learning.",
        "Nils Nilsson called these approaches \"sub-symbolic\".",
        "In 1982, physicist John Hopfield was able to prove that a form of neural network (now called a \"Hopfield net\") could learn and process information, and provably converges after enough time under any fixed condition.",
        "It was a breakthrough, as it was previously thought that nonlinear networks would, in general, evolve chaotically.",
        "Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called \"backpropagation\".",
        "These two developments helped to revive the exploration of artificial neural networks.",
        "Neural networks, along with several other similar models, received widespread attention after the 1986 publication of the Parallel Distributed Processing, a two volume collection of papers edited by Rumelhart and psychologist James McClelland.",
        "The new field was christened \"connectionism\" and there was a considerable debate between advocates of symbolic AI and the \"connectionists\".",
        "Hinton called symbols the \"luminous aether of AI\" - that is, an unworkable and misleading model of intelligence.",
        "This was a direct attack on the principles that inspired the cognitive revolution.",
        "Neural networks started to advance state of the art in some specialist areas such as protein structure prediction.",
        "Following pioneering work from Terry Sejnowski, cascading multilayer perceptrons such as PhD and PsiPred reached near-theoretical maximum accuracy in predicting secondary structure.",
        "In 1990, Yann LeCun at Bell Labs used convolutional neural networks to recognize handwritten digits.",
        "The system was used widely in 90s, reading zip codes and personal checks.",
        "This was the first genuinely useful application of neural networks.",
        "Rodney Brooks, Hans Moravec and others argued that, in order to show real intelligence, a machine needs to have a body - it needs to perceive, move, survive and deal with the world.",
        "Sensorimotor skills are essential to higher level skills such as commonsense reasoning.",
        "They can't be efficiently implemented using abstract symbolic reasoning, so AI should solve the problems of perception, mobility, manipulation and survival without using symbolic representation at all.",
        "These robotics researchers advocated building intelligence \"from the bottom up\".",
        "A precursor to this idea was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision.",
        "He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place.",
        "(Marr's work would be cut short by leukemia in 1980.)",
        "In his 1990 paper \"Elephants Don't Play Chess,\" robotics researcher Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since \"the world is its own best model.",
        "It is always exactly up to date.",
        "It always has every detail there is to be known.",
        "The trick is to sense it appropriately and often enough.\"",
        "In the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the \"embodied mind thesis\".",
        "Soft computing uses methods that work with incomplete and imprecise information.",
        "They do not attempt to give precise, logical answers, but give results that are only \"probably\" correct.",
        "This allowed them to solve problems that precise symbolic methods could not handle.",
        "Press accounts often claimed these tools could \"think like a human\".",
        "Judea Pearl's Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, an influential 1988 book brought probability and decision theory into AI.",
        "Fuzzy logic, developed by Lofti Zadeh in the 60s, began to be more widely used in AI and robotics.",
        "Evolutionary computation and artificial neural networks also handle imprecise information, and are classified as \"soft\".",
        "In the 90s and early 2000s many other soft computing tools were developed and put into use, including Bayesian networks, hidden Markov models, information theory and stochastic modeling.",
        "These tools in turn depended on advanced mathematical techniques such as classical optimization.",
        "For a time in the 1990s and early 2000s, these soft tools were studied by a subfield of AI called \"computational intelligence\".",
        "Reinforcement learning gives an agent a reward every time it performs a desired action well, and may give negative rewards (or \"punishments\") when it performs poorly.",
        "It was described in the first half of the twentieth century by psychologists using animal models, such as Thorndike, Pavlov and Skinner.",
        "In the 1950s, Alan Turing and Arthur Samuel foresaw the role of reinforcement learning in AI.",
        "A successful and influential research program was led by Richard Sutton and Andrew Barto beginning 1972.",
        "Their collaboration revolutionized the study of reinforcement learning and decision making over the four decades.",
        "In 1988, Sutton described machine learning in terms of decision theory (i.e., the Markov decision process).",
        "This gave the subject a solid theoretical foundation and access to a large body of theoretical results developed in the field of operations research.",
        "Also in 1988, Sutton and Barto developed the \"temporal difference\" (TD) learning algorithm, where the agent is rewarded only when its predictions about the future show improvement.",
        "It significantly outperformed previous algorithms.",
        "TD-learning was used by Gerald Tesauro in 1992 in the program TD-Gammon, which played backgammon as well as the best human players.",
        "The program learned the game by playing against itself with zero prior knowledge.",
        "In an interesting case of interdisciplinary convergence, neurologists discovered in 1997 that the dopamine reward system in brains also uses a version of the TD-learning algorithm.",
        "TD learning would be become highly influential in the 21st century, used in both AlphaGo and AlphaZero.",
        "The business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble.",
        "As dozens of companies failed, the perception in the business world was that the technology was not viable.",
        "The damage to AI's reputation would last into the 21st century.",
        "Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s.",
        "Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of \"artificial intelligence\".",
        "Over the next 20 years, AI consistently delivered working solutions to specific isolated problems.",
        "By the late 1990s, it was being used throughout the technology industry, although somewhat behind the scenes.",
        "The success was due to increasing computer power, by collaboration with other fields (such as mathematical optimization and statistics) and using the highest standards of scientific accountability.",
        "By 2000, AI had achieved some of its oldest goals.",
        "The field was both more cautious and more successful than it had ever been.",
        "The term \"AI winter\" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow.",
        "Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.",
        "The first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987.",
        "Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others.",
        "There was no longer a good reason to buy them.",
        "An entire industry worth half a billion dollars was demolished overnight.",
        "Eventually the earliest successful expert systems, such as XCON, proved too expensive to maintain.",
        "They were difficult to update, they could not learn, and they were \"brittle\" (i.e., they could make grotesque mistakes when given unusual inputs).",
        "Expert systems proved useful, but only in a few special contexts.",
        "In the late 1980s, the Strategic Computing Initiative cut funding to AI \"deeply and brutally\".",
        "New leadership at DARPA had decided that AI was not \"the next wave\" and directed funds towards projects that seemed more likely to produce immediate results.",
        "By 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met.",
        "Indeed, some of them, like \"carry on a casual conversation\" would not be accomplished for another 30 years.",
        "As with other AI projects, expectations had run much higher than what was actually possible.",
        "Over 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI.",
        "In 1994, HP Newquist stated in The Brain Makers that \"The immediate future of artificial intelligence-in its commercial form-seems to rest in part on the continued success of neural networks.\"",
        "In the 1990s, algorithms originally developed by AI researchers began to appear as parts of larger systems.",
        "AI had solved a lot of very difficult problems and their solutions proved to be useful throughout the technology industry, such as data mining, industrial robotics, logistics, speech recognition, banking software, medical diagnosis and Google's search engine.",
        "The field of AI received little or no credit for these successes in the 1990s and early 2000s.",
        "Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science.",
        "Nick Bostrom explains: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"",
        "Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, \"cognitive systems\" or computational intelligence.",
        "In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding.",
        "In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: \"Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\"",
        "AI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past.",
        "Most of the new directions in AI relied heavily on mathematical models, including artificial neural networks, probabilistic reasoning, soft computing and reinforcement learning.",
        "In the 90s and 2000s, many other highly mathematical tools were adapted for AI.",
        "These tools were applied to machine learning, perception and mobility.",
        "There was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like statistics, mathematics, electrical engineering, economics or operations research.",
        "The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous \"scientific\" discipline.",
        "Another key reason for the success in the 90s was that AI researchers focussed on specific problems with verifiable solutions (an approach later derided as narrow AI).",
        "This provided useful tools in the present, rather than speculation about the future.",
        "A new paradigm called \"intelligent agents\" became widely accepted during the 1990s.",
        "Although earlier researchers had proposed modular \"divide and conquer\" approaches to AI, the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI.",
        "When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.",
        "An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success.",
        "By this definition, simple programs that solve specific problems are \"intelligent agents\", as are human beings and organizations of human beings, such as firms.",
        "The intelligent agent paradigm defines AI research as \"the study of intelligent agents\".",
        "This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence.",
        "The paradigm gave researchers license to study isolated problems and to disagree about methods, but still retain hope that their work could be combined into an agent architecture that would be capable of general intelligence.",
        "On May 11, 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov.",
        "In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail.",
        "Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an urban environment while responding to traffic hazards and adhering to traffic laws.",
        "These successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computers by the 90s.",
        "In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951.",
        "This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years.",
        "The fundamental problem of \"raw computer power\" was slowly being overcome.",
        "Electronic literature experiments such as The Impermanence Agent (1998 -2002) and digital art such as Agent Ruby, used AI in their art and literature, \"laying bare the bias accompanying forms of technology that feign objectivity.\"",
        "In the first decades of the 21st century, access to large amounts of data (known as \"big data\"), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy.",
        "A turning point was the success of deep learning around 2012 which improved the performance of machine learning on many tasks, including image and video processing, text analysis, and speech recognition.",
        "Investment in AI increased along with its capabilities, and by 2016, the market for AI-related products, hardware, and software reached more than $8 billion, and the New York Times reported that interest in AI had reached a \"frenzy\".",
        "In 2002, Ben Goertzel and others became concerned that AI had largely abandoned its original goal of producing versatile, fully intelligent machines, and argued in favor of more direct research into artificial general intelligence.",
        "By the mid-2010s several companies and institutions had been founded to pursue Artificial General Intelligence (AGI), such as OpenAI and Google's DeepMind.",
        "During the same period, new insights into superintelligence raised concerns that AI was an existential threat.",
        "The risks and unintended consequences of AI technology became an area of serious academic research after 2016.",
        "The success of machine learning in the 2000s depended on the availability of vast amounts of training data and faster computers.",
        "Russell and Norvig wrote that the \"improvement in performance obtained by increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be made by tweaking the algorithm.\"",
        "Geoffrey Hinton recalled that back in the 90s, the problem was that \"our labeled datasets were thousands of times too small.",
        "[And] our computers were millions of times too slow.\"",
        "This was no longer true by 2010.",
        "The most useful data in the 2000s came from curated, labeled data sets created specifically for machine learning and AI.",
        "In 2007, a group at UMass Amherst released Labeled Faces in the Wild, an annotated set of images of faces that was widely used to train and test face recognition systems for the next several decades.",
        "Fei-Fei Li developed ImageNet, a database of three million images captioned by volunteers using the Amazon Mechanical Turk.",
        "Released in 2009, it was a useful body of training data and a benchmark for testing for the next generation of image processing systems.",
        "Google released word2vec in 2013 as an open source resource.",
        "It used large amounts of data text scraped from the internet and word embedding to create a numeric vector to represent each word.",
        "Users were surprised at how well it was able to capture word meanings, for example, ordinary vector addition would give equivalences like China + River = Yangtze, London-England+France = Paris.",
        "This database in particular would be essential for the development of large language models in the late 2010s.",
        "The explosive growth of the internet gave machine learning programs access to billions of pages of text and images that could be scraped.",
        "And, for specific problems, large privately held databases contained the relevant data.",
        "McKinsey Global Institute reported that \"by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data\".",
        "This collection of information was known in the 2000s as big data.",
        "In a Jeopardy!",
        "exhibition match in February 2011, IBM's question answering system Watson defeated the two best Jeopardy!",
        "champions, Brad Rutter and Ken Jennings, by a significant margin.",
        "Watson's expertise would have been impossible without the information available on the internet.",
        "In 2012, AlexNet, a deep learning model, developed by Alex Krizhevsky, won the ImageNet Large Scale Visual Recognition Challenge, with significantly fewer errors than the second-place winner.",
        "Krizhevsky worked with Geoffrey Hinton at the University of Toronto.",
        "This was a turning point in machine learning: over the next few years dozens of other approaches to image recognition were abandoned in favor of deep learning.",
        "Deep learning uses a multi-layer perceptron.",
        "Although this architecture has been known since the 60s, getting it to work requires powerful hardware and large amounts of training data.",
        "Before these became available, improving performance of image processing systems required hand-crafted ad hoc features that were difficult to implement.",
        "Deep learning was simpler and more general.",
        "Deep learning was applied to dozens of problems over the next few years (such as speech recognition, machine translation, medical diagnosis, and game playing).",
        "In every case it showed enormous gains in performance.",
        "Investment and interest in AI boomed as a result.",
        "It became fashionable in the 2000s to begin talking about the future of AI again and several popular books considered the possibility of superintelligent machines and what they might mean for human society.",
        "Some of this was optimistic (such as Ray Kurzweil's The Singularity is Near), but others warned that a sufficiently powerful AI was existential threat to humanity, such as Nick Bostrom and Eliezer Yudkowsky.",
        "The topic became widely covered in the press and many leading intellectuals and politicians commented on the issue.",
        "AI programs in the 21st century are defined by their goals - the specific measures that they are designed to optimize.",
        "Nick Bostrom's influential 2014 book Superintelligence argued that, if one isn't careful about defining these goals, the machine may cause harm to humanity in the process of achieving a goal.",
        "Stuart J. Russell used the example of an intelligent robot that kills its owner to prevent it from being unplugged, reasoning \"you can't fetch the coffee if you're dead\".",
        "(This problem is known by the technical term \"instrumental convergence\".)",
        "The solution is to align the machine's goal function with the goals of its owner and humanity in general.",
        "Thus, the problem of mitigating the risks and unintended consequences of AI became known as \"the value alignment problem\" or AI alignment.",
        "At the same time, machine learning systems had begun to have disturbing unintended consequences.",
        "Cathy O'Neil explained how statistical algorithms had been among the causes of the 2008 economic crash, Julia Angwin of ProPublica argued that the COMPAS system used by the criminal justice system exhibited racial bias under some measures, others showed that many machine learning systems exhibited some form of racial bias, and there were many other examples of dangerous outcomes that had resulted from machine learning systems.",
        "In 2016, the election of Donald Trump and the controversy over the COMPAS system illuminated several problems with the current technological infrastructure, including misinformation, social media algorithms designed to maximize engagement, the misuse of personal data and the trustworthiness of predictive models.",
        "Issues of fairness and unintended consequences became significantly more popular at AI conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues.",
        "The value alignment problem became a serious field of academic study.",
        "In the early 2000s, several researchers became concerned that mainstream AI was too focused on \"measurable performance in specific applications\" (known as \"narrow AI\") and had abandoned AI's original goal of creating versatile, fully intelligent machines.",
        "An early critic was Nils Nilsson in 1995, and similar opinions were published by AI elder statesmen John McCarthy, Marvin Minsky, and Patrick Winston in 2007-2009.",
        "Minsky organized a symposium on \"human-level AI\" in 2004.",
        "Ben Goertzel adopted the term \"artificial general intelligence\" for the new sub-field, founding a journal and holding conferences beginning in 2008.",
        "The new field grew rapidly, buoyed by the continuing success of artificial neural networks and the hope that it was the key to AGI.",
        "Several competing companies, laboratories and foundations were founded to develop AGI in the 2010s.",
        "DeepMind was founded in 2010 by three English scientists, Demis Hassabis, Shane Legg and Mustafa Suleyman, with funding from Peter Thiel and later Elon Musk.",
        "The founders and financiers were deeply concerned about AI safety and the existential risk of AI.",
        "DeepMind's founders had a personal connection with Yudkowsky and Musk was among those who was actively raising the alarm.",
        "Hassabis was both worried about the dangers of AGI and optimistic about its power; he hoped they could \"solve AI, then solve everything else.\"",
        "The New York Times wrote in 2023 \"At the heart of this competition is a brain-stretching paradox.",
        "The people who say they are most worried about AI are among the most determined to create it and enjoy its riches.",
        "They have justified their ambition with their strong belief that they alone can keep AI from endangering Earth.\"",
        "In 2012, Geoffrey Hinton (who been leading neural network research since the 80s) was approached by Baidu, which wanted to hire him and all his students for an enormous sum.",
        "Hinton decided to hold an auction and, at a Lake Tahoe AI conference, they sold themselves to Google for a price of $44 million.",
        "Hassabis took notice and sold DeepMind to Google in 2014, on the condition that it would not accept military contracts and would be overseen by an ethics board.",
        "Larry Page of Google, unlike Musk and Hassabis, was an optimist about the future of AI.",
        "Musk and Paige became embroiled in an argument about the risk of AGI at Musk's 2015 birthday party.",
        "They had been friends for decades but stopped speaking to each other shortly afterwards.",
        "Musk attended the one and only meeting of the DeepMind's ethics board, where it became clear that Google was uninterested in mitigating the harm of AGI.",
        "Frustrated by his lack of influence he founded OpenAI in 2015, enlisting Sam Altman to run it and hiring top scientists.",
        "OpenAI began as a non-profit, \"free from the economic incentives that were driving Google and other corporations.\"",
        "Musk became frustrated again and left the company in 2018.",
        "OpenAI turned to Microsoft for continued financial support and Altman and OpenAI formed a for-profit version of the company with more than $1 billion in financing.",
        "In 2021, Dario Amodei and 14 other scientists left OpenAI over concerns that the company was putting profits above safety.",
        "They formed Anthropic, which soon had $6 billion in financing from Microsoft and Google.",
        "The AI boom started with the initial development of key architectures and algorithms such as the transformer architecture in 2017, leading to the scaling and development of large language models exhibiting human-like traits of knowledge, attention and creativity.",
        "The new AI era began since 2020, with the public release of scaled large language models (LLMs) such as ChatGPT.",
        "In 2017, the transformer architecture was proposed by Google researchers.",
        "It exploits an attention mechanism and became widely used in large language models.",
        "Large language models, based on the transformer, were developed by AGI companies: OpenAI released GPT-3 in 2020, and DeepMind released Gato in 2022.",
        "These are foundation models: they are trained on vast quantities of unlabeled data and can be adapted to a wide range of downstream tasks.",
        "These models can discuss a huge number of topics and display general knowledge.",
        "The question naturally arises: are these models an example of artificial general intelligence?",
        "Bill Gates was skeptical of the new technology and the hype that surrounded AGI.",
        "However, Altman presented him with a live demo of ChatGPT4 passing an advanced biology test.",
        "Gates was convinced.",
        "In 2023, Microsoft Research tested the model with a large variety of tasks, and concluded that \"it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system\".",
        "In 2024, OpenAI o3, a type of advanced reasoning model developed by OpenAI was announced.",
        "On the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) benchmark developed by François Chollet in 2019, the model achieved an unofficial score of 87.5% on the semi-private test, surpassing the typical human score of 84%.",
        "The benchmark is supposed to be a necessary, but not sufficient test for AGI.",
        "Speaking of the benchmark, Chollet has said \"You’ll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.\"",
        "Investment in AI grew exponentially after 2020, with venture capital funding for generative AI companies increasing dramatically.",
        "Total AI investments rose from $18 billion in 2014 to $119 billion in 2021, with generative AI accounting for approximately 30% of investments by 2023.",
        "According to metrics from 2017 to 2021, the United States outranked the rest of the world in terms of venture capital funding, number of startups, and AI patents granted.",
        "The commercial AI scene became dominated by American Big Tech companies, whose investments in this area surpassed those from U.S.-based venture capitalists.",
        "OpenAI's valuation reached $86 billion by early 2024, while NVIDIA's market capitalization surpassed $3.3 trillion by mid-2024, making it the world's largest company by market capitalization as the demand for AI-capable GPUs surged.",
        "15.ai, launched in March 2020 by an anonymous MIT researcher, was one of the earliest examples of generative AI gaining widespread public attention during the initial stages of the AI boom.",
        "The free web application demonstrated the ability to clone character voices using neural networks with minimal training data, requiring as little as 15 seconds of audio to reproduce a voice-a capability later corroborated by OpenAI in 2024.",
        "The service went viral on social media platforms in early 2021, allowing users to generate speech for characters from popular media franchises, and became particularly notable for its pioneering role in popularizing AI voice synthesis for creative content and memes.",
        "ChatGPT was launched on November 30, 2022, marking a pivotal moment in artificial intelligence's public adoption.",
        "Within days of its release it went viral, gaining over 100 million users in two months and becoming the fastest-growing consumer software application in history.",
        "The chatbot's ability to engage in human-like conversations, write code, and generate creative content captured public imagination and led to rapid adoption across various sectors including education, business, and research.",
        "ChatGPT's success prompted unprecedented responses from major technology companies-Google declared a \"code red\" and rapidly launched Gemini (formerly known as Google Bard), while Microsoft incorporated the technology into Bing Chat.",
        "The rapid adoption of these AI technologies sparked intense debate about their implications.",
        "Notable AI researchers and industry leaders voiced both optimism and concern about the accelerating pace of development.",
        "In March 2023, over 20,000 signatories, including computer scientist Yoshua Bengio, Elon Musk, and Apple co-founder Steve Wozniak, signed an open letter calling for a pause in advanced AI development, citing \"profound risks to society and humanity.\"",
        "However, other prominent researchers like Juergen Schmidhuber took a more optimistic view, emphasizing that the majority of AI research aims to make \"human lives longer and healthier and easier.\"",
        "By mid-2024, however, the financial sector began to scrutinize AI companies more closely, particularly questioning their capacity to produce a return on investment commensurate with their massive valuations.",
        "Some prominent investors raised concerns about market expectations becoming disconnected from fundamental business realities.",
        "Jeremy Grantham, co-founder of GMO LLC, warned investors to \"be quite careful\" and drew parallels to previous technology-driven market bubbles.",
        "Similarly, Jeffrey Gundlach, CEO of DoubleLine Capital, explicitly compared the AI boom to the dot-com bubble of the late 1990s, suggesting that investor enthusiasm might be outpacing realistic near-term capabilities and revenue potential.",
        "These concerns were amplified by the substantial market capitalizations of AI-focused companies, many of which had yet to demonstrate sustainable profitability models.",
        "In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus.",
        "The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google.",
        "In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.",
        "In 2024, the Royal Swedish Academy of Sciences awarded Nobel Prizes in recognition of groundbreaking contributions to artificial intelligence.",
        "The recipients included: In physics: John Hopfield for his work on physics-inspired Hopfield networks, and Geoffrey Hinton for foundational contributions to Boltzmann machines and deep learning.",
        "In chemistry: David Baker, Demis Hassabis, and John Jumper for their advancements in protein folding predictions.",
        "See AlphaFold.",
        "In January 2025, OpenAI announced a new AI, ChatGPT-Gov, which would be specifically designed for US government agencies to use securely.",
        "Open AI said that agencies could utilize ChatGPT Gov on a Microsoft Azure cloud or Azure Government cloud, \"on top of Microsoft’s Azure’s OpenAI Service.\"",
        "OpenAI's announcement stated that \"Self-hosting ChatGPT Gov enables agencies to more easily manage their own security, privacy, and compliance requirements, such as stringent cybersecurity frameworks (IL5, CJIS, ITAR, FedRAMP High).",
        "Additionally, we believe this infrastructure will expedite internal authorization of OpenAI’s tools for the handling of non-public sensitive data.\"",
        "Countries have invested in policies and funding to deploy autonomous robots in an attempt to address labor shortages and enhancing efficiency, while also implementing regulatory frameworks for ethical and safe development.",
        "In 2025, China invested approximately 730 billion yuan (roughly $100 billion USD) to advance AI and robotics in smart manufacturing and healthcare.",
        "The \"14th Five-Year Plan\" (2021-2025) prioritized service robots, with AI systems enabling robots to perform complex tasks like assisting in surgeries or automating factory assembly lines.",
        "Some funding also supported defense applications, such as autonomous drones.",
        "Starting in September 2025, China mandated labeling of AI-generated content to ensure transparency and public trust in these technologies.",
        "In January 2025, Stargate LLC was formed as a joint venture of OpenAI, SoftBank, Oracle, and MGX, who announced plans to invest US$500 billion in AI infrastructure in the United States by 2029.",
        "Stargate LLC started with a US$100 billion investment in re-industrialization and national security capabilities.",
        "The venture was formally announced by U.S. President Donald Trump on January 21, 2025, with SoftBank CEO Masayoshi Son appointed as chairman.",
        "The U.S. government allocated approximately $2 billion to integrate AI and robotics in manufacturing and logistics.",
        "State governments supplemented this with funding for service robots, such as those deployed in warehouses to fulfill verbal commands for inventory management or in eldercare facilities to respond to residents' requests for assistance.",
        "Some funds were directed to defense, including Lethal autonomous weapon and Military robot.",
        "In January 2025, Executive Order 14179 established an \"AI Action Plan\" to accelerate innovation and deployment of these technologies."
      ],
      "metadata": {
        "title": "History of artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/History_of_artificial_intelligence",
        "word_count": 12004,
        "char_count": 77771,
        "sentence_count": 558,
        "scraped_at": "2025-08-09T14:46:47.086336",
        "language": "en",
        "processing_time": 0.016120195388793945,
        "source_hash": "05ba9f965dee108a0f01c19a9e48e0a9"
      }
    },
    {
      "title": "Natural language processing",
      "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
      "raw_text": "Natural language processing (NLP) is the processing of natural language information by a computer. The study of NLP, a subfield of computer science, is generally associated with artificial intelligence. NLP is related to information retrieval, knowledge representation, computational linguistics, and more broadly with linguistics.\nMajor processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation.\n\n\n== History ==\n\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\n\n\n=== Symbolic NLP (1950s – early 1990s) ===\nThe premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n\n1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed.\n1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer  memory at the time.\n1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, the first chatterbots were written (e.g., PARRY).\n1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.\n\n\n=== Statistical NLP (1990s–present) ===\nUp until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. \n\n1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models.  These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms.  Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.  Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.  However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical.\n2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.)\n2010: Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling, and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling and parsing. This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care or protect patient privacy.\n\n\n== Approaches: Symbolic, statistical, neural networks ==\nSymbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: such as by writing grammars or devising heuristic rules for stemming.\nMachine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: \n\nboth statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally.\nlanguage models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce.\nthe larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems.\nRule-based systems are commonly used:\n\nwhen the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system,\nfor preprocessing in NLP pipelines, e.g., tokenization, or\nfor postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.\n\n\n=== Statistical approach ===\nIn the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.\nThe earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches.\nOnly the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\n\n\n=== Neural networks ===\n\nA major drawback of statistical methods is that they require elaborate feature engineering. Since 2015, the statistical approach has been replaced by the neural networks approach, using semantic networks and word embeddings to capture semantic properties of words.  \nIntermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. \nNeural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.\n\n\n== Common NLP tasks ==\nThe following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\nThough natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\n\n\n=== Text and speech processing ===\nOptical character recognition (OCR)\nGiven an image representing printed text, determine the corresponding text.\nSpeech recognition\nGiven a sound clip of a person or people speaking, determine the textual representation of the speech.  This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above).  In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.\nSpeech segmentation\nGiven a sound clip of a person or people speaking, separate it into words.  A subtask of speech recognition and typically grouped with it.\nText-to-speech\nGiven a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.\nWord segmentation (Tokenization)\nTokenization is a process used in text analysis that divides text into individual words or word fragments. This technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods.\nFor a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.\n\n\n=== Morphological analysis ===\nLemmatization\nThe task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.\nMorphological segmentation\nSeparate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.\nPart-of-speech tagging\nGiven a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five different parts of speech.\nStemming\nThe process of reducing inflected (or sometimes derived) words to a base form (e.g., \"close\" will be the root for \"closed\", \"closing\", \"close\", \"closer\" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary.\n\n\n=== Syntactic analysis ===\n\nGrammar induction\nGenerate a formal grammar that describes a language's syntax.\nSentence breaking (also known as \"sentence boundary disambiguation\")\nGiven a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations).\nParsing\nDetermine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar).\n\n\n=== Lexical semantics (of individual words in context) ===\nLexical semantics\nWhat is the computational meaning of individual words in context?\nDistributional semantics\nHow can we learn semantic representations from data?\nNamed entity recognition (NER)\nGiven a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient.  For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized.  Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives. Another name for this task is token classification.\nSentiment analysis (see also Multimodal sentiment analysis)\nSentiment analysis is a computational method used to identify and classify the emotional intent behind text. This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms.\nTerminology extraction\nThe goal of terminology extraction is to automatically extract relevant terms from a given corpus.\nWord-sense disambiguation (WSD)\nMany words have more than one meaning; we have to select the meaning which makes the most sense in context.  For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet.\nEntity linking\nMany words—typically proper names—refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context.\n\n\n=== Relational semantics (semantics of individual sentences) ===\nRelationship extraction\nGiven a chunk of text, identify the relationships among named entities (e.g. who is married to whom).\nSemantic parsing\nGiven a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below).\nSemantic role labelling (see also implicit semantic role labelling below)\nGiven a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles).\n\n\n=== Discourse (semantics beyond individual sentences) ===\nCoreference resolution\nGiven a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving referring expressions. For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).\nDiscourse analysis\nThis rubric includes several related tasks.  One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast).  Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes–no question, content question, statement, assertion, etc.).\nImplicit semantic role labelling\nGiven a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages.\nRecognizing textual entailment\nGiven two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.\nTopic segmentation and recognition\nGiven a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.\nArgument mining\nThe goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse.\n\n\n=== Higher-level NLP applications ===\nAutomatic summarization (text summarization)\nProduce a readable summary of a chunk of text.  Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper.\nGrammatical error correction\nGrammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications.\nLogic translation\nTranslate a text from a natural language into formal logic.\nMachine translation (MT)\nAutomatically translate text from one human language to another.  This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly.\nNatural language understanding (NLU)\nConvert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.\nNatural language generation (NLG):\nConvert information from computer databases or semantic intents into readable human language.\nBook generation\nNot an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed). The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization.\nDocument AI\nA Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants.\nDialogue management\nComputer systems intended to converse with a human.\nQuestion answering\nGiven a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada?\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\").\nText-to-image generation\nGiven a description of an image, generate an image that matches the description.\nText-to-scene generation\nGiven a description of a scene, generate a 3D model of the scene.\nText-to-video\nGiven a description of a video, generate a video that matches the description.\n\n\n== General tendencies and (possible) future directions ==\nBased on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:\n\nInterest on increasingly abstract, \"cognitive\" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).\nIncreasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)\nElimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)\n\n\n=== Cognition ===\nMost higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\nCognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\" Cognitive science is the interdisciplinary, scientific study of the mind and its processes. Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\nAs an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects:\n\nApply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author. For example, consider the English word big. When used in a comparison (\"That is a big tree\"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience.  When used metaphorically (\"Tomorrow is a big day\"), the author's intent to imply importance.  The intent behind other usages, like in \"She is a big person\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.\nAssign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in  US Patent 9269353:\n\n  \n    \n      \n        \n          R\n          M\n          M\n          (\n          t\n          o\n          k\n          e\n          \n            n\n            \n              N\n            \n          \n          )\n        \n        =\n        \n          P\n          M\n          M\n          (\n          t\n          o\n          k\n          e\n          \n            n\n            \n              N\n            \n          \n          )\n        \n        ×\n        \n          \n            1\n            \n              2\n              d\n            \n          \n        \n        \n          (\n          \n            \n              ∑\n              \n                i\n                =\n                −\n                d\n              \n              \n                d\n              \n            \n            \n              (\n              (\n              P\n              M\n              M\n              (\n              t\n              o\n              k\n              e\n              \n                n\n                \n                  N\n                \n              \n              )\n            \n            ×\n            \n              P\n              F\n              (\n              t\n              o\n              k\n              e\n              \n                n\n                \n                  N\n                  −\n                  i\n                \n              \n              ,\n              t\n              o\n              k\n              e\n              \n                n\n                \n                  N\n                \n              \n              ,\n              t\n              o\n              k\n              e\n              \n                n\n                \n                  N\n                  +\n                  i\n                \n              \n              )\n              \n                )\n                \n                  i\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {RMM(token_{N})}={PMM(token_{N})}\\times {\\frac {1}{2d}}\\left(\\sum _{i=-d}^{d}{((PMM(token_{N})}\\times {PF(token_{N-i},token_{N},token_{N+i}))_{i}}\\right)}\n  \n\nWhere\nRMM is the relative measure of meaning\ntoken is any block of text, sentence, phrase or word\nN is the number of tokens being analyzed\nPMM is the probable measure of meaning based on a corpora\nd is the non zero location of the token along the sequence of N tokens\nPF is the probability function specific to a language\nTies with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, functional grammar, construction grammar, computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\". Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit) and developments in artificial intelligence, specifically tools and technologies using large language model approaches and new directions in artificial general intelligence based on the free energy principle by British neuroscientist and theoretician at University College London Karl J. Friston.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n Media related to Natural language processing at Wikimedia Commons",
      "cleaned_text": "Natural language processing (NLP) is the processing of natural language information by a computer. The study of NLP, a subfield of computer science, is generally associated with artificial intelligence. NLP is related to information retrieval, knowledge representation, computational linguistics, and more broadly with linguistics. Major processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation. Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. 1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed. 1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time. 1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY). 1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period. Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. 1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data. 2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical. 2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.) 2010: Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling, and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling and parsing. This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care or protect patient privacy. Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: such as by writing grammars or devising heuristic rules for stemming. Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally. language models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce. the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems. Rule-based systems are commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses. In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches. The earliest decision trees, producing systems of hard if-then rules, were still very similar to the old rule-based approaches. Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach. A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015, the statistical approach has been replaced by the neural networks approach, using semantic networks and word embeddings to capture semantic properties of words. Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. Neural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation. The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below. Optical character recognition (OCR) Given an image representing printed text, determine the corresponding text. Speech recognition Given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent. Speech segmentation Given a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition and typically grouped with it. Text-to-speech Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired. Word segmentation (Tokenization) Tokenization is a process used in text analysis that divides text into individual words or word fragments. This technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining. Lemmatization The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form. Morphological segmentation Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms. Part-of-speech tagging Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five different parts of speech. Stemming The process of reducing inflected (or sometimes derived) words to a base form (e.g., \"close\" will be the root for \"closed\", \"closing\", \"close\", \"closer\" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary. Grammar induction Generate a formal grammar that describes a language's syntax. Sentence breaking (also known as \"sentence boundary disambiguation\") Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations). Parsing Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar). Lexical semantics What is the computational meaning of individual words in context? Distributional semantics How can we learn semantic representations from data? Named entity recognition (NER) Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient. For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives. Another name for this task is token classification. Sentiment analysis (see also Multimodal sentiment analysis) Sentiment analysis is a computational method used to identify and classify the emotional intent behind text. This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet. Entity linking Many words-typically proper names-refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context. Relationship extraction Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom). Semantic parsing Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below). Semantic role labelling (see also implicit semantic role labelling below) Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles). Coreference resolution Given a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving referring expressions. For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to). Discourse analysis This rubric includes several related tasks. One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes-no question, content question, statement, assertion, etc.). Implicit semantic role labelling Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages. Recognizing textual entailment Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false. Topic segmentation and recognition Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment. Argument mining The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse. Automatic summarization (text summarization) Produce a readable summary of a chunk of text. Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper. Grammatical error correction Grammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications. Logic translation Translate a text from a natural language into formal logic. Machine translation (MT) Automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly. Natural language understanding (NLU) Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization. Natural language generation (NLG): Convert information from computer databases or semantic intents into readable human language. Book generation Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed). The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization. Document AI A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants. Dialogue management Computer systems intended to converse with a human. Question answering Given a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada?\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). Text-to-image generation Given a description of an image, generate an image that matches the description. Text-to-scene generation Given a description of a scene, generate a 3D model of the scene. Text-to-video Given a description of a video, generate a video that matches the description. Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: Interest on increasingly abstract, \"cognitive\" aspects of natural language (1999-2001: shallow parsing, 2002-03: named entity recognition, 2006-09/2017-18: dependency syntax, 2004-05/2008-09 semantic role labelling, 2011-12 coreference, 2015-16: discourse parsing, 2019: semantic parsing). Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages) Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems) Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above). Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\" Cognitive science is the interdisciplinary, scientific study of the mind and its processes. Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects: Apply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author. For example, consider the English word big. When used in a comparison (\"That is a big tree\"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience. When used metaphorically (\"Tomorrow is a big day\"), the author's intent to imply importance. The intent behind other usages, like in \"She is a big person\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information. Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in US Patent 9269353: R M M ( t o k e n N ) = P M M ( t o k e n N ) × 1 2 d ( ∑ i = − d d ( ( P M M ( t o k e n N ) × P F ( t o k e n N − i , t o k e n N , t o k e n N + i ) ) i ) {\\displaystyle {RMM(token_{N})}={PMM(token_{N})} imes {\\frac {1}{2d}}\\left(\\sum _{i=-d}^{d}{((PMM(token_{N})} imes {PF(token_{N-i},token_{N},token_{N+i}))_{i}}\\right)} Where RMM is the relative measure of meaning token is any block of text, sentence, phrase or word N is the number of tokens being analyzed PMM is the probable measure of meaning based on a corpora d is the non zero location of the token along the sequence of N tokens PF is the probability function specific to a language Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, functional grammar, construction grammar, computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\". Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit) and developments in artificial intelligence, specifically tools and technologies using large language model approaches and new directions in artificial general intelligence based on the free energy principle by British neuroscientist and theoretician at University College London Karl J. Friston.",
      "sentences": [
        "Natural language processing (NLP) is the processing of natural language information by a computer.",
        "The study of NLP, a subfield of computer science, is generally associated with artificial intelligence.",
        "NLP is related to information retrieval, knowledge representation, computational linguistics, and more broadly with linguistics.",
        "Major processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation.",
        "Natural language processing has its roots in the 1950s.",
        "Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence.",
        "The proposed test includes a task that involves the automated interpretation and generation of natural language.",
        "The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.",
        "1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English.",
        "The authors claimed that within three or five years, machine translation would be a solved problem.",
        "However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.",
        "Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed.",
        "1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.",
        "Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction.",
        "When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".",
        "Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time.",
        "1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data.",
        "During this time, the first chatterbots were written (e.g., PARRY).",
        "1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP.",
        "Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky.",
        "An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.",
        "Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.",
        "Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.",
        "This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g.",
        "transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.",
        "1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models.",
        "These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.",
        "However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems.",
        "As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.",
        "2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s.",
        "Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms.",
        "Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.",
        "Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.",
        "However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical.",
        "2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.)",
        "2010: Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling, and in the following years he went on to develop Word2vec.",
        "In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing.",
        "That popularity was due partly to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling and parsing.",
        "This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care or protect patient privacy.",
        "Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: such as by writing grammars or devising heuristic rules for stemming.",
        "Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally.",
        "language models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g.",
        "containing words or structures that have not been seen before) and erroneous input (e.g.",
        "with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce.",
        "the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems.",
        "Rule-based systems are commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.",
        "In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.",
        "The earliest decision trees, producing systems of hard if-then rules, were still very similar to the old rule-based approaches.",
        "Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.",
        "A major drawback of statistical methods is that they require elaborate feature engineering.",
        "Since 2015, the statistical approach has been replaced by the neural networks approach, using semantic networks and word embeddings to capture semantic properties of words.",
        "Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore.",
        "Neural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.",
        "The following is a list of some of the most commonly researched tasks in natural language processing.",
        "Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.",
        "Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience.",
        "A coarse division is given below.",
        "Optical character recognition (OCR) Given an image representing printed text, determine the corresponding text.",
        "Speech recognition Given a sound clip of a person or people speaking, determine the textual representation of the speech.",
        "This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above).",
        "In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below).",
        "In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process.",
        "Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.",
        "Speech segmentation Given a sound clip of a person or people speaking, separate it into words.",
        "A subtask of speech recognition and typically grouped with it.",
        "Text-to-speech Given a text, transform those units and produce a spoken representation.",
        "Text-to-speech can be used to aid the visually impaired.",
        "Word segmentation (Tokenization) Tokenization is a process used in text analysis that divides text into individual words or word fragments.",
        "This technique results in two key components: a word index and tokenized text.",
        "The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token.",
        "These numerical tokens are then used in various deep learning methods.",
        "For a language like English, this is fairly trivial, since words are usually separated by spaces.",
        "However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language.",
        "Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.",
        "Lemmatization The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma.",
        "Lemmatization is another technique for reducing words to their normalized form.",
        "But in this case, the transformation actually uses a dictionary to map words to their actual form.",
        "Morphological segmentation Separate words into individual morphemes and identify the class of the morphemes.",
        "The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered.",
        "English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words.",
        "In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.",
        "Part-of-speech tagging Given a sentence, determine the part of speech (POS) for each word.",
        "Many words, especially common ones, can serve as multiple parts of speech.",
        "For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five different parts of speech.",
        "Stemming The process of reducing inflected (or sometimes derived) words to a base form (e.g., \"close\" will be the root for \"closed\", \"closing\", \"close\", \"closer\" etc.).",
        "Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary.",
        "Grammar induction Generate a formal grammar that describes a language's syntax.",
        "Sentence breaking (also known as \"sentence boundary disambiguation\") Given a chunk of text, find the sentence boundaries.",
        "Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations).",
        "Parsing Determine the parse tree (grammatical analysis) of a given sentence.",
        "The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human).",
        "There are two primary types of parsing: dependency parsing and constituency parsing.",
        "Lexical semantics What is the computational meaning of individual words in context?",
        "Distributional semantics How can we learn semantic representations from data?",
        "Named entity recognition (NER) Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g.",
        "person, location, organization).",
        "Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient.",
        "For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized.",
        "Furthermore, many other languages in non-Western scripts (e.g.",
        "Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names.",
        "For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives.",
        "Another name for this task is token classification.",
        "Sentiment analysis (see also Multimodal sentiment analysis) Sentiment analysis is a computational method used to identify and classify the emotional intent behind text.",
        "This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral.",
        "Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences.",
        "The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms.",
        "Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus.",
        "Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the meaning which makes the most sense in context.",
        "For this problem, we are typically given a list of words and associated word senses, e.g.",
        "from a dictionary or an online resource such as WordNet.",
        "Entity linking Many words-typically proper names-refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.)",
        "which is referred to in context.",
        "Relationship extraction Given a chunk of text, identify the relationships among named entities (e.g.",
        "who is married to whom).",
        "This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below).",
        "Coreference resolution Given a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\").",
        "Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer.",
        "The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving referring expressions.",
        "For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).",
        "Discourse analysis This rubric includes several related tasks.",
        "One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e.",
        "the nature of the discourse relationships between sentences (e.g.",
        "elaboration, explanation, contrast).",
        "Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g.",
        "yes-no question, content question, statement, assertion, etc.).",
        "Implicit semantic role labelling Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above).",
        "Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text.",
        "A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages.",
        "Recognizing textual entailment Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.",
        "Topic segmentation and recognition Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.",
        "Argument mining The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs.",
        "Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse.",
        "Automatic summarization (text summarization) Produce a readable summary of a chunk of text.",
        "Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper.",
        "Grammatical error correction Grammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics).",
        "Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language.",
        "It has thus been subject to a number of shared tasks since 2011.",
        "As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications.",
        "Logic translation Translate a text from a natural language into formal logic.",
        "Machine translation (MT) Automatically translate text from one human language to another.",
        "This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e.",
        "requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.)",
        "to solve properly.",
        "Natural language understanding (NLU) Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate.",
        "Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts.",
        "Introduction and creation of language metamodel and ontology are efficient however empirical solutions.",
        "An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.",
        "Natural language generation (NLG): Convert information from computer databases or semantic intents into readable human language.",
        "Book generation Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books.",
        "The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed).",
        "The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words.",
        "Both these systems are basically elaborate but non-sensical (semantics-free) language models.",
        "The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham).",
        "Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization.",
        "Document AI A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types.",
        "NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants.",
        "Dialogue management Computer systems intended to converse with a human.",
        "Question answering Given a human-language question, determine its answer.",
        "Typical questions have a specific right answer (such as \"What is the capital of Canada?",
        "\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\").",
        "Text-to-image generation Given a description of an image, generate an image that matches the description.",
        "Text-to-scene generation Given a description of a scene, generate a 3D model of the scene.",
        "Text-to-video Given a description of a video, generate a video that matches the description.",
        "Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP.",
        "As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: Interest on increasingly abstract, \"cognitive\" aspects of natural language (1999-2001: shallow parsing, 2002-03: named entity recognition, 2006-09/2017-18: dependency syntax, 2004-05/2008-09 semantic role labelling, 2011-12 coreference, 2015-16: discourse parsing, 2019: semantic parsing).",
        "More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).",
        "Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"",
        "Cognitive science is the interdisciplinary, scientific study of the mind and its processes.",
        "Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.",
        "Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.",
        "As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects: Apply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author.",
        "For example, consider the English word big.",
        "When used in a comparison (\"That is a big tree\"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience.",
        "When used metaphorically (\"Tomorrow is a big day\"), the author's intent to imply importance.",
        "The intent behind other usages, like in \"She is a big person\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.",
        "Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG).",
        "Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, functional grammar, construction grammar, computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences of the ACL).",
        "More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".",
        "Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit) and developments in artificial intelligence, specifically tools and technologies using large language model approaches and new directions in artificial general intelligence based on the free energy principle by British neuroscientist and theoretician at University College London Karl J. Friston."
      ],
      "metadata": {
        "title": "Natural language processing",
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "word_count": 4444,
        "char_count": 29782,
        "sentence_count": 179,
        "scraped_at": "2025-08-09T14:46:52.516012",
        "language": "en",
        "processing_time": 0.015396833419799805,
        "source_hash": "804ed00e497a95700bf4860d634c5f15"
      }
    },
    {
      "title": "Natural language",
      "url": "https://en.wikipedia.org/wiki/Natural_language",
      "raw_text": "A natural language or ordinary language is a language that occurs organically in a human community by a process of use, repetition, and change and in forms such as written, spoken and signed. Categorization as natural includes languages associated with linguistic prescriptivism or language regulation, but excludes constructed and formal languages such as those used for computer programming and logic. Nonstandard dialects can be viewed as a wild type in comparison with standard languages. An official language with a regulating academy such as Standard French, overseen by the Académie Française, is classified as a natural language (e.g. in the field of natural language processing), as its prescriptive aspects do not make it constructed enough to be a constructed language or controlled enough to be a controlled natural language.\nCategorization as natural excludes:\n\nArtificial and constructed languages\nConstructed international auxiliary languages\nNon-human communication systems in nature such as whale and other marine mammal vocalizations or honey bees' waggle dance.\n\n\n== Controlled languages ==\n\nControlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce ambiguity and complexity. This may be accomplished by decreasing usage of superlative or adverbial forms, or irregular verbs. Typical purposes for developing and implementing a controlled natural language are to aid understanding by non-native speakers or to ease computer processing. An example of a widely used controlled natural language is Simplified Technical English, which was originally developed for aerospace and avionics industry manuals.\n\n\n== International constructed languages ==\n\nBeing constructed, International auxiliary languages such as Esperanto and Interlingua are not considered natural languages, with the possible exception of true native speakers of such languages. Natural languages evolve, through fluctuations in vocabulary and syntax, to incrementally improve human communication. In contrast, Esperanto was created by Polish ophthalmologist L. L. Zamenhof in the late 19th century.\nSome natural languages have become organically \"standardized\" through the synthesis of two or more pre-existing natural languages over a relatively short period of time through the development of a pidgin, which is not considered a language, into a stable creole language. A creole such as Haitian Creole has its own grammar, vocabulary and literature. It is spoken by over 10 million people worldwide and is one of the two official languages of the Republic of Haiti.\nAs of 1996, there were 350 attested families with one or more native speakers of Esperanto. Latino sine flexione, another international auxiliary language, is no longer widely spoken.\n\n\n== See also ==\nLanguage acquisition – Process in which a first language is being acquired\nOrigin of language – Relationship between language and human evolution\nFormal semantics (natural language) – Formal study of linguistic meaning\nWhistled language – Emulation of speech by whistling\n\n\n== Notes ==\n\n\n== References ==",
      "cleaned_text": "A natural language or ordinary language is a language that occurs organically in a human community by a process of use, repetition, and change and in forms such as written, spoken and signed. Categorization as natural includes languages associated with linguistic prescriptivism or language regulation, but excludes constructed and formal languages such as those used for computer programming and logic. Nonstandard dialects can be viewed as a wild type in comparison with standard languages. An official language with a regulating academy such as Standard French, overseen by the Académie Française, is classified as a natural language (e.g. in the field of natural language processing), as its prescriptive aspects do not make it constructed enough to be a constructed language or controlled enough to be a controlled natural language. Categorization as natural excludes: Artificial and constructed languages Constructed international auxiliary languages Non-human communication systems in nature such as whale and other marine mammal vocalizations or honey bees' waggle dance. Controlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce ambiguity and complexity. This may be accomplished by decreasing usage of superlative or adverbial forms, or irregular verbs. Typical purposes for developing and implementing a controlled natural language are to aid understanding by non-native speakers or to ease computer processing. An example of a widely used controlled natural language is Simplified Technical English, which was originally developed for aerospace and avionics industry manuals. Being constructed, International auxiliary languages such as Esperanto and Interlingua are not considered natural languages, with the possible exception of true native speakers of such languages. Natural languages evolve, through fluctuations in vocabulary and syntax, to incrementally improve human communication. In contrast, Esperanto was created by Polish ophthalmologist L. L. Zamenhof in the late 19th century. Some natural languages have become organically \"standardized\" through the synthesis of two or more pre-existing natural languages over a relatively short period of time through the development of a pidgin, which is not considered a language, into a stable creole language. A creole such as Haitian Creole has its own grammar, vocabulary and literature. It is spoken by over 10 million people worldwide and is one of the two official languages of the Republic of Haiti. As of 1996, there were 350 attested families with one or more native speakers of Esperanto. Latino sine flexione, another international auxiliary language, is no longer widely spoken.",
      "sentences": [
        "A natural language or ordinary language is a language that occurs organically in a human community by a process of use, repetition, and change and in forms such as written, spoken and signed.",
        "Categorization as natural includes languages associated with linguistic prescriptivism or language regulation, but excludes constructed and formal languages such as those used for computer programming and logic.",
        "Nonstandard dialects can be viewed as a wild type in comparison with standard languages.",
        "An official language with a regulating academy such as Standard French, overseen by the Académie Française, is classified as a natural language (e.g.",
        "in the field of natural language processing), as its prescriptive aspects do not make it constructed enough to be a constructed language or controlled enough to be a controlled natural language.",
        "Categorization as natural excludes: Artificial and constructed languages Constructed international auxiliary languages Non-human communication systems in nature such as whale and other marine mammal vocalizations or honey bees' waggle dance.",
        "Controlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce ambiguity and complexity.",
        "This may be accomplished by decreasing usage of superlative or adverbial forms, or irregular verbs.",
        "Typical purposes for developing and implementing a controlled natural language are to aid understanding by non-native speakers or to ease computer processing.",
        "An example of a widely used controlled natural language is Simplified Technical English, which was originally developed for aerospace and avionics industry manuals.",
        "Being constructed, International auxiliary languages such as Esperanto and Interlingua are not considered natural languages, with the possible exception of true native speakers of such languages.",
        "Natural languages evolve, through fluctuations in vocabulary and syntax, to incrementally improve human communication.",
        "In contrast, Esperanto was created by Polish ophthalmologist L. L. Zamenhof in the late 19th century.",
        "Some natural languages have become organically \"standardized\" through the synthesis of two or more pre-existing natural languages over a relatively short period of time through the development of a pidgin, which is not considered a language, into a stable creole language.",
        "A creole such as Haitian Creole has its own grammar, vocabulary and literature.",
        "It is spoken by over 10 million people worldwide and is one of the two official languages of the Republic of Haiti.",
        "As of 1996, there were 350 attested families with one or more native speakers of Esperanto.",
        "Latino sine flexione, another international auxiliary language, is no longer widely spoken."
      ],
      "metadata": {
        "title": "Natural language",
        "url": "https://en.wikipedia.org/wiki/Natural_language",
        "word_count": 400,
        "char_count": 2733,
        "sentence_count": 18,
        "scraped_at": "2025-08-09T14:46:52.517393",
        "language": "en",
        "processing_time": 0.0010821819305419922,
        "source_hash": "d6cd91695bd18fcc8b038e9c47dbefb0"
      }
    },
    {
      "title": "Quantum natural language processing",
      "url": "https://en.wikipedia.org/wiki/Quantum_natural_language_processing",
      "raw_text": "Quantum natural language processing (QNLP) is the application of quantum computing to natural language processing (NLP). It computes word embeddings as parameterised quantum circuits that can solve NLP tasks faster than any classical computer. It is inspired by categorical quantum mechanics and the DisCoCat framework, making use of string diagrams to translate from grammatical structure to quantum processes.\n\n\n== Theory ==\nThe first quantum algorithm for natural language processing used the DisCoCat framework and Grover's algorithm to show a quadratic quantum speedup for a text classification task. It was later shown that quantum language processing is BQP-Complete, i.e. quantum language models are more expressive than their classical counterpart, unless quantum mechanics can be efficiently simulated by classical computers.\nThese two theoretical results assume fault-tolerant quantum computation and a QRAM, i.e. an efficient way to load classical data on a quantum computer. Thus, they are not applicable to the noisy intermediate-scale quantum (NISQ) computers available today.\n\n\n== Experiments ==\nThe algorithm of Zeng and Coecke was adapted to the constraints of NISQ computers and implemented on IBM quantum computers to solve binary classification tasks. Instead of loading classical word vectors onto a quantum memory, the word vectors are computed directly as the parameters of quantum circuits. These parameters are optimised using methods from quantum machine learning to solve data-driven tasks such as question answering, machine translation and even algorithmic music composition.\n\n\n== See also ==\nCategorical quantum mechanics\nNatural language processing\nQuantum machine learning\nApplied category theory\nString diagram\n\n\n== References ==\n\n\n== External links ==\nDisCoPy, a Python toolkit for computing with string diagrams\nlambeq, a Python library for quantum natural language processing",
      "cleaned_text": "Quantum natural language processing (QNLP) is the application of quantum computing to natural language processing (NLP). It computes word embeddings as parameterised quantum circuits that can solve NLP tasks faster than any classical computer. It is inspired by categorical quantum mechanics and the DisCoCat framework, making use of string diagrams to translate from grammatical structure to quantum processes. The first quantum algorithm for natural language processing used the DisCoCat framework and Grover's algorithm to show a quadratic quantum speedup for a text classification task. It was later shown that quantum language processing is BQP-Complete, i.e. quantum language models are more expressive than their classical counterpart, unless quantum mechanics can be efficiently simulated by classical computers. These two theoretical results assume fault-tolerant quantum computation and a QRAM, i.e. an efficient way to load classical data on a quantum computer. Thus, they are not applicable to the noisy intermediate-scale quantum (NISQ) computers available today. The algorithm of Zeng and Coecke was adapted to the constraints of NISQ computers and implemented on IBM quantum computers to solve binary classification tasks. Instead of loading classical word vectors onto a quantum memory, the word vectors are computed directly as the parameters of quantum circuits. These parameters are optimised using methods from quantum machine learning to solve data-driven tasks such as question answering, machine translation and even algorithmic music composition.",
      "sentences": [
        "Quantum natural language processing (QNLP) is the application of quantum computing to natural language processing (NLP).",
        "It computes word embeddings as parameterised quantum circuits that can solve NLP tasks faster than any classical computer.",
        "It is inspired by categorical quantum mechanics and the DisCoCat framework, making use of string diagrams to translate from grammatical structure to quantum processes.",
        "The first quantum algorithm for natural language processing used the DisCoCat framework and Grover's algorithm to show a quadratic quantum speedup for a text classification task.",
        "It was later shown that quantum language processing is BQP-Complete, i.e.",
        "quantum language models are more expressive than their classical counterpart, unless quantum mechanics can be efficiently simulated by classical computers.",
        "These two theoretical results assume fault-tolerant quantum computation and a QRAM, i.e.",
        "an efficient way to load classical data on a quantum computer.",
        "Thus, they are not applicable to the noisy intermediate-scale quantum (NISQ) computers available today.",
        "The algorithm of Zeng and Coecke was adapted to the constraints of NISQ computers and implemented on IBM quantum computers to solve binary classification tasks.",
        "Instead of loading classical word vectors onto a quantum memory, the word vectors are computed directly as the parameters of quantum circuits.",
        "These parameters are optimised using methods from quantum machine learning to solve data-driven tasks such as question answering, machine translation and even algorithmic music composition."
      ],
      "metadata": {
        "title": "Quantum natural language processing",
        "url": "https://en.wikipedia.org/wiki/Quantum_natural_language_processing",
        "word_count": 224,
        "char_count": 1570,
        "sentence_count": 12,
        "scraped_at": "2025-08-09T14:46:52.518100",
        "language": "en",
        "processing_time": 0.0006041526794433594,
        "source_hash": "8e060ec9b37a44ae15530b29aafef0fe"
      }
    },
    {
      "title": "History of natural language processing",
      "url": "https://en.wikipedia.org/wiki/History_of_natural_language_processing",
      "raw_text": "The history of natural language processing describes the advances of natural language processing. There is some overlap with the history of machine translation, the history of speech recognition, and the history of artificial intelligence.\n\n\n== Early history ==\nThe history of machine translation dates back to the seventeenth century, when philosophers such as Leibniz and Descartes put forward proposals for codes which would relate words between languages. All of these proposals remained theoretical, and none resulted in the development of an actual machine.\nThe first patents for \"translating machines\" were applied for in the mid-1930s. One proposal, by Georges Artsrouni was simply an automatic bilingual dictionary using paper tape. The other proposal, by Peter Troyanskii, a Russian, was more detailed. Troyanski proposal included both the bilingual dictionary, and a method for dealing with grammatical roles between languages, based on Esperanto.\n\n\n== Logical period ==\nIn 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge, sufficiently well that the judge is unable to distinguish reliably — on the basis of the conversational content alone — between the program and a real human.\nIn 1957, Noam Chomsky’s Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule-based system of syntactic structures.\nThe Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.  Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\nSome notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies.\nIn 1969 Roger Schank introduced the conceptual dependency theory for natural language understanding. This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.\nIn 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input. Instead of phrase structure rules ATNs used an equivalent set of finite-state automata that were called recursively. ATNs and their more general format called \"generalized ATNs\" continued to be used for a number of years. During the 1970s many programmers began to write 'conceptual ontologies', which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n\n\n== Statistical period ==\n\nUp to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing.  This was due both to the steady increase in computational power resulting from Moore's law and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.  Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.  Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n\n\n=== Datasets ===\nThe emergence of statistical approaches was aided by both increase in computing power and the availability of large datasets. At that time, large multilingual corpora were starting to emerge. Notably, some were produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.\nMany of the notable early successes occurred in the field of machine translation. In 1993, the IBM alignment models were used for statistical machine translation. Compared to previous machine translation systems, which were symbolic systems manually coded by computational linguists, these systems were statistical, which allowed them to automatically learn from large textual corpora. Though these systems do not work well in situations where only small corpora is available, so data-efficient methods continue to be an area of research and development.\nIn 2001, a one-billion-word large text corpus, scraped from the Internet, referred to as \"very very large\" at the time, was used for word disambiguation.\nTo take advantage of large, unlabelled datasets, algorithms were developed for unsupervised and self-supervised learning. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.  However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.\n\n\n== Neural period ==\n\nNeural language models were developed in 1990s. In 1990, the Elman network, using a recurrent neural network, encoded each word in a training set as a vector, called a word embedding, and the whole vocabulary as a vector database, allowing it to perform such tasks as sequence-predictions that are beyond the power of a simple multilayer perceptron. A shortcoming of the static embeddings was that they didn't differentiate between multiple meanings of homonyms.\nYoshua Bengio developed the first neural probabilistic language model in 2000 \nIn recent years, advancements in deep learning and large language models have significantly enhanced the capabilities of natural language processing, leading to widespread applications in areas such as healthcare, customer service, and content generation.\n\n\n== Software ==\n\n\n== References ==\n\n\n== Bibliography ==\nCrevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3.\nMcCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 978-1-56881-205-2, OCLC 52197627.\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.",
      "cleaned_text": "The history of natural language processing describes the advances of natural language processing. There is some overlap with the history of machine translation, the history of speech recognition, and the history of artificial intelligence. The history of machine translation dates back to the seventeenth century, when philosophers such as Leibniz and Descartes put forward proposals for codes which would relate words between languages. All of these proposals remained theoretical, and none resulted in the development of an actual machine. The first patents for \"translating machines\" were applied for in the mid-1930s. One proposal, by Georges Artsrouni was simply an automatic bilingual dictionary using paper tape. The other proposal, by Peter Troyanskii, a Russian, was more detailed. Troyanski proposal included both the bilingual dictionary, and a method for dealing with grammatical roles between languages, based on Esperanto. In 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge, sufficiently well that the judge is unable to distinguish reliably - on the basis of the conversational content alone - between the program and a real human. In 1957, Noam Chomsky’s Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule-based system of syntactic structures. The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed. Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies. In 1969 Roger Schank introduced the conceptual dependency theory for natural language understanding. This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner. In 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input. Instead of phrase structure rules ATNs used an equivalent set of finite-state automata that were called recursively. ATNs and their more general format called \"generalized ATNs\" continued to be used for a number of years. During the 1970s many programmers began to write 'conceptual ontologies', which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky. Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due both to the steady increase in computational power resulting from Moore's law and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks. The emergence of statistical approaches was aided by both increase in computing power and the availability of large datasets. At that time, large multilingual corpora were starting to emerge. Notably, some were produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. Many of the notable early successes occurred in the field of machine translation. In 1993, the IBM alignment models were used for statistical machine translation. Compared to previous machine translation systems, which were symbolic systems manually coded by computational linguists, these systems were statistical, which allowed them to automatically learn from large textual corpora. Though these systems do not work well in situations where only small corpora is available, so data-efficient methods continue to be an area of research and development. In 2001, a one-billion-word large text corpus, scraped from the Internet, referred to as \"very very large\" at the time, was used for word disambiguation. To take advantage of large, unlabelled datasets, algorithms were developed for unsupervised and self-supervised learning. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results. Neural language models were developed in 1990s. In 1990, the Elman network, using a recurrent neural network, encoded each word in a training set as a vector, called a word embedding, and the whole vocabulary as a vector database, allowing it to perform such tasks as sequence-predictions that are beyond the power of a simple multilayer perceptron. A shortcoming of the static embeddings was that they didn't differentiate between multiple meanings of homonyms. Yoshua Bengio developed the first neural probabilistic language model in 2000 In recent years, advancements in deep learning and large language models have significantly enhanced the capabilities of natural language processing, leading to widespread applications in areas such as healthcare, customer service, and content generation.",
      "sentences": [
        "The history of natural language processing describes the advances of natural language processing.",
        "There is some overlap with the history of machine translation, the history of speech recognition, and the history of artificial intelligence.",
        "The history of machine translation dates back to the seventeenth century, when philosophers such as Leibniz and Descartes put forward proposals for codes which would relate words between languages.",
        "All of these proposals remained theoretical, and none resulted in the development of an actual machine.",
        "The first patents for \"translating machines\" were applied for in the mid-1930s.",
        "One proposal, by Georges Artsrouni was simply an automatic bilingual dictionary using paper tape.",
        "The other proposal, by Peter Troyanskii, a Russian, was more detailed.",
        "Troyanski proposal included both the bilingual dictionary, and a method for dealing with grammatical roles between languages, based on Esperanto.",
        "In 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.",
        "This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge, sufficiently well that the judge is unable to distinguish reliably - on the basis of the conversational content alone - between the program and a real human.",
        "In 1957, Noam Chomsky’s Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule-based system of syntactic structures.",
        "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English.",
        "The authors claimed that within three or five years, machine translation would be a solved problem.",
        "However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.",
        "Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.",
        "Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies.",
        "In 1969 Roger Schank introduced the conceptual dependency theory for natural language understanding.",
        "This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.",
        "In 1970, William A.",
        "Woods introduced the augmented transition network (ATN) to represent natural language input.",
        "Instead of phrase structure rules ATNs used an equivalent set of finite-state automata that were called recursively.",
        "ATNs and their more general format called \"generalized ATNs\" continued to be used for a number of years.",
        "During the 1970s many programmers began to write 'conceptual ontologies', which structured real-world information into computer-understandable data.",
        "During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.",
        "Up to the 1980s, most NLP systems were based on complex sets of hand-written rules.",
        "Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing.",
        "This was due both to the steady increase in computational power resulting from Moore's law and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g.",
        "transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.",
        "Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.",
        "Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.",
        "The cache language models upon which many speech recognition systems now rely are examples of such statistical models.",
        "Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.",
        "The emergence of statistical approaches was aided by both increase in computing power and the availability of large datasets.",
        "At that time, large multilingual corpora were starting to emerge.",
        "Notably, some were produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.",
        "Many of the notable early successes occurred in the field of machine translation.",
        "In 1993, the IBM alignment models were used for statistical machine translation.",
        "Compared to previous machine translation systems, which were symbolic systems manually coded by computational linguists, these systems were statistical, which allowed them to automatically learn from large textual corpora.",
        "Though these systems do not work well in situations where only small corpora is available, so data-efficient methods continue to be an area of research and development.",
        "In 2001, a one-billion-word large text corpus, scraped from the Internet, referred to as \"very very large\" at the time, was used for word disambiguation.",
        "To take advantage of large, unlabelled datasets, algorithms were developed for unsupervised and self-supervised learning.",
        "Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.",
        "However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.",
        "Neural language models were developed in 1990s.",
        "In 1990, the Elman network, using a recurrent neural network, encoded each word in a training set as a vector, called a word embedding, and the whole vocabulary as a vector database, allowing it to perform such tasks as sequence-predictions that are beyond the power of a simple multilayer perceptron.",
        "A shortcoming of the static embeddings was that they didn't differentiate between multiple meanings of homonyms.",
        "Yoshua Bengio developed the first neural probabilistic language model in 2000 In recent years, advancements in deep learning and large language models have significantly enhanced the capabilities of natural language processing, leading to widespread applications in areas such as healthcare, customer service, and content generation."
      ],
      "metadata": {
        "title": "History of natural language processing",
        "url": "https://en.wikipedia.org/wiki/History_of_natural_language_processing",
        "word_count": 1029,
        "char_count": 7036,
        "sentence_count": 47,
        "scraped_at": "2025-08-09T14:46:52.520734",
        "language": "en",
        "processing_time": 0.0024788379669189453,
        "source_hash": "d4ed9e07740762764a4f49f8902b061f"
      }
    },
    {
      "title": "Outline of natural language processing",
      "url": "https://en.wikipedia.org/wiki/Outline_of_natural_language_processing",
      "raw_text": "The following outline is provided as an overview of and topical guide to natural-language processing:\nnatural-language processing – computer activity in which computers are entailed to analyze, understand, alter, or generate natural language.  This includes the automation of any or all linguistic forms, activities, or methods of communication, such as conversation, correspondence, reading, written composition, dictation, publishing, translation, lip reading, and so on.  Natural-language processing is also the name of the branch of computer science, artificial intelligence, and linguistics concerned with enabling computers to engage in communication using natural language(s) in all forms, including but not limited to speech, print, writing, and signing.\n\n\n== Natural-language processing ==\nNatural-language processing can be described as all of the following:\n\nA field of science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.\nAn applied science – field that applies human knowledge to build or design useful things.\nA field of computer science – scientific and practical approach to computation and its applications.\nA branch of artificial intelligence – intelligence of machines and robots and the branch of computer science that aims to create it.\nA subfield of computational linguistics – interdisciplinary field dealing with the statistical or rule-based modeling of natural language from a computational perspective.\nAn application of engineering – science, skill, and profession of acquiring and applying scientific, economic, social, and practical knowledge, in order to design and also build structures, machines, devices, systems, materials and processes.\nAn application of software engineering – application of a systematic, disciplined, quantifiable  approach to the design, development, operation, and maintenance of software, and the study of these approaches; that is, the application of engineering to software.\nA subfield of computer programming – process of designing, writing, testing, debugging, and maintaining the source code of computer programs. This source code is written in one or more programming languages (such as Java, C++, C#, Python, etc.). The purpose of programming is to create a set of instructions that computers use to perform specific operations or to exhibit desired behaviors.\nA subfield of artificial intelligence programming –\nA type of system – set of interacting or interdependent components forming an integrated whole or a set of elements (often called 'components' ) and relationships which are different from relationships of the set or its elements to other elements or sets.\nA system that includes software – software is a collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it. Software refers to one or more computer programs and data held in the storage of the computer. In other words, software is a set of programs, procedures, algorithms and its documentation concerned with the operation of a data processing system.\nA type of technology – making, modification, usage, and knowledge of tools, machines, techniques, crafts, systems, methods of organization, in order to solve a problem, improve a preexisting solution to a problem, achieve a goal, handle an applied input/output relation or perform a specific function. It can also refer to the collection of such tools, machinery, modifications, arrangements and procedures. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.\nA form of computer technology – computers and their application. NLP makes use of computers, image scanners, microphones, and many types of software programs.\nLanguage technology – consists of natural-language processing (NLP) and computational linguistics (CL) on the one hand, and speech technology on the other. It also includes many application oriented aspects of these. It is often called human language technology (HLT).\n\n\n== Prerequisite technologies ==\nThe following technologies make natural-language processing possible:\n\nCommunication – the activity of a source sending a message to a receiver\nLanguage –\nSpeech –\nWriting –\nComputing –\nComputers –\nComputer programming –\nInformation extraction –\nUser interface –\nSoftware –\nText editing – program used to edit plain text files\nWord processing – piece of software used for composing, editing, formatting, printing documents\nInput devices – pieces of hardware for sending data to a computer to be processed\nComputer keyboard – typewriter style input device whose input is converted into various data depending on the circumstances\nImage scanners –\n\n\n== Subfields of natural-language processing ==\nInformation extraction (IE) – field concerned in general with the extraction of semantic information from text.  This covers tasks such as named-entity recognition, coreference resolution, relationship extraction, etc.\nOntology engineering – field that studies the methods and methodologies for building ontologies, which are formal representations of a set of concepts within a domain and the relationships between those concepts.\nSpeech processing – field that covers speech recognition, text-to-speech and related tasks.\nStatistical natural-language processing –\nStatistical semantics – a subfield of computational semantics that establishes semantic relations between words to examine their contexts.\nDistributional semantics – a subfield of statistical semantics that examines the semantic relationship of words across a corpora or in large samples of data.\n\n\n== Related fields ==\nNatural-language processing contributes to, and makes use of (the theories, tools, and methodologies from), the following fields:\n\nAutomated reasoning – area of computer science and mathematical logic dedicated to understanding various aspects of reasoning, and producing software which allows computers to reason completely, or nearly completely, automatically. A sub-field of artificial intelligence, automatic reasoning is also grounded in theoretical computer science and philosophy of mind.\nLinguistics – scientific study of human language. Natural-language processing requires understanding of the structure and application of language, and therefore it draws heavily from linguistics.\nApplied linguistics – interdisciplinary field of study that identifies, investigates, and offers solutions to language-related real-life problems. Some of the academic fields related to applied linguistics are education, linguistics, psychology, computer science, anthropology, and sociology. Some of the subfields of applied linguistics relevant to natural-language processing are:\nBilingualism / Multilingualism –\nComputer-mediated communication (CMC) – any communicative transaction that occurs through the use of two or more networked computers. Research on CMC focuses largely on the social effects of different computer-supported communication technologies. Many recent studies involve Internet-based social networking supported by social software.\nContrastive linguistics – practice-oriented linguistic approach that seeks to describe the differences and similarities between a pair of languages.\nConversation analysis (CA) – approach to the study of social interaction, embracing both verbal and non-verbal conduct, in situations of everyday life. Turn-taking is one aspect of language use that is studied by CA.\nDiscourse analysis – various approaches to analyzing written, vocal, or sign language use or any significant semiotic event.\nForensic linguistics – application of linguistic knowledge, methods and insights to the forensic context of law, language, crime investigation, trial, and judicial procedure.\nInterlinguistics – study of improving communications between people of different first languages with the use of ethnic and auxiliary languages (lingua franca). For instance by use of intentional international auxiliary languages, such as Esperanto or Interlingua, or spontaneous interlanguages known as pidgin languages.\nLanguage assessment – assessment of first, second or other language in the school, college, or university context; assessment of language use in the workplace; and assessment of language in the immigration, citizenship, and asylum contexts. The assessment may include analyses of listening, speaking, reading, writing or cultural understanding, with respect to understanding how the language works theoretically and the ability to use the language practically.\nLanguage pedagogy – science and art of language education, including approaches and methods of language teaching and study. Natural-language processing is used in programs designed to teach language, including first- and second-language training.\nLanguage planning –\nLanguage policy –\nLexicography –\nLiteracies –\nPragmatics –\nSecond-language acquisition –\nStylistics –\nTranslation –\nComputational linguistics – interdisciplinary field dealing with the statistical or rule-based modeling of natural language from a computational perspective. The models and tools of computational linguistics are used extensively in the field of natural-language processing, and vice versa.\nComputational semantics –\nCorpus linguistics – study of language as expressed in samples (corpora) of \"real world\" text.  Corpora is the plural of corpus, and a corpus is a specifically selected collection of texts (or speech segments) composed of natural language. After it is constructed (gathered or composed), a corpus is analyzed with the methods of computational linguistics to infer the meaning and context of its components (words, phrases, and sentences), and the relationships between them.  Optionally, a corpus can be annotated (\"tagged\") with data (manually or automatically) to make the corpus easier to understand (e.g., part-of-speech tagging).  This data is then applied to make sense of user input, for example, to make better (automated) guesses of what people are talking about or saying, perhaps to achieve more narrowly focused web searches, or for speech recognition.\nMetalinguistics –\nSign linguistics – scientific study and analysis of natural sign languages, their features, their structure (phonology, morphology, syntax, and semantics), their acquisition (as a primary or secondary language), how they develop independently of other languages, their application in communication, their relationships to other languages (including spoken languages), and many other aspects.\nHuman–computer interaction – the intersection of computer science and behavioral sciences, this field involves the study, planning, and design of the interaction between people (users) and computers.  Attention to human-machine interaction is important, because poorly designed human-machine interfaces can lead to many unexpected problems. A classic example of this is the Three Mile Island accident where investigations concluded that the design of the human–machine interface was at least partially responsible for the disaster.\nInformation retrieval (IR) – field concerned with storing, searching and retrieving information. It is a separate field within computer science (closer to databases), but IR relies on some NLP methods (for example, stemming). Some current research and applications seek to bridge the gap between IR and NLP.\nKnowledge representation (KR) – area of artificial intelligence research aimed at representing knowledge in symbols to facilitate inferencing from those knowledge elements, creating new elements of knowledge. Knowledge Representation research involves analysis of how to reason accurately and effectively and how best to use a set of symbols to represent a set of facts within a knowledge domain.\nSemantic network – study of semantic relations between concepts.\nSemantic Web –\nMachine learning – subfield of computer science that examines pattern recognition and computational learning theory in artificial intelligence. There are three broad approaches to machine learning.  Supervised learning occurs when the machine is given example inputs and outputs by a teacher so that it can learn a rule that maps inputs to outputs. Unsupervised learning occurs when the machine determines the inputs structure without being provided example inputs or outputs. Reinforcement learning occurs when a machine must perform a goal without teacher feedback.\nPattern recognition – branch of machine learning that examines how machines recognize regularities in data. As with machine learning, teachers can train machines to recognize patterns by providing them with example inputs and outputs (i.e. Supervised Learning), or the machines can recognize patterns without being trained on any example inputs or outputs (i.e. Unsupervised Learning).\nStatistical classification –\n\n\n== Structures used in natural-language processing ==\nAnaphora – type of expression whose reference depends upon another referential element. E.g., in the sentence 'Sally preferred the company of herself', 'herself' is an anaphoric expression in that it is coreferential with 'Sally', the sentence's subject.\nContext-free language –\nControlled natural language – a natural language with a restriction introduced on its grammar and vocabulary in order to eliminate ambiguity and complexity\nCorpus – body of data, optionally tagged (for example, through part-of-speech tagging), providing real world samples for analysis and comparison.\nText corpus – large and structured set of texts, nowadays usually electronically stored and processed. They are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific subject (or domain).\nSpeech corpus – database of speech audio files and text transcriptions. In Speech technology, speech corpora are used, among other things, to create acoustic models (which can then be used with a speech recognition engine). In Linguistics, spoken corpora are used to do research into phonetic, conversation analysis, dialectology and other fields.\nGrammar –\nContext-free grammar (CFG) –\nConstraint grammar (CG) –\nDefinite clause grammar (DCG) –\nFunctional unification grammar (FUG) –\nGeneralized phrase structure grammar (GPSG) –\nHead-driven phrase structure grammar (HPSG) –\nLexical functional grammar (LFG) –\nProbabilistic context-free grammar (PCFG) – another name for stochastic context-free grammar.\nStochastic context-free grammar (SCFG) –\nSystemic functional grammar (SFG) –\nTree-adjoining grammar (TAG) –\nNatural language –\nn-gram – sequence of n number of tokens, where a \"token\" is a character, syllable, or word. The n is replaced by a number. Therefore, a 5-gram is an n-gram of 5 letters, syllables, or words. \"Eat this\" is a 2-gram (also known as a bigram).\nBigram – n-gram of 2 tokens. Every sequence of 2 adjacent elements in a string of tokens is a bigram. Bigrams are used for speech recognition, they can be used to solve cryptograms, and bigram frequency is one approach to statistical language identification.\nTrigram –  special case of the n-gram, where n is 3.\nOntology – formal representation of a set of concepts within a domain and the relationships between those concepts.\nTaxonomy – practice and science of classification, including the principles underlying classification, and the methods of classifying things or concepts.\nHyponymy and hypernymy – the linguistics of hyponyms and hypernyms.  A hyponym shares a type-of relationship with its hypernym. For example, pigeon, crow, eagle and seagull are all hyponyms of bird (their hypernym); which, in turn, is a hyponym of animal.\nTaxonomy for search engines – typically called a \"taxonomy of entities\". It is a tree in which nodes are labelled with entities which are expected to occur in a web search query. These trees are used to match keywords from a search query with the keywords from relevant answers (or snippets).\nTextual entailment – directional relation between text fragments. The relation holds whenever the truth of one text fragment follows from another text. In the TE framework, the entailing and entailed texts are termed text (t) and hypothesis (h), respectively. The relation is directional because even if \"t entails h\", the reverse \"h entails t\" is much less certain.\nTriphone – sequence of three phonemes. Triphones are useful in models of natural-language processing where they are used to establish the various contexts in which a phoneme can occur in a particular natural language.\n\n\n== Processes of NLP ==\n\n\n=== Applications ===\nAutomated essay scoring (AES) – the use of specialized computer programs to assign grades to essays written in an educational setting. It is a method of educational assessment and an application of natural-language processing. Its objective is to classify a large set of textual entities into a small number of discrete categories, corresponding to the possible grades—for example, the numbers 1 to 6. Therefore, it can be considered a problem of statistical classification.\nAutomatic image annotation – process by which a computer system automatically assigns textual metadata in the form of captioning or keywords to a digital image. The annotations are used in image retrieval systems to organize and locate images of interest from a database.\nAutomatic summarization – process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document. Often used to provide summaries of text of a known type, such as articles in the financial section of a newspaper.\nTypes\nKeyphrase extraction –\nDocument summarization –\nMulti-document summarization –\nMethods and techniques\nExtraction-based summarization –\nAbstraction-based summarization –\nMaximum entropy-based summarization –\nSentence extraction –\nAided summarization –\nHuman aided machine summarization (HAMS) –\nMachine aided human summarization (MAHS) –\nAutomatic taxonomy induction – automated construction of tree structures from a corpus. This may be applied to building taxonomical classification systems for reading by end users, such as web directories or subject outlines.\nCoreference resolution – in order to derive the correct interpretation of text, or even to estimate the relative importance of various mentioned subjects, pronouns and other referring expressions need to be connected to the right individuals or objects. Given a sentence or larger chunk of text, coreference resolution determines which words (\"mentions\") refer to which objects (\"entities\") included in the text.\nAnaphora resolution – concerned with matching up pronouns with the nouns or names that they refer to. For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).\nDialog system –\nForeign-language reading aid – computer program that assists a non-native language user to read properly in their target language. The proper reading means that the pronunciation should be correct and stress to different parts of the words should be proper.\nForeign-language writing aid – computer program or any other instrument that assists a non-native language user (also referred to as a foreign-language learner) in writing decently in their target language. Assistive operations can be classified into two categories: on-the-fly prompts and post-writing checks.\nGrammar checking – the act of verifying the grammatical correctness of written text, especially if this act is performed by a computer program.\nInformation retrieval –\nCross-language information retrieval –\nMachine translation (MT) – aims to automatically translate text from one human language to another.  This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) in order to solve properly.\nClassical approach of machine translation – rules-based machine translation.\nComputer-assisted translation –\nInteractive machine translation –\nTranslation memory – database that stores so-called \"segments\", which can be sentences, paragraphs or sentence-like units (headings, titles or elements in a list) that have previously been translated, in order to aid human translators.\nExample-based machine translation –\nRule-based machine translation –\nNatural-language programming – interpreting and compiling instructions communicated in natural language into computer instructions (machine code).\nNatural-language search –\nOptical character recognition (OCR) – given an image representing printed text, determine the corresponding text.\nQuestion answering – given a human-language question, determine its answer.  Typical questions have a specific right answer (such as \"What is the capital of Canada?\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\").\nOpen domain question answering –\nSpam filtering –\nSentiment analysis – extracts subjective information usually from a set of documents, often using online reviews to determine \"polarity\" about specific objects. It is especially useful for identifying trends of public opinion in the social media, for the purpose of marketing.\nSpeech recognition – given a sound clip of a person or people speaking, determine the textual representation of the speech.  This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above).  In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process.\nSpeech synthesis (Text-to-speech) –\nText-proofing –\nText simplification – automated editing a document to include fewer words, or use easier words, while retaining its underlying meaning and information.\n\n\n=== Component processes ===\nNatural-language understanding – converts chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural-language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural-language expression which usually takes the form of organized notations of natural-languages concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural-languages semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.\nNatural-language generation – task of converting information from computer databases into readable  human language.\n\n\n==== Component processes of natural-language understanding ====\nAutomatic document classification (text categorization) –\nAutomatic language identification –\nCompound term processing – category of techniques that identify compound terms and match them to their definitions. Compound terms are built by combining two (or more) simple terms, for example \"triple\" is a single word term but \"triple heart bypass\" is a compound term.\nAutomatic taxonomy induction –\nCorpus processing –\nAutomatic acquisition of lexicon –\nText normalization –\nText simplification –\nDeep linguistic processing –\nDiscourse analysis – includes a number of related tasks.  One task is identifying the discourse structure of connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast).  Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes–no questions, content questions, statements, assertions, orders, suggestions, etc.).\nInformation extraction –\nText mining – process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning.\nBiomedical text mining – (also known as BioNLP), this is text mining applied to texts and literature of the biomedical and molecular biology domain. It is a rather recent research field drawing elements from natural-language processing, bioinformatics, medical informatics and computational linguistics. There is an increasing interest in text mining and information extraction strategies applied to the biomedical and molecular biology literature due to the increasing number of electronically available publications stored in databases such as PubMed.\nDecision tree learning –\nSentence extraction –\nTerminology extraction –\nLatent semantic indexing –\nLemmatisation – groups together all like terms that share a same lemma such that they are classified as a single item.\nMorphological segmentation – separates words into individual morphemes and identifies the class of the morphemes.  The difficulty of this task depends greatly on the complexity of the morphology (i.e. the structure of words) of the language being considered.  English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g. \"open, opens, opened, opening\") as separate words.  In languages such as Turkish, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.\nNamed-entity recognition (NER) – given a stream of text, determines which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization).  Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case is often inaccurate or insufficient.  For example, the first word of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized.  Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names.  For example, German capitalizes all nouns, regardless of whether they refer to names, and French and Spanish do not capitalize names that serve as adjectives.\nOntology learning – automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between those concepts from a corpus of natural-language text, and encoding them with an ontology language for easy retrieval. Also called \"ontology extraction\", \"ontology generation\", and \"ontology acquisition\".\nParsing – determines the parse tree (grammatical analysis) of a given sentence.  The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses.  In fact, perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human).\nShallow parsing –\nPart-of-speech tagging – given a sentence, determines the part of speech for each word.  Many words, especially common ones, can serve as multiple parts of speech.  For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five different parts of speech. Some languages have more such ambiguity than others.  Languages with little inflectional morphology, such as English are particularly prone to such ambiguity. Chinese is prone to such ambiguity because it is a tonal language during verbalization. Such inflection is not readily conveyed via the entities employed within the orthography to convey intended meaning.\nQuery expansion –\nRelationship extraction – given a chunk of text, identifies the relationships among named entities (e.g. who is the wife of whom).\nSemantic analysis (computational) – formal analysis of meaning, and \"computational\" refers to approaches that in principle support effective implementation.\nExplicit semantic analysis –\nLatent semantic analysis –\nSemantic analytics –\nSentence breaking (also known as sentence boundary disambiguation and sentence detection) – given a chunk of text, finds the sentence boundaries.  Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g. marking abbreviations).\nSpeech segmentation – given a sound clip of a person or people speaking, separates it into words.  A subtask of speech recognition and typically grouped with it.\nStemming – reduces an inflected or derived word into its word stem, base, or root form.\nText chunking –\nTokenization – given a chunk of text, separates it into distinct words, symbols, sentences, or other units\nTopic segmentation and recognition – given a chunk of text, separates it into segments each of which is devoted to a topic, and identifies the topic of the segment.\nTruecasing –\nWord segmentation – separates a chunk of continuous text into separate words.  For a language like English, this is fairly trivial, since words are usually separated by spaces.  However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language.\nWord-sense disambiguation (WSD) – because many words have more than one meaning, word-sense disambiguation is used to select the meaning which makes the most sense in context.  For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or from an online resource such as WordNet.\nWord-sense induction – open problem of natural-language processing, which concerns the automatic identification of the senses of a word (i.e. meanings). Given that the output of word-sense induction is a set of senses for the target word (sense inventory), this task is strictly related to that of word-sense disambiguation (WSD), which relies on a predefined sense inventory and aims to solve the ambiguity of words in context.\nAutomatic acquisition of sense-tagged corpora –\nW-shingling – set of unique \"shingles\"—contiguous subsequences of tokens in a document—that can be used to gauge the similarity of two documents. The w denotes the number of tokens in each shingle in the set.\n\n\n==== Component processes of natural-language generation ====\nNatural-language generation – task of converting information from computer databases into readable  human language.\n\nAutomatic taxonomy induction (ATI) – automated building of tree structures from a corpus. While ATI is used to construct the core of ontologies (and doing so makes it a component process of natural-language understanding), when the ontologies being constructed are end user readable (such as a subject outline), and these are used for the construction of further documentation (such as using an outline as the basis to construct a report or treatise) this also becomes a component process of natural-language generation.\nDocument structuring –\n\n\n== History of natural-language processing ==\nHistory of natural-language processing\n\nHistory of machine translation\nHistory of automated essay scoring\nHistory of natural-language user interface\nHistory of natural-language understanding\nHistory of optical character recognition\nHistory of question answering\nHistory of speech synthesis\nTuring test – test of a machine's ability to exhibit intelligent behavior, equivalent to or indistinguishable from, that of an actual human. In the original illustrative example, a human judge engages in a natural-language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being. All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test. The test was introduced by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence,\" which opens with the words: \"I propose to consider the question, 'Can machines think?'\"\nUniversal grammar – theory in linguistics, usually credited to Noam Chomsky, proposing that the ability to learn grammar is hard-wired into the brain. The theory suggests that linguistic ability manifests itself without being taught (see poverty of the stimulus), and that there are properties that all natural human languages share. It is a matter of observation and experimentation to determine precisely what abilities are innate and what properties are shared by all languages.\nALPAC – was a committee of seven scientists led by John R. Pierce, established in 1964 by the U. S. Government in order to evaluate the progress in computational linguistics in general and machine translation in particular. Its report, issued in 1966, gained notoriety for being very skeptical of research done in machine translation so far, and emphasizing the need for basic research in computational linguistics; this eventually caused the U. S. Government to reduce its funding of the topic dramatically.\nConceptual dependency theory – a model of natural-language understanding used in artificial intelligence systems.  Roger Schank at Stanford University introduced the model in 1969, in the early days of artificial intelligence. This model was extensively used by Schank's students at Yale University such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.\nAugmented transition network – type of graph theoretic structure used in the operational definition of formal languages, used especially in parsing relatively complex natural languages, and having wide application in artificial intelligence.  Introduced by William A. Woods in 1970.\nDistributed Language Translation (project) –\n\n\n=== Timeline of NLP software ===\n\n\n== General natural-language processing concepts ==\nSukhotin's algorithm – statistical classification algorithm for classifying characters in a text as vowels or consonants. It was initially created by Boris V. Sukhotin.\nT9 (predictive text) – stands for \"Text on 9 keys\", is a USA-patented predictive text technology for mobile phones (specifically those that contain a 3x4 numeric keypad), originally developed by Tegic Communications, now part of Nuance Communications.\nTatoeba – free collaborative online database of example sentences geared towards foreign-language learners.\nTeragram Corporation –  fully owned subsidiary of SAS Institute, a major producer of statistical analysis software, headquartered in Cary, North Carolina, USA. Teragram is based in Cambridge, Massachusetts and specializes in the application of computational linguistics to multilingual natural-language processing.\nTipTop Technologies – company that developed TipTop Search, a real-time web, social search engine with a unique platform for semantic analysis of natural language. TipTop Search provides results capturing individual and group sentiment, opinions, and experiences from content of various sorts including real-time messages from Twitter or consumer product reviews on Amazon.com.\nTransderivational search – when a search is being conducted for a fuzzy match across a broad field. In computing the equivalent function can be performed using content-addressable memory.\nVocabulary mismatch – common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.\nLRE Map –\nReification (linguistics) –\nSemantic Web –\nMetadata –\nSpoken dialogue system –\nAffix grammar over a finite lattice –\nAggregation (linguistics) –\nBag-of-words model – model that represents a text as a bag (multiset) of its words that disregards grammar and word sequence, but maintains multiplicity. This model is a commonly used to train document classifiers\nBrill tagger –\nCache language model –\nChaSen, MeCab – provide morphological analysis and word splitting for Japanese\nClassic monolingual WSD –\nClearForest –\nCMU Pronouncing Dictionary – also known as cmudict, is a public domain pronouncing dictionary designed for uses in speech technology, and was created by Carnegie Mellon University (CMU). It defines a mapping from English words to their North American pronunciations, and is commonly used in speech processing applications such as the Festival Speech Synthesis System and the CMU Sphinx speech recognition system.\nConcept mining –\nContent determination –\nDATR –\nDBpedia Spotlight –\nDeep linguistic processing –\nDiscourse relation –\nDocument-term matrix –\nDragomir R. Radev –\nETBLAST –\nFiltered-popping recursive transition network –\nRobby Garner –\nGeneRIF –\nGorn address –\nGrammar induction –\nGrammatik –\nHashing-Trick –\nHidden Markov model –\nHuman language technology –\nInformation extraction –\nInternational Conference on Language Resources and Evaluation –\nKleene star –\nLanguage Computer Corporation –\nLanguage model –\nLanguageWare –\nLatent semantic mapping –\nLegal information retrieval –\nLesk algorithm –\nLessac Technologies –\nLexalytics –\nLexical choice –\nLexical Markup Framework –\nLexical substitution –\nLKB –\nLogic form –\nLRE Map –\nMachine translation software usability –\nMAREC –\nMaximum entropy –\nMessage Understanding Conference –\nMETEOR –\nMinimal recursion semantics –\nMorphological pattern –\nMulti-document summarization –\nMultilingual notation –\nNaive semantics –\nNatural language –\nNatural-language interface –\nNatural-language user interface –\nNews analytics –\nNondeterministic polynomial –\nOpen domain question answering –\nOptimality theory –\nPaco Nathan –\nPhrase structure grammar –\nPowerset (company) –\nProduction (computer science) –\nPropBank –\nQuestion answering –\nRealization (linguistics) –\nRecursive transition network –\nReferring expression generation –\nRewrite rule –\nSemantic compression –\nSemantic neural network –\nSemEval –\nSPL notation –\nStemming – reduces an inflected or derived word into its word stem, base, or root form.\nString kernel –\n\n\n== Natural-language processing tools ==\nGoogle Ngram Viewer – graphs n-gram usage from a corpus of more than 5.2 million books\n\n\n=== Corpora ===\nText corpus (see list) – large and structured set of texts (nowadays usually electronically stored and processed). They are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.\nBank of English\nBritish National Corpus\nCorpus of Contemporary American English (COCA)\nOxford English Corpus\n\n\n=== Natural-language processing toolkits ===\nThe following natural-language processing toolkits are notable collections of natural-language processing software. They are suites of libraries, frameworks, and applications for symbolic, statistical natural-language and speech processing.\n\n\n=== Named-entity recognizers ===\nABNER (A Biomedical Named-Entity Recognizer) – open source text mining program that uses linear-chain conditional random field sequence models. It automatically tags genes, proteins and other entity names in text. Written by Burr Settles of the University of Wisconsin-Madison.\nStanford NER (Named-Entity Recognizer) — Java implementation of a Named-Entity Recognizer that uses linear-chain conditional random field sequence models. It automatically tags persons, organizations, and locations in text in English, German, Chinese, and Spanish languages. Written by Jenny Finkel and other members of the Stanford NLP Group at Stanford University.\n\n\n=== Translation software ===\nComparison of machine translation applications\nMachine translation applications\nGoogle Translate\nDeepL\nLinguee – web service that provides an online dictionary for a number of language pairs. Unlike similar services, such as LEO, Linguee incorporates a search engine that provides access to large amounts of bilingual, translated sentence pairs, which come from the World Wide Web. As a translation aid, Linguee therefore differs from machine translation services like Babelfish and is more similar in function to a translation memory.\nUNL Universal Networking Language\nYahoo! Babel Fish\nReverso\n\n\n=== Other software ===\nCTAKES – open-source natural-language processing system for information extraction from electronic medical record clinical free-text. It processes clinical notes, identifying types of clinical named entities — drugs, diseases/disorders, signs/symptoms, anatomical sites and procedures. Each named entity has attributes for the text span, the ontology mapping code, context (family history of, current, unrelated to patient), and negated/not negated. Also known as Apache cTAKES.\nDMAP –\nETAP-3 – proprietary linguistic processing system focusing on English and Russian. It is a rule-based system which uses the Meaning-Text Theory as its theoretical foundation.\nJAPE – the Java Annotation Patterns Engine, a component of the open-source General Architecture for Text Engineering (GATE) platform. JAPE is a finite state transducer that operates over annotations based on regular expressions.\nLOLITA – \"Large-scale, Object-based, Linguistic Interactor, Translator and Analyzer\". LOLITA was developed by Roberto Garigliano and colleagues between 1986 and 2000. It was designed as a general-purpose tool for processing unrestricted text that could be the basis of a wide variety of applications. At its core was a semantic network containing some 90,000 interlinked concepts.\nMaluuba – intelligent personal assistant for Android devices, that uses a contextual approach to search which takes into account the user's geographic location, contacts, and language.\nMETAL MT – machine translation system developed in the 1980s at the University of Texas and at Siemens which ran on Lisp Machines.\nNever-Ending Language Learning – semantic machine learning system developed by a research team at Carnegie Mellon University, and supported by grants from DARPA, Google, and the NSF, with portions of the system running on a supercomputing cluster provided by Yahoo!. NELL was programmed by its developers to be able to identify a basic set of fundamental semantic relationships between a few hundred predefined categories of data, such as cities, companies, emotions and sports teams. Since the beginning of 2010, the Carnegie Mellon research team has been running NELL around the clock, sifting through hundreds of millions of web pages looking for connections between the information it already knows and what it finds through its search process – to make new connections in a manner that is intended to mimic the way humans learn new information.\nNLTK –\nOnline-translator.com –\nRegulus Grammar Compiler – software system for compiling unification grammars into grammars for speech recognition systems.\nS Voice –\nSiri (software) –\nSpeaktoit –\nTeLQAS –\nWeka's classification tools –\nword2vec – models that were developed by a team of researchers led by Thomas Milkov at Google to generate word embeddings that can reconstruct some of the linguistic context of words using shallow, two dimensional neural nets derived from a much larger vector space.\nFestival Speech Synthesis System –\nCMU Sphinx speech recognition system –\nLanguage Grid – Open source platform for language web services, which can customize language services by combining existing language services.\n\n\n=== Chatterbots ===\n\nChatterbot – a text-based conversation agent that can interact with human users through some medium, such as an instant message service. Some chatterbots are designed for specific purposes, while others converse with human users on a wide range of topics.\n\n\n==== Classic chatterbots ====\nDr. Sbaitso\nELIZA\nPARRY\nRacter (or Claude Chatterbot)\nMark V Shaney\n\n\n==== General chatterbots ====\nAlbert One – 1998 and 1999 Loebner winner, by Robby Garner.\nA.L.I.C.E. – 2001, 2002, and 2004 Loebner Prize winner developed by Richard Wallace.\nCharlix\nCleverbot (winner of the 2010 Mechanical Intelligence Competition)\nElbot – 2008 Loebner Prize winner, by Fred Roberts.\nEugene Goostman – 2012 Turing 100 winner, by Vladimir Veselov.\nFred – an early chatterbot by Robby Garner.\nJabberwacky\nJeeney AI\nMegaHAL\nMitsuku, 2013 and 2016 Loebner Prize winner\nRose - ... 2015 - 3x Loebner Prize winner, by Bruce Wilcox.\nSimSimi – A popular artificial intelligence conversation program that was created in 2002 by ISMaker.\nSpookitalk – A chatterbot used for NPCs in Douglas Adams' Starship Titanic video game.\nUltra Hal – 2007 Loebner Prize winner, by Robert Medeksza.\nVerbot\n\n\n==== Instant messenger chatterbots ====\nGooglyMinotaur, specializing in Radiohead, the first bot released by ActiveBuddy (June 2001-March 2002)\nSmarterChild, developed by ActiveBuddy and released in June 2001\nInfobot, an assistant on IRC channels such as #perl, primarily to help out with answering Frequently Asked Questions (June 1995-today)\nNegobot, a bot designed to catch online pedophiles by posing as a young girl and attempting to elicit personal details from people it speaks to.\n\n\n== Natural-language processing organizations ==\nAFNLP (Asian Federation of Natural Language Processing Associations) – the organization for coordinating the natural-language processing related activities and events in the Asia-Pacific region.\nAustralasian Language Technology Association –\nAssociation for Computational Linguistics – international scientific and professional society for people working on problems involving natural-language processing.\n\n\n=== Natural-language processing-related conferences ===\nAnnual Meeting of the Association for Computational Linguistics (ACL)\nInternational Conference on Intelligent Text Processing and Computational Linguistics (CICLing)\nInternational Conference on Language Resources and Evaluation – biennial conference organised by the European Language Resources Association with the support of institutions and organisations involved in natural-language processing\nAnnual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)\nText, Speech and Dialogue (TSD) – annual conference\nText Retrieval Conference (TREC) – on-going series of workshops focusing on various information retrieval (IR) research areas, or tracks\n\n\n=== Companies involved in natural-language processing ===\nAlchemyAPI – service provider of a natural-language processing API.\nGoogle, Inc. – the Google search engine is an example of automatic summarization, utilizing keyphrase extraction.\nCalais (Reuters product) – provider of a natural-language processing services.\nWolfram Research, Inc. developer of natural-language processing computation engine Wolfram Alpha.\n\n\n== Natural-language processing publications ==\n\n\n=== Books ===\nConnectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing  – Wermter, S., Riloff E. and Scheler, G. (editors). First book that addressed statistical and neural network learning of language.\nSpeech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics – by Daniel Jurafsky and James H. Martin. Introductory book on language technology.\n\n\n==== Book series ====\nStudies in Natural Language Processing – book series of the Association for Computational Linguistics, published by Cambridge University Press.\n\n\n=== Journals ===\nComputational Linguistics – peer-reviewed academic journal in the field of computational linguistics. It is published quarterly by MIT Press for the Association for Computational Linguistics (ACL)\n\n\n== People influential in natural-language processing ==\nDaniel Bobrow –\nRollo Carpenter – creator of Jabberwacky and Cleverbot.\nNoam Chomsky – author of the seminal work Syntactic Structures, which revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\nKenneth Colby –\nDavid Ferrucci – principal investigator of the team that created Watson, IBM's AI computer that won the quiz show Jeopardy!\nLyn Frazier –\nDaniel Jurafsky – Professor of Linguistics and Computer Science at Stanford University. With James H. Martin, he wrote the textbook Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics\nRoger Schank – introduced the conceptual dependency theory for natural-language understanding.\nJean E. Fox Tree –\nAlan Turing – originator of the Turing Test.\nJoseph Weizenbaum – author of the ELIZA chatterbot.\nTerry Winograd – professor of computer science at Stanford University, and co-director of the Stanford Human-Computer Interaction Group. He is known within the philosophy of mind and artificial intelligence fields for his work on natural language using the SHRDLU program.\nWilliam Aaron Woods –\nMaurice Gross – author of the concept of local grammar, taking finite automata as the competence model of language.\nStephen Wolfram – CEO and founder of Wolfram Research, creator of the programming language (natural-language understanding) Wolfram Language, and natural-language processing computation engine Wolfram Alpha.\nVictor Yngve –\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nCrevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3.\nMcCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 978-1-56881-205-2, OCLC 52197627.\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.\n\n\n== External links ==",
      "cleaned_text": "The following outline is provided as an overview of and topical guide to natural-language processing: natural-language processing - computer activity in which computers are entailed to analyze, understand, alter, or generate natural language. This includes the automation of any or all linguistic forms, activities, or methods of communication, such as conversation, correspondence, reading, written composition, dictation, publishing, translation, lip reading, and so on. Natural-language processing is also the name of the branch of computer science, artificial intelligence, and linguistics concerned with enabling computers to engage in communication using natural language(s) in all forms, including but not limited to speech, print, writing, and signing. Natural-language processing can be described as all of the following: A field of science - systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe. An applied science - field that applies human knowledge to build or design useful things. A field of computer science - scientific and practical approach to computation and its applications. A branch of artificial intelligence - intelligence of machines and robots and the branch of computer science that aims to create it. A subfield of computational linguistics - interdisciplinary field dealing with the statistical or rule-based modeling of natural language from a computational perspective. An application of engineering - science, skill, and profession of acquiring and applying scientific, economic, social, and practical knowledge, in order to design and also build structures, machines, devices, systems, materials and processes. An application of software engineering - application of a systematic, disciplined, quantifiable approach to the design, development, operation, and maintenance of software, and the study of these approaches; that is, the application of engineering to software. A subfield of computer programming - process of designing, writing, testing, debugging, and maintaining the source code of computer programs. This source code is written in one or more programming languages (such as Java, C++, C#, Python, etc.). The purpose of programming is to create a set of instructions that computers use to perform specific operations or to exhibit desired behaviors. A subfield of artificial intelligence programming - A type of system - set of interacting or interdependent components forming an integrated whole or a set of elements (often called 'components' ) and relationships which are different from relationships of the set or its elements to other elements or sets. A system that includes software - software is a collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it. Software refers to one or more computer programs and data held in the storage of the computer. In other words, software is a set of programs, procedures, algorithms and its documentation concerned with the operation of a data processing system. A type of technology - making, modification, usage, and knowledge of tools, machines, techniques, crafts, systems, methods of organization, in order to solve a problem, improve a preexisting solution to a problem, achieve a goal, handle an applied input/output relation or perform a specific function. It can also refer to the collection of such tools, machinery, modifications, arrangements and procedures. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments. A form of computer technology - computers and their application. NLP makes use of computers, image scanners, microphones, and many types of software programs. Language technology - consists of natural-language processing (NLP) and computational linguistics (CL) on the one hand, and speech technology on the other. It also includes many application oriented aspects of these. It is often called human language technology (HLT). The following technologies make natural-language processing possible: Communication - the activity of a source sending a message to a receiver Language - Speech - Writing - Computing - Computers - Computer programming - Information extraction - User interface - Software - Text editing - program used to edit plain text files Word processing - piece of software used for composing, editing, formatting, printing documents Input devices - pieces of hardware for sending data to a computer to be processed Computer keyboard - typewriter style input device whose input is converted into various data depending on the circumstances Image scanners - Information extraction (IE) - field concerned in general with the extraction of semantic information from text. This covers tasks such as named-entity recognition, coreference resolution, relationship extraction, etc. Ontology engineering - field that studies the methods and methodologies for building ontologies, which are formal representations of a set of concepts within a domain and the relationships between those concepts. Speech processing - field that covers speech recognition, text-to-speech and related tasks. Statistical natural-language processing - Statistical semantics - a subfield of computational semantics that establishes semantic relations between words to examine their contexts. Distributional semantics - a subfield of statistical semantics that examines the semantic relationship of words across a corpora or in large samples of data. Natural-language processing contributes to, and makes use of (the theories, tools, and methodologies from), the following fields: Automated reasoning - area of computer science and mathematical logic dedicated to understanding various aspects of reasoning, and producing software which allows computers to reason completely, or nearly completely, automatically. A sub-field of artificial intelligence, automatic reasoning is also grounded in theoretical computer science and philosophy of mind. Linguistics - scientific study of human language. Natural-language processing requires understanding of the structure and application of language, and therefore it draws heavily from linguistics. Applied linguistics - interdisciplinary field of study that identifies, investigates, and offers solutions to language-related real-life problems. Some of the academic fields related to applied linguistics are education, linguistics, psychology, computer science, anthropology, and sociology. Some of the subfields of applied linguistics relevant to natural-language processing are: Bilingualism / Multilingualism - Computer-mediated communication (CMC) - any communicative transaction that occurs through the use of two or more networked computers. Research on CMC focuses largely on the social effects of different computer-supported communication technologies. Many recent studies involve Internet-based social networking supported by social software. Contrastive linguistics - practice-oriented linguistic approach that seeks to describe the differences and similarities between a pair of languages. Conversation analysis (CA) - approach to the study of social interaction, embracing both verbal and non-verbal conduct, in situations of everyday life. Turn-taking is one aspect of language use that is studied by CA. Discourse analysis - various approaches to analyzing written, vocal, or sign language use or any significant semiotic event. Forensic linguistics - application of linguistic knowledge, methods and insights to the forensic context of law, language, crime investigation, trial, and judicial procedure. Interlinguistics - study of improving communications between people of different first languages with the use of ethnic and auxiliary languages (lingua franca). For instance by use of intentional international auxiliary languages, such as Esperanto or Interlingua, or spontaneous interlanguages known as pidgin languages. Language assessment - assessment of first, second or other language in the school, college, or university context; assessment of language use in the workplace; and assessment of language in the immigration, citizenship, and asylum contexts. The assessment may include analyses of listening, speaking, reading, writing or cultural understanding, with respect to understanding how the language works theoretically and the ability to use the language practically. Language pedagogy - science and art of language education, including approaches and methods of language teaching and study. Natural-language processing is used in programs designed to teach language, including first- and second-language training. Language planning - Language policy - Lexicography - Literacies - Pragmatics - Second-language acquisition - Stylistics - Translation - Computational linguistics - interdisciplinary field dealing with the statistical or rule-based modeling of natural language from a computational perspective. The models and tools of computational linguistics are used extensively in the field of natural-language processing, and vice versa. Computational semantics - Corpus linguistics - study of language as expressed in samples (corpora) of \"real world\" text. Corpora is the plural of corpus, and a corpus is a specifically selected collection of texts (or speech segments) composed of natural language. After it is constructed (gathered or composed), a corpus is analyzed with the methods of computational linguistics to infer the meaning and context of its components (words, phrases, and sentences), and the relationships between them. Optionally, a corpus can be annotated (\"tagged\") with data (manually or automatically) to make the corpus easier to understand (e.g., part-of-speech tagging). This data is then applied to make sense of user input, for example, to make better (automated) guesses of what people are talking about or saying, perhaps to achieve more narrowly focused web searches, or for speech recognition. Metalinguistics - Sign linguistics - scientific study and analysis of natural sign languages, their features, their structure (phonology, morphology, syntax, and semantics), their acquisition (as a primary or secondary language), how they develop independently of other languages, their application in communication, their relationships to other languages (including spoken languages), and many other aspects. Human-computer interaction - the intersection of computer science and behavioral sciences, this field involves the study, planning, and design of the interaction between people (users) and computers. Attention to human-machine interaction is important, because poorly designed human-machine interfaces can lead to many unexpected problems. A classic example of this is the Three Mile Island accident where investigations concluded that the design of the human-machine interface was at least partially responsible for the disaster. Information retrieval (IR) - field concerned with storing, searching and retrieving information. It is a separate field within computer science (closer to databases), but IR relies on some NLP methods (for example, stemming). Some current research and applications seek to bridge the gap between IR and NLP. Knowledge representation (KR) - area of artificial intelligence research aimed at representing knowledge in symbols to facilitate inferencing from those knowledge elements, creating new elements of knowledge. Knowledge Representation research involves analysis of how to reason accurately and effectively and how best to use a set of symbols to represent a set of facts within a knowledge domain. Semantic network - study of semantic relations between concepts. Semantic Web - Machine learning - subfield of computer science that examines pattern recognition and computational learning theory in artificial intelligence. There are three broad approaches to machine learning. Supervised learning occurs when the machine is given example inputs and outputs by a teacher so that it can learn a rule that maps inputs to outputs. Unsupervised learning occurs when the machine determines the inputs structure without being provided example inputs or outputs. Reinforcement learning occurs when a machine must perform a goal without teacher feedback. Pattern recognition - branch of machine learning that examines how machines recognize regularities in data. As with machine learning, teachers can train machines to recognize patterns by providing them with example inputs and outputs (i.e. Supervised Learning), or the machines can recognize patterns without being trained on any example inputs or outputs (i.e. Unsupervised Learning). Statistical classification - Anaphora - type of expression whose reference depends upon another referential element. E.g., in the sentence 'Sally preferred the company of herself', 'herself' is an anaphoric expression in that it is coreferential with 'Sally', the sentence's subject. Context-free language - Controlled natural language - a natural language with a restriction introduced on its grammar and vocabulary in order to eliminate ambiguity and complexity Corpus - body of data, optionally tagged (for example, through part-of-speech tagging), providing real world samples for analysis and comparison. Text corpus - large and structured set of texts, nowadays usually electronically stored and processed. They are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific subject (or domain). Speech corpus - database of speech audio files and text transcriptions. In Speech technology, speech corpora are used, among other things, to create acoustic models (which can then be used with a speech recognition engine). In Linguistics, spoken corpora are used to do research into phonetic, conversation analysis, dialectology and other fields. Grammar - Context-free grammar (CFG) - Constraint grammar (CG) - Definite clause grammar (DCG) - Functional unification grammar (FUG) - Generalized phrase structure grammar (GPSG) - Head-driven phrase structure grammar (HPSG) - Lexical functional grammar (LFG) - Probabilistic context-free grammar (PCFG) - another name for stochastic context-free grammar. Stochastic context-free grammar (SCFG) - Systemic functional grammar (SFG) - Tree-adjoining grammar (TAG) - Natural language - n-gram - sequence of n number of tokens, where a \"token\" is a character, syllable, or word. The n is replaced by a number. Therefore, a 5-gram is an n-gram of 5 letters, syllables, or words. \"Eat this\" is a 2-gram (also known as a bigram). Bigram - n-gram of 2 tokens. Every sequence of 2 adjacent elements in a string of tokens is a bigram. Bigrams are used for speech recognition, they can be used to solve cryptograms, and bigram frequency is one approach to statistical language identification. Trigram - special case of the n-gram, where n is 3. Ontology - formal representation of a set of concepts within a domain and the relationships between those concepts. Taxonomy - practice and science of classification, including the principles underlying classification, and the methods of classifying things or concepts. Hyponymy and hypernymy - the linguistics of hyponyms and hypernyms. A hyponym shares a type-of relationship with its hypernym. For example, pigeon, crow, eagle and seagull are all hyponyms of bird (their hypernym); which, in turn, is a hyponym of animal. Taxonomy for search engines - typically called a \"taxonomy of entities\". It is a tree in which nodes are labelled with entities which are expected to occur in a web search query. These trees are used to match keywords from a search query with the keywords from relevant answers (or snippets). Textual entailment - directional relation between text fragments. The relation holds whenever the truth of one text fragment follows from another text. In the TE framework, the entailing and entailed texts are termed text (t) and hypothesis (h), respectively. The relation is directional because even if \"t entails h\", the reverse \"h entails t\" is much less certain. Triphone - sequence of three phonemes. Triphones are useful in models of natural-language processing where they are used to establish the various contexts in which a phoneme can occur in a particular natural language. Automated essay scoring (AES) - the use of specialized computer programs to assign grades to essays written in an educational setting. It is a method of educational assessment and an application of natural-language processing. Its objective is to classify a large set of textual entities into a small number of discrete categories, corresponding to the possible grades-for example, the numbers 1 to 6. Therefore, it can be considered a problem of statistical classification. Automatic image annotation - process by which a computer system automatically assigns textual metadata in the form of captioning or keywords to a digital image. The annotations are used in image retrieval systems to organize and locate images of interest from a database. Automatic summarization - process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document. Often used to provide summaries of text of a known type, such as articles in the financial section of a newspaper. Types Keyphrase extraction - Document summarization - Multi-document summarization - Methods and techniques Extraction-based summarization - Abstraction-based summarization - Maximum entropy-based summarization - Sentence extraction - Aided summarization - Human aided machine summarization (HAMS) - Machine aided human summarization (MAHS) - Automatic taxonomy induction - automated construction of tree structures from a corpus. This may be applied to building taxonomical classification systems for reading by end users, such as web directories or subject outlines. Coreference resolution - in order to derive the correct interpretation of text, or even to estimate the relative importance of various mentioned subjects, pronouns and other referring expressions need to be connected to the right individuals or objects. Given a sentence or larger chunk of text, coreference resolution determines which words (\"mentions\") refer to which objects (\"entities\") included in the text. Anaphora resolution - concerned with matching up pronouns with the nouns or names that they refer to. For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to). Dialog system - Foreign-language reading aid - computer program that assists a non-native language user to read properly in their target language. The proper reading means that the pronunciation should be correct and stress to different parts of the words should be proper. Foreign-language writing aid - computer program or any other instrument that assists a non-native language user (also referred to as a foreign-language learner) in writing decently in their target language. Assistive operations can be classified into two categories: on-the-fly prompts and post-writing checks. Grammar checking - the act of verifying the grammatical correctness of written text, especially if this act is performed by a computer program. Information retrieval - Cross-language information retrieval - Machine translation (MT) - aims to automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) in order to solve properly. Classical approach of machine translation - rules-based machine translation. Computer-assisted translation - Interactive machine translation - Translation memory - database that stores so-called \"segments\", which can be sentences, paragraphs or sentence-like units (headings, titles or elements in a list) that have previously been translated, in order to aid human translators. Example-based machine translation - Rule-based machine translation - Natural-language programming - interpreting and compiling instructions communicated in natural language into computer instructions (machine code). Natural-language search - Optical character recognition (OCR) - given an image representing printed text, determine the corresponding text. Question answering - given a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada?\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). Open domain question answering - Spam filtering - Sentiment analysis - extracts subjective information usually from a set of documents, often using online reviews to determine \"polarity\" about specific objects. It is especially useful for identifying trends of public opinion in the social media, for the purpose of marketing. Speech recognition - given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Speech synthesis (Text-to-speech) - Text-proofing - Text simplification - automated editing a document to include fewer words, or use easier words, while retaining its underlying meaning and information. Natural-language understanding - converts chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural-language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural-language expression which usually takes the form of organized notations of natural-languages concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural-languages semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization. Natural-language generation - task of converting information from computer databases into readable human language. Automatic document classification (text categorization) - Automatic language identification - Compound term processing - category of techniques that identify compound terms and match them to their definitions. Compound terms are built by combining two (or more) simple terms, for example \"triple\" is a single word term but \"triple heart bypass\" is a compound term. Automatic taxonomy induction - Corpus processing - Automatic acquisition of lexicon - Text normalization - Text simplification - Deep linguistic processing - Discourse analysis - includes a number of related tasks. One task is identifying the discourse structure of connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes-no questions, content questions, statements, assertions, orders, suggestions, etc.). Information extraction - Text mining - process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Biomedical text mining - (also known as BioNLP), this is text mining applied to texts and literature of the biomedical and molecular biology domain. It is a rather recent research field drawing elements from natural-language processing, bioinformatics, medical informatics and computational linguistics. There is an increasing interest in text mining and information extraction strategies applied to the biomedical and molecular biology literature due to the increasing number of electronically available publications stored in databases such as PubMed. Decision tree learning - Sentence extraction - Terminology extraction - Latent semantic indexing - Lemmatisation - groups together all like terms that share a same lemma such that they are classified as a single item. Morphological segmentation - separates words into individual morphemes and identifies the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e. the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g. \"open, opens, opened, opening\") as separate words. In languages such as Turkish, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms. Named-entity recognition (NER) - given a stream of text, determines which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case is often inaccurate or insufficient. For example, the first word of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they refer to names, and French and Spanish do not capitalize names that serve as adjectives. Ontology learning - automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between those concepts from a corpus of natural-language text, and encoding them with an ontology language for easy retrieval. Also called \"ontology extraction\", \"ontology generation\", and \"ontology acquisition\". Parsing - determines the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses. In fact, perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). Shallow parsing - Part-of-speech tagging - given a sentence, determines the part of speech for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five different parts of speech. Some languages have more such ambiguity than others. Languages with little inflectional morphology, such as English are particularly prone to such ambiguity. Chinese is prone to such ambiguity because it is a tonal language during verbalization. Such inflection is not readily conveyed via the entities employed within the orthography to convey intended meaning. Query expansion - Relationship extraction - given a chunk of text, identifies the relationships among named entities (e.g. who is the wife of whom). Semantic analysis (computational) - formal analysis of meaning, and \"computational\" refers to approaches that in principle support effective implementation. Explicit semantic analysis - Latent semantic analysis - Semantic analytics - Sentence breaking (also known as sentence boundary disambiguation and sentence detection) - given a chunk of text, finds the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g. marking abbreviations). Speech segmentation - given a sound clip of a person or people speaking, separates it into words. A subtask of speech recognition and typically grouped with it. Stemming - reduces an inflected or derived word into its word stem, base, or root form. Text chunking - Tokenization - given a chunk of text, separates it into distinct words, symbols, sentences, or other units Topic segmentation and recognition - given a chunk of text, separates it into segments each of which is devoted to a topic, and identifies the topic of the segment. Truecasing - Word segmentation - separates a chunk of continuous text into separate words. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Word-sense disambiguation (WSD) - because many words have more than one meaning, word-sense disambiguation is used to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or from an online resource such as WordNet. Word-sense induction - open problem of natural-language processing, which concerns the automatic identification of the senses of a word (i.e. meanings). Given that the output of word-sense induction is a set of senses for the target word (sense inventory), this task is strictly related to that of word-sense disambiguation (WSD), which relies on a predefined sense inventory and aims to solve the ambiguity of words in context. Automatic acquisition of sense-tagged corpora - W-shingling - set of unique \"shingles\"-contiguous subsequences of tokens in a document-that can be used to gauge the similarity of two documents. The w denotes the number of tokens in each shingle in the set. Natural-language generation - task of converting information from computer databases into readable human language. Automatic taxonomy induction (ATI) - automated building of tree structures from a corpus. While ATI is used to construct the core of ontologies (and doing so makes it a component process of natural-language understanding), when the ontologies being constructed are end user readable (such as a subject outline), and these are used for the construction of further documentation (such as using an outline as the basis to construct a report or treatise) this also becomes a component process of natural-language generation. Document structuring - History of natural-language processing History of machine translation History of automated essay scoring History of natural-language user interface History of natural-language understanding History of optical character recognition History of question answering History of speech synthesis Turing test - test of a machine's ability to exhibit intelligent behavior, equivalent to or indistinguishable from, that of an actual human. In the original illustrative example, a human judge engages in a natural-language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being. All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test. The test was introduced by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence,\" which opens with the words: \"I propose to consider the question, 'Can machines think?'\" Universal grammar - theory in linguistics, usually credited to Noam Chomsky, proposing that the ability to learn grammar is hard-wired into the brain. The theory suggests that linguistic ability manifests itself without being taught (see poverty of the stimulus), and that there are properties that all natural human languages share. It is a matter of observation and experimentation to determine precisely what abilities are innate and what properties are shared by all languages. ALPAC - was a committee of seven scientists led by John R. Pierce, established in 1964 by the U. S. Government in order to evaluate the progress in computational linguistics in general and machine translation in particular. Its report, issued in 1966, gained notoriety for being very skeptical of research done in machine translation so far, and emphasizing the need for basic research in computational linguistics; this eventually caused the U. S. Government to reduce its funding of the topic dramatically. Conceptual dependency theory - a model of natural-language understanding used in artificial intelligence systems. Roger Schank at Stanford University introduced the model in 1969, in the early days of artificial intelligence. This model was extensively used by Schank's students at Yale University such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner. Augmented transition network - type of graph theoretic structure used in the operational definition of formal languages, used especially in parsing relatively complex natural languages, and having wide application in artificial intelligence. Introduced by William A. Woods in 1970. Distributed Language Translation (project) - Sukhotin's algorithm - statistical classification algorithm for classifying characters in a text as vowels or consonants. It was initially created by Boris V. Sukhotin. T9 (predictive text) - stands for \"Text on 9 keys\", is a USA-patented predictive text technology for mobile phones (specifically those that contain a 3x4 numeric keypad), originally developed by Tegic Communications, now part of Nuance Communications. Tatoeba - free collaborative online database of example sentences geared towards foreign-language learners. Teragram Corporation - fully owned subsidiary of SAS Institute, a major producer of statistical analysis software, headquartered in Cary, North Carolina, USA. Teragram is based in Cambridge, Massachusetts and specializes in the application of computational linguistics to multilingual natural-language processing. TipTop Technologies - company that developed TipTop Search, a real-time web, social search engine with a unique platform for semantic analysis of natural language. TipTop Search provides results capturing individual and group sentiment, opinions, and experiences from content of various sorts including real-time messages from Twitter or consumer product reviews on Amazon.com. Transderivational search - when a search is being conducted for a fuzzy match across a broad field. In computing the equivalent function can be performed using content-addressable memory. Vocabulary mismatch - common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently. LRE Map - Reification (linguistics) - Semantic Web - Metadata - Spoken dialogue system - Affix grammar over a finite lattice - Aggregation (linguistics) - Bag-of-words model - model that represents a text as a bag (multiset) of its words that disregards grammar and word sequence, but maintains multiplicity. This model is a commonly used to train document classifiers Brill tagger - Cache language model - ChaSen, MeCab - provide morphological analysis and word splitting for Japanese Classic monolingual WSD - ClearForest - CMU Pronouncing Dictionary - also known as cmudict, is a public domain pronouncing dictionary designed for uses in speech technology, and was created by Carnegie Mellon University (CMU). It defines a mapping from English words to their North American pronunciations, and is commonly used in speech processing applications such as the Festival Speech Synthesis System and the CMU Sphinx speech recognition system. Concept mining - Content determination - DATR - DBpedia Spotlight - Deep linguistic processing - Discourse relation - Document-term matrix - Dragomir R. Radev - ETBLAST - Filtered-popping recursive transition network - Robby Garner - GeneRIF - Gorn address - Grammar induction - Grammatik - Hashing-Trick - Hidden Markov model - Human language technology - Information extraction - International Conference on Language Resources and Evaluation - Kleene star - Language Computer Corporation - Language model - LanguageWare - Latent semantic mapping - Legal information retrieval - Lesk algorithm - Lessac Technologies - Lexalytics - Lexical choice - Lexical Markup Framework - Lexical substitution - LKB - Logic form - LRE Map - Machine translation software usability - MAREC - Maximum entropy - Message Understanding Conference - METEOR - Minimal recursion semantics - Morphological pattern - Multi-document summarization - Multilingual notation - Naive semantics - Natural language - Natural-language interface - Natural-language user interface - News analytics - Nondeterministic polynomial - Open domain question answering - Optimality theory - Paco Nathan - Phrase structure grammar - Powerset (company) - Production (computer science) - PropBank - Question answering - Realization (linguistics) - Recursive transition network - Referring expression generation - Rewrite rule - Semantic compression - Semantic neural network - SemEval - SPL notation - Stemming - reduces an inflected or derived word into its word stem, base, or root form. String kernel - Google Ngram Viewer - graphs n-gram usage from a corpus of more than 5.2 million books Text corpus (see list) - large and structured set of texts (nowadays usually electronically stored and processed). They are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory. Bank of English British National Corpus Corpus of Contemporary American English (COCA) Oxford English Corpus The following natural-language processing toolkits are notable collections of natural-language processing software. They are suites of libraries, frameworks, and applications for symbolic, statistical natural-language and speech processing. ABNER (A Biomedical Named-Entity Recognizer) - open source text mining program that uses linear-chain conditional random field sequence models. It automatically tags genes, proteins and other entity names in text. Written by Burr Settles of the University of Wisconsin-Madison. Stanford NER (Named-Entity Recognizer) - Java implementation of a Named-Entity Recognizer that uses linear-chain conditional random field sequence models. It automatically tags persons, organizations, and locations in text in English, German, Chinese, and Spanish languages. Written by Jenny Finkel and other members of the Stanford NLP Group at Stanford University. Comparison of machine translation applications Machine translation applications Google Translate DeepL Linguee - web service that provides an online dictionary for a number of language pairs. Unlike similar services, such as LEO, Linguee incorporates a search engine that provides access to large amounts of bilingual, translated sentence pairs, which come from the World Wide Web. As a translation aid, Linguee therefore differs from machine translation services like Babelfish and is more similar in function to a translation memory. UNL Universal Networking Language Yahoo! Babel Fish Reverso CTAKES - open-source natural-language processing system for information extraction from electronic medical record clinical free-text. It processes clinical notes, identifying types of clinical named entities - drugs, diseases/disorders, signs/symptoms, anatomical sites and procedures. Each named entity has attributes for the text span, the ontology mapping code, context (family history of, current, unrelated to patient), and negated/not negated. Also known as Apache cTAKES. DMAP - ETAP-3 - proprietary linguistic processing system focusing on English and Russian. It is a rule-based system which uses the Meaning-Text Theory as its theoretical foundation. JAPE - the Java Annotation Patterns Engine, a component of the open-source General Architecture for Text Engineering (GATE) platform. JAPE is a finite state transducer that operates over annotations based on regular expressions. LOLITA - \"Large-scale, Object-based, Linguistic Interactor, Translator and Analyzer\". LOLITA was developed by Roberto Garigliano and colleagues between 1986 and 2000. It was designed as a general-purpose tool for processing unrestricted text that could be the basis of a wide variety of applications. At its core was a semantic network containing some 90,000 interlinked concepts. Maluuba - intelligent personal assistant for Android devices, that uses a contextual approach to search which takes into account the user's geographic location, contacts, and language. METAL MT - machine translation system developed in the 1980s at the University of Texas and at Siemens which ran on Lisp Machines. Never-Ending Language Learning - semantic machine learning system developed by a research team at Carnegie Mellon University, and supported by grants from DARPA, Google, and the NSF, with portions of the system running on a supercomputing cluster provided by Yahoo!. NELL was programmed by its developers to be able to identify a basic set of fundamental semantic relationships between a few hundred predefined categories of data, such as cities, companies, emotions and sports teams. Since the beginning of 2010, the Carnegie Mellon research team has been running NELL around the clock, sifting through hundreds of millions of web pages looking for connections between the information it already knows and what it finds through its search process - to make new connections in a manner that is intended to mimic the way humans learn new information. NLTK - Online-translator.com - Regulus Grammar Compiler - software system for compiling unification grammars into grammars for speech recognition systems. S Voice - Siri (software) - Speaktoit - TeLQAS - Weka's classification tools - word2vec - models that were developed by a team of researchers led by Thomas Milkov at Google to generate word embeddings that can reconstruct some of the linguistic context of words using shallow, two dimensional neural nets derived from a much larger vector space. Festival Speech Synthesis System - CMU Sphinx speech recognition system - Language Grid - Open source platform for language web services, which can customize language services by combining existing language services. Chatterbot - a text-based conversation agent that can interact with human users through some medium, such as an instant message service. Some chatterbots are designed for specific purposes, while others converse with human users on a wide range of topics. Dr. Sbaitso ELIZA PARRY Racter (or Claude Chatterbot) Mark V Shaney Albert One - 1998 and 1999 Loebner winner, by Robby Garner. A.L.I.C.E. - 2001, 2002, and 2004 Loebner Prize winner developed by Richard Wallace. Charlix Cleverbot (winner of the 2010 Mechanical Intelligence Competition) Elbot - 2008 Loebner Prize winner, by Fred Roberts. Eugene Goostman - 2012 Turing 100 winner, by Vladimir Veselov. Fred - an early chatterbot by Robby Garner. Jabberwacky Jeeney AI MegaHAL Mitsuku, 2013 and 2016 Loebner Prize winner Rose - ... 2015 - 3x Loebner Prize winner, by Bruce Wilcox. SimSimi - A popular artificial intelligence conversation program that was created in 2002 by ISMaker. Spookitalk - A chatterbot used for NPCs in Douglas Adams' Starship Titanic video game. Ultra Hal - 2007 Loebner Prize winner, by Robert Medeksza. Verbot GooglyMinotaur, specializing in Radiohead, the first bot released by ActiveBuddy (June 2001-March 2002) SmarterChild, developed by ActiveBuddy and released in June 2001 Infobot, an assistant on IRC channels such as #perl, primarily to help out with answering Frequently Asked Questions (June 1995-today) Negobot, a bot designed to catch online pedophiles by posing as a young girl and attempting to elicit personal details from people it speaks to. AFNLP (Asian Federation of Natural Language Processing Associations) - the organization for coordinating the natural-language processing related activities and events in the Asia-Pacific region. Australasian Language Technology Association - Association for Computational Linguistics - international scientific and professional society for people working on problems involving natural-language processing. Annual Meeting of the Association for Computational Linguistics (ACL) International Conference on Intelligent Text Processing and Computational Linguistics (CICLing) International Conference on Language Resources and Evaluation - biennial conference organised by the European Language Resources Association with the support of institutions and organisations involved in natural-language processing Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) Text, Speech and Dialogue (TSD) - annual conference Text Retrieval Conference (TREC) - on-going series of workshops focusing on various information retrieval (IR) research areas, or tracks AlchemyAPI - service provider of a natural-language processing API. Google, Inc. - the Google search engine is an example of automatic summarization, utilizing keyphrase extraction. Calais (Reuters product) - provider of a natural-language processing services. Wolfram Research, Inc. developer of natural-language processing computation engine Wolfram Alpha. Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing - Wermter, S., Riloff E. and Scheler, G. (editors). First book that addressed statistical and neural network learning of language. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics - by Daniel Jurafsky and James H. Martin. Introductory book on language technology. Studies in Natural Language Processing - book series of the Association for Computational Linguistics, published by Cambridge University Press. Computational Linguistics - peer-reviewed academic journal in the field of computational linguistics. It is published quarterly by MIT Press for the Association for Computational Linguistics (ACL) Daniel Bobrow - Rollo Carpenter - creator of Jabberwacky and Cleverbot. Noam Chomsky - author of the seminal work Syntactic Structures, which revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures. Kenneth Colby - David Ferrucci - principal investigator of the team that created Watson, IBM's AI computer that won the quiz show Jeopardy! Lyn Frazier - Daniel Jurafsky - Professor of Linguistics and Computer Science at Stanford University. With James H. Martin, he wrote the textbook Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics Roger Schank - introduced the conceptual dependency theory for natural-language understanding. Jean E. Fox Tree - Alan Turing - originator of the Turing Test. Joseph Weizenbaum - author of the ELIZA chatterbot. Terry Winograd - professor of computer science at Stanford University, and co-director of the Stanford Human-Computer Interaction Group. He is known within the philosophy of mind and artificial intelligence fields for his work on natural language using the SHRDLU program. William Aaron Woods - Maurice Gross - author of the concept of local grammar, taking finite automata as the competence model of language. Stephen Wolfram - CEO and founder of Wolfram Research, creator of the programming language (natural-language understanding) Wolfram Language, and natural-language processing computation engine Wolfram Alpha. Victor Yngve -",
      "sentences": [
        "The following outline is provided as an overview of and topical guide to natural-language processing: natural-language processing - computer activity in which computers are entailed to analyze, understand, alter, or generate natural language.",
        "This includes the automation of any or all linguistic forms, activities, or methods of communication, such as conversation, correspondence, reading, written composition, dictation, publishing, translation, lip reading, and so on.",
        "Natural-language processing is also the name of the branch of computer science, artificial intelligence, and linguistics concerned with enabling computers to engage in communication using natural language(s) in all forms, including but not limited to speech, print, writing, and signing.",
        "Natural-language processing can be described as all of the following: A field of science - systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.",
        "An applied science - field that applies human knowledge to build or design useful things.",
        "A field of computer science - scientific and practical approach to computation and its applications.",
        "A branch of artificial intelligence - intelligence of machines and robots and the branch of computer science that aims to create it.",
        "A subfield of computational linguistics - interdisciplinary field dealing with the statistical or rule-based modeling of natural language from a computational perspective.",
        "An application of engineering - science, skill, and profession of acquiring and applying scientific, economic, social, and practical knowledge, in order to design and also build structures, machines, devices, systems, materials and processes.",
        "An application of software engineering - application of a systematic, disciplined, quantifiable approach to the design, development, operation, and maintenance of software, and the study of these approaches; that is, the application of engineering to software.",
        "A subfield of computer programming - process of designing, writing, testing, debugging, and maintaining the source code of computer programs.",
        "This source code is written in one or more programming languages (such as Java, C++, C#, Python, etc.).",
        "The purpose of programming is to create a set of instructions that computers use to perform specific operations or to exhibit desired behaviors.",
        "A subfield of artificial intelligence programming - A type of system - set of interacting or interdependent components forming an integrated whole or a set of elements (often called 'components' ) and relationships which are different from relationships of the set or its elements to other elements or sets.",
        "A system that includes software - software is a collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it.",
        "Software refers to one or more computer programs and data held in the storage of the computer.",
        "In other words, software is a set of programs, procedures, algorithms and its documentation concerned with the operation of a data processing system.",
        "A type of technology - making, modification, usage, and knowledge of tools, machines, techniques, crafts, systems, methods of organization, in order to solve a problem, improve a preexisting solution to a problem, achieve a goal, handle an applied input/output relation or perform a specific function.",
        "It can also refer to the collection of such tools, machinery, modifications, arrangements and procedures.",
        "Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.",
        "A form of computer technology - computers and their application.",
        "NLP makes use of computers, image scanners, microphones, and many types of software programs.",
        "Language technology - consists of natural-language processing (NLP) and computational linguistics (CL) on the one hand, and speech technology on the other.",
        "It also includes many application oriented aspects of these.",
        "It is often called human language technology (HLT).",
        "This covers tasks such as named-entity recognition, coreference resolution, relationship extraction, etc.",
        "Ontology engineering - field that studies the methods and methodologies for building ontologies, which are formal representations of a set of concepts within a domain and the relationships between those concepts.",
        "Speech processing - field that covers speech recognition, text-to-speech and related tasks.",
        "Statistical natural-language processing - Statistical semantics - a subfield of computational semantics that establishes semantic relations between words to examine their contexts.",
        "Distributional semantics - a subfield of statistical semantics that examines the semantic relationship of words across a corpora or in large samples of data.",
        "Natural-language processing contributes to, and makes use of (the theories, tools, and methodologies from), the following fields: Automated reasoning - area of computer science and mathematical logic dedicated to understanding various aspects of reasoning, and producing software which allows computers to reason completely, or nearly completely, automatically.",
        "A sub-field of artificial intelligence, automatic reasoning is also grounded in theoretical computer science and philosophy of mind.",
        "Linguistics - scientific study of human language.",
        "Natural-language processing requires understanding of the structure and application of language, and therefore it draws heavily from linguistics.",
        "Applied linguistics - interdisciplinary field of study that identifies, investigates, and offers solutions to language-related real-life problems.",
        "Some of the academic fields related to applied linguistics are education, linguistics, psychology, computer science, anthropology, and sociology.",
        "Some of the subfields of applied linguistics relevant to natural-language processing are: Bilingualism / Multilingualism - Computer-mediated communication (CMC) - any communicative transaction that occurs through the use of two or more networked computers.",
        "Research on CMC focuses largely on the social effects of different computer-supported communication technologies.",
        "Many recent studies involve Internet-based social networking supported by social software.",
        "Contrastive linguistics - practice-oriented linguistic approach that seeks to describe the differences and similarities between a pair of languages.",
        "Conversation analysis (CA) - approach to the study of social interaction, embracing both verbal and non-verbal conduct, in situations of everyday life.",
        "Turn-taking is one aspect of language use that is studied by CA.",
        "Discourse analysis - various approaches to analyzing written, vocal, or sign language use or any significant semiotic event.",
        "Forensic linguistics - application of linguistic knowledge, methods and insights to the forensic context of law, language, crime investigation, trial, and judicial procedure.",
        "Interlinguistics - study of improving communications between people of different first languages with the use of ethnic and auxiliary languages (lingua franca).",
        "For instance by use of intentional international auxiliary languages, such as Esperanto or Interlingua, or spontaneous interlanguages known as pidgin languages.",
        "Language assessment - assessment of first, second or other language in the school, college, or university context; assessment of language use in the workplace; and assessment of language in the immigration, citizenship, and asylum contexts.",
        "The assessment may include analyses of listening, speaking, reading, writing or cultural understanding, with respect to understanding how the language works theoretically and the ability to use the language practically.",
        "Language pedagogy - science and art of language education, including approaches and methods of language teaching and study.",
        "Natural-language processing is used in programs designed to teach language, including first- and second-language training.",
        "Language planning - Language policy - Lexicography - Literacies - Pragmatics - Second-language acquisition - Stylistics - Translation - Computational linguistics - interdisciplinary field dealing with the statistical or rule-based modeling of natural language from a computational perspective.",
        "The models and tools of computational linguistics are used extensively in the field of natural-language processing, and vice versa.",
        "Computational semantics - Corpus linguistics - study of language as expressed in samples (corpora) of \"real world\" text.",
        "Corpora is the plural of corpus, and a corpus is a specifically selected collection of texts (or speech segments) composed of natural language.",
        "After it is constructed (gathered or composed), a corpus is analyzed with the methods of computational linguistics to infer the meaning and context of its components (words, phrases, and sentences), and the relationships between them.",
        "This data is then applied to make sense of user input, for example, to make better (automated) guesses of what people are talking about or saying, perhaps to achieve more narrowly focused web searches, or for speech recognition.",
        "Human-computer interaction - the intersection of computer science and behavioral sciences, this field involves the study, planning, and design of the interaction between people (users) and computers.",
        "Attention to human-machine interaction is important, because poorly designed human-machine interfaces can lead to many unexpected problems.",
        "A classic example of this is the Three Mile Island accident where investigations concluded that the design of the human-machine interface was at least partially responsible for the disaster.",
        "Information retrieval (IR) - field concerned with storing, searching and retrieving information.",
        "It is a separate field within computer science (closer to databases), but IR relies on some NLP methods (for example, stemming).",
        "Some current research and applications seek to bridge the gap between IR and NLP.",
        "Knowledge representation (KR) - area of artificial intelligence research aimed at representing knowledge in symbols to facilitate inferencing from those knowledge elements, creating new elements of knowledge.",
        "Knowledge Representation research involves analysis of how to reason accurately and effectively and how best to use a set of symbols to represent a set of facts within a knowledge domain.",
        "Semantic network - study of semantic relations between concepts.",
        "Semantic Web - Machine learning - subfield of computer science that examines pattern recognition and computational learning theory in artificial intelligence.",
        "There are three broad approaches to machine learning.",
        "Supervised learning occurs when the machine is given example inputs and outputs by a teacher so that it can learn a rule that maps inputs to outputs.",
        "Unsupervised learning occurs when the machine determines the inputs structure without being provided example inputs or outputs.",
        "Reinforcement learning occurs when a machine must perform a goal without teacher feedback.",
        "Pattern recognition - branch of machine learning that examines how machines recognize regularities in data.",
        "As with machine learning, teachers can train machines to recognize patterns by providing them with example inputs and outputs (i.e.",
        "Supervised Learning), or the machines can recognize patterns without being trained on any example inputs or outputs (i.e.",
        "Unsupervised Learning).",
        "Statistical classification - Anaphora - type of expression whose reference depends upon another referential element.",
        "E.g., in the sentence 'Sally preferred the company of herself', 'herself' is an anaphoric expression in that it is coreferential with 'Sally', the sentence's subject.",
        "Context-free language - Controlled natural language - a natural language with a restriction introduced on its grammar and vocabulary in order to eliminate ambiguity and complexity Corpus - body of data, optionally tagged (for example, through part-of-speech tagging), providing real world samples for analysis and comparison.",
        "Text corpus - large and structured set of texts, nowadays usually electronically stored and processed.",
        "They are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific subject (or domain).",
        "Speech corpus - database of speech audio files and text transcriptions.",
        "In Speech technology, speech corpora are used, among other things, to create acoustic models (which can then be used with a speech recognition engine).",
        "In Linguistics, spoken corpora are used to do research into phonetic, conversation analysis, dialectology and other fields.",
        "The n is replaced by a number.",
        "Therefore, a 5-gram is an n-gram of 5 letters, syllables, or words.",
        "\"Eat this\" is a 2-gram (also known as a bigram).",
        "Bigram - n-gram of 2 tokens.",
        "Every sequence of 2 adjacent elements in a string of tokens is a bigram.",
        "Bigrams are used for speech recognition, they can be used to solve cryptograms, and bigram frequency is one approach to statistical language identification.",
        "Trigram - special case of the n-gram, where n is 3.",
        "Ontology - formal representation of a set of concepts within a domain and the relationships between those concepts.",
        "Taxonomy - practice and science of classification, including the principles underlying classification, and the methods of classifying things or concepts.",
        "Hyponymy and hypernymy - the linguistics of hyponyms and hypernyms.",
        "A hyponym shares a type-of relationship with its hypernym.",
        "For example, pigeon, crow, eagle and seagull are all hyponyms of bird (their hypernym); which, in turn, is a hyponym of animal.",
        "Taxonomy for search engines - typically called a \"taxonomy of entities\".",
        "It is a tree in which nodes are labelled with entities which are expected to occur in a web search query.",
        "These trees are used to match keywords from a search query with the keywords from relevant answers (or snippets).",
        "Textual entailment - directional relation between text fragments.",
        "The relation holds whenever the truth of one text fragment follows from another text.",
        "In the TE framework, the entailing and entailed texts are termed text (t) and hypothesis (h), respectively.",
        "The relation is directional because even if \"t entails h\", the reverse \"h entails t\" is much less certain.",
        "Triphone - sequence of three phonemes.",
        "Triphones are useful in models of natural-language processing where they are used to establish the various contexts in which a phoneme can occur in a particular natural language.",
        "Automated essay scoring (AES) - the use of specialized computer programs to assign grades to essays written in an educational setting.",
        "It is a method of educational assessment and an application of natural-language processing.",
        "Its objective is to classify a large set of textual entities into a small number of discrete categories, corresponding to the possible grades-for example, the numbers 1 to 6.",
        "Therefore, it can be considered a problem of statistical classification.",
        "Automatic image annotation - process by which a computer system automatically assigns textual metadata in the form of captioning or keywords to a digital image.",
        "The annotations are used in image retrieval systems to organize and locate images of interest from a database.",
        "Automatic summarization - process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.",
        "Often used to provide summaries of text of a known type, such as articles in the financial section of a newspaper.",
        "Types Keyphrase extraction - Document summarization - Multi-document summarization - Methods and techniques Extraction-based summarization - Abstraction-based summarization - Maximum entropy-based summarization - Sentence extraction - Aided summarization - Human aided machine summarization (HAMS) - Machine aided human summarization (MAHS) - Automatic taxonomy induction - automated construction of tree structures from a corpus.",
        "This may be applied to building taxonomical classification systems for reading by end users, such as web directories or subject outlines.",
        "Coreference resolution - in order to derive the correct interpretation of text, or even to estimate the relative importance of various mentioned subjects, pronouns and other referring expressions need to be connected to the right individuals or objects.",
        "Given a sentence or larger chunk of text, coreference resolution determines which words (\"mentions\") refer to which objects (\"entities\") included in the text.",
        "Anaphora resolution - concerned with matching up pronouns with the nouns or names that they refer to.",
        "For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).",
        "Dialog system - Foreign-language reading aid - computer program that assists a non-native language user to read properly in their target language.",
        "The proper reading means that the pronunciation should be correct and stress to different parts of the words should be proper.",
        "Foreign-language writing aid - computer program or any other instrument that assists a non-native language user (also referred to as a foreign-language learner) in writing decently in their target language.",
        "Assistive operations can be classified into two categories: on-the-fly prompts and post-writing checks.",
        "Grammar checking - the act of verifying the grammatical correctness of written text, especially if this act is performed by a computer program.",
        "Information retrieval - Cross-language information retrieval - Machine translation (MT) - aims to automatically translate text from one human language to another.",
        "This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e.",
        "requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.)",
        "in order to solve properly.",
        "Classical approach of machine translation - rules-based machine translation.",
        "Computer-assisted translation - Interactive machine translation - Translation memory - database that stores so-called \"segments\", which can be sentences, paragraphs or sentence-like units (headings, titles or elements in a list) that have previously been translated, in order to aid human translators.",
        "Example-based machine translation - Rule-based machine translation - Natural-language programming - interpreting and compiling instructions communicated in natural language into computer instructions (machine code).",
        "Natural-language search - Optical character recognition (OCR) - given an image representing printed text, determine the corresponding text.",
        "Question answering - given a human-language question, determine its answer.",
        "Typical questions have a specific right answer (such as \"What is the capital of Canada?",
        "\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\").",
        "Open domain question answering - Spam filtering - Sentiment analysis - extracts subjective information usually from a set of documents, often using online reviews to determine \"polarity\" about specific objects.",
        "It is especially useful for identifying trends of public opinion in the social media, for the purpose of marketing.",
        "Speech recognition - given a sound clip of a person or people speaking, determine the textual representation of the speech.",
        "This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above).",
        "In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below).",
        "In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process.",
        "Speech synthesis (Text-to-speech) - Text-proofing - Text simplification - automated editing a document to include fewer words, or use easier words, while retaining its underlying meaning and information.",
        "Natural-language understanding - converts chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate.",
        "Natural-language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural-language expression which usually takes the form of organized notations of natural-languages concepts.",
        "Introduction and creation of language metamodel and ontology are efficient however empirical solutions.",
        "An explicit formalization of natural-languages semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.",
        "Natural-language generation - task of converting information from computer databases into readable human language.",
        "Automatic document classification (text categorization) - Automatic language identification - Compound term processing - category of techniques that identify compound terms and match them to their definitions.",
        "Compound terms are built by combining two (or more) simple terms, for example \"triple\" is a single word term but \"triple heart bypass\" is a compound term.",
        "Automatic taxonomy induction - Corpus processing - Automatic acquisition of lexicon - Text normalization - Text simplification - Deep linguistic processing - Discourse analysis - includes a number of related tasks.",
        "One task is identifying the discourse structure of connected text, i.e.",
        "the nature of the discourse relationships between sentences (e.g.",
        "elaboration, explanation, contrast).",
        "Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g.",
        "yes-no questions, content questions, statements, assertions, orders, suggestions, etc.).",
        "Information extraction - Text mining - process of deriving high-quality information from text.",
        "High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning.",
        "Biomedical text mining - (also known as BioNLP), this is text mining applied to texts and literature of the biomedical and molecular biology domain.",
        "It is a rather recent research field drawing elements from natural-language processing, bioinformatics, medical informatics and computational linguistics.",
        "There is an increasing interest in text mining and information extraction strategies applied to the biomedical and molecular biology literature due to the increasing number of electronically available publications stored in databases such as PubMed.",
        "Decision tree learning - Sentence extraction - Terminology extraction - Latent semantic indexing - Lemmatisation - groups together all like terms that share a same lemma such that they are classified as a single item.",
        "Morphological segmentation - separates words into individual morphemes and identifies the class of the morphemes.",
        "The difficulty of this task depends greatly on the complexity of the morphology (i.e.",
        "the structure of words) of the language being considered.",
        "English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g.",
        "\"open, opens, opened, opening\") as separate words.",
        "In languages such as Turkish, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.",
        "Named-entity recognition (NER) - given a stream of text, determines which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g.",
        "person, location, organization).",
        "Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case is often inaccurate or insufficient.",
        "For example, the first word of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized.",
        "Furthermore, many other languages in non-Western scripts (e.g.",
        "Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names.",
        "For example, German capitalizes all nouns, regardless of whether they refer to names, and French and Spanish do not capitalize names that serve as adjectives.",
        "Ontology learning - automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between those concepts from a corpus of natural-language text, and encoding them with an ontology language for easy retrieval.",
        "Also called \"ontology extraction\", \"ontology generation\", and \"ontology acquisition\".",
        "Parsing - determines the parse tree (grammatical analysis) of a given sentence.",
        "The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses.",
        "In fact, perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human).",
        "Shallow parsing - Part-of-speech tagging - given a sentence, determines the part of speech for each word.",
        "Many words, especially common ones, can serve as multiple parts of speech.",
        "For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five different parts of speech.",
        "Some languages have more such ambiguity than others.",
        "Languages with little inflectional morphology, such as English are particularly prone to such ambiguity.",
        "Chinese is prone to such ambiguity because it is a tonal language during verbalization.",
        "Such inflection is not readily conveyed via the entities employed within the orthography to convey intended meaning.",
        "Query expansion - Relationship extraction - given a chunk of text, identifies the relationships among named entities (e.g.",
        "who is the wife of whom).",
        "Semantic analysis (computational) - formal analysis of meaning, and \"computational\" refers to approaches that in principle support effective implementation.",
        "Explicit semantic analysis - Latent semantic analysis - Semantic analytics - Sentence breaking (also known as sentence boundary disambiguation and sentence detection) - given a chunk of text, finds the sentence boundaries.",
        "Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g.",
        "marking abbreviations).",
        "Speech segmentation - given a sound clip of a person or people speaking, separates it into words.",
        "A subtask of speech recognition and typically grouped with it.",
        "Stemming - reduces an inflected or derived word into its word stem, base, or root form.",
        "Text chunking - Tokenization - given a chunk of text, separates it into distinct words, symbols, sentences, or other units Topic segmentation and recognition - given a chunk of text, separates it into segments each of which is devoted to a topic, and identifies the topic of the segment.",
        "Truecasing - Word segmentation - separates a chunk of continuous text into separate words.",
        "For a language like English, this is fairly trivial, since words are usually separated by spaces.",
        "However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language.",
        "Word-sense disambiguation (WSD) - because many words have more than one meaning, word-sense disambiguation is used to select the meaning which makes the most sense in context.",
        "For this problem, we are typically given a list of words and associated word senses, e.g.",
        "from a dictionary or from an online resource such as WordNet.",
        "Word-sense induction - open problem of natural-language processing, which concerns the automatic identification of the senses of a word (i.e.",
        "meanings).",
        "Given that the output of word-sense induction is a set of senses for the target word (sense inventory), this task is strictly related to that of word-sense disambiguation (WSD), which relies on a predefined sense inventory and aims to solve the ambiguity of words in context.",
        "Automatic acquisition of sense-tagged corpora - W-shingling - set of unique \"shingles\"-contiguous subsequences of tokens in a document-that can be used to gauge the similarity of two documents.",
        "The w denotes the number of tokens in each shingle in the set.",
        "Natural-language generation - task of converting information from computer databases into readable human language.",
        "Automatic taxonomy induction (ATI) - automated building of tree structures from a corpus.",
        "Document structuring - History of natural-language processing History of machine translation History of automated essay scoring History of natural-language user interface History of natural-language understanding History of optical character recognition History of question answering History of speech synthesis Turing test - test of a machine's ability to exhibit intelligent behavior, equivalent to or indistinguishable from, that of an actual human.",
        "In the original illustrative example, a human judge engages in a natural-language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being.",
        "All participants are separated from one another.",
        "If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test.",
        "The test was introduced by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence,\" which opens with the words: \"I propose to consider the question, 'Can machines think?'\"",
        "Universal grammar - theory in linguistics, usually credited to Noam Chomsky, proposing that the ability to learn grammar is hard-wired into the brain.",
        "The theory suggests that linguistic ability manifests itself without being taught (see poverty of the stimulus), and that there are properties that all natural human languages share.",
        "It is a matter of observation and experimentation to determine precisely what abilities are innate and what properties are shared by all languages.",
        "ALPAC - was a committee of seven scientists led by John R. Pierce, established in 1964 by the U. S. Government in order to evaluate the progress in computational linguistics in general and machine translation in particular.",
        "Its report, issued in 1966, gained notoriety for being very skeptical of research done in machine translation so far, and emphasizing the need for basic research in computational linguistics; this eventually caused the U. S. Government to reduce its funding of the topic dramatically.",
        "Conceptual dependency theory - a model of natural-language understanding used in artificial intelligence systems.",
        "Roger Schank at Stanford University introduced the model in 1969, in the early days of artificial intelligence.",
        "This model was extensively used by Schank's students at Yale University such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.",
        "Augmented transition network - type of graph theoretic structure used in the operational definition of formal languages, used especially in parsing relatively complex natural languages, and having wide application in artificial intelligence.",
        "Introduced by William A.",
        "Woods in 1970.",
        "Distributed Language Translation (project) - Sukhotin's algorithm - statistical classification algorithm for classifying characters in a text as vowels or consonants.",
        "It was initially created by Boris V. Sukhotin.",
        "T9 (predictive text) - stands for \"Text on 9 keys\", is a USA-patented predictive text technology for mobile phones (specifically those that contain a 3x4 numeric keypad), originally developed by Tegic Communications, now part of Nuance Communications.",
        "Tatoeba - free collaborative online database of example sentences geared towards foreign-language learners.",
        "Teragram Corporation - fully owned subsidiary of SAS Institute, a major producer of statistical analysis software, headquartered in Cary, North Carolina, USA.",
        "Teragram is based in Cambridge, Massachusetts and specializes in the application of computational linguistics to multilingual natural-language processing.",
        "TipTop Technologies - company that developed TipTop Search, a real-time web, social search engine with a unique platform for semantic analysis of natural language.",
        "TipTop Search provides results capturing individual and group sentiment, opinions, and experiences from content of various sorts including real-time messages from Twitter or consumer product reviews on Amazon.com.",
        "Transderivational search - when a search is being conducted for a fuzzy match across a broad field.",
        "In computing the equivalent function can be performed using content-addressable memory.",
        "Vocabulary mismatch - common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.",
        "This model is a commonly used to train document classifiers Brill tagger - Cache language model - ChaSen, MeCab - provide morphological analysis and word splitting for Japanese Classic monolingual WSD - ClearForest - CMU Pronouncing Dictionary - also known as cmudict, is a public domain pronouncing dictionary designed for uses in speech technology, and was created by Carnegie Mellon University (CMU).",
        "It defines a mapping from English words to their North American pronunciations, and is commonly used in speech processing applications such as the Festival Speech Synthesis System and the CMU Sphinx speech recognition system.",
        "String kernel - Google Ngram Viewer - graphs n-gram usage from a corpus of more than 5.2 million books Text corpus (see list) - large and structured set of texts (nowadays usually electronically stored and processed).",
        "They are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.",
        "Bank of English British National Corpus Corpus of Contemporary American English (COCA) Oxford English Corpus The following natural-language processing toolkits are notable collections of natural-language processing software.",
        "They are suites of libraries, frameworks, and applications for symbolic, statistical natural-language and speech processing.",
        "ABNER (A Biomedical Named-Entity Recognizer) - open source text mining program that uses linear-chain conditional random field sequence models.",
        "It automatically tags genes, proteins and other entity names in text.",
        "Written by Burr Settles of the University of Wisconsin-Madison.",
        "Stanford NER (Named-Entity Recognizer) - Java implementation of a Named-Entity Recognizer that uses linear-chain conditional random field sequence models.",
        "It automatically tags persons, organizations, and locations in text in English, German, Chinese, and Spanish languages.",
        "Written by Jenny Finkel and other members of the Stanford NLP Group at Stanford University.",
        "Comparison of machine translation applications Machine translation applications Google Translate DeepL Linguee - web service that provides an online dictionary for a number of language pairs.",
        "Unlike similar services, such as LEO, Linguee incorporates a search engine that provides access to large amounts of bilingual, translated sentence pairs, which come from the World Wide Web.",
        "As a translation aid, Linguee therefore differs from machine translation services like Babelfish and is more similar in function to a translation memory.",
        "UNL Universal Networking Language Yahoo!",
        "Babel Fish Reverso CTAKES - open-source natural-language processing system for information extraction from electronic medical record clinical free-text.",
        "It processes clinical notes, identifying types of clinical named entities - drugs, diseases/disorders, signs/symptoms, anatomical sites and procedures.",
        "Each named entity has attributes for the text span, the ontology mapping code, context (family history of, current, unrelated to patient), and negated/not negated.",
        "Also known as Apache cTAKES.",
        "DMAP - ETAP-3 - proprietary linguistic processing system focusing on English and Russian.",
        "It is a rule-based system which uses the Meaning-Text Theory as its theoretical foundation.",
        "JAPE - the Java Annotation Patterns Engine, a component of the open-source General Architecture for Text Engineering (GATE) platform.",
        "JAPE is a finite state transducer that operates over annotations based on regular expressions.",
        "LOLITA - \"Large-scale, Object-based, Linguistic Interactor, Translator and Analyzer\".",
        "LOLITA was developed by Roberto Garigliano and colleagues between 1986 and 2000.",
        "It was designed as a general-purpose tool for processing unrestricted text that could be the basis of a wide variety of applications.",
        "At its core was a semantic network containing some 90,000 interlinked concepts.",
        "Maluuba - intelligent personal assistant for Android devices, that uses a contextual approach to search which takes into account the user's geographic location, contacts, and language.",
        "METAL MT - machine translation system developed in the 1980s at the University of Texas and at Siemens which ran on Lisp Machines.",
        "Never-Ending Language Learning - semantic machine learning system developed by a research team at Carnegie Mellon University, and supported by grants from DARPA, Google, and the NSF, with portions of the system running on a supercomputing cluster provided by Yahoo!.",
        "NELL was programmed by its developers to be able to identify a basic set of fundamental semantic relationships between a few hundred predefined categories of data, such as cities, companies, emotions and sports teams.",
        "Since the beginning of 2010, the Carnegie Mellon research team has been running NELL around the clock, sifting through hundreds of millions of web pages looking for connections between the information it already knows and what it finds through its search process - to make new connections in a manner that is intended to mimic the way humans learn new information.",
        "NLTK - Online-translator.com - Regulus Grammar Compiler - software system for compiling unification grammars into grammars for speech recognition systems.",
        "S Voice - Siri (software) - Speaktoit - TeLQAS - Weka's classification tools - word2vec - models that were developed by a team of researchers led by Thomas Milkov at Google to generate word embeddings that can reconstruct some of the linguistic context of words using shallow, two dimensional neural nets derived from a much larger vector space.",
        "Festival Speech Synthesis System - CMU Sphinx speech recognition system - Language Grid - Open source platform for language web services, which can customize language services by combining existing language services.",
        "Chatterbot - a text-based conversation agent that can interact with human users through some medium, such as an instant message service.",
        "Some chatterbots are designed for specific purposes, while others converse with human users on a wide range of topics.",
        "Dr. Sbaitso ELIZA PARRY Racter (or Claude Chatterbot) Mark V Shaney Albert One - 1998 and 1999 Loebner winner, by Robby Garner.",
        "- 2001, 2002, and 2004 Loebner Prize winner developed by Richard Wallace.",
        "Charlix Cleverbot (winner of the 2010 Mechanical Intelligence Competition) Elbot - 2008 Loebner Prize winner, by Fred Roberts.",
        "Eugene Goostman - 2012 Turing 100 winner, by Vladimir Veselov.",
        "Fred - an early chatterbot by Robby Garner.",
        "Jabberwacky Jeeney AI MegaHAL Mitsuku, 2013 and 2016 Loebner Prize winner Rose - ... 2015 - 3x Loebner Prize winner, by Bruce Wilcox.",
        "SimSimi - A popular artificial intelligence conversation program that was created in 2002 by ISMaker.",
        "Spookitalk - A chatterbot used for NPCs in Douglas Adams' Starship Titanic video game.",
        "Ultra Hal - 2007 Loebner Prize winner, by Robert Medeksza.",
        "Verbot GooglyMinotaur, specializing in Radiohead, the first bot released by ActiveBuddy (June 2001-March 2002) SmarterChild, developed by ActiveBuddy and released in June 2001 Infobot, an assistant on IRC channels such as #perl, primarily to help out with answering Frequently Asked Questions (June 1995-today) Negobot, a bot designed to catch online pedophiles by posing as a young girl and attempting to elicit personal details from people it speaks to.",
        "AFNLP (Asian Federation of Natural Language Processing Associations) - the organization for coordinating the natural-language processing related activities and events in the Asia-Pacific region.",
        "Australasian Language Technology Association - Association for Computational Linguistics - international scientific and professional society for people working on problems involving natural-language processing.",
        "Google, Inc. - the Google search engine is an example of automatic summarization, utilizing keyphrase extraction.",
        "Calais (Reuters product) - provider of a natural-language processing services.",
        "Wolfram Research, Inc. developer of natural-language processing computation engine Wolfram Alpha.",
        "Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing - Wermter, S., Riloff E. and Scheler, G. (editors).",
        "First book that addressed statistical and neural network learning of language.",
        "Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics - by Daniel Jurafsky and James H. Martin.",
        "Introductory book on language technology.",
        "Studies in Natural Language Processing - book series of the Association for Computational Linguistics, published by Cambridge University Press.",
        "Computational Linguistics - peer-reviewed academic journal in the field of computational linguistics.",
        "It is published quarterly by MIT Press for the Association for Computational Linguistics (ACL) Daniel Bobrow - Rollo Carpenter - creator of Jabberwacky and Cleverbot.",
        "Noam Chomsky - author of the seminal work Syntactic Structures, which revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.",
        "Kenneth Colby - David Ferrucci - principal investigator of the team that created Watson, IBM's AI computer that won the quiz show Jeopardy!",
        "Lyn Frazier - Daniel Jurafsky - Professor of Linguistics and Computer Science at Stanford University.",
        "With James H. Martin, he wrote the textbook Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics Roger Schank - introduced the conceptual dependency theory for natural-language understanding.",
        "Jean E. Fox Tree - Alan Turing - originator of the Turing Test.",
        "Joseph Weizenbaum - author of the ELIZA chatterbot.",
        "Terry Winograd - professor of computer science at Stanford University, and co-director of the Stanford Human-Computer Interaction Group.",
        "He is known within the philosophy of mind and artificial intelligence fields for his work on natural language using the SHRDLU program.",
        "William Aaron Woods - Maurice Gross - author of the concept of local grammar, taking finite automata as the competence model of language.",
        "Stephen Wolfram - CEO and founder of Wolfram Research, creator of the programming language (natural-language understanding) Wolfram Language, and natural-language processing computation engine Wolfram Alpha.",
        "Victor Yngve -"
      ],
      "metadata": {
        "title": "Outline of natural language processing",
        "url": "https://en.wikipedia.org/wiki/Outline_of_natural_language_processing",
        "word_count": 7084,
        "char_count": 48490,
        "sentence_count": 305,
        "scraped_at": "2025-08-09T14:46:52.535448",
        "language": "en",
        "processing_time": 0.014183759689331055,
        "source_hash": "00cc3daaa821879b58af4e8c68ff4f93"
      }
    },
    {
      "title": "Computer vision",
      "url": "https://en.wikipedia.org/wiki/Computer_vision",
      "raw_text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\nThe scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.\nSubdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.\n\n\n== Definition ==\nComputer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Machine vision refers to a systems engineering discipline, especially in the context of factory automation. In more recent times, the terms computer vision and machine vision have converged to a greater degree.\n\n\n== History ==\nIn the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system as a stepping stone to endowing robots with intelligent behavior. In 1966, it was believed that this could be achieved through an undergraduate summer project, by attaching a camera to a computer and having it \"describe what it saw\".\nWhat distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.\nThe next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.\nBy the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.\nRecent work has seen the resurgence of feature-based methods used in conjunction with machine learning techniques and complex optimization frameworks. \nThe advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed prior methods.\n\n\n== Related fields ==\n\n\n=== Solid-state physics ===\nSolid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible, infrared or ultraviolet light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids.\n\n\n=== Neurobiology ===\n\nNeurobiology has greatly influenced the development of computer vision algorithms. Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing of visual stimuli in both humans and various animals. This has led to a coarse yet convoluted description of how natural vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology.  The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex.\nSome strands of computer vision research are closely related to the study of biological vision—indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.\n\n\n=== Signal processing ===\nYet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.\n\n\n=== Robotic navigation ===\nRobot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot\n\n\n=== Visual computing ===\n\n\n=== Other fields ===\nBesides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry.\n\n\n=== Distinctions ===\nThe fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis.\nComputer graphics produces image data from 3D models, and computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality.\nThe following characterizations appear relevant but should not be taken as universally accepted:\n\nImage processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content.\nComputer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image.\nMachine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software. It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms.\nThere is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications. Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology.\nFinally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. A significant part of this field is devoted to applying these methods to image data.\nPhotogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.\n\n\n== Applications ==\nApplications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:\n\nAutomatic inspection, e.g., in manufacturing applications;\nAssisting humans in identification tasks, e.g., a species identification system;\nControlling processes, e.g., an industrial robot;\nDetecting events, e.g., for visual surveillance or people counting, e.g., in the restaurant industry;\nInteraction, e.g., as the input to a device for computer-human interaction;\nmonitoring agricultural crops, e.g. an open-source vision transformers model  has been developed to help farmers automatically detect strawberry diseases with 98.4% accuracy.\nModeling objects or environments, e.g., medical image analysis or topographical modeling;\nNavigation, e.g., by an autonomous vehicle or mobile robot;\nOrganizing information, e.g., for indexing databases of images and image sequences.\nTracking surfaces or planes in 3D coordinates for allowing Augmented Reality experiences.\nAnalyzing the condition of facilities in industry or construction.\nAutomatic real-time lip-reading for devices and apps to assist people with disabilities.\nFor 2024, the leading areas of computer vision were industry (market size US$5.22 billion), medicine (market size US$2.6 billion), military (market size US$996.2 million).\n\n\n=== Medicine ===\n\nOne of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain or the quality of medical treatments. Applications of computer vision in the medical area also include enhancement of images interpreted by humans—ultrasonic images or X-ray images, for example—to reduce the influence of noise.\n\n\n=== Machine vision ===\nA second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process called optical sorting.\n\n\n=== Military ===\nThe obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.\n\n\n=== Autonomous vehicles ===\n\nOne of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles. It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, cameras and LiDAR sensors in vehicles, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover.\n\n\n=== Tactile feedback ===\n\nMaterials such as rubber and silicon are being used to create sensors that allow for applications such as detecting microundulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins are being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data on imperfections on a very large surface. Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data.\nOther application areas include:\n\nSupport of visual effects creation for cinema and broadcast, e.g., camera tracking (match moving).\nSurveillance.\nDriver drowsiness detection\nTracking and counting organisms in the biological sciences\n\n\n== Typical tasks ==\nEach of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.\nComputer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n\n\n=== Recognition ===\nThe classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of recognition problem are described in the literature.\n\nObject recognition (also called object classification) – one or several pre-specified or learned objects or object classes can be recognized, usually together with their 2D positions in the image or 3D poses in the scene. Blippar, Google Goggles, and LikeThat provide stand-alone programs that illustrate this functionality.\nIdentification – an individual instance of an object is recognized. Examples include identification of a specific person's face or fingerprint, identification of handwritten digits, or the identification of a specific vehicle.\nDetection – the image data are scanned for specific objects along with their locations. Examples include the detection of an obstacle in the car's field of view and possible abnormal cells or tissues in medical images or the detection of a vehicle in an automatic road toll system. Detection based on relatively simple and fast computations is sometimes used for finding smaller regions of interesting image data which can be further analyzed by more computationally demanding techniques to produce a correct interpretation.\nCurrently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition. Performance of convolutional neural networks on the ImageNet tests is now close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.\nSeveral specialized tasks based on recognition exist, such as:\n\nContent-based image retrieval – finding all images in a larger set of images which have a specific content. The content can be specified in different ways, for example in terms of similarity relative to a target image (give me all images similar to image X) by utilizing reverse image search techniques, or in terms of high-level search criteria given as text input (give me all images which contain many houses, are taken during winter and have no cars in them).\n\nPose estimation – estimating the position or orientation of a specific object relative to the camera. An example application for this technique would be assisting a robot arm in retrieving objects from a conveyor belt in an assembly line situation or picking parts from a bin.\nOptical character recognition (OCR) – identifying characters in images of printed or handwritten text, usually with a view to encoding the text in a format more amenable to editing or indexing (e.g. ASCII). A related task is reading of 2D codes such as data matrix and QR codes.\nFacial recognition –  a technology that enables the matching of faces in digital images or video frames to a face database, which is now widely used for mobile phone facelock, smart door locking, etc.\nEmotion recognition – a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces.\nShape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects.\nHuman activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking.\n\n\n=== Motion analysis ===\nSeveral tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images. Examples of such tasks are:\n\nEgomotion – determining the 3D rigid motion (rotation and translation) of the camera from an image sequence produced by the camera.\nTracking – following the movements of a (usually) smaller set of interest points or objects (e.g., vehicles, objects, humans or other organisms) in the image sequence. This has vast industry applications as most high-running machinery can be monitored in this way.\nOptical flow – to determine, for each point in the image, how that point is moving relative to the image plane, i.e., its apparent motion. This motion is a result of both how the corresponding 3D point is moving in the scene and how the camera is moving relative to the scene.\n\n\n=== Scene reconstruction ===\nGiven one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.\n\n\n=== Image restoration ===\nImage restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise. When the images are degraded or damaged, the information to be extracted from them also gets damaged. Therefore, we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.\nAn example in this field is inpainting.\n\n\n== System methods ==\nThe organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.\n\nImage acquisition – A digital image is produced by one or several image sensors, which, besides various types of light-sensitive cameras, include range sensors, tomography devices, radar, ultra-sonic cameras, etc. Depending on the type of sensor, the resulting image data is an ordinary 2D image, a 3D volume, or an image sequence. The pixel values typically correspond to light intensity in one or several spectral bands (gray images or colour images) but can also be related to various physical measures, such as depth, absorption or reflectance of sonic or electromagnetic waves, or magnetic resonance imaging.\nPre-processing – Before a computer vision method can be applied to image data in order to extract some specific piece of information, it is usually necessary to process the data in order to ensure that it satisfies certain assumptions implied by the method. Examples are:\nRe-sampling to ensure that the image coordinate system is correct.\nNoise reduction to ensure that sensor noise does not introduce false information.\nContrast enhancement to ensure that relevant information can be detected.\nScale space representation to enhance image structures at locally appropriate scales.\nFeature extraction – Image features at various levels of complexity are extracted from the image data. Typical examples of such features are:\nLines, edges and ridges.\nLocalized interest points such as corners, blobs or points.\nMore complex features may be related to texture, shape, or motion.\nDetection/segmentation – At some point in the processing, a decision is made about which image points or regions of the image are relevant for further processing. Examples are:\nSelection of a specific set of interest points.\nSegmentation of one or multiple image regions that contain a specific object of interest.\nSegmentation of image into nested scene architecture comprising foreground, object groups, single objects or salient object parts (also referred to as spatial-taxon scene hierarchy), while the visual salience is often implemented as spatial and temporal attention.\nSegmentation or co-segmentation of one or multiple videos into a series of per-frame foreground masks while maintaining its temporal semantic continuity.\nHigh-level processing – At this step, the input is typically a small set of data, for example, a set of points or an image region, which is assumed to contain a specific object. The remaining processing deals with, for example:\nVerification that the data satisfies model-based and application-specific assumptions.\nEstimation of application-specific parameters, such as object pose or object size.\nImage recognition – classifying a detected object into different categories.\nImage registration – comparing and combining two different views of the same object.\nDecision making Making the final decision required for the application, for example:\nPass/fail on automatic inspection applications.\nMatch/no-match in recognition applications.\nFlag for further human review in medical, military, security and recognition applications.\n\n\n=== Image-understanding systems ===\nImage-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research.\nThe representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.\nWhile inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.\n\n\n== Hardware ==\n\nThere are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors.\nMost computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).\nA few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images.\nWhile traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized.\nEgocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective.\nAs of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role.\n\n\n== See also ==\n\n\n=== Lists ===\nOutline of computer vision\nList of emerging technologies\nOutline of artificial intelligence\n\n\n== References ==\n\n\n== Further reading ==\nJames E. Dobson (2023). The Birth of Computer Vision. University of Minnesota Press. ISBN 978-1-5179-1421-9.\nDavid Marr (1982). Vision. W. H. Freeman and Company. ISBN 978-0-7167-1284-8.\nAzriel Rosenfeld; Avinash Kak (1982). Digital Picture Processing. Academic Press. ISBN 978-0-12-597301-4.\nBarghout, Lauren; Lawrence W. Lee (2003). Perceptual information processing system. U.S. Patent Application 10/618,543. ISBN 978-0-262-08159-7.\nBerthold K.P. Horn (1986). Robot Vision. MIT Press. ISBN 978-0-262-08159-7.\nMichael C. Fairhurst (1988). Computer Vision for robotic systems. Prentice Hall. ISBN 978-0-13-166919-2.\nOlivier Faugeras (1993). Three-Dimensional Computer Vision, A Geometric Viewpoint. MIT Press. ISBN 978-0-262-06158-2.\nTony Lindeberg (1994). Scale-Space Theory in Computer Vision. Springer. ISBN 978-0-7923-9418-1.\nJames L. Crowley; Henrik I. Christensen, eds. (1995). Vision as Process. Springer-Verlag. ISBN 978-3-540-58143-7.\nGösta H. Granlund; Hans Knutsson (1995). Signal Processing for Computer Vision. Kluwer Academic Publisher. ISBN 978-0-7923-9530-0.\nReinhard Klette; Karsten Schluens; Andreas Koschan (1998). Computer Vision – Three-Dimensional Data from Images. Springer, Singapore. ISBN 978-981-3083-71-4.\nEmanuele Trucco; Alessandro Verri (1998). Introductory Techniques for 3-D Computer Vision. Prentice Hall. ISBN 978-0-13-261108-4.\nBernd Jähne (2002). Digital Image Processing. Springer. ISBN 978-3-540-67754-3.\nRichard Hartley and Andrew Zisserman (2003). Multiple View Geometry in Computer Vision. Cambridge University Press. ISBN 978-0-521-54051-3.\nGérard Medioni; Sing Bing Kang (2004). Emerging Topics in Computer Vision. Prentice Hall. ISBN 978-0-13-101366-7.\nR. Fisher; K Dawson-Howe; A. Fitzgibbon; C. Robertson; E. Trucco (2005). Dictionary of Computer Vision and Image Processing. John Wiley. ISBN 978-0-470-01526-1.\nNikos Paragios and Yunmei Chen and Olivier Faugeras (2005). Handbook of Mathematical Models in Computer Vision. Springer. ISBN 978-0-387-26371-7.\nWilhelm Burger; Mark J. Burge (2007). Digital Image Processing: An Algorithmic Approach Using Java. Springer. ISBN 978-1-84628-379-6. Archived from the original on 2014-05-17. Retrieved 2007-06-13.\nPedram Azad; Tilo Gockel; Rüdiger Dillmann (2008). Computer Vision – Principles and Practice. Elektor International Media BV. ISBN 978-0-905705-71-2.\nRichard Szeliski (2010). Computer Vision: Algorithms and Applications. Springer-Verlag. ISBN 978-1848829343.\nJ. R. Parker (2011). Algorithms for Image Processing and Computer Vision (2nd ed.). Wiley. ISBN 978-0470643853.\nRichard J. Radke (2013). Computer Vision for Visual Effects. Cambridge University Press. ISBN 978-0-521-76687-6.\nNixon, Mark; Aguado, Alberto (2019). Feature Extraction and Image Processing for Computer Vision (4th ed.). Academic Press. ISBN 978-0128149768.\n\n\n== External links ==\nUSC Iris computer vision conference list\nComputer vision papers on the web – a complete list of papers of the most relevant computer vision conferences.\nComputer Vision Online Archived 2011-11-30 at the Wayback Machine – news, source code, datasets and job offers related to computer vision\nCVonline – Bob Fisher's Compendium of Computer Vision.\nBritish Machine Vision Association – supporting computer vision research within the UK via the BMVC and MIUA conferences, Annals of the BMVA (open-source journal), BMVA Summer School and one-day meetings\nComputer Vision Container, Joe Hoeller GitHub: Widely adopted open-source container for GPU accelerated computer vision applications. Used by researchers, universities, private companies, as well as the U.S. Gov't.",
      "cleaned_text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems. Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Machine vision refers to a systems engineering discipline, especially in the context of factory automation. In more recent times, the terms computer vision and machine vision have converged to a greater degree. In the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system as a stepping stone to endowing robots with intelligent behavior. In 1966, it was believed that this could be achieved through an undergraduate summer project, by attaching a camera to a computer and having it \"describe what it saw\". What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields. By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering. Recent work has seen the resurgence of feature-based methods used in conjunction with machine learning techniques and complex optimization frameworks. The advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed prior methods. Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible, infrared or ultraviolet light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids. Neurobiology has greatly influenced the development of computer vision algorithms. Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing of visual stimuli in both humans and various animals. This has led to a coarse yet convoluted description of how natural vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology. The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex. Some strands of computer vision research are closely related to the study of biological vision-indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields. Yet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision. Robot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot Besides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry. The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis. Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality. The following characterizations appear relevant but should not be taken as universally accepted: Image processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image. Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software. It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms. There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications. Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology. Finally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. A significant part of this field is devoted to applying these methods to image data. Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision. Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for: Automatic inspection, e.g., in manufacturing applications; Assisting humans in identification tasks, e.g., a species identification system; Controlling processes, e.g., an industrial robot; Detecting events, e.g., for visual surveillance or people counting, e.g., in the restaurant industry; Interaction, e.g., as the input to a device for computer-human interaction; monitoring agricultural crops, e.g. an open-source vision transformers model has been developed to help farmers automatically detect strawberry diseases with 98.4% accuracy. Modeling objects or environments, e.g., medical image analysis or topographical modeling; Navigation, e.g., by an autonomous vehicle or mobile robot; Organizing information, e.g., for indexing databases of images and image sequences. Tracking surfaces or planes in 3D coordinates for allowing Augmented Reality experiences. Analyzing the condition of facilities in industry or construction. Automatic real-time lip-reading for devices and apps to assist people with disabilities. For 2024, the leading areas of computer vision were industry (market size US$5.22 billion), medicine (market size US$2.6 billion), military (market size US$996.2 million). One of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain or the quality of medical treatments. Applications of computer vision in the medical area also include enhancement of images interpreted by humans-ultrasonic images or X-ray images, for example-to reduce the influence of noise. A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process called optical sorting. The obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability. One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles. It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, cameras and LiDAR sensors in vehicles, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover. Materials such as rubber and silicon are being used to create sensors that allow for applications such as detecting microundulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins are being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data on imperfections on a very large surface. Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data. Other application areas include: Support of visual effects creation for cinema and broadcast, e.g., camera tracking (match moving). Surveillance. Driver drowsiness detection Tracking and counting organisms in the biological sciences Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below. Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of recognition problem are described in the literature. Object recognition (also called object classification) - one or several pre-specified or learned objects or object classes can be recognized, usually together with their 2D positions in the image or 3D poses in the scene. Blippar, Google Goggles, and LikeThat provide stand-alone programs that illustrate this functionality. Identification - an individual instance of an object is recognized. Examples include identification of a specific person's face or fingerprint, identification of handwritten digits, or the identification of a specific vehicle. Detection - the image data are scanned for specific objects along with their locations. Examples include the detection of an obstacle in the car's field of view and possible abnormal cells or tissues in medical images or the detection of a vehicle in an automatic road toll system. Detection based on relatively simple and fast computations is sometimes used for finding smaller regions of interesting image data which can be further analyzed by more computationally demanding techniques to produce a correct interpretation. Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition. Performance of convolutional neural networks on the ImageNet tests is now close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease. Several specialized tasks based on recognition exist, such as: Content-based image retrieval - finding all images in a larger set of images which have a specific content. The content can be specified in different ways, for example in terms of similarity relative to a target image (give me all images similar to image X) by utilizing reverse image search techniques, or in terms of high-level search criteria given as text input (give me all images which contain many houses, are taken during winter and have no cars in them). Pose estimation - estimating the position or orientation of a specific object relative to the camera. An example application for this technique would be assisting a robot arm in retrieving objects from a conveyor belt in an assembly line situation or picking parts from a bin. Optical character recognition (OCR) - identifying characters in images of printed or handwritten text, usually with a view to encoding the text in a format more amenable to editing or indexing (e.g. ASCII). A related task is reading of 2D codes such as data matrix and QR codes. Facial recognition - a technology that enables the matching of faces in digital images or video frames to a face database, which is now widely used for mobile phone facelock, smart door locking, etc. Emotion recognition - a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces. Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking. Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images. Examples of such tasks are: Egomotion - determining the 3D rigid motion (rotation and translation) of the camera from an image sequence produced by the camera. Tracking - following the movements of a (usually) smaller set of interest points or objects (e.g., vehicles, objects, humans or other organisms) in the image sequence. This has vast industry applications as most high-running machinery can be monitored in this way. Optical flow - to determine, for each point in the image, how that point is moving relative to the image plane, i.e., its apparent motion. This motion is a result of both how the corresponding 3D point is moving in the scene and how the camera is moving relative to the scene. Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models. Image restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise. When the images are degraded or damaged, the information to be extracted from them also gets damaged. Therefore, we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches. An example in this field is inpainting. The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems. Image acquisition - A digital image is produced by one or several image sensors, which, besides various types of light-sensitive cameras, include range sensors, tomography devices, radar, ultra-sonic cameras, etc. Depending on the type of sensor, the resulting image data is an ordinary 2D image, a 3D volume, or an image sequence. The pixel values typically correspond to light intensity in one or several spectral bands (gray images or colour images) but can also be related to various physical measures, such as depth, absorption or reflectance of sonic or electromagnetic waves, or magnetic resonance imaging. Pre-processing - Before a computer vision method can be applied to image data in order to extract some specific piece of information, it is usually necessary to process the data in order to ensure that it satisfies certain assumptions implied by the method. Examples are: Re-sampling to ensure that the image coordinate system is correct. Noise reduction to ensure that sensor noise does not introduce false information. Contrast enhancement to ensure that relevant information can be detected. Scale space representation to enhance image structures at locally appropriate scales. Feature extraction - Image features at various levels of complexity are extracted from the image data. Typical examples of such features are: Lines, edges and ridges. Localized interest points such as corners, blobs or points. More complex features may be related to texture, shape, or motion. Detection/segmentation - At some point in the processing, a decision is made about which image points or regions of the image are relevant for further processing. Examples are: Selection of a specific set of interest points. Segmentation of one or multiple image regions that contain a specific object of interest. Segmentation of image into nested scene architecture comprising foreground, object groups, single objects or salient object parts (also referred to as spatial-taxon scene hierarchy), while the visual salience is often implemented as spatial and temporal attention. Segmentation or co-segmentation of one or multiple videos into a series of per-frame foreground masks while maintaining its temporal semantic continuity. High-level processing - At this step, the input is typically a small set of data, for example, a set of points or an image region, which is assumed to contain a specific object. The remaining processing deals with, for example: Verification that the data satisfies model-based and application-specific assumptions. Estimation of application-specific parameters, such as object pose or object size. Image recognition - classifying a detected object into different categories. Image registration - comparing and combining two different views of the same object. Decision making Making the final decision required for the application, for example: Pass/fail on automatic inspection applications. Match/no-match in recognition applications. Flag for further human review in medical, military, security and recognition applications. Image-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research. The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation. While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction. There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors. Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower). A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images. While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized. Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective. As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role.",
      "sentences": [
        "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g.",
        "in the form of decisions.",
        "\"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action.",
        "This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.",
        "The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images.",
        "Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices.",
        "The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.",
        "Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.",
        "Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos.",
        "From the perspective of engineering, it seeks to automate tasks that the human visual system can do.",
        "\"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images.",
        "It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\"",
        "As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images.",
        "The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.",
        "As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.",
        "Machine vision refers to a systems engineering discipline, especially in the context of factory automation.",
        "In more recent times, the terms computer vision and machine vision have converged to a greater degree.",
        "In the late 1960s, computer vision began at universities that were pioneering artificial intelligence.",
        "It was meant to mimic the human visual system as a stepping stone to endowing robots with intelligent behavior.",
        "In 1966, it was believed that this could be achieved through an undergraduate summer project, by attaching a camera to a computer and having it \"describe what it saw\".",
        "What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding.",
        "Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.",
        "The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision.",
        "These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes.",
        "Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.",
        "By the 1990s, some of the previous research topics became more active than others.",
        "Research in projective 3-D reconstructions led to better understanding of camera calibration.",
        "With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry.",
        "This led to methods for sparse 3-D reconstructions of scenes from multiple images.",
        "Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques.",
        "At the same time, variations of graph cut were used to solve image segmentation.",
        "This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface).",
        "Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision.",
        "This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.",
        "Recent work has seen the resurgence of feature-based methods used in conjunction with machine learning techniques and complex optimization frameworks.",
        "The advancement of Deep Learning techniques has brought further life to the field of computer vision.",
        "The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed prior methods.",
        "Solid-state physics is another field that is closely related to computer vision.",
        "Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible, infrared or ultraviolet light.",
        "The sensors are designed using quantum physics.",
        "The process by which light interacts with surfaces is explained using physics.",
        "Physics explains the behavior of optics which are a core part of most imaging systems.",
        "Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process.",
        "Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids.",
        "Neurobiology has greatly influenced the development of computer vision algorithms.",
        "Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing of visual stimuli in both humans and various animals.",
        "This has led to a coarse yet convoluted description of how natural vision systems operate in order to solve certain vision-related tasks.",
        "These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity.",
        "Also, some of the learning-based methods developed within computer vision (e.g.",
        "neural net and deep learning based image and feature analysis and classification) have their background in neurobiology.",
        "The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex.",
        "Some strands of computer vision research are closely related to the study of biological vision-indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information.",
        "The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals.",
        "Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems.",
        "An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.",
        "Yet another field related to computer vision is signal processing.",
        "Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision.",
        "However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals.",
        "Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.",
        "Robot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment.",
        "A detailed understanding of these environments is required to navigate through them.",
        "Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot Besides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view.",
        "For example, many methods in computer vision are based on statistics, optimization or geometry.",
        "Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance.",
        "Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry.",
        "The fields most closely related to computer vision are image processing, image analysis and machine vision.",
        "There is a significant overlap in the range of techniques and applications that these cover.",
        "This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names.",
        "On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented.",
        "In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis.",
        "Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data.",
        "There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality.",
        "The following characterizations appear relevant but should not be taken as universally accepted: Image processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image.",
        "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content.",
        "Computer vision includes 3D analysis from 2D images.",
        "This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images.",
        "Computer vision often relies on more or less complex assumptions about the scene depicted in an image.",
        "Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance in industrial applications.",
        "Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking).",
        "This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software.",
        "It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms.",
        "There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images.",
        "For example, medical imaging includes substantial work on the analysis of image data in medical applications.",
        "Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology.",
        "Finally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks.",
        "A significant part of this field is devoted to applying these methods to image data.",
        "Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.",
        "Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them.",
        "The computer vision and machine vision fields have significant overlap.",
        "Computer vision covers the core technology of automated image analysis which is used in many fields.",
        "Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications.",
        "In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.",
        "Examples of applications of computer vision include systems for: Automatic inspection, e.g., in manufacturing applications; Assisting humans in identification tasks, e.g., a species identification system; Controlling processes, e.g., an industrial robot; Detecting events, e.g., for visual surveillance or people counting, e.g., in the restaurant industry; Interaction, e.g., as the input to a device for computer-human interaction; monitoring agricultural crops, e.g.",
        "an open-source vision transformers model has been developed to help farmers automatically detect strawberry diseases with 98.4% accuracy.",
        "Modeling objects or environments, e.g., medical image analysis or topographical modeling; Navigation, e.g., by an autonomous vehicle or mobile robot; Organizing information, e.g., for indexing databases of images and image sequences.",
        "Tracking surfaces or planes in 3D coordinates for allowing Augmented Reality experiences.",
        "Analyzing the condition of facilities in industry or construction.",
        "Automatic real-time lip-reading for devices and apps to assist people with disabilities.",
        "One of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient.",
        "An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc.",
        "are another example.",
        "It also supports medical research by providing new information: e.g., about the structure of the brain or the quality of medical treatments.",
        "Applications of computer vision in the medical area also include enhancement of images interpreted by humans-ultrasonic images or X-ray images, for example-to reduce the influence of noise.",
        "A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process.",
        "One example is quality control where details or final products are being automatically inspected in order to find defects.",
        "One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner.",
        "Another example is a measurement of the position and orientation of details to be picked up by a robot arm.",
        "Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process called optical sorting.",
        "The obvious examples are the detection of enemy soldiers or vehicles and missile guidance.",
        "More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data.",
        "Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions.",
        "In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.",
        "One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV).",
        "The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations.",
        "Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles.",
        "It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires.",
        "Examples of supporting systems are obstacle warning systems in cars, cameras and LiDAR sensors in vehicles, and systems for autonomous landing of aircraft.",
        "Several car manufacturers have demonstrated systems for autonomous driving of cars.",
        "There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance.",
        "Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover.",
        "Materials such as rubber and silicon are being used to create sensors that allow for applications such as detecting microundulations and calibrating robotic hands.",
        "Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges.",
        "The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins.",
        "A user can then wear the finger mold and trace a surface.",
        "A computer can then read the data from the strain gauges and measure if one or more of the pins are being pushed upward.",
        "If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface.",
        "This sort of technology is useful in order to receive accurate data on imperfections on a very large surface.",
        "Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon.",
        "The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced.",
        "These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data.",
        "Other application areas include: Support of visual effects creation for cinema and broadcast, e.g., camera tracking (match moving).",
        "Surveillance.",
        "Driver drowsiness detection Tracking and counting organisms in the biological sciences Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods.",
        "Some examples of typical computer vision tasks are presented below.",
        "Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.",
        "Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action.",
        "This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.",
        "The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity.",
        "Different varieties of recognition problem are described in the literature.",
        "Object recognition (also called object classification) - one or several pre-specified or learned objects or object classes can be recognized, usually together with their 2D positions in the image or 3D poses in the scene.",
        "Blippar, Google Goggles, and LikeThat provide stand-alone programs that illustrate this functionality.",
        "Identification - an individual instance of an object is recognized.",
        "Examples include identification of a specific person's face or fingerprint, identification of handwritten digits, or the identification of a specific vehicle.",
        "Detection - the image data are scanned for specific objects along with their locations.",
        "Examples include the detection of an obstacle in the car's field of view and possible abnormal cells or tissues in medical images or the detection of a vehicle in an automatic road toll system.",
        "Detection based on relatively simple and fast computations is sometimes used for finding smaller regions of interesting image data which can be further analyzed by more computationally demanding techniques to produce a correct interpretation.",
        "Currently, the best algorithms for such tasks are based on convolutional neural networks.",
        "An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition.",
        "Performance of convolutional neural networks on the ImageNet tests is now close to that of humans.",
        "The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand.",
        "They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras).",
        "By contrast, those kinds of images rarely trouble humans.",
        "Humans, however, tend to have trouble with other issues.",
        "For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.",
        "Several specialized tasks based on recognition exist, such as: Content-based image retrieval - finding all images in a larger set of images which have a specific content.",
        "The content can be specified in different ways, for example in terms of similarity relative to a target image (give me all images similar to image X) by utilizing reverse image search techniques, or in terms of high-level search criteria given as text input (give me all images which contain many houses, are taken during winter and have no cars in them).",
        "Pose estimation - estimating the position or orientation of a specific object relative to the camera.",
        "An example application for this technique would be assisting a robot arm in retrieving objects from a conveyor belt in an assembly line situation or picking parts from a bin.",
        "Optical character recognition (OCR) - identifying characters in images of printed or handwritten text, usually with a view to encoding the text in a format more amenable to editing or indexing (e.g.",
        "A related task is reading of 2D codes such as data matrix and QR codes.",
        "Facial recognition - a technology that enables the matching of faces in digital images or video frames to a face database, which is now widely used for mobile phone facelock, smart door locking, etc.",
        "Emotion recognition - a subset of facial recognition, emotion recognition refers to the process of classifying human emotions.",
        "Psychologists caution, however, that internal emotions cannot be reliably detected from faces.",
        "Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects.",
        "Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking.",
        "Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images.",
        "Examples of such tasks are: Egomotion - determining the 3D rigid motion (rotation and translation) of the camera from an image sequence produced by the camera.",
        "Tracking - following the movements of a (usually) smaller set of interest points or objects (e.g., vehicles, objects, humans or other organisms) in the image sequence.",
        "This has vast industry applications as most high-running machinery can be monitored in this way.",
        "Optical flow - to determine, for each point in the image, how that point is moving relative to the image plane, i.e., its apparent motion.",
        "This motion is a result of both how the corresponding 3D point is moving in the scene and how the camera is moving relative to the scene.",
        "Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene.",
        "In the simplest case, the model can be a set of 3D points.",
        "More sophisticated methods produce a complete 3D surface model.",
        "The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field.",
        "Grid-based 3D sensing can be used to acquire 3D images from multiple angles.",
        "Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.",
        "Image restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise.",
        "When the images are degraded or damaged, the information to be extracted from them also gets damaged.",
        "Therefore, we need to recover or restore the image as it was intended to be.",
        "The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.)",
        "from images.",
        "The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters.",
        "More sophisticated methods assume a model of how the local image structures look to distinguish them from noise.",
        "By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.",
        "An example in this field is inpainting.",
        "The organization of a computer vision system is highly application-dependent.",
        "Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc.",
        "The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation.",
        "Many functions are unique to the application.",
        "There are, however, typical functions that are found in many computer vision systems.",
        "Image acquisition - A digital image is produced by one or several image sensors, which, besides various types of light-sensitive cameras, include range sensors, tomography devices, radar, ultra-sonic cameras, etc.",
        "Depending on the type of sensor, the resulting image data is an ordinary 2D image, a 3D volume, or an image sequence.",
        "The pixel values typically correspond to light intensity in one or several spectral bands (gray images or colour images) but can also be related to various physical measures, such as depth, absorption or reflectance of sonic or electromagnetic waves, or magnetic resonance imaging.",
        "Pre-processing - Before a computer vision method can be applied to image data in order to extract some specific piece of information, it is usually necessary to process the data in order to ensure that it satisfies certain assumptions implied by the method.",
        "Examples are: Re-sampling to ensure that the image coordinate system is correct.",
        "Noise reduction to ensure that sensor noise does not introduce false information.",
        "Contrast enhancement to ensure that relevant information can be detected.",
        "Scale space representation to enhance image structures at locally appropriate scales.",
        "Feature extraction - Image features at various levels of complexity are extracted from the image data.",
        "Typical examples of such features are: Lines, edges and ridges.",
        "Localized interest points such as corners, blobs or points.",
        "More complex features may be related to texture, shape, or motion.",
        "Detection/segmentation - At some point in the processing, a decision is made about which image points or regions of the image are relevant for further processing.",
        "Examples are: Selection of a specific set of interest points.",
        "Segmentation of one or multiple image regions that contain a specific object of interest.",
        "Segmentation of image into nested scene architecture comprising foreground, object groups, single objects or salient object parts (also referred to as spatial-taxon scene hierarchy), while the visual salience is often implemented as spatial and temporal attention.",
        "Segmentation or co-segmentation of one or multiple videos into a series of per-frame foreground masks while maintaining its temporal semantic continuity.",
        "High-level processing - At this step, the input is typically a small set of data, for example, a set of points or an image region, which is assumed to contain a specific object.",
        "The remaining processing deals with, for example: Verification that the data satisfies model-based and application-specific assumptions.",
        "Estimation of application-specific parameters, such as object pose or object size.",
        "Image recognition - classifying a detected object into different categories.",
        "Image registration - comparing and combining two different views of the same object.",
        "Decision making Making the final decision required for the application, for example: Pass/fail on automatic inspection applications.",
        "Match/no-match in recognition applications.",
        "Flag for further human review in medical, military, security and recognition applications.",
        "Image-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events.",
        "Many of these requirements are entirely topics for further research.",
        "The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.",
        "While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing.",
        "Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.",
        "There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.",
        "), a processor, and control and communication cables or some kind of wireless interconnection mechanism.",
        "In addition, a practical vision system contains software, as well as a display in order to monitor the system.",
        "Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment.",
        "Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors.",
        "Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).",
        "A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc.",
        "Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images.",
        "While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second.",
        "For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms.",
        "When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized.",
        "Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective.",
        "As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role."
      ],
      "metadata": {
        "title": "Computer vision",
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "word_count": 5185,
        "char_count": 34463,
        "sentence_count": 234,
        "scraped_at": "2025-08-09T14:46:57.512840",
        "language": "en",
        "processing_time": 0.01432490348815918,
        "source_hash": "48b7032d53fd17f1bae17291bfe4c232"
      }
    },
    {
      "title": "Feature (computer vision)",
      "url": "https://en.wikipedia.org/wiki/Feature_(computer_vision)",
      "raw_text": "In computer vision and image processing, a feature is a piece of information about the content of an image; typically about whether a certain region of the image has certain properties. Features may be specific structures in the image such as points, edges or objects. Features may also be the result of a general neighborhood operation or feature detection applied to the image. Other examples of features are related to motion in image sequences, or to shapes defined in terms of curves or boundaries between different image regions.\nMore broadly a feature is any piece of information that is relevant for solving the computational task related to a certain application. This is the same sense as feature in machine learning and pattern recognition generally, though image processing has a very sophisticated collection of features. The feature concept is very general and the choice of features in a particular computer vision system may be highly dependent on the specific problem at hand.\n\n\n== Definition ==\nThere is no universal or exact definition of what constitutes a feature, and the exact definition often depends on the problem or the type of application. Nevertheless, a feature is typically defined as an \"interesting\" part of an image, and features are used as a starting point for many computer vision algorithms. \nSince features are used as the starting point and main primitives for subsequent algorithms, the overall algorithm will often only be as good as its feature detector. Consequently, the desirable property for a feature detector is repeatability: whether or not the same feature will be detected in two or more different images of the same scene.\nFeature detection is a low-level image processing operation. That is, it is usually performed as the first operation on an image and examines every pixel to see if there is a feature present at that pixel. If this is part of a larger algorithm, then the algorithm will typically only examine the image in the region of the features. As a built-in pre-requisite to feature detection, the input image is usually smoothed by a Gaussian kernel in a scale-space representation and one or several feature images are computed, often expressed in terms of local image derivative operations.\nOccasionally, when feature detection is computationally expensive and there are time constraints, a higher-level algorithm may be used to guide the feature detection stage so that only certain parts of the image are searched for features.\nThere are many computer vision algorithms that use feature detection as the initial step, so as a result, a very large number of feature detectors have been developed. These vary widely in the kinds of feature detected, the computational complexity and the repeatability.\nWhen features are defined in terms of local neighborhood operations applied to an image, a procedure commonly referred to as feature extraction, one can distinguish between feature detection approaches that produce local decisions whether there is a feature of a given type at a given image point or not, and those who produce non-binary data as result.  The distinction becomes relevant when the resulting detected features are relatively sparse. Although local decisions are made, the output from a feature detection step does not need to be a binary image. The result is often represented in terms of sets of (connected or unconnected) coordinates of the image points where features have been detected, sometimes with subpixel accuracy.\nWhen feature extraction is done without local decision making, the result is often referred to as a feature image.  Consequently, a feature image can be seen as an image in the sense that it is a function of the same spatial (or temporal) variables as the original image, but where the pixel values hold information about image features instead of intensity or color.  This means that a feature image can be processed in a similar way as an ordinary image generated by an image sensor. Feature images are also often computed as integrated step in algorithms for feature detection.\n\n\n=== Feature vectors and feature spaces ===\nIn some applications, it is not sufficient to extract only one type of feature to obtain the relevant information from the image data.  Instead, two or more different features are extracted, resulting in two or more feature descriptors at each image point.  A common practice is to organize the information provided by all these descriptors as the elements of one single vector, commonly referred to as a feature vector.  The set of all possible feature vectors constitutes a feature space.\nA common example of feature vectors appears when each image point is to be classified as belonging to a specific class.  Assuming that each image point has a corresponding feature vector based on a suitable set of features, meaning that each class is well separated in the corresponding feature space, the classification of each image point can be done using standard classification method.\n\nAnother and related example occurs when neural network-based processing is applied to images.  The input data fed to the neural network is often given in terms of a feature vector from each image point, where the vector is constructed from several different features extracted from the image data.  During a learning phase, the network can itself find which combinations of different features are useful for solving the problem at hand.\n\n\n== Types ==\n\n\n=== Edges ===\nEdges are points where there is a boundary (or an edge) between two image regions. In general, an edge can be of almost arbitrary shape, and may include junctions. In practice, edges are usually defined as sets of points in the image that have a strong gradient magnitude. Furthermore, some common algorithms will then chain high gradient points together to form a more complete description of an edge. These algorithms usually place some constraints on the properties of an edge, such as shape, smoothness, and gradient value.\nLocally, edges have a one-dimensional structure.\n\n\n=== Corners/interest points ===\nThe terms corners and interest points are used somewhat interchangeably and refer to point-like features in an image, which have a local two-dimensional structure. The name \"Corner\" arose since early algorithms first performed edge detection, and then analyzed the edges to find rapid changes in direction (corners). These algorithms were then developed so that explicit edge detection was no longer required, for instance by looking for high levels of curvature in the image gradient. It was then noticed that the so-called corners were also being detected on parts of the image that were not corners in the traditional sense (for instance a small bright spot on a dark background may be detected). These points are frequently known as interest points, but the term \"corner\" is used by tradition.\n\n\n=== Blobs / regions of interest points ===\nBlobs provide a complementary description of image structures in terms of regions, as opposed to corners that are more point-like. Nevertheless, blob descriptors may often contain a preferred point (a local maximum of an operator response or a center of gravity) which means that many blob detectors may also be regarded as interest point operators. Blob detectors can detect areas in an image that are too smooth to be detected by a corner detector.\nConsider shrinking an image and then performing corner detection. The detector will respond to points that are sharp in the shrunk image, but may be smooth in the original image. It is at this point that the difference between a corner detector and a blob detector becomes somewhat vague. To a large extent, this distinction can be remedied by including an appropriate notion of scale. Nevertheless, due to their response properties to different types of image structures at different scales, the LoG and DoH  blob detectors are also mentioned in the article on corner detection.\n\n\n=== Ridges ===\nFor elongated objects, the notion of ridges is a natural tool. A ridge descriptor computed from a grey-level image can be seen as a generalization of a medial axis. From a practical viewpoint, a ridge can be thought of as a one-dimensional curve that represents an axis of symmetry, and in addition has an attribute of local ridge width associated with each ridge point. Unfortunately, however, it is algorithmically harder to extract ridge features from general classes of grey-level images than edge-, corner- or blob features. Nevertheless, ridge descriptors are frequently used for road extraction in aerial images and for extracting blood vessels in medical images—see ridge detection.\n\n\n== Detection ==\n\nFeature detection includes methods for computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not. The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions.\nThe extraction of features are sometimes made over several scalings. One of these methods is the scale-invariant feature transform (SIFT).\n\n\n== Extraction ==\n\nOnce features have been detected, a local image patch around the feature can be extracted. This extraction may involve quite considerable amounts of image processing. The result is known as a feature descriptor or feature vector. Among the approaches that are used to feature description, one can mention N-jets and local histograms (see scale-invariant feature transform for one example of a local histogram descriptor). In addition to such attribute information, the feature detection step by itself may also provide complementary attributes, such as the edge orientation and gradient magnitude in edge detection and the polarity and the strength of the blob in blob detection.\n\n\n=== Low-level ===\nEdge detection\nCorner detection\nBlob detection\nRidge detection\nScale-invariant feature transform\n\n\n==== Curvature ====\nEdge direction, changing intensity, autocorrelation.\n\n\n==== Image motion ====\nMotion detection. Area based, differential approach. Optical flow.\n\n\n=== Shape based ===\nThresholding\nBlob extraction\nTemplate matching\nHough transform\nLines\nCircles/ellipses\nArbitrary shapes (generalized Hough transform)\nWorks with any parameterizable feature (class variables, cluster detection, etc..)\nGeneralised Hough transform\n\n\n=== Flexible methods ===\nDeformable, parameterized shapes\nActive contours (snakes)\n\n\n== Representation ==\n\nA specific image feature, defined in terms of a specific structure in the image data, can often be represented in different ways.  For example, an edge can be represented as a Boolean variable in each image point that describes whether an edge is present at that point.  Alternatively, we can instead use a representation that provides a certainty measure instead of a Boolean statement of the edge's existence and combine this with information about the orientation of the edge.  Similarly, the color of a specific region can either be represented in terms of the average color (three scalars) or a color histogram (three functions).\nWhen a computer vision system or computer vision algorithm is designed the choice of feature representation can be a critical issue.  In some cases, a higher level of detail in the description of a feature may be necessary for solving the problem, but this comes at the cost of having to deal with more data and more demanding processing.  Below, some of the factors which are relevant for choosing a suitable representation are discussed.  In this discussion, an instance of a feature representation is referred to as a feature descriptor, or simply descriptor.\n\n\n=== Certainty or confidence ===\nTwo examples of image features are local edge orientation and local velocity in an image sequence.  In the case of orientation, the value of this feature may be more or less undefined if more than one edge are present in the corresponding neighborhood.  Local velocity is undefined if the corresponding image region does not contain any spatial variation.  As a consequence of this observation, it may be relevant to use a feature representation that includes a measure of certainty or confidence related to the statement about the feature value.  Otherwise, it is a typical situation that the same descriptor is used to represent feature values of low certainty and feature values close to zero, with a resulting ambiguity in the interpretation of this descriptor.  Depending on the application, such an ambiguity may or may not be acceptable.\nIn particular, if a featured image will be used in subsequent processing, it may be a good idea to employ a feature representation that includes information about certainty or confidence.  This enables a new feature descriptor to be computed from several descriptors, for example, computed at the same image point but at different scales, or from different but neighboring points, in terms of a weighted average where the weights are derived from the corresponding certainties.  In the simplest case, the corresponding computation can be implemented as a low-pass filtering of the featured image.  The resulting feature image will, in general, be more stable to noise.\n\n\n=== Averageability ===\nIn addition to having certainty measures included in the representation, the representation of the corresponding feature values may itself be suitable for an averaging operation or not. Most feature representations can be averaged in practice, but only in certain cases can the resulting descriptor be given a correct interpretation in terms of a feature value.  Such representations are referred to as averageable.\nFor example, if the orientation of an edge is represented in terms of an angle, this representation must have a discontinuity where the angle wraps from its maximal value to its minimal value.  Consequently, it can happen that two similar orientations are represented by angles that have a mean that does not lie close to either of the original angles and, hence, this representation is not averageable.  There are other representations of edge orientation, such as the structure tensor, which are averageable.\nAnother example relates to motion, where in some cases only the normal velocity relative to some edge can be extracted.  If two such features have been extracted and they can be assumed to refer to same true velocity, this velocity is not given as the average of the normal velocity vectors.  Hence, normal velocity vectors are not averageable.  Instead, there are other representations of motions, using matrices or tensors, that give the true velocity in terms of an average operation of the normal velocity descriptors.\n\n\n== Matching ==\n\nFeatures detected in each image can be matched across multiple images to establish corresponding features such as corresponding points.\nThe algorithm is based on comparing and analyzing point correspondences between the reference image and the target image. If any part of the cluttered scene shares correspondences greater than the threshold, that part of the cluttered scene image is targeted and considered to include the reference object there.\n\n\n== See also ==\nComputer vision\nAutomatic image annotation\nFeature learning\nFeature selection\nForeground detection\nVectorization (image tracing)\n\n\n== References ==\n\n\n== Further reading ==\nT. Lindeberg (2009). \"Scale-space\". In Benjamin Wah (ed.). Encyclopedia of Computer Science and Engineering. Vol. IV. John Wiley and Sons. pp. 2495–2504. doi:10.1002/9780470050118.ecse609. ISBN 978-0470050118. (summary and review of a number of feature detectors formulated based on scale-space operations)",
      "cleaned_text": "In computer vision and image processing, a feature is a piece of information about the content of an image; typically about whether a certain region of the image has certain properties. Features may be specific structures in the image such as points, edges or objects. Features may also be the result of a general neighborhood operation or feature detection applied to the image. Other examples of features are related to motion in image sequences, or to shapes defined in terms of curves or boundaries between different image regions. More broadly a feature is any piece of information that is relevant for solving the computational task related to a certain application. This is the same sense as feature in machine learning and pattern recognition generally, though image processing has a very sophisticated collection of features. The feature concept is very general and the choice of features in a particular computer vision system may be highly dependent on the specific problem at hand. There is no universal or exact definition of what constitutes a feature, and the exact definition often depends on the problem or the type of application. Nevertheless, a feature is typically defined as an \"interesting\" part of an image, and features are used as a starting point for many computer vision algorithms. Since features are used as the starting point and main primitives for subsequent algorithms, the overall algorithm will often only be as good as its feature detector. Consequently, the desirable property for a feature detector is repeatability: whether or not the same feature will be detected in two or more different images of the same scene. Feature detection is a low-level image processing operation. That is, it is usually performed as the first operation on an image and examines every pixel to see if there is a feature present at that pixel. If this is part of a larger algorithm, then the algorithm will typically only examine the image in the region of the features. As a built-in pre-requisite to feature detection, the input image is usually smoothed by a Gaussian kernel in a scale-space representation and one or several feature images are computed, often expressed in terms of local image derivative operations. Occasionally, when feature detection is computationally expensive and there are time constraints, a higher-level algorithm may be used to guide the feature detection stage so that only certain parts of the image are searched for features. There are many computer vision algorithms that use feature detection as the initial step, so as a result, a very large number of feature detectors have been developed. These vary widely in the kinds of feature detected, the computational complexity and the repeatability. When features are defined in terms of local neighborhood operations applied to an image, a procedure commonly referred to as feature extraction, one can distinguish between feature detection approaches that produce local decisions whether there is a feature of a given type at a given image point or not, and those who produce non-binary data as result. The distinction becomes relevant when the resulting detected features are relatively sparse. Although local decisions are made, the output from a feature detection step does not need to be a binary image. The result is often represented in terms of sets of (connected or unconnected) coordinates of the image points where features have been detected, sometimes with subpixel accuracy. When feature extraction is done without local decision making, the result is often referred to as a feature image. Consequently, a feature image can be seen as an image in the sense that it is a function of the same spatial (or temporal) variables as the original image, but where the pixel values hold information about image features instead of intensity or color. This means that a feature image can be processed in a similar way as an ordinary image generated by an image sensor. Feature images are also often computed as integrated step in algorithms for feature detection. In some applications, it is not sufficient to extract only one type of feature to obtain the relevant information from the image data. Instead, two or more different features are extracted, resulting in two or more feature descriptors at each image point. A common practice is to organize the information provided by all these descriptors as the elements of one single vector, commonly referred to as a feature vector. The set of all possible feature vectors constitutes a feature space. A common example of feature vectors appears when each image point is to be classified as belonging to a specific class. Assuming that each image point has a corresponding feature vector based on a suitable set of features, meaning that each class is well separated in the corresponding feature space, the classification of each image point can be done using standard classification method. Another and related example occurs when neural network-based processing is applied to images. The input data fed to the neural network is often given in terms of a feature vector from each image point, where the vector is constructed from several different features extracted from the image data. During a learning phase, the network can itself find which combinations of different features are useful for solving the problem at hand. Edges are points where there is a boundary (or an edge) between two image regions. In general, an edge can be of almost arbitrary shape, and may include junctions. In practice, edges are usually defined as sets of points in the image that have a strong gradient magnitude. Furthermore, some common algorithms will then chain high gradient points together to form a more complete description of an edge. These algorithms usually place some constraints on the properties of an edge, such as shape, smoothness, and gradient value. Locally, edges have a one-dimensional structure. The terms corners and interest points are used somewhat interchangeably and refer to point-like features in an image, which have a local two-dimensional structure. The name \"Corner\" arose since early algorithms first performed edge detection, and then analyzed the edges to find rapid changes in direction (corners). These algorithms were then developed so that explicit edge detection was no longer required, for instance by looking for high levels of curvature in the image gradient. It was then noticed that the so-called corners were also being detected on parts of the image that were not corners in the traditional sense (for instance a small bright spot on a dark background may be detected). These points are frequently known as interest points, but the term \"corner\" is used by tradition. Blobs provide a complementary description of image structures in terms of regions, as opposed to corners that are more point-like. Nevertheless, blob descriptors may often contain a preferred point (a local maximum of an operator response or a center of gravity) which means that many blob detectors may also be regarded as interest point operators. Blob detectors can detect areas in an image that are too smooth to be detected by a corner detector. Consider shrinking an image and then performing corner detection. The detector will respond to points that are sharp in the shrunk image, but may be smooth in the original image. It is at this point that the difference between a corner detector and a blob detector becomes somewhat vague. To a large extent, this distinction can be remedied by including an appropriate notion of scale. Nevertheless, due to their response properties to different types of image structures at different scales, the LoG and DoH blob detectors are also mentioned in the article on corner detection. For elongated objects, the notion of ridges is a natural tool. A ridge descriptor computed from a grey-level image can be seen as a generalization of a medial axis. From a practical viewpoint, a ridge can be thought of as a one-dimensional curve that represents an axis of symmetry, and in addition has an attribute of local ridge width associated with each ridge point. Unfortunately, however, it is algorithmically harder to extract ridge features from general classes of grey-level images than edge-, corner- or blob features. Nevertheless, ridge descriptors are frequently used for road extraction in aerial images and for extracting blood vessels in medical images-see ridge detection. Feature detection includes methods for computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not. The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions. The extraction of features are sometimes made over several scalings. One of these methods is the scale-invariant feature transform (SIFT). Once features have been detected, a local image patch around the feature can be extracted. This extraction may involve quite considerable amounts of image processing. The result is known as a feature descriptor or feature vector. Among the approaches that are used to feature description, one can mention N-jets and local histograms (see scale-invariant feature transform for one example of a local histogram descriptor). In addition to such attribute information, the feature detection step by itself may also provide complementary attributes, such as the edge orientation and gradient magnitude in edge detection and the polarity and the strength of the blob in blob detection. Edge detection Corner detection Blob detection Ridge detection Scale-invariant feature transform Edge direction, changing intensity, autocorrelation. Motion detection. Area based, differential approach. Optical flow. Thresholding Blob extraction Template matching Hough transform Lines Circles/ellipses Arbitrary shapes (generalized Hough transform) Works with any parameterizable feature (class variables, cluster detection, etc..) Generalised Hough transform Deformable, parameterized shapes Active contours (snakes) A specific image feature, defined in terms of a specific structure in the image data, can often be represented in different ways. For example, an edge can be represented as a Boolean variable in each image point that describes whether an edge is present at that point. Alternatively, we can instead use a representation that provides a certainty measure instead of a Boolean statement of the edge's existence and combine this with information about the orientation of the edge. Similarly, the color of a specific region can either be represented in terms of the average color (three scalars) or a color histogram (three functions). When a computer vision system or computer vision algorithm is designed the choice of feature representation can be a critical issue. In some cases, a higher level of detail in the description of a feature may be necessary for solving the problem, but this comes at the cost of having to deal with more data and more demanding processing. Below, some of the factors which are relevant for choosing a suitable representation are discussed. In this discussion, an instance of a feature representation is referred to as a feature descriptor, or simply descriptor. Two examples of image features are local edge orientation and local velocity in an image sequence. In the case of orientation, the value of this feature may be more or less undefined if more than one edge are present in the corresponding neighborhood. Local velocity is undefined if the corresponding image region does not contain any spatial variation. As a consequence of this observation, it may be relevant to use a feature representation that includes a measure of certainty or confidence related to the statement about the feature value. Otherwise, it is a typical situation that the same descriptor is used to represent feature values of low certainty and feature values close to zero, with a resulting ambiguity in the interpretation of this descriptor. Depending on the application, such an ambiguity may or may not be acceptable. In particular, if a featured image will be used in subsequent processing, it may be a good idea to employ a feature representation that includes information about certainty or confidence. This enables a new feature descriptor to be computed from several descriptors, for example, computed at the same image point but at different scales, or from different but neighboring points, in terms of a weighted average where the weights are derived from the corresponding certainties. In the simplest case, the corresponding computation can be implemented as a low-pass filtering of the featured image. The resulting feature image will, in general, be more stable to noise. In addition to having certainty measures included in the representation, the representation of the corresponding feature values may itself be suitable for an averaging operation or not. Most feature representations can be averaged in practice, but only in certain cases can the resulting descriptor be given a correct interpretation in terms of a feature value. Such representations are referred to as averageable. For example, if the orientation of an edge is represented in terms of an angle, this representation must have a discontinuity where the angle wraps from its maximal value to its minimal value. Consequently, it can happen that two similar orientations are represented by angles that have a mean that does not lie close to either of the original angles and, hence, this representation is not averageable. There are other representations of edge orientation, such as the structure tensor, which are averageable. Another example relates to motion, where in some cases only the normal velocity relative to some edge can be extracted. If two such features have been extracted and they can be assumed to refer to same true velocity, this velocity is not given as the average of the normal velocity vectors. Hence, normal velocity vectors are not averageable. Instead, there are other representations of motions, using matrices or tensors, that give the true velocity in terms of an average operation of the normal velocity descriptors. Features detected in each image can be matched across multiple images to establish corresponding features such as corresponding points. The algorithm is based on comparing and analyzing point correspondences between the reference image and the target image. If any part of the cluttered scene shares correspondences greater than the threshold, that part of the cluttered scene image is targeted and considered to include the reference object there.",
      "sentences": [
        "In computer vision and image processing, a feature is a piece of information about the content of an image; typically about whether a certain region of the image has certain properties.",
        "Features may be specific structures in the image such as points, edges or objects.",
        "Features may also be the result of a general neighborhood operation or feature detection applied to the image.",
        "Other examples of features are related to motion in image sequences, or to shapes defined in terms of curves or boundaries between different image regions.",
        "More broadly a feature is any piece of information that is relevant for solving the computational task related to a certain application.",
        "This is the same sense as feature in machine learning and pattern recognition generally, though image processing has a very sophisticated collection of features.",
        "The feature concept is very general and the choice of features in a particular computer vision system may be highly dependent on the specific problem at hand.",
        "There is no universal or exact definition of what constitutes a feature, and the exact definition often depends on the problem or the type of application.",
        "Nevertheless, a feature is typically defined as an \"interesting\" part of an image, and features are used as a starting point for many computer vision algorithms.",
        "Since features are used as the starting point and main primitives for subsequent algorithms, the overall algorithm will often only be as good as its feature detector.",
        "Consequently, the desirable property for a feature detector is repeatability: whether or not the same feature will be detected in two or more different images of the same scene.",
        "Feature detection is a low-level image processing operation.",
        "That is, it is usually performed as the first operation on an image and examines every pixel to see if there is a feature present at that pixel.",
        "If this is part of a larger algorithm, then the algorithm will typically only examine the image in the region of the features.",
        "As a built-in pre-requisite to feature detection, the input image is usually smoothed by a Gaussian kernel in a scale-space representation and one or several feature images are computed, often expressed in terms of local image derivative operations.",
        "Occasionally, when feature detection is computationally expensive and there are time constraints, a higher-level algorithm may be used to guide the feature detection stage so that only certain parts of the image are searched for features.",
        "There are many computer vision algorithms that use feature detection as the initial step, so as a result, a very large number of feature detectors have been developed.",
        "These vary widely in the kinds of feature detected, the computational complexity and the repeatability.",
        "When features are defined in terms of local neighborhood operations applied to an image, a procedure commonly referred to as feature extraction, one can distinguish between feature detection approaches that produce local decisions whether there is a feature of a given type at a given image point or not, and those who produce non-binary data as result.",
        "The distinction becomes relevant when the resulting detected features are relatively sparse.",
        "Although local decisions are made, the output from a feature detection step does not need to be a binary image.",
        "The result is often represented in terms of sets of (connected or unconnected) coordinates of the image points where features have been detected, sometimes with subpixel accuracy.",
        "When feature extraction is done without local decision making, the result is often referred to as a feature image.",
        "Consequently, a feature image can be seen as an image in the sense that it is a function of the same spatial (or temporal) variables as the original image, but where the pixel values hold information about image features instead of intensity or color.",
        "This means that a feature image can be processed in a similar way as an ordinary image generated by an image sensor.",
        "Feature images are also often computed as integrated step in algorithms for feature detection.",
        "In some applications, it is not sufficient to extract only one type of feature to obtain the relevant information from the image data.",
        "Instead, two or more different features are extracted, resulting in two or more feature descriptors at each image point.",
        "A common practice is to organize the information provided by all these descriptors as the elements of one single vector, commonly referred to as a feature vector.",
        "The set of all possible feature vectors constitutes a feature space.",
        "A common example of feature vectors appears when each image point is to be classified as belonging to a specific class.",
        "Assuming that each image point has a corresponding feature vector based on a suitable set of features, meaning that each class is well separated in the corresponding feature space, the classification of each image point can be done using standard classification method.",
        "Another and related example occurs when neural network-based processing is applied to images.",
        "The input data fed to the neural network is often given in terms of a feature vector from each image point, where the vector is constructed from several different features extracted from the image data.",
        "During a learning phase, the network can itself find which combinations of different features are useful for solving the problem at hand.",
        "Edges are points where there is a boundary (or an edge) between two image regions.",
        "In general, an edge can be of almost arbitrary shape, and may include junctions.",
        "In practice, edges are usually defined as sets of points in the image that have a strong gradient magnitude.",
        "Furthermore, some common algorithms will then chain high gradient points together to form a more complete description of an edge.",
        "These algorithms usually place some constraints on the properties of an edge, such as shape, smoothness, and gradient value.",
        "Locally, edges have a one-dimensional structure.",
        "The terms corners and interest points are used somewhat interchangeably and refer to point-like features in an image, which have a local two-dimensional structure.",
        "The name \"Corner\" arose since early algorithms first performed edge detection, and then analyzed the edges to find rapid changes in direction (corners).",
        "These algorithms were then developed so that explicit edge detection was no longer required, for instance by looking for high levels of curvature in the image gradient.",
        "It was then noticed that the so-called corners were also being detected on parts of the image that were not corners in the traditional sense (for instance a small bright spot on a dark background may be detected).",
        "These points are frequently known as interest points, but the term \"corner\" is used by tradition.",
        "Blobs provide a complementary description of image structures in terms of regions, as opposed to corners that are more point-like.",
        "Nevertheless, blob descriptors may often contain a preferred point (a local maximum of an operator response or a center of gravity) which means that many blob detectors may also be regarded as interest point operators.",
        "Blob detectors can detect areas in an image that are too smooth to be detected by a corner detector.",
        "Consider shrinking an image and then performing corner detection.",
        "The detector will respond to points that are sharp in the shrunk image, but may be smooth in the original image.",
        "It is at this point that the difference between a corner detector and a blob detector becomes somewhat vague.",
        "To a large extent, this distinction can be remedied by including an appropriate notion of scale.",
        "Nevertheless, due to their response properties to different types of image structures at different scales, the LoG and DoH blob detectors are also mentioned in the article on corner detection.",
        "For elongated objects, the notion of ridges is a natural tool.",
        "A ridge descriptor computed from a grey-level image can be seen as a generalization of a medial axis.",
        "From a practical viewpoint, a ridge can be thought of as a one-dimensional curve that represents an axis of symmetry, and in addition has an attribute of local ridge width associated with each ridge point.",
        "Unfortunately, however, it is algorithmically harder to extract ridge features from general classes of grey-level images than edge-, corner- or blob features.",
        "Nevertheless, ridge descriptors are frequently used for road extraction in aerial images and for extracting blood vessels in medical images-see ridge detection.",
        "Feature detection includes methods for computing abstractions of image information and making local decisions at every image point whether there is an image feature of a given type at that point or not.",
        "The resulting features will be subsets of the image domain, often in the form of isolated points, continuous curves or connected regions.",
        "The extraction of features are sometimes made over several scalings.",
        "One of these methods is the scale-invariant feature transform (SIFT).",
        "Once features have been detected, a local image patch around the feature can be extracted.",
        "This extraction may involve quite considerable amounts of image processing.",
        "The result is known as a feature descriptor or feature vector.",
        "Among the approaches that are used to feature description, one can mention N-jets and local histograms (see scale-invariant feature transform for one example of a local histogram descriptor).",
        "In addition to such attribute information, the feature detection step by itself may also provide complementary attributes, such as the edge orientation and gradient magnitude in edge detection and the polarity and the strength of the blob in blob detection.",
        "Edge detection Corner detection Blob detection Ridge detection Scale-invariant feature transform Edge direction, changing intensity, autocorrelation.",
        "Motion detection.",
        "Area based, differential approach.",
        "Optical flow.",
        "For example, an edge can be represented as a Boolean variable in each image point that describes whether an edge is present at that point.",
        "Alternatively, we can instead use a representation that provides a certainty measure instead of a Boolean statement of the edge's existence and combine this with information about the orientation of the edge.",
        "Similarly, the color of a specific region can either be represented in terms of the average color (three scalars) or a color histogram (three functions).",
        "When a computer vision system or computer vision algorithm is designed the choice of feature representation can be a critical issue.",
        "In some cases, a higher level of detail in the description of a feature may be necessary for solving the problem, but this comes at the cost of having to deal with more data and more demanding processing.",
        "Below, some of the factors which are relevant for choosing a suitable representation are discussed.",
        "In this discussion, an instance of a feature representation is referred to as a feature descriptor, or simply descriptor.",
        "Two examples of image features are local edge orientation and local velocity in an image sequence.",
        "In the case of orientation, the value of this feature may be more or less undefined if more than one edge are present in the corresponding neighborhood.",
        "Local velocity is undefined if the corresponding image region does not contain any spatial variation.",
        "As a consequence of this observation, it may be relevant to use a feature representation that includes a measure of certainty or confidence related to the statement about the feature value.",
        "Otherwise, it is a typical situation that the same descriptor is used to represent feature values of low certainty and feature values close to zero, with a resulting ambiguity in the interpretation of this descriptor.",
        "Depending on the application, such an ambiguity may or may not be acceptable.",
        "In particular, if a featured image will be used in subsequent processing, it may be a good idea to employ a feature representation that includes information about certainty or confidence.",
        "This enables a new feature descriptor to be computed from several descriptors, for example, computed at the same image point but at different scales, or from different but neighboring points, in terms of a weighted average where the weights are derived from the corresponding certainties.",
        "In the simplest case, the corresponding computation can be implemented as a low-pass filtering of the featured image.",
        "The resulting feature image will, in general, be more stable to noise.",
        "In addition to having certainty measures included in the representation, the representation of the corresponding feature values may itself be suitable for an averaging operation or not.",
        "Most feature representations can be averaged in practice, but only in certain cases can the resulting descriptor be given a correct interpretation in terms of a feature value.",
        "Such representations are referred to as averageable.",
        "For example, if the orientation of an edge is represented in terms of an angle, this representation must have a discontinuity where the angle wraps from its maximal value to its minimal value.",
        "Consequently, it can happen that two similar orientations are represented by angles that have a mean that does not lie close to either of the original angles and, hence, this representation is not averageable.",
        "There are other representations of edge orientation, such as the structure tensor, which are averageable.",
        "Another example relates to motion, where in some cases only the normal velocity relative to some edge can be extracted.",
        "If two such features have been extracted and they can be assumed to refer to same true velocity, this velocity is not given as the average of the normal velocity vectors.",
        "Hence, normal velocity vectors are not averageable.",
        "Instead, there are other representations of motions, using matrices or tensors, that give the true velocity in terms of an average operation of the normal velocity descriptors.",
        "Features detected in each image can be matched across multiple images to establish corresponding features such as corresponding points.",
        "The algorithm is based on comparing and analyzing point correspondences between the reference image and the target image.",
        "If any part of the cluttered scene shares correspondences greater than the threshold, that part of the cluttered scene image is targeted and considered to include the reference object there."
      ],
      "metadata": {
        "title": "Feature (computer vision)",
        "url": "https://en.wikipedia.org/wiki/Feature_(computer_vision)",
        "word_count": 2342,
        "char_count": 14746,
        "sentence_count": 102,
        "scraped_at": "2025-08-09T14:46:57.517936",
        "language": "en",
        "processing_time": 0.004740238189697266,
        "source_hash": "1f8d2051d2b5af61de6c479a13d1eeb6"
      }
    },
    {
      "title": "Homography (computer vision)",
      "url": "https://en.wikipedia.org/wiki/Homography_(computer_vision)",
      "raw_text": "In the field of computer vision, any two images of the same planar surface in space are related by a homography (assuming a pinhole camera model).  This has many practical applications, such as image rectification, image registration, or camera motion—rotation and translation—between two images.   Once camera resectioning has been done from an estimated homography matrix, this information may be used for navigation, or to insert models of 3D objects into an image or video, so that they are rendered with the correct perspective and appear to have been part of the original scene (see Augmented reality).\n\n\n== 3D plane to plane equation ==\nWe have two cameras a and b, looking at points \n  \n    \n      \n        \n          P\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle P_{i}}\n  \n in a plane.\nPassing from the projection \n  \n    \n      \n        \n          \n\n          \n          \n            b\n          \n        \n        \n          p\n          \n            i\n          \n        \n        =\n        \n          (\n          \n            \n              \n\n              \n              \n                b\n              \n            \n            \n              u\n              \n                i\n              \n            \n            ;\n            \n              \n\n              \n              \n                b\n              \n            \n            \n              v\n              \n                i\n              \n            \n            ;\n            1\n          \n          )\n        \n      \n    \n    {\\displaystyle {}^{b}p_{i}=\\left({}^{b}u_{i};{}^{b}v_{i};1\\right)}\n  \n of \n  \n    \n      \n        \n          P\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle P_{i}}\n  \n in b to the projection \n  \n    \n      \n        \n          \n\n          \n          \n            a\n          \n        \n        \n          p\n          \n            i\n          \n        \n        =\n        \n          (\n          \n            \n              \n\n              \n              \n                a\n              \n            \n            \n              u\n              \n                i\n              \n            \n            ;\n            \n              \n\n              \n              \n                a\n              \n            \n            \n              v\n              \n                i\n              \n            \n            ;\n            1\n          \n          )\n        \n      \n    \n    {\\displaystyle {}^{a}p_{i}=\\left({}^{a}u_{i};{}^{a}v_{i};1\\right)}\n  \n of \n  \n    \n      \n        \n          P\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle P_{i}}\n  \n in a:\n\n  \n    \n      \n        \n          \n\n          \n          \n            a\n          \n        \n        \n          p\n          \n            i\n          \n        \n        =\n        \n          \n            \n              \n                \n\n                \n                \n                  b\n                \n              \n              \n                z\n                \n                  i\n                \n              \n            \n            \n              \n                \n\n                \n                \n                  a\n                \n              \n              \n                z\n                \n                  i\n                \n              \n            \n          \n        \n        \n          K\n          \n            a\n          \n        \n        ⋅\n        \n          H\n          \n            a\n            b\n          \n        \n        ⋅\n        \n          K\n          \n            b\n          \n          \n            −\n            1\n          \n        \n        ⋅\n        \n          \n\n          \n          \n            b\n          \n        \n        \n          p\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {}^{a}p_{i}={\\frac {{}^{b}z_{i}}{{}^{a}z_{i}}}K_{a}\\cdot H_{ab}\\cdot K_{b}^{-1}\\cdot {}^{b}p_{i}}\n  \n\nwhere \n  \n    \n      \n        \n          \n\n          \n          \n            a\n          \n        \n        \n          z\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {}^{a}z_{i}}\n  \n and \n  \n    \n      \n        \n          \n\n          \n          \n            b\n          \n        \n        \n          z\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {}^{b}z_{i}}\n  \n are the z coordinates of P in each camera frame and where the homography matrix \n  \n    \n      \n        \n          H\n          \n            a\n            b\n          \n        \n      \n    \n    {\\displaystyle H_{ab}}\n  \n is given by\n\n  \n    \n      \n        \n          H\n          \n            a\n            b\n          \n        \n        =\n        R\n        −\n        \n          \n            \n              t\n              \n                n\n                \n                  T\n                \n              \n            \n            d\n          \n        \n      \n    \n    {\\displaystyle H_{ab}=R-{\\frac {tn^{T}}{d}}}\n  \n.\n\n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n is the rotation matrix by which b is rotated in relation to a; t is the translation vector from a to b; n and d are the normal vector of the plane and the distance from origin to the plane respectively.\nKa and Kb are the cameras' intrinsic parameter matrices. \n\nThe figure shows camera b looking at the plane at distance d.\nNote: From above figure, assuming \n  \n    \n      \n        \n          n\n          \n            T\n          \n        \n        \n          P\n          \n            i\n          \n        \n        +\n        d\n        =\n        0\n      \n    \n    {\\displaystyle n^{T}P_{i}+d=0}\n  \n as plane model, \n  \n    \n      \n        \n          n\n          \n            T\n          \n        \n        \n          P\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n^{T}P_{i}}\n  \n is the projection of vector \n  \n    \n      \n        \n          P\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle P_{i}}\n  \n along \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n, and equal to \n  \n    \n      \n        −\n        d\n      \n    \n    {\\displaystyle -d}\n  \n. So \n  \n    \n      \n        t\n        =\n        t\n        ⋅\n        1\n        =\n        t\n        \n          (\n          \n            −\n            \n              \n                \n                  \n                    n\n                    \n                      T\n                    \n                  \n                  \n                    P\n                    \n                      i\n                    \n                  \n                \n                d\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle t=t\\cdot 1=t\\left(-{\\frac {n^{T}P_{i}}{d}}\\right)}\n  \n. And we have \n  \n    \n      \n        \n          H\n          \n            a\n            b\n          \n        \n        \n          P\n          \n            i\n          \n        \n        =\n        R\n        \n          P\n          \n            i\n          \n        \n        +\n        t\n      \n    \n    {\\displaystyle H_{ab}P_{i}=RP_{i}+t}\n  \n where \n  \n    \n      \n        \n          H\n          \n            a\n            b\n          \n        \n        =\n        R\n        −\n        \n          \n            \n              t\n              \n                n\n                \n                  T\n                \n              \n            \n            d\n          \n        \n      \n    \n    {\\displaystyle H_{ab}=R-{\\frac {tn^{T}}{d}}}\n  \n.\nThis formula is only valid if camera b has no rotation and no translation. In the general case where \n  \n    \n      \n        \n          R\n          \n            a\n          \n        \n        ,\n        \n          R\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle R_{a},R_{b}}\n  \n and \n  \n    \n      \n        \n          t\n          \n            a\n          \n        \n        ,\n        \n          t\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle t_{a},t_{b}}\n  \n are the respective rotations and translations of camera a and b, \n  \n    \n      \n        R\n        =\n        \n          R\n          \n            a\n          \n        \n        \n          R\n          \n            b\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle R=R_{a}R_{b}^{T}}\n  \n and the homography matrix \n  \n    \n      \n        \n          H\n          \n            a\n            b\n          \n        \n      \n    \n    {\\displaystyle H_{ab}}\n  \n becomes\n\n  \n    \n      \n        \n          H\n          \n            a\n            b\n          \n        \n        =\n        \n          R\n          \n            a\n          \n        \n        \n          R\n          \n            b\n          \n          \n            T\n          \n        \n        −\n        \n          \n            \n              (\n              −\n              \n                R\n                \n                  a\n                \n              \n              ∗\n              \n                R\n                \n                  b\n                \n                \n                  T\n                \n              \n              ∗\n              \n                t\n                \n                  b\n                \n              \n              +\n              \n                t\n                \n                  a\n                \n              \n              )\n              \n                n\n                \n                  T\n                \n              \n            \n            d\n          \n        \n      \n    \n    {\\displaystyle H_{ab}=R_{a}R_{b}^{T}-{\\frac {(-R_{a}*R_{b}^{T}*t_{b}+t_{a})n^{T}}{d}}}\n  \n\nwhere d is the distance of the camera b to the plane.\n\n\n== Affine homography ==\nWhen the image region in which the homography is computed is small or the image has been acquired with a large focal length, an affine homography is a more appropriate model of image displacements. An affine homography is a special type of a general homography whose last row is fixed to  \n\n  \n    \n      \n        \n          h\n          \n            31\n          \n        \n        =\n        \n          h\n          \n            32\n          \n        \n        =\n        0\n        ,\n        \n        \n          h\n          \n            33\n          \n        \n        =\n        1.\n      \n    \n    {\\displaystyle h_{31}=h_{32}=0,\\;h_{33}=1.}\n  \n\n\n== See also ==\nDirect linear transformation\nEpipolar geometry\nFeature (computer vision)\nFundamental matrix (computer vision)\nPose (computer vision)\nPhotogrammetry\n\n\n== References ==\n\nO. Chum and T. Pajdla and P. Sturm (2005). \"The Geometric Error for Homographies\" (PDF). Computer Vision and Image Understanding. 97 (1): 86–102. doi:10.1016/j.cviu.2004.03.004.\n\n\n== Toolboxes ==\nhomest is a GPL C/C++ library for robust, non-linear (based on the Levenberg–Marquardt algorithm) homography estimation from matched point pairs (Manolis Lourakis).\nOpenCV is a complete (open and free) computer vision software library that has many routines related to homography estimation (cvFindHomography) and re-projection (cvPerspectiveTransform).\n\n\n== External links ==\nSerge Belongie & David Kriegman (2007) Explanation of Homography Estimation from Department of Computer Science and Engineering, University of California, San Diego.\nA. Criminisi, I. Reid & A. Zisserman (1997) \"A Plane Measuring Device\", §3 Computing the Plane to Plane Homography, from Visual Geometry Group, Department of Engineering Science, University of Oxford.\nElan Dubrofsky (2009) Homography Estimation, Master's thesis, from Department of Computer Science, University of British Columbia.\nRichard Hartley & Andrew Zisserman (2004) Multiple View Geometry from Visual Geometry Group, Oxford. Includes Matlab Functions for calculating a homography and the fundamental matrix (computer vision).\nGIMP Tutorial – using the Perspective Tool by Billy Kerr on YouTube. Shows how to do a perspective transform using GIMP.\nAllan Jepson (2010) Planar Homographies from Department of Computer Science, University of Toronto. Includes 2D homography from four pairs of corresponding points, mosaics in image processing, removing perspective distortion in computer vision, rendering textures in computer graphics, and computing planar shadows.\nPlane transfer homography Course notes from CSE576 at University of Washington in Seattle.\nEtienne Vincent & Robert Laganiere (2000) Detecting Planar Homographies in an Image Pair Archived 2016-03-04 at the Wayback Machine from School of Information Technology and Engineering,  University of Ottawa. Describes an algorithm for detecting planes in images, uses random sample consensus (RANSAC) method, describes heuristics and iteration.",
      "cleaned_text": "In the field of computer vision, any two images of the same planar surface in space are related by a homography (assuming a pinhole camera model). This has many practical applications, such as image rectification, image registration, or camera motion-rotation and translation-between two images. Once camera resectioning has been done from an estimated homography matrix, this information may be used for navigation, or to insert models of 3D objects into an image or video, so that they are rendered with the correct perspective and appear to have been part of the original scene (see Augmented reality). We have two cameras a and b, looking at points P i {\\displaystyle P_{i}} in a plane. Passing from the projection b p i = ( b u i ; b v i ; 1 ) {\\displaystyle {}^{b}p_{i}=\\left({}^{b}u_{i};{}^{b}v_{i};1\\right)} of P i {\\displaystyle P_{i}} in b to the projection a p i = ( a u i ; a v i ; 1 ) {\\displaystyle {}^{a}p_{i}=\\left({}^{a}u_{i};{}^{a}v_{i};1\\right)} of P i {\\displaystyle P_{i}} in a: a p i = b z i a z i K a ⋅ H a b ⋅ K b − 1 ⋅ b p i {\\displaystyle {}^{a}p_{i}={\\frac {{}^{b}z_{i}}{{}^{a}z_{i}}}K_{a}\\cdot H_{ab}\\cdot K_{b}^{-1}\\cdot {}^{b}p_{i}} where a z i {\\displaystyle {}^{a}z_{i}} and b z i {\\displaystyle {}^{b}z_{i}} are the z coordinates of P in each camera frame and where the homography matrix H a b {\\displaystyle H_{ab}} is given by H a b = R − t n T d {\\displaystyle H_{ab}=R-{\\frac {tn^{T}}{d}}} . R {\\displaystyle R} is the rotation matrix by which b is rotated in relation to a; t is the translation vector from a to b; n and d are the normal vector of the plane and the distance from origin to the plane respectively. Ka and Kb are the cameras' intrinsic parameter matrices. The figure shows camera b looking at the plane at distance d. Note: From above figure, assuming n T P i + d = 0 {\\displaystyle n^{T}P_{i}+d=0} as plane model, n T P i {\\displaystyle n^{T}P_{i}} is the projection of vector P i {\\displaystyle P_{i}} along n {\\displaystyle n} , and equal to − d {\\displaystyle -d} . So t = t ⋅ 1 = t ( − n T P i d ) {\\displaystyle t=t\\cdot 1=t\\left(-{\\frac {n^{T}P_{i}}{d}}\\right)} . And we have H a b P i = R P i + t {\\displaystyle H_{ab}P_{i}=RP_{i}+t} where H a b = R − t n T d {\\displaystyle H_{ab}=R-{\\frac {tn^{T}}{d}}} . This formula is only valid if camera b has no rotation and no translation. In the general case where R a , R b {\\displaystyle R_{a},R_{b}} and t a , t b {\\displaystyle t_{a},t_{b}} are the respective rotations and translations of camera a and b, R = R a R b T {\\displaystyle R=R_{a}R_{b}^{T}} and the homography matrix H a b {\\displaystyle H_{ab}} becomes H a b = R a R b T − ( − R a ∗ R b T ∗ t b + t a ) n T d {\\displaystyle H_{ab}=R_{a}R_{b}^{T}-{\\frac {(-R_{a}*R_{b}^{T}*t_{b}+t_{a})n^{T}}{d}}} where d is the distance of the camera b to the plane. When the image region in which the homography is computed is small or the image has been acquired with a large focal length, an affine homography is a more appropriate model of image displacements. An affine homography is a special type of a general homography whose last row is fixed to h 31 = h 32 = 0 , h 33 = 1. {\\displaystyle h_{31}=h_{32}=0,\\;h_{33}=1.}",
      "sentences": [
        "In the field of computer vision, any two images of the same planar surface in space are related by a homography (assuming a pinhole camera model).",
        "This has many practical applications, such as image rectification, image registration, or camera motion-rotation and translation-between two images.",
        "Once camera resectioning has been done from an estimated homography matrix, this information may be used for navigation, or to insert models of 3D objects into an image or video, so that they are rendered with the correct perspective and appear to have been part of the original scene (see Augmented reality).",
        "We have two cameras a and b, looking at points P i {\\displaystyle P_{i}} in a plane.",
        "R {\\displaystyle R} is the rotation matrix by which b is rotated in relation to a; t is the translation vector from a to b; n and d are the normal vector of the plane and the distance from origin to the plane respectively.",
        "Ka and Kb are the cameras' intrinsic parameter matrices.",
        "The figure shows camera b looking at the plane at distance d. Note: From above figure, assuming n T P i + d = 0 {\\displaystyle n^{T}P_{i}+d=0} as plane model, n T P i {\\displaystyle n^{T}P_{i}} is the projection of vector P i {\\displaystyle P_{i}} along n {\\displaystyle n} , and equal to − d {\\displaystyle -d} .",
        "So t = t ⋅ 1 = t ( − n T P i d ) {\\displaystyle t=t\\cdot 1=t\\left(-{\\frac {n^{T}P_{i}}{d}}\\right)} .",
        "And we have H a b P i = R P i + t {\\displaystyle H_{ab}P_{i}=RP_{i}+t} where H a b = R − t n T d {\\displaystyle H_{ab}=R-{\\frac {tn^{T}}{d}}} .",
        "This formula is only valid if camera b has no rotation and no translation.",
        "In the general case where R a , R b {\\displaystyle R_{a},R_{b}} and t a , t b {\\displaystyle t_{a},t_{b}} are the respective rotations and translations of camera a and b, R = R a R b T {\\displaystyle R=R_{a}R_{b}^{T}} and the homography matrix H a b {\\displaystyle H_{ab}} becomes H a b = R a R b T − ( − R a ∗ R b T ∗ t b + t a ) n T d {\\displaystyle H_{ab}=R_{a}R_{b}^{T}-{\\frac {(-R_{a}*R_{b}^{T}*t_{b}+t_{a})n^{T}}{d}}} where d is the distance of the camera b to the plane.",
        "When the image region in which the homography is computed is small or the image has been acquired with a large focal length, an affine homography is a more appropriate model of image displacements.",
        "An affine homography is a special type of a general homography whose last row is fixed to h 31 = h 32 = 0 , h 33 = 1."
      ],
      "metadata": {
        "title": "Homography (computer vision)",
        "url": "https://en.wikipedia.org/wiki/Homography_(computer_vision)",
        "word_count": 583,
        "char_count": 3180,
        "sentence_count": 13,
        "scraped_at": "2025-08-09T14:46:57.523522",
        "language": "en",
        "processing_time": 0.005406856536865234,
        "source_hash": "13cf620e1a31cf42e8f1c1c848236fbc"
      }
    },
    {
      "title": "Computer vision dazzle",
      "url": "https://en.wikipedia.org/wiki/Computer_vision_dazzle",
      "raw_text": "Computer vision dazzle, also known as CV dazzle, dazzle makeup, or anti-surveillance makeup, is a type of camouflage used to hamper facial recognition software, inspired by dazzle camouflage used by vehicles such as ships and planes. \n\n\n== Methods ==\nCV dazzle combines stylized makeup, asymmetric hair, and sometimes infrared lights built in to glasses or clothing to break up detectable facial patterns recognized by computer vision algorithms in much the same way that warships contrasted color and used sloping lines and curves to distort the structure of a vessel. \nIt has been shown to be somewhat successful at defeating face detection software in common use, including that employed by Facebook. CV dazzle attempts to block detection by facial recognition technologies such as DeepFace \"by creating an 'anti-face'\". It uses occlusion, covering certain facial features; transformation, altering the shape or colour of parts of the face; and a combination of the two. Prominent artists employing this technique include Adam Harvey and Jillian Mayer.\n\n\n== Use in protests ==\nComputer vision dazzle makeup has been used by rioters in several different protest movements. Its use as a protesting aid has often been found ineffective. It may be effective to thwart computer technology, but draws human attention, is easy for human monitors to spot on security cameras, and makes it hard for rioters to blend in within a crowd. Advances in facial recognition technology make dazzle makeup increasingly ineffective.\n\n\n== See also ==\nAdversarial machine learning\n\n\n== References ==",
      "cleaned_text": "Computer vision dazzle, also known as CV dazzle, dazzle makeup, or anti-surveillance makeup, is a type of camouflage used to hamper facial recognition software, inspired by dazzle camouflage used by vehicles such as ships and planes. CV dazzle combines stylized makeup, asymmetric hair, and sometimes infrared lights built in to glasses or clothing to break up detectable facial patterns recognized by computer vision algorithms in much the same way that warships contrasted color and used sloping lines and curves to distort the structure of a vessel. It has been shown to be somewhat successful at defeating face detection software in common use, including that employed by Facebook. CV dazzle attempts to block detection by facial recognition technologies such as DeepFace \"by creating an 'anti-face'\". It uses occlusion, covering certain facial features; transformation, altering the shape or colour of parts of the face; and a combination of the two. Prominent artists employing this technique include Adam Harvey and Jillian Mayer. Computer vision dazzle makeup has been used by rioters in several different protest movements. Its use as a protesting aid has often been found ineffective. It may be effective to thwart computer technology, but draws human attention, is easy for human monitors to spot on security cameras, and makes it hard for rioters to blend in within a crowd. Advances in facial recognition technology make dazzle makeup increasingly ineffective.",
      "sentences": [
        "Computer vision dazzle, also known as CV dazzle, dazzle makeup, or anti-surveillance makeup, is a type of camouflage used to hamper facial recognition software, inspired by dazzle camouflage used by vehicles such as ships and planes.",
        "CV dazzle combines stylized makeup, asymmetric hair, and sometimes infrared lights built in to glasses or clothing to break up detectable facial patterns recognized by computer vision algorithms in much the same way that warships contrasted color and used sloping lines and curves to distort the structure of a vessel.",
        "It has been shown to be somewhat successful at defeating face detection software in common use, including that employed by Facebook.",
        "CV dazzle attempts to block detection by facial recognition technologies such as DeepFace \"by creating an 'anti-face'\".",
        "It uses occlusion, covering certain facial features; transformation, altering the shape or colour of parts of the face; and a combination of the two.",
        "Prominent artists employing this technique include Adam Harvey and Jillian Mayer.",
        "Computer vision dazzle makeup has been used by rioters in several different protest movements.",
        "Its use as a protesting aid has often been found ineffective.",
        "It may be effective to thwart computer technology, but draws human attention, is easy for human monitors to spot on security cameras, and makes it hard for rioters to blend in within a crowd.",
        "Advances in facial recognition technology make dazzle makeup increasingly ineffective."
      ],
      "metadata": {
        "title": "Computer vision dazzle",
        "url": "https://en.wikipedia.org/wiki/Computer_vision_dazzle",
        "word_count": 228,
        "char_count": 1473,
        "sentence_count": 10,
        "scraped_at": "2025-08-09T14:46:57.524250",
        "language": "en",
        "processing_time": 0.0005197525024414062,
        "source_hash": "28ff3c23a6471f95fc68fcb7e22869d1"
      }
    },
    {
      "title": "Computer vision syndrome",
      "url": "https://en.wikipedia.org/wiki/Computer_vision_syndrome",
      "raw_text": "Computer vision syndrome (CVS) is a condition resulting from focusing the eyes on a computer or other display device for protracted, uninterrupted periods of time and the eye's muscles being unable to recover from the constant tension required to maintain focus on a close object. \n\n\n== Symptoms ==\nSome symptoms of CVS include headaches, blurred vision, neck pain, fatigue, eye strain, dry eyes, irritated eyes, double vision, vertigo/dizziness, polyopia, and difficulty refocusing the eyes.  These symptoms can be further aggravated by improper lighting conditions (i.e. glare, strong blue-spectrum backlights, or bright overhead lighting) or air moving past the eyes (e.g. overhead vents, or direct air from a fan).\n\n\n== Therapy ==\nAsthenopic (eye strain) symptoms in the eye are responsible for much of the severity in CVS. Proper rest to the eye and its muscles is recommended to relieve the associated eye strain. Observations from persons experiencing chronic eye strain have shown that most people who claim to be getting enough sleep are actually not. This, unaware to them, causes the eye strain to build up over a period of time, when if they had obtained seven to eight hours of uninterrupted sleep, their eye muscles would have recovered during the sleep and the strain would not have built up.\nComputer workers are often advised to take breaks and look at distant objects. A routinely recommended approach is to consciously blink the eyes every now and then (this helps replenish the tear film) and to look out the window to a distant object or to the sky—doing so provides rest to the ciliary muscles. One of the catch phrases is the \"20–20–20 rule\": every 20 minutes, focus the eyes on an object 20 feet (6 meters) away for 20 seconds. This basically gives a convenient distance and timeframe for a person to follow the advice from the optometrist and ophthalmologist.\nA number of computer and smartphone applications adjust the computer video color temperature, reducing the amount of blue light emitted by the screen, particularly at night.\nDry eye is a symptom that is targeted in the therapy of CVS. The use of over-the-counter artificial-tear solutions can reduce the effects of dry eye in CVS. Prior to using artificial tear solutions, it is necessary to check if dry eye is the actual cause of the problem (measured by a tear meniscus test) or whether there are no actual symptoms of dry eye at all.\nDry eyes because of CVS can also be treated using moisture chamber glasses or humidifier machines. Office spaces with artificially dry air can worsen CVS syndromes, in which case, a desktop or a room humidifier can help the eyes keep a healthy moisture level.\nAt night, CVS can become worse. It is recommended to use a dark user interface while working at night on the computer. Several browser and OS settings or add-ons exist to darken the user interface.\nA 2017 randomized controlled trial evaluated macular carotenoid supplements (lutein, zeaxanthin, and mesozeaxanthin) in people with high screen time usage. The supplement group had statistically significant reduction in self-reported headache, eye strain, eye fatigue and sleep complaints, but no reduction in neck strain or blurry vision.\nA 2021 review investigated suggested therapies for CVS and found little supporting evidence for the following: switching to bi- or multi-focal glasses to reduce eye strain, or using glasses that block blue light. The same review reported \"low-certainty\" in omega-3 supplements as a method to combat CVS.\n\n\n=== Eyeglasses ===\nDecreased focusing capability is mitigated by wearing a small plus-powered (+1.00 to +1.50) over-the-counter pair of eyeglasses. Wearing these eyeglasses helps such patients regain their ability to focus on near objects. People who are engaged in other occupations—such as tailors engaged in embroidery—can experience similar symptoms and can be helped by these glasses.\nA Pacific University research study of 36 participants found significant differences in irritation or burning of the eyes, tearing, or watery eyes, dry eyes, and tired eyes, that were each improved by amber colored lenses versus placebo lenses, but in a follow-up study in 2008, the same team was not able to reproduce the results of the first study.\nA study sponsored by the lens industry has shown blue light-filtering lenses decrease specific aspects of light emissions. Theoretical reductions in phototoxicity were 10.6% to 23.6%. Additionally, melatonin suppression was reduced by 5.8% to 15.0% and scotopic sensitivity by 2.4% to 9.6%. Over 70% of the participants in this testing were unable to detect these changes. The expansion of technology has led to more individuals utilizing computers and televisions which increase the overall exposure to blue light. Double-blind trials however, have shown no evidence to support the use of blue light filtering lenses for digital eye strain caused by blue light from electronic screens.\nAmber-tinted lenses have been shown to affect the circadian rhythm and treat delayed sleep phase disorder.\n\n\n== Prevalence ==\nAccording to the US National Institute for Occupational Safety and Health, computer vision syndrome affects about 90% of the people who spend three hours or more a day at a computer.\nAnother study in Malaysia was conducted on 795 university students aged between 18 and 25. The students experienced headaches along with eyestrain, with 89.9% of the students surveyed feeling any type of symptom of CVS.\n\n\n== See also ==\nAsthenopia (eye strain)\nComputer-induced medical problems\nEffects of blue light technology\nElectronic media and sleep\nList of repetitive strain injury software (break reminders)\nOcular neurosis\nPhotophobia\nRepetitive strain injury\nPresbyopia\nVisual looming syndrome\nVisual snow\n\n\n== References ==\n\n\n== External links ==\n\n\"Computer Vision Syndrome (CVS)\". American Optometric Association. 2017.\nYan, Zheng; Hu, Liang; Chen, Hao; Lu, Han (September 2008). \"Computer Vision Syndrome: A widely spreading but largely unknown epidemic among users\". Computers in Human Behavior. 24 (5): 2026–2042. doi:10.1016/j.chb.2007.09.004.",
      "cleaned_text": "Computer vision syndrome (CVS) is a condition resulting from focusing the eyes on a computer or other display device for protracted, uninterrupted periods of time and the eye's muscles being unable to recover from the constant tension required to maintain focus on a close object. Some symptoms of CVS include headaches, blurred vision, neck pain, fatigue, eye strain, dry eyes, irritated eyes, double vision, vertigo/dizziness, polyopia, and difficulty refocusing the eyes. These symptoms can be further aggravated by improper lighting conditions (i.e. glare, strong blue-spectrum backlights, or bright overhead lighting) or air moving past the eyes (e.g. overhead vents, or direct air from a fan). Asthenopic (eye strain) symptoms in the eye are responsible for much of the severity in CVS. Proper rest to the eye and its muscles is recommended to relieve the associated eye strain. Observations from persons experiencing chronic eye strain have shown that most people who claim to be getting enough sleep are actually not. This, unaware to them, causes the eye strain to build up over a period of time, when if they had obtained seven to eight hours of uninterrupted sleep, their eye muscles would have recovered during the sleep and the strain would not have built up. Computer workers are often advised to take breaks and look at distant objects. A routinely recommended approach is to consciously blink the eyes every now and then (this helps replenish the tear film) and to look out the window to a distant object or to the sky-doing so provides rest to the ciliary muscles. One of the catch phrases is the \"20-20-20 rule\": every 20 minutes, focus the eyes on an object 20 feet (6 meters) away for 20 seconds. This basically gives a convenient distance and timeframe for a person to follow the advice from the optometrist and ophthalmologist. A number of computer and smartphone applications adjust the computer video color temperature, reducing the amount of blue light emitted by the screen, particularly at night. Dry eye is a symptom that is targeted in the therapy of CVS. The use of over-the-counter artificial-tear solutions can reduce the effects of dry eye in CVS. Prior to using artificial tear solutions, it is necessary to check if dry eye is the actual cause of the problem (measured by a tear meniscus test) or whether there are no actual symptoms of dry eye at all. Dry eyes because of CVS can also be treated using moisture chamber glasses or humidifier machines. Office spaces with artificially dry air can worsen CVS syndromes, in which case, a desktop or a room humidifier can help the eyes keep a healthy moisture level. At night, CVS can become worse. It is recommended to use a dark user interface while working at night on the computer. Several browser and OS settings or add-ons exist to darken the user interface. A 2017 randomized controlled trial evaluated macular carotenoid supplements (lutein, zeaxanthin, and mesozeaxanthin) in people with high screen time usage. The supplement group had statistically significant reduction in self-reported headache, eye strain, eye fatigue and sleep complaints, but no reduction in neck strain or blurry vision. A 2021 review investigated suggested therapies for CVS and found little supporting evidence for the following: switching to bi- or multi-focal glasses to reduce eye strain, or using glasses that block blue light. The same review reported \"low-certainty\" in omega-3 supplements as a method to combat CVS. Decreased focusing capability is mitigated by wearing a small plus-powered (+1.00 to +1.50) over-the-counter pair of eyeglasses. Wearing these eyeglasses helps such patients regain their ability to focus on near objects. People who are engaged in other occupations-such as tailors engaged in embroidery-can experience similar symptoms and can be helped by these glasses. A Pacific University research study of 36 participants found significant differences in irritation or burning of the eyes, tearing, or watery eyes, dry eyes, and tired eyes, that were each improved by amber colored lenses versus placebo lenses, but in a follow-up study in 2008, the same team was not able to reproduce the results of the first study. A study sponsored by the lens industry has shown blue light-filtering lenses decrease specific aspects of light emissions. Theoretical reductions in phototoxicity were 10.6% to 23.6%. Additionally, melatonin suppression was reduced by 5.8% to 15.0% and scotopic sensitivity by 2.4% to 9.6%. Over 70% of the participants in this testing were unable to detect these changes. The expansion of technology has led to more individuals utilizing computers and televisions which increase the overall exposure to blue light. Double-blind trials however, have shown no evidence to support the use of blue light filtering lenses for digital eye strain caused by blue light from electronic screens. Amber-tinted lenses have been shown to affect the circadian rhythm and treat delayed sleep phase disorder. According to the US National Institute for Occupational Safety and Health, computer vision syndrome affects about 90% of the people who spend three hours or more a day at a computer. Another study in Malaysia was conducted on 795 university students aged between 18 and 25. The students experienced headaches along with eyestrain, with 89.9% of the students surveyed feeling any type of symptom of CVS.",
      "sentences": [
        "Computer vision syndrome (CVS) is a condition resulting from focusing the eyes on a computer or other display device for protracted, uninterrupted periods of time and the eye's muscles being unable to recover from the constant tension required to maintain focus on a close object.",
        "Some symptoms of CVS include headaches, blurred vision, neck pain, fatigue, eye strain, dry eyes, irritated eyes, double vision, vertigo/dizziness, polyopia, and difficulty refocusing the eyes.",
        "These symptoms can be further aggravated by improper lighting conditions (i.e.",
        "glare, strong blue-spectrum backlights, or bright overhead lighting) or air moving past the eyes (e.g.",
        "overhead vents, or direct air from a fan).",
        "Asthenopic (eye strain) symptoms in the eye are responsible for much of the severity in CVS.",
        "Proper rest to the eye and its muscles is recommended to relieve the associated eye strain.",
        "Observations from persons experiencing chronic eye strain have shown that most people who claim to be getting enough sleep are actually not.",
        "This, unaware to them, causes the eye strain to build up over a period of time, when if they had obtained seven to eight hours of uninterrupted sleep, their eye muscles would have recovered during the sleep and the strain would not have built up.",
        "Computer workers are often advised to take breaks and look at distant objects.",
        "A routinely recommended approach is to consciously blink the eyes every now and then (this helps replenish the tear film) and to look out the window to a distant object or to the sky-doing so provides rest to the ciliary muscles.",
        "One of the catch phrases is the \"20-20-20 rule\": every 20 minutes, focus the eyes on an object 20 feet (6 meters) away for 20 seconds.",
        "This basically gives a convenient distance and timeframe for a person to follow the advice from the optometrist and ophthalmologist.",
        "A number of computer and smartphone applications adjust the computer video color temperature, reducing the amount of blue light emitted by the screen, particularly at night.",
        "Dry eye is a symptom that is targeted in the therapy of CVS.",
        "The use of over-the-counter artificial-tear solutions can reduce the effects of dry eye in CVS.",
        "Prior to using artificial tear solutions, it is necessary to check if dry eye is the actual cause of the problem (measured by a tear meniscus test) or whether there are no actual symptoms of dry eye at all.",
        "Dry eyes because of CVS can also be treated using moisture chamber glasses or humidifier machines.",
        "Office spaces with artificially dry air can worsen CVS syndromes, in which case, a desktop or a room humidifier can help the eyes keep a healthy moisture level.",
        "At night, CVS can become worse.",
        "It is recommended to use a dark user interface while working at night on the computer.",
        "Several browser and OS settings or add-ons exist to darken the user interface.",
        "A 2017 randomized controlled trial evaluated macular carotenoid supplements (lutein, zeaxanthin, and mesozeaxanthin) in people with high screen time usage.",
        "The supplement group had statistically significant reduction in self-reported headache, eye strain, eye fatigue and sleep complaints, but no reduction in neck strain or blurry vision.",
        "A 2021 review investigated suggested therapies for CVS and found little supporting evidence for the following: switching to bi- or multi-focal glasses to reduce eye strain, or using glasses that block blue light.",
        "The same review reported \"low-certainty\" in omega-3 supplements as a method to combat CVS.",
        "Decreased focusing capability is mitigated by wearing a small plus-powered (+1.00 to +1.50) over-the-counter pair of eyeglasses.",
        "Wearing these eyeglasses helps such patients regain their ability to focus on near objects.",
        "People who are engaged in other occupations-such as tailors engaged in embroidery-can experience similar symptoms and can be helped by these glasses.",
        "A Pacific University research study of 36 participants found significant differences in irritation or burning of the eyes, tearing, or watery eyes, dry eyes, and tired eyes, that were each improved by amber colored lenses versus placebo lenses, but in a follow-up study in 2008, the same team was not able to reproduce the results of the first study.",
        "A study sponsored by the lens industry has shown blue light-filtering lenses decrease specific aspects of light emissions.",
        "Theoretical reductions in phototoxicity were 10.6% to 23.6%.",
        "Additionally, melatonin suppression was reduced by 5.8% to 15.0% and scotopic sensitivity by 2.4% to 9.6%.",
        "Over 70% of the participants in this testing were unable to detect these changes.",
        "The expansion of technology has led to more individuals utilizing computers and televisions which increase the overall exposure to blue light.",
        "Double-blind trials however, have shown no evidence to support the use of blue light filtering lenses for digital eye strain caused by blue light from electronic screens.",
        "Amber-tinted lenses have been shown to affect the circadian rhythm and treat delayed sleep phase disorder.",
        "According to the US National Institute for Occupational Safety and Health, computer vision syndrome affects about 90% of the people who spend three hours or more a day at a computer.",
        "Another study in Malaysia was conducted on 795 university students aged between 18 and 25.",
        "The students experienced headaches along with eyestrain, with 89.9% of the students surveyed feeling any type of symptom of CVS."
      ],
      "metadata": {
        "title": "Computer vision syndrome",
        "url": "https://en.wikipedia.org/wiki/Computer_vision_syndrome",
        "word_count": 867,
        "char_count": 5408,
        "sentence_count": 40,
        "scraped_at": "2025-08-09T14:46:57.526139",
        "language": "en",
        "processing_time": 0.0017590522766113281,
        "source_hash": "fe0628095c9aa98767dcc56def5d3e89"
      }
    },
    {
      "title": "Deep learning",
      "url": "https://en.wikipedia.org/wiki/Deep_learning",
      "raw_text": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised.\nSome common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\n\n\n== Overview ==\nMost modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\nFundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.\nImportantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.\nThe word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\nDeep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.\nThe term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated.\n\n\n== Interpretations ==\nDeep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.\nThe classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.\nThe universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.\nThe probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.\n\n\n== History ==\n\n\n=== Before 1980 ===\nThere are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other early recurrent neural networks were published by Kaoru Nakano in 1971. Already in 1948, Alan Turing produced work on \"Intelligent Machinery\"  that was not published in his lifetime, containing \"ideas related to artificial evolution and learning RNNs\".\nFrank Rosenblatt (1958) proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight). The book cites an earlier network by R. D. Joseph (1960) \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion.\nThe first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\".\nThe first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\nIn 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning.\nDeep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. \nBackpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.\n\n\n=== 1980s-2000s ===\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.  In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. \nIn 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.\nRecurrent neural networks (RNN) were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study problems in cognitive psychology.\nIn the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below. This \"neural history compressor\" uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by  distilling a higher level chunker network into a lower level automatizer network. In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. The \"P\" in ChatGPT refers to such pre-training.\nSepp Hochreiter's diploma thesis (1991) implemented the neural history compressor, and identified and analyzed the vanishing gradient problem.  Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995. LSTM can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999, which became the standard RNN architecture.\nIn 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in generative adversarial networks (GANs).\nDuring 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 ). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.\nBoth shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power.\nMost speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark. It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.\nThe principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.\n\n\n=== 2000s ===\nNeural networks entered a lull, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.\nIn 2003, LSTM became competitive with traditional speech recognizers on certain tasks. In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTMs. In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition.\nIn 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation. They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow.\nThe impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.\nThe 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.\nIn 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.\n\n\n=== Deep learning revolution ===\n\nThe deep learning revolution started around CNN- and GPU-based computer vision.\nAlthough CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.\nA key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.\nIn 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly.\nIn 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.\nIn October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.\nThe success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.\nIn 2014, the state of the art was training “very deep neural network” with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the Highway Network was published in May 2015, and the residual neural network (ResNet) in Dec 2015. ResNet behaves like an open-gated Highway Net.\nAround the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015), both of which were based on pretrained image classification neural networks, such as VGG-19.\nGenerative adversarial network (GAN) by (Ian Goodfellow et al., 2014) (based on  Jürgen Schmidhuber's principle of artificial curiosity)\nbecame state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.  Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).\nIn 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone.\nDeep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by LSTM. but are more successful in computer vision.\nYoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\".\n\n\n== Neural networks ==\n\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.\nAn ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.\nTypically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\nThe original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\nNeural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\nAs of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\").\n\n\n=== Deep neural networks ===\nA deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.\nFor example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer,  and complex DNN have many layers, hence the name \"deep\" networks. \nDNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.\nDeep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.\nDNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.\nRecurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.\nConvolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).\n\n\n==== Challenges ====\nAs with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.\nDNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (\n  \n    \n      \n        \n          ℓ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\ell _{2}}\n  \n-regularization) or sparsity (\n  \n    \n      \n        \n          ℓ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\ell _{1}}\n  \n-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.\nDNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.\nAlternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.\n\n\n== Hardware ==\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.\nSpecial electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).\nAtomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.\nIn 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).\nIn 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.\n\n\n== Applications ==\n\n\n=== Automatic speech recognition ===\n\nLarge-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.\nThe initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.\n\nThe debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:\n\nScale-up/out and accelerated DNN training and decoding\nSequence discriminative training\nFeature processing by deep models with solid understanding of the underlying mechanisms\nAdaptation of DNNs and related deep models\nMulti-task and transfer learning by DNNs and related deep models\nCNNs and how to design them to best exploit domain knowledge of speech\nRNN and its rich LSTM variants\nOther types of deep models including tensor-based models and integrated deep generative/discriminative models.\nMore recent speech recognition models use Transformers or Temporal Convolution Networks with significant success and widespread applications. All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.\n\n\n=== Image recognition ===\n\nA common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.\nDeep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.\nDeep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\n\n\n=== Visual art processing ===\n\nClosely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of\n\nidentifying the style period of a given painting\nNeural Style Transfer –  capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video\ngenerating striking imagery based on random visual input fields.\n\n\n=== Natural language processing ===\n\nNeural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.\nOther key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others.\nRecent developments generalize word embedding to sentence embedding.\nGoogle Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\". It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages. The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\". GT uses English as an intermediate between most language pairs.\n\n\n=== Drug discovery and toxicology ===\n\nA large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.\nAtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.\nIn 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.\n\n\n=== Customer relationship management ===\n\nDeep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.\n\n\n=== Recommendation systems ===\n\nRecommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.\n\n\n=== Bioinformatics ===\n\nAn autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.\nIn medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.\nDeep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.\n\n\n=== Deep Neural Network Estimations ===\nDeep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.\n\n\n=== Medical image analysis ===\nDeep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.\n\n\n=== Mobile advertising ===\nFinding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\n\n\n=== Image restoration ===\nDeep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.\n\n\n=== Financial fraud detection ===\nDeep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering.\n\n\n=== Materials science ===\nIn November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.\n\n\n=== Military ===\nThe United States Department of Defense applied deep learning to train robots in new tasks through observation.\n\n\n=== Partial differential equations ===\nPhysics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner. One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on.\n\n\n=== Deep backward stochastic differential equation method ===\nDeep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden.\nIn addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems.\n\n\n=== Image reconstruction ===\nImage reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging  and ultrasound imaging.\n\n\n=== Weather prediction ===\nTraditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to  predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.\n\n\n=== Epigenetic clock ===\n\nAn epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.\n\n\n== Relation to human cognitive and brain development ==\nDeep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\".\nA variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.\nAlthough a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.\n\n\n== Commercial activity ==\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.\nGoogle's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages.\nIn 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.\nAs of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\".\n\n\n== Criticism and comment ==\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science.\n\n\n=== Theory ===\n\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.\nIn further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website.\nFurthermore, some researchers have argued that standard loss functions and differentiable architectures in deep learning may limit the discovery of deeper causal or generative mechanisms. Building on Algorithmic information theory (AIT), Hernández-Orozco et al. (2021) proposed an algorithmic loss function to measure the discrepancy between predicted and observed system behavior. Their approach integrates AIT with Machine learning to formulate a framework for learning generative rules in non-differentiable spaces, bridging discrete algorithmic theory with continuous optimization techniques. This framework provides a new perspective on generalization and model interpretability by grounding learning dynamics in algorithmic complexity.  \n\n\n=== Errors ===\nSome deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).\n\n\n=== Cyber threat ===\nAs deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an \"adversarial attack\".\nIn 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.\nAnother group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.\nANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.\nIn 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\".\nIn \"data poisoning\", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.\n\n\n=== Data collection ethics ===\nThe deep learning systems that are trained using supervised learning often rely on data that is created or annotated by humans, or both. It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.\n\n\n== See also ==\nApplications of artificial intelligence\nComparison of deep learning software\nCompressed sensing\nDifferentiable programming\nEcho state network\nList of artificial intelligence projects\nLiquid state machine\nList of datasets for machine-learning research\nReservoir computing\nScale space and deep learning\nSparse coding\nStochastic parrot\nTopological deep learning\n\n\n== References ==\n\n\n== Further reading ==",
      "cleaned_text": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised. Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose. Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction. The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively. Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance. Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks. The term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated. Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference. The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit. The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator. The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop. There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other early recurrent neural networks were published by Kaoru Nakano in 1971. Already in 1948, Alan Turing produced work on \"Intelligent Machinery\" that was not published in his lifetime, containing \"ideas related to artificial evolution and learning RNNs\". Frank Rosenblatt (1958) proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight). The book cites an earlier network by R. D. Joseph (1960) \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\". The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. Recurrent neural networks (RNN) were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study problems in cognitive psychology. In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below. This \"neural history compressor\" uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network. In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. The \"P\" in ChatGPT refers to such pre-training. Sepp Hochreiter's diploma thesis (1991) implemented the neural history compressor, and identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995. LSTM can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999, which became the standard RNN architecture. In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in generative adversarial networks (GANs). During 1985-1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 ). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics. Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark. It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning. The principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results. Neural networks entered a lull, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks. In 2003, LSTM became competitive with traditional speech recognizers on certain tasks. In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTMs. In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition. In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation. They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow. The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009-2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees. The deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning. A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly. In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos. In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs. In 2014, the state of the art was training “very deep neural network” with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the Highway Network was published in May 2015, and the residual neural network (ResNet) in Dec 2015. ResNet behaves like an open-gated Highway Net. Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015), both of which were based on pretrained image classification neural networks, such as VGG-19. Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014) (based on Jürgen Schmidhuber's principle of artificial curiosity) became state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022). In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone. Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by LSTM. but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\". Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream. Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\"). A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name \"deep\" networks. DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks. Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets. DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data. Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use. Convolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR). As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time. DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay ( ℓ 2 {\\displaystyle \\ell _{2}} -regularization) or sparsity ( ℓ 1 {\\displaystyle \\ell _{1}} -regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting. DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations. Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved. Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months. Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2). Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage. In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications. Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks. The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991. The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003-2007, accelerated progress in eight major areas: Scale-up/out and accelerated DNN training and decoding Sequence discriminative training Feature processing by deep models with solid understanding of the underlying mechanisms Adaptation of DNNs and related deep models Multi-task and transfer learning by DNNs and related deep models CNNs and how to design them to best exploit domain knowledge of speech RNN and its rich LSTM variants Other types of deep models including tensor-based models and integrated deep generative/discriminative models. More recent speech recognition models use Transformers or Temporal Convolution Networks with significant success and widespread applications. All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning. A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available. Deep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces. Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes. Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of identifying the style period of a given painting Neural Style Transfer - capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video generating striking imagery based on random visual input fields. Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling. Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others. Recent developments generalize word embedding to sentence embedding. Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\". It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages. The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\". GT uses English as an intermediate between most language pairs. A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs. AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis. In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice. Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value. Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks. An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships. In medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data. Deep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods. Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes. Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency. Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection. Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration. Deep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering. In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds. The United States Department of Defense applied deep learning to train robots in new tasks through observation. Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner. One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on. Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden. In addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems. Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging and ultrasound imaging. Traditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems. An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity. Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\". A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex. Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories. As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\". Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science. A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website. Furthermore, some researchers have argued that standard loss functions and differentiable architectures in deep learning may limit the discovery of deeper causal or generative mechanisms. Building on Algorithmic information theory (AIT), Hernández-Orozco et al. (2021) proposed an algorithmic loss function to measure the discrepancy between predicted and observed system behavior. Their approach integrates AIT with Machine learning to formulate a framework for learning generative rules in non-differentiable spaces, bridging discrete algorithmic theory with continuous optimization techniques. This framework provides a new perspective on generalization and model interpretability by grounding learning dynamics in algorithmic complexity. Some deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an \"adversarial attack\". In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken. Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them. ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target. In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\". In \"data poisoning\", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery. The deep learning systems that are trained using supervised learning often rely on data that is created or annotated by humans, or both. It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.",
      "sentences": [
        "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning.",
        "The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data.",
        "The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network.",
        "Methods used can be supervised, semi-supervised or unsupervised.",
        "Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields.",
        "These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.",
        "Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain.",
        "However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.",
        "Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.",
        "Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation.",
        "For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels).",
        "The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.",
        "Importantly, a deep learning process can learn which features to optimally place at which level on its own.",
        "Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on.",
        "In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically.",
        "This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.",
        "The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed.",
        "More precisely, deep learning systems have a substantial credit assignment path (CAP) depth.",
        "The CAP is the chain of transformations from input to output.",
        "CAPs describe potentially causal connections between input and output.",
        "For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized).",
        "For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.",
        "No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two.",
        "CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function.",
        "Beyond that, more layers do not add to the function approximator ability of the network.",
        "Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.",
        "Deep learning architectures can be constructed with a greedy layer-by-layer method.",
        "Deep learning helps to disentangle these abstractions and pick out which features improve performance.",
        "Deep learning algorithms can be applied to unsupervised learning tasks.",
        "This is an important benefit because unlabeled data is more abundant than the labeled data.",
        "Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.",
        "The term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.",
        "Although the history of its appearance is apparently more complicated.",
        "Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.",
        "The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.",
        "In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.",
        "Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.",
        "The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow.",
        "proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.",
        "The probabilistic interpretation derives from the field of machine learning.",
        "It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively.",
        "More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.",
        "The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.",
        "The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.",
        "RNNs have cycles in their connectivity structure, FNNs don't.",
        "In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements.",
        "In 1972, Shun'ichi Amari made this architecture adaptive.",
        "His learning RNN was republished by John Hopfield in 1982.",
        "Other early recurrent neural networks were published by Kaoru Nakano in 1971.",
        "Already in 1948, Alan Turing produced work on \"Intelligent Machinery\" that was not published in his lifetime, containing \"ideas related to artificial evolution and learning RNNs\".",
        "Frank Rosenblatt (1958) proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer.",
        "He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight).",
        "The book cites an earlier network by R. D. Joseph (1960) \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times).",
        "Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units?",
        "Unfortunately, the learning algorithm was not a functional one, and fell into oblivion.",
        "The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965.",
        "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships.",
        "A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis.",
        "Superfluous hidden units are pruned using a separate validation set.",
        "Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\".",
        "The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.",
        "In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes.",
        "Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.",
        "In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.",
        "The rectifier has become the most popular activation function for deep learning.",
        "Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.",
        "Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes.",
        "The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.",
        "The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970).",
        "Ostrovski et al.",
        "republished it in 1971.",
        "Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm).",
        "In 1986, David E. Rumelhart et al.",
        "popularised backpropagation but did not cite the original work.",
        "The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition.",
        "It used convolutions, weight sharing, and backpropagation.",
        "In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.",
        "In 1989, Yann LeCun et al.",
        "created a CNN called LeNet for recognizing handwritten ZIP codes on mail.",
        "Training required 3 days.",
        "In 1990, Wei Zhang implemented a CNN on optical computing hardware.",
        "In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms.",
        "LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images.",
        "Recurrent neural networks (RNN) were further developed in the 1980s.",
        "Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer.",
        "Consequently, they have similar properties and issues, and their developments had mutual influences.",
        "In RNN, two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study problems in cognitive psychology.",
        "In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths.",
        "To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below.",
        "This \"neural history compressor\" uses predictive coding to learn internal representations at multiple self-organizing time scales.",
        "This can substantially facilitate downstream deep learning.",
        "The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network.",
        "In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.",
        "The \"P\" in ChatGPT refers to such pre-training.",
        "Sepp Hochreiter's diploma thesis (1991) implemented the neural history compressor, and identified and analyzed the vanishing gradient problem.",
        "Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem.",
        "This led to the long short-term memory (LSTM), published in 1995.",
        "LSTM can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before.",
        "That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999, which became the standard RNN architecture.",
        "In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.",
        "The first network is a generative model that models a probability distribution over output patterns.",
        "The second network learns by gradient descent to predict the reactions of the environment to these patterns.",
        "This was called \"artificial curiosity\".",
        "In 2014, this principle was used in generative adversarial networks (GANs).",
        "During 1985-1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm.",
        "These were designed for unsupervised learning of deep generative models.",
        "However, those were more computationally expensive compared to backpropagation.",
        "Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986.",
        "A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.",
        "Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years.",
        "These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.",
        "Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models.",
        "Additional difficulties were the lack of training data and limited computing power.",
        "Most speech recognition researchers moved away from neural nets to pursue generative modeling.",
        "An exception was at SRI International in the late 1990s.",
        "Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition.",
        "The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark.",
        "It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.",
        "The principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms.",
        "The raw features of speech, waveforms, later produced excellent larger-scale results.",
        "Neural networks entered a lull, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.",
        "In 2003, LSTM became competitive with traditional speech recognizers on certain tasks.",
        "In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTMs.",
        "In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition.",
        "In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh deep belief networks were developed for generative modeling.",
        "They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation.",
        "They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow.",
        "The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.",
        "Industrial applications of deep learning to large-scale speech recognition started around 2010.",
        "The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical.",
        "It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets.",
        "However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.",
        "The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.",
        "Analysis around 2009-2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.",
        "That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.",
        "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.",
        "The deep learning revolution started around CNN- and GPU-based computer vision.",
        "Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision.",
        "Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.",
        "A key advance for the deep learning revolution was hardware advances, especially GPU.",
        "Some early work dated back to 2004.",
        "In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning.",
        "They reported up to 70 times faster training.",
        "In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.",
        "It then won more contests.",
        "They also showed how max-pooling CNNs on GPU improved performance significantly.",
        "In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.",
        "In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods.",
        "Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.",
        "The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.",
        "In 2014, the state of the art was training “very deep neural network” with 20 to 30 layers.",
        "Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem.",
        "In 2015, two techniques were developed to train very deep networks: the Highway Network was published in May 2015, and the residual neural network (ResNet) in Dec 2015.",
        "ResNet behaves like an open-gated Highway Net.",
        "Around the same time, deep learning started impacting the field of art.",
        "Early examples included Google DeepDream (2015), and neural style transfer (2015), both of which were based on pretrained image classification neural networks, such as VGG-19.",
        "Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al.",
        "Here the GAN generator is grown from small to large scale in a pyramidal fashion.",
        "Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.",
        "In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone.",
        "Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR).",
        "Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.",
        "Convolutional neural networks were superseded for ASR by LSTM.",
        "but are more successful in computer vision.",
        "Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\".",
        "Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains.",
        "Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming.",
        "For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images.",
        "They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.",
        "An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain).",
        "Each connection (synapse) between neurons can transmit a signal to another neuron.",
        "The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it.",
        "Neurons may have state, generally represented by real numbers, typically between 0 and 1.",
        "Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.",
        "Typically, neurons are organized in layers.",
        "Different layers may perform different kinds of transformations on their inputs.",
        "Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.",
        "The original goal of the neural network approach was to solve problems in the same way that a human brain would.",
        "Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.",
        "Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.",
        "As of 2017, neural networks typically have a few thousand to a few million units and millions of connections.",
        "Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\").",
        "A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers.",
        "There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.",
        "These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.",
        "For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed.",
        "The user can review the results and select which probabilities the network should display (above a certain threshold, etc.)",
        "and return the proposed label.",
        "Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name \"deep\" networks.",
        "DNNs can model complex non-linear relationships.",
        "DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.",
        "The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.",
        "For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.",
        "Deep architectures include many variants of a few basic approaches.",
        "Each architecture has found success in specific domains.",
        "It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.",
        "DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back.",
        "At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them.",
        "The weights and inputs are multiplied and return an output between 0 and 1.",
        "If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.",
        "That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.",
        "Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling.",
        "Long short-term memory is particularly effective for this use.",
        "Convolutional neural networks (CNNs) are used in computer vision.",
        "CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).",
        "As with ANNs, many issues can arise with naively trained DNNs.",
        "Two common issues are overfitting and computation time.",
        "DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data.",
        "Regularization methods such as Ivakhnenko's unit pruning or weight decay ( ℓ 2 {\\displaystyle \\ell _{2}} -regularization) or sparsity ( ℓ 1 {\\displaystyle \\ell _{1}} -regularization) can be applied during training to combat overfitting.",
        "Alternatively dropout regularization randomly omits units from the hidden layers during training.",
        "This helps to exclude rare dependencies.",
        "Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled.",
        "This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction.",
        "Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.",
        "DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights.",
        "Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources.",
        "Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation.",
        "Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.",
        "Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms.",
        "CMAC (cerebellar model articulation controller) is one such kind of neural network.",
        "It doesn't require learning rates or randomized initial weights.",
        "The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.",
        "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.",
        "By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI .",
        "OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.",
        "Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms.",
        "Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform.",
        "Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).",
        "Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.",
        "In 2020, Marega et al.",
        "published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).",
        "In 2021, J. Feldmann et al.",
        "proposed an integrated photonic hardware accelerator for parallel convolutional processing.",
        "The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds.",
        "Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.",
        "Large-scale automatic speech recognition is the first and most convincing successful case of deep learning.",
        "LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.",
        "The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT.",
        "The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.",
        "Its small size lets many configurations be tried.",
        "More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models.",
        "This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed.",
        "The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.",
        "More recent speech recognition models use Transformers or Temporal Convolution Networks with significant success and widespread applications.",
        "All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.)",
        "are based on deep learning.",
        "A common evaluation set for image classification is the MNIST database data set.",
        "MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples.",
        "As with TIMIT, its small size lets users test multiple configurations.",
        "A comprehensive list of results on this set is available.",
        "Deep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants.",
        "This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.",
        "Deep learning-trained vehicles now interpret 360° camera views.",
        "Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.",
        "Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks.",
        "DNNs have proven themselves capable, for example, of identifying the style period of a given painting Neural Style Transfer - capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video generating striking imagery based on random visual input fields.",
        "Neural networks have been used for implementing language models since the early 2000s.",
        "LSTM helped to improve machine translation and language modeling.",
        "Other key techniques in this field are negative sampling and word embedding.",
        "Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space.",
        "Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar.",
        "A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.",
        "Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.",
        "Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others.",
        "Recent developments generalize word embedding to sentence embedding.",
        "Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network.",
        "Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\".",
        "It translates \"whole sentences at a time, rather than pieces\".",
        "Google Translate supports over one hundred languages.",
        "The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\".",
        "GT uses English as an intermediate between most language pairs.",
        "A large percentage of candidate drugs fail to win regulatory approval.",
        "These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.",
        "Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.",
        "AtomNet is a deep learning system for structure-based rational drug design.",
        "AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.",
        "In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set.",
        "In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.",
        "Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables.",
        "The estimated value function was shown to have a natural interpretation as customer lifetime value.",
        "Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.",
        "Multi-view deep learning has been applied for learning user preferences from multiple domains.",
        "The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.",
        "An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.",
        "In medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.",
        "Deep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up.",
        "In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.",
        "Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE).",
        "Such an estimation provides insights on the effects of input random variables on an independent random variable.",
        "Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X.",
        "For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes.",
        "In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds.",
        "It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.",
        "Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement.",
        "Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.",
        "Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.",
        "Deep learning has been used to interpret large, many-dimensioned advertising datasets.",
        "Many data points are collected during the request/serve/click internet advertising cycle.",
        "This information can form the basis of machine learning to improve ad selection.",
        "Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.",
        "These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.",
        "Deep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering.",
        "In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME.",
        "This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe.",
        "GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures.",
        "The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%.",
        "The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications.",
        "This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development.",
        "The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.",
        "The United States Department of Defense applied deep learning to train robots in new tasks through observation.",
        "Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner.",
        "One example is the reconstructing fluid flow governed by the Navier-Stokes equations.",
        "Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on.",
        "Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE).",
        "This method is particularly useful for solving high-dimensional problems in financial mathematics.",
        "By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings.",
        "Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions.",
        "Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden.",
        "In addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture.",
        "This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations.",
        "PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems.",
        "Image reconstruction is the reconstruction of the underlying images from the image-related measurements.",
        "Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging and ultrasound imaging.",
        "Traditional weather prediction systems solve a very complex system of partial differential equations.",
        "GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time.",
        "It is able to predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.",
        "An epigenetic clock is a biochemical test that can be used to measure age.",
        "Galkin et al.",
        "used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples.",
        "The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity.",
        "The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.",
        "Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.",
        "These developmental theories were instantiated in computational models, making them predecessors of deep learning systems.",
        "These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models.",
        "Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers.",
        "This process yields a self-organizing stack of transducers, well-tuned to their operating environment.",
        "A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\".",
        "A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective.",
        "On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.",
        "Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.",
        "In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.",
        "Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported.",
        "For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations.",
        "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.",
        "Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.",
        "Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input.",
        "In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.",
        "Google Translate uses a neural network to translate between more than 100 languages.",
        "In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.",
        "As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.",
        "First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers.",
        "Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation.",
        "Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person.",
        "The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\".",
        "Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.",
        "A main criticism concerns the lack of theory surrounding some methods.",
        "Learning in the most common deep architectures is implemented using well-understood gradient descent.",
        "However, the theory surrounding other algorithms, such as contrastive divergence is less clear.",
        "(e.g., Does it converge?",
        "If so, how fast?",
        "What is it approximating?)",
        "Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.",
        "Furthermore, some researchers have argued that standard loss functions and differentiable architectures in deep learning may limit the discovery of deeper causal or generative mechanisms.",
        "Building on Algorithmic information theory (AIT), Hernández-Orozco et al.",
        "(2021) proposed an algorithmic loss function to measure the discrepancy between predicted and observed system behavior.",
        "Their approach integrates AIT with Machine learning to formulate a framework for learning generative rules in non-differentiable spaces, bridging discrete algorithmic theory with continuous optimization techniques.",
        "This framework provides a new perspective on generalization and model interpretability by grounding learning dynamics in algorithmic complexity.",
        "Some deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013).",
        "Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.",
        "These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events.",
        "Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).",
        "As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception.",
        "By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize.",
        "For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target.",
        "Such manipulation is termed an \"adversarial attack\".",
        "In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it.",
        "The modified images looked no different to human eyes.",
        "Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.",
        "One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it.",
        "A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.",
        "Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another.",
        "In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.",
        "ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry.",
        "ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.",
        "In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\".",
        "In \"data poisoning\", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.",
        "The deep learning systems that are trained using supervised learning often rely on data that is created or annotated by humans, or both.",
        "It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such.",
        "CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g.",
        "tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g.",
        "by leveraging quantified-self devices such as activity trackers) and (5) clickwork."
      ],
      "metadata": {
        "title": "Deep learning",
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "word_count": 8144,
        "char_count": 55424,
        "sentence_count": 391,
        "scraped_at": "2025-08-09T14:47:02.845411",
        "language": "en",
        "processing_time": 0.016392946243286133,
        "source_hash": "2bc9bde5ffc8f59397e3cb9894aaae3e"
      }
    },
    {
      "title": "Transformer (deep learning architecture)",
      "url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)",
      "raw_text": "In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. \nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.\n\nThe modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\n\n\n== History ==\n\n\n=== Predecessors ===\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\nA key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\nHowever, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.\nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.\n\n\n=== Attention with seq2seq ===\n\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.\nA 380M-parameter model for machine translation uses two long short-term memories (LSTM). Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.\nThe RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\".\nThe relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.\nIn 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.\n\n\n=== Parallelizing attention ===\n\nSeq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title \"attention is all you need\". That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical. In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.\nIn 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.\n\n\n=== AI boom era ===\nAlready in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.\nIn language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model.\nStarting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models.\nSince 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The vision transformer, in turn, stimulated new developments in convolutional neural networks. Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024), and Sora (2024), use Transformers to analyse input data (like text prompts) by breaking it down into \"tokens\" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data.\n\n\n== Training ==\n\n\n=== Methods for stabilizing training ===\nThe plain transformer architecture had difficulty converging. In the original paper the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.\nA 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.\n\n\n=== Pretrain-finetune ===\nTransformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:\n\nlanguage modeling\nnext-sentence prediction\nquestion answering\nreading comprehension\nsentiment analysis\nparaphrasing\nThe T5 transformer report documents a large number of natural language pretraining tasks. Some examples are:\n\nrestoring or repairing incomplete or corrupted text. For example, the input, \"Thank you ~~ me to your party ~~ week\", might generate the output, \"Thank you for inviting me to your party last week\".\ntranslation between natural languages (machine translation)\njudging the pragmatic acceptability of natural language. For example, the following sentence might be judged \"not acceptable\", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well.\nNote that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.\n\n\n=== Tasks ===\n\nIn general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\". These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.\nIn a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: \n  \n    \n      \n        \n          Loss\n        \n        =\n        −\n        \n          ∑\n          \n            t\n            ∈\n            \n              masked tokens\n            \n          \n        \n        ln\n        ⁡\n        (\n        \n          probability of \n        \n        t\n        \n           conditional on its context\n        \n        )\n      \n    \n    {\\displaystyle {\\text{Loss}}=-\\sum _{t\\in {\\text{masked tokens}}}\\ln({\\text{probability of }}t{\\text{ conditional on its context}})}\n  \nand the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.\nIn an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.\nIn a prefixLM task, the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.\nNote that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model).\n\n\n== Architecture ==\nAll transformers have the same primary components:\n\nTokenizers, which convert text into tokens.\nEmbedding layer, which converts tokens and positions of the tokens into vector representations.\nTransformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. These consist of alternating attention and feedforward layers. There are two major types of transformer layers: encoder layers and decoder layers, with further variants.\nUn-embedding layer, which converts the final vector representations back to a probability distribution over the tokens.\nThe following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section.\nBy convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as \n  \n    \n      \n        x\n        W\n      \n    \n    {\\displaystyle xW}\n  \n.\n\n\n=== Tokenization ===\n\nAs the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences is a tokenizer.\nThe set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size \n  \n    \n      \n        \n          n\n          \n            vocabulary\n          \n        \n      \n    \n    {\\displaystyle n_{\\text{vocabulary}}}\n  \n. When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\".\nSome commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.\n\n\n=== Embedding ===\n\nEach token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n. For example, if the input token is \n  \n    \n      \n        3\n      \n    \n    {\\displaystyle 3}\n  \n, then the one-hot representation is \n  \n    \n      \n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        …\n        ]\n      \n    \n    {\\displaystyle [0,0,0,1,0,0,\\dots ]}\n  \n, and its embedding vector is\n  \n    \n      \n        \n          E\n          m\n          b\n          e\n          d\n        \n        (\n        3\n        )\n        =\n        [\n        0\n        ,\n        0\n        ,\n        0\n        ,\n        1\n        ,\n        0\n        ,\n        0\n        ,\n        …\n        ]\n        M\n      \n    \n    {\\displaystyle \\mathrm {Embed} (3)=[0,0,0,1,0,0,\\dots ]M}\n  \nThe token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors.\nThe number of dimensions in an embedding vector is called hidden size or embedding size and written as \n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb}}}\n  \n. This size is written as \n  \n    \n      \n        \n          d\n          \n            model\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{model}}}\n  \n in the original Transformer paper.\n\n\n=== Un-embedding ===\nAn un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.\nThe un-embedding layer is a linear-softmax layer:\n  \n    \n      \n        \n          U\n          n\n          E\n          m\n          b\n          e\n          d\n        \n        (\n        x\n        )\n        =\n        \n          s\n          o\n          f\n          t\n          m\n          a\n          x\n        \n        (\n        x\n        W\n        +\n        b\n        )\n      \n    \n    {\\displaystyle \\mathrm {UnEmbed} (x)=\\mathrm {softmax} (xW+b)}\n  \nThe matrix has shape \n  \n    \n      \n        (\n        \n          d\n          \n            emb\n          \n        \n        ,\n        \n          n\n          \n            vocabulary\n          \n        \n        )\n      \n    \n    {\\displaystyle (d_{\\text{emb}},n_{\\text{vocabulary}})}\n  \n. The embedding matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n and the un-embedding matrix \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n are sometimes required to be transposes of each other, a practice called weight tying.\n\n\n=== Positional encoding ===\n\nA positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This induces a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\".\nThe positional encoding is defined as a function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        →\n        \n          \n            R\n          \n          \n            d\n          \n        \n        ;\n        d\n        ∈\n        \n          Z\n        \n        ,\n        d\n        >\n        0\n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {R} ^{d};d\\in \\mathbb {Z} ,d>0}\n  \n, where \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n is a positive even integer. The full positional encoding defined in the original paper is:\n  \n    \n      \n        (\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n          \n        \n        ,\n        f\n        (\n        t\n        \n          )\n          \n            2\n            k\n            +\n            1\n          \n        \n        )\n        =\n        (\n        sin\n        ⁡\n        (\n        θ\n        )\n        ,\n        cos\n        ⁡\n        (\n        θ\n        )\n        )\n        \n        ∀\n        k\n        ∈\n        {\n        0\n        ,\n        1\n        ,\n        …\n        ,\n        d\n        \n          /\n        \n        2\n        −\n        1\n        }\n      \n    \n    {\\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\\sin(\\theta ),\\cos(\\theta ))\\quad \\forall k\\in \\{0,1,\\ldots ,d/2-1\\}}\n  \nwhere \n  \n    \n      \n        θ\n        =\n        \n          \n            t\n            \n              r\n              \n                k\n              \n            \n          \n        \n        ,\n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle \\theta ={\\frac {t}{r^{k}}},r=N^{2/d}}\n  \n.\nHere, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is a free parameter that should be significantly larger than the biggest \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n that would be input into the positional encoding function. The original paper uses \n  \n    \n      \n        N\n        =\n        10000\n      \n    \n    {\\displaystyle N=10000}\n  \n.\nThe function is in a simpler form when written as a complex function of type \n  \n    \n      \n        f\n        :\n        \n          R\n        \n        →\n        \n          \n            C\n          \n          \n            d\n            \n              /\n            \n            2\n          \n        \n      \n    \n    {\\displaystyle f:\\mathbb {R} \\to \\mathbb {C} ^{d/2}}\n  \n\n  \n    \n      \n        f\n        (\n        t\n        )\n        =\n        \n          \n            (\n            \n              e\n              \n                i\n                t\n                \n                  /\n                \n                \n                  r\n                  \n                    k\n                  \n                \n              \n            \n            )\n          \n          \n            k\n            =\n            0\n            ,\n            1\n            ,\n            …\n            ,\n            \n              \n                d\n                2\n              \n            \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle f(t)=\\left(e^{it/r^{k}}\\right)_{k=0,1,\\ldots ,{\\frac {d}{2}}-1}}\n  \nwhere \n  \n    \n      \n        r\n        =\n        \n          N\n          \n            2\n            \n              /\n            \n            d\n          \n        \n      \n    \n    {\\displaystyle r=N^{2/d}}\n  \n.\nThe main reason for using this positional encoding function is that using it, shifts are linear transformations:\n  \n    \n      \n        f\n        (\n        t\n        +\n        Δ\n        t\n        )\n        =\n        \n          d\n          i\n          a\n          g\n        \n        (\n        f\n        (\n        Δ\n        t\n        )\n        )\n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle f(t+\\Delta t)=\\mathrm {diag} (f(\\Delta t))f(t)}\n  \nwhere \n  \n    \n      \n        Δ\n        t\n        ∈\n        \n          R\n        \n      \n    \n    {\\displaystyle \\Delta t\\in \\mathbb {R} }\n  \n is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.\nBy taking a linear sum, any convolution can also be implemented as linear transformations:\n  \n    \n      \n        \n          ∑\n          \n            j\n          \n        \n        \n          c\n          \n            j\n          \n        \n        f\n        (\n        t\n        +\n        Δ\n        \n          t\n          \n            j\n          \n        \n        )\n        =\n        \n          (\n          \n            \n              ∑\n              \n                j\n              \n            \n            \n              c\n              \n                j\n              \n            \n            \n            \n              d\n              i\n              a\n              g\n            \n            (\n            f\n            (\n            Δ\n            \n              t\n              \n                j\n              \n            \n            )\n            )\n          \n          )\n        \n        f\n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\sum _{j}c_{j}f(t+\\Delta t_{j})=\\left(\\sum _{j}c_{j}\\,\\mathrm {diag} (f(\\Delta t_{j}))\\right)f(t)}\n  \nfor any constants \n  \n    \n      \n        \n          c\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle c_{j}}\n  \n. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\"\nIn typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.\n\n\n=== Encoder-decoder (overview) ===\n\nLike earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.\nThe purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for \"mixing\" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).\nBoth the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps. These feed-forward layers contain most of the parameters in a Transformer model.\n\n\n=== Feedforward network ===\n\nThe feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons:\n  \n    \n      \n        \n          F\n          F\n          N\n        \n        (\n        x\n        )\n        =\n        ϕ\n        (\n        x\n        \n          W\n          \n            (\n            1\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            1\n            )\n          \n        \n        )\n        \n          W\n          \n            (\n            2\n            )\n          \n        \n        +\n        \n          b\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {FFN} (x)=\\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}\n  \nwhere \n  \n    \n      \n        \n          W\n          \n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle W^{(1)}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle W^{(2)}}\n  \n are weight matrices and \n  \n    \n      \n        \n          b\n          \n            (\n            1\n            )\n          \n        \n      \n    \n    {\\displaystyle b^{(1)}}\n  \n and  \n  \n    \n      \n        \n          b\n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle b^{(2)}}\n  \n are bias vectors, and \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n is its activation function. The original Transformer used ReLU activation.\nThe number of neurons in the middle layer is called intermediate size (GPT), filter size (BERT), or feedforward size (BERT). It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: \n  \n    \n      \n        \n          d\n          \n            ffn\n          \n        \n        =\n        4\n        \n          d\n          \n            emb\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{ffn}}=4d_{\\text{emb}}}\n  \n.\n\n\n=== Scaled dot-product attention ===\n\n\n==== Attention head ====\n\nThe attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n, the key weights \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n, and the value weights \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n.\nThe module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length \n  \n    \n      \n        \n          ℓ\n          \n            seq, query\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, query}}}\n  \n, and each entry is a vector of dimension \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}}\n  \n. Similarly for the key and value sequences.\nFor each vector \n  \n    \n      \n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n      \n    \n    {\\displaystyle x_{i,{\\text{query}}}}\n  \n in the query sequence, it is multiplied by a matrix \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n to produce a query vector \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        =\n        \n          x\n          \n            i\n            ,\n            \n              query\n            \n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle q_{i}=x_{i,{\\text{query}}}W^{Q}}\n  \n. The matrix of all query vectors is the query matrix:\n  \n    \n      \n        Q\n        =\n        \n          X\n          \n            query\n          \n        \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle Q=X_{\\text{query}}W^{Q}}\n  \nSimilarly, we construct the key matrix \n  \n    \n      \n        K\n        =\n        \n          X\n          \n            key\n          \n        \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle K=X_{\\text{key}}W^{K}}\n  \n and the value matrix \n  \n    \n      \n        V\n        =\n        \n          X\n          \n            value\n          \n        \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle V=X_{\\text{value}}W^{V}}\n  \n.\nIt is usually the case that all \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{Q},W^{K},W^{V}}\n  \n are square matrices, meaning \n  \n    \n      \n        \n          d\n          \n            emb, query\n          \n        \n        =\n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{emb, query}}=d_{\\text{query}}}\n  \n, etc.\nAttention weights are calculated using the query and key vectors: the attention weight \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n is the dot product between \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n and \n  \n    \n      \n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle k_{j}}\n  \n. The attention weights are divided by the square root of the dimension of the key vectors, \n  \n    \n      \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\sqrt {d_{k}}}}\n  \n, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n are different matrices allows attention to be non-symmetric: if token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n attends to token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n        ⋅\n        \n          k\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle q_{i}\\cdot k_{j}}\n  \n is large), this does not necessarily mean that token \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n will attend to token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n (i.e. \n  \n    \n      \n        \n          q\n          \n            j\n          \n        \n        ⋅\n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{j}\\cdot k_{i}}\n  \n could be small). The output of the attention unit for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n is the weighted sum of the value vectors of all tokens, weighted by \n  \n    \n      \n        \n          a\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle a_{ij}}\n  \n, the attention from token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n to each token.\nThe attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n, \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n are defined as the matrices where the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \nth rows are vectors \n  \n    \n      \n        \n          q\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle q_{i}}\n  \n, \n  \n    \n      \n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle k_{i}}\n  \n, and \n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle v_{i}}\n  \n respectively. Then we can represent the attention as\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        Q\n                        \n                          K\n                          \n                            \n                              T\n                            \n                          \n                        \n                      \n                      \n                        \n                          d\n                          \n                            k\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \n\nwhere the softmax is applied over each of the rows of the matrix.\nThe number of dimensions in a query vector is query size \n  \n    \n      \n        \n          d\n          \n            query\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{query}}}\n  \n and similarly for the key size \n  \n    \n      \n        \n          d\n          \n            key\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{key}}}\n  \n and value size \n  \n    \n      \n        \n          d\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{value}}}\n  \n. The output dimension of an attention head is its head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n. The attention mechanism requires the following three equalities to hold:\n  \n    \n      \n        \n          ℓ\n          \n            seq, key\n          \n        \n        =\n        \n          ℓ\n          \n            seq, value\n          \n        \n        ,\n        \n        \n          d\n          \n            query\n          \n        \n        =\n        \n          d\n          \n            key\n          \n        \n        ,\n        \n        \n          d\n          \n            value\n          \n        \n        =\n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle \\ell _{\\text{seq, key}}=\\ell _{\\text{seq, value}},\\;d_{\\text{query}}=d_{\\text{key}},\\;d_{\\text{value}}=d_{\\text{head}}}\n  \nbut is otherwise unconstrained.\nIf the attention head is used in a self-attention fashion, then \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        =\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}=X_{\\text{key}}=X_{\\text{value}}}\n  \n. If the attention head is used in a cross-attention fashion, then usually \n  \n    \n      \n        \n          X\n          \n            query\n          \n        \n        ≠\n        \n          X\n          \n            key\n          \n        \n        =\n        \n          X\n          \n            value\n          \n        \n      \n    \n    {\\displaystyle X_{\\text{query}}\\neq X_{\\text{key}}=X_{\\text{value}}}\n  \n. It is theoretically possible for all three to be different, but that is rarely the case in practice.\n\n\n==== Multiheaded attention ====\n\nOne set of \n  \n    \n      \n        \n          (\n          \n            \n              W\n              \n                Q\n              \n            \n            ,\n            \n              W\n              \n                K\n              \n            \n            ,\n            \n              W\n              \n                V\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)}\n  \n matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\". Specifically, the query and key projection matrices, \n  \n    \n      \n        \n          W\n          \n            Q\n          \n        \n      \n    \n    {\\displaystyle W^{Q}}\n  \n and \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle W^{K}}\n  \n , which are involved in the attention score computation, defines the \"relevance\". Meanwhile, the value projection matrix \n  \n    \n      \n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{V}}\n  \n, in combination with the part of the output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects. The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.\nConcretely, let the multiple attention heads be indexed by \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, then we have\n  \n    \n      \n        \n          MultiheadedAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            ∈\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        (\n        \n          Attention\n        \n        (\n        X\n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        X\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        X\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n        )\n        )\n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V}))W^{O}}\n  \n where the matrix \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  \n is the concatenation of word embeddings, and the matrices \n  \n    \n      \n        \n          W\n          \n            i\n          \n          \n            Q\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            i\n          \n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}\n  \n are \"projection matrices\" owned by individual attention head \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, and \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle W^{O}}\n  \n is a final projection matrix owned by the whole multi-headed attention head.\nIt is theoretically possible for each attention head to have a different head dimension \n  \n    \n      \n        \n          d\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{head}}}\n  \n, but that is rarely the case in practice.\nAs an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:\n  \n    \n      \n        \n          d\n          \n            emb\n          \n        \n        =\n        768\n        ,\n        \n          n\n          \n            head\n          \n        \n        =\n        12\n        ,\n        \n          d\n          \n            head\n          \n        \n        =\n        64\n      \n    \n    {\\displaystyle d_{\\text{emb}}=768,n_{\\text{head}}=12,d_{\\text{head}}=64}\n  \nSince \n  \n    \n      \n        12\n        ×\n        64\n        =\n        768\n      \n    \n    {\\displaystyle 12\\times 64=768}\n  \n, its output projection matrix \n  \n    \n      \n        \n          W\n          \n            O\n          \n        \n        ∈\n        \n          \n            R\n          \n          \n            (\n            12\n            ×\n            64\n            )\n            ×\n            768\n          \n        \n      \n    \n    {\\displaystyle W^{O}\\in \\mathbb {R} ^{(12\\times 64)\\times 768}}\n  \n is a square matrix.\n\n\n==== Masked attention ====\nThe Transformer architecture is constructed to calculate output tokens iteratively. Assuming \n  \n    \n      \n        t\n        =\n        0\n      \n    \n    {\\displaystyle t=0}\n  \n refers to the calculation of the first output token \n  \n    \n      \n        i\n        =\n        0\n      \n    \n    {\\displaystyle i=0}\n  \n, for step \n  \n    \n      \n        t\n        >\n        0\n      \n    \n    {\\displaystyle t>0}\n  \n, the output token \n  \n    \n      \n        i\n        =\n        0\n      \n    \n    {\\displaystyle i=0}\n  \n shall remain constant. This ensures properties of the model similar to autoregressive models. Therefore, at every time step \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, the calculation for all outputs \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n should not have access to tokens at position \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n for \n  \n    \n      \n        j\n        >=\n        i\n      \n    \n    {\\displaystyle j>=i}\n  \n (as it naturally is the case for time step \n  \n    \n      \n        t\n        =\n        i\n      \n    \n    {\\displaystyle t=i}\n  \n, when tokens \n  \n    \n      \n        j\n        >\n        t\n      \n    \n    {\\displaystyle j>t}\n  \n are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n that is \n  \n    \n      \n        −\n        ∞\n      \n    \n    {\\displaystyle -\\infty }\n  \n at entries where the attention link must be cut, and \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n at other places:\n  \n    \n      \n        \n          \n            \n              \n                \n                  MaskedAttention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    M\n                    +\n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{MaskedAttention}}(Q,K,V)={\\text{softmax}}\\left(M+{\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n  \n The following matrix is commonly used in decoder self-attention modules, called \"causal masking\":\n  \n    \n      \n        \n          M\n          \n            causal\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  −\n                  ∞\n                \n                \n                  −\n                  ∞\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  −\n                  ∞\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  …\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{causal}}={\\begin{bmatrix}0&-\\infty &-\\infty &\\dots &-\\infty \\\\0&0&-\\infty &\\dots &-\\infty \\\\0&0&0&\\dots &-\\infty \\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix}}}\n  \n\nIn words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form \n  \n    \n      \n        P\n        \n          M\n          \n            causal\n          \n        \n        \n          P\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle PM_{\\text{causal}}P^{-1}}\n  \n, where \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is a random permutation matrix.\n\n\n=== Encoder ===\n\nAn encoder consists of an embedding layer, followed by multiple encoder layers.\nEach encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  given input vectors \n                \n              \n              \n                \n                  h\n                  \n                    0\n                  \n                \n                ,\n                \n                  h\n                  \n                    1\n                  \n                \n                ,\n                …\n              \n            \n            \n              \n                \n                  combine them into a matrix \n                \n                H\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            h\n                            \n                              0\n                            \n                          \n                        \n                      \n                      \n                        \n                          \n                            h\n                            \n                              1\n                            \n                          \n                        \n                      \n                      \n                        \n                          ⋮\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n            \n              \n                \n                  EncoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  \n                    [\n                    \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadedAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              0\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          \n                            FFN\n                          \n                          (\n                          \n                            MultiheadedAttention\n                          \n                          (\n                          H\n                          ,\n                          H\n                          ,\n                          H\n                          \n                            )\n                            \n                              1\n                            \n                          \n                          )\n                        \n                      \n                      \n                        \n                          ⋮\n                        \n                      \n                    \n                    ]\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{given input vectors }}&h_{0},h_{1},\\dots \\\\{\\text{combine them into a matrix }}H&={\\begin{bmatrix}h_{0}\\\\h_{1}\\\\\\vdots \\end{bmatrix}}\\\\{\\text{EncoderLayer}}(H)&={\\begin{bmatrix}{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{0})\\\\{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{1})\\\\\\vdots \\end{bmatrix}}\\\\\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n stands for \"feed-forward network\". We can more succinctly write it as\n  \n    \n      \n        \n          EncoderLayer\n        \n        (\n        H\n        )\n        =\n        \n          FFN\n        \n        (\n        \n          MultiheadedAttention\n        \n        (\n        H\n        ,\n        H\n        ,\n        H\n        )\n        )\n      \n    \n    {\\displaystyle {\\text{EncoderLayer}}(H)={\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H))}\n  \nwith the implicit convention that the \n  \n    \n      \n        \n          FFN\n        \n      \n    \n    {\\displaystyle {\\text{FFN}}}\n  \n is applied to each row of the matrix individually.\nThe encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.\nAs the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.\n\n\n=== Decoder ===\n\nA decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.\nEach decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention.\nLike the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow. This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.\nIn contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.\nSchematically, we have:\n  \n    \n      \n        \n          \n            \n              \n                \n                  H\n                  ′\n                \n              \n              \n                \n                =\n                \n                  MaskedMultiheadedAttention\n                \n                (\n                H\n                ,\n                H\n                ,\n                H\n                )\n              \n            \n            \n              \n                \n                  DecoderLayer\n                \n                (\n                H\n                )\n              \n              \n                \n                =\n                \n                  FFN\n                \n                (\n                \n                  MultiheadedAttention\n                \n                (\n                \n                  H\n                  ′\n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                ,\n                \n                  H\n                  \n                    E\n                  \n                \n                )\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}H'&={\\text{MaskedMultiheadedAttention}}(H,H,H)\\\\{\\text{DecoderLayer}}(H)&={\\text{FFN}}({\\text{MultiheadedAttention}}(H',H^{E},H^{E}))\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        \n          H\n          \n            E\n          \n        \n      \n    \n    {\\displaystyle H^{E}}\n  \n is the matrix with rows being the output vectors from the encoder.\nThe last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.\n\n\n=== Adapted architectures ===\nMany large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture. Early GPT models are decoder-only models trained to predict the next token in a sequence. BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence.\n\n\n== Full transformer architecture ==\n\n\n=== Sublayers ===\nEach encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.\n\nThe final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence.\nThe residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y = F(x) + x. The expression indicates that an output y is the sum of the transformation of input x (F(x)) and the input itself (x). Adding the input x can preserve the input information and avoid issues when the gradient of F(x) is close to zero.\nSimilarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector.\nThere are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is \n  \n    \n      \n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle \\mathrm {LayerNorm} (x+\\mathrm {Sublayer} (x))}\n  \nwhere \n  \n    \n      \n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\mathrm {Sublayer} (x)}\n  \n is the function implemented by the sublayer itself.\nIn the pre-LN convention, the output of each sublayer is\n  \n    \n      \n        x\n        +\n        \n          S\n          u\n          b\n          l\n          a\n          y\n          e\n          r\n        \n        (\n        \n          L\n          a\n          y\n          e\n          r\n          N\n          o\n          r\n          m\n        \n        (\n        x\n        )\n        )\n      \n    \n    {\\displaystyle x+\\mathrm {Sublayer} (\\mathrm {LayerNorm} (x))}\n  \nThe original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence.\n\n\n=== Pseudocode ===\nThe following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from\n\ninput: Encoder input t_e\n       Decoder input t_d\noutput: Array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence))\n\n/* encoder */\nz_e ← encoder.tokenizer(t_e)\n\nfor each t in 1:length(z_e) do\n    z_e[t] ← encoder.embedding(z_e[t]) + encoder.positional_embedding(t)\n\nfor each l in 1:length(encoder.layers) do\n    layer ← encoder.layers[l]\n\n    /* first sublayer */\n    z_e_copy ← copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← layer.layer_norm(z_e[t])\n    z_e ← layer.multiheaded_attention(z_e, z_e, z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← z_e[t] + z_e_copy[t]\n\n    /* second sublayer */\n    z_e_copy ← copy(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← layer.layer_norm(z_e[t])\n    z_e ← layer.feedforward(z_e)\n    for each t in 1:length(z_e) do\n        z_e[t] ← z_e[t] + z_e_copy[t]\n\nfor each t in 1:length(z_e) do\n    z_e[t] ← encoder.final_layer_norm(z_e[t])\n\n/* decoder */\nz_d ← decoder.tokenizer(t_d)\n\nfor each t in 1:length(z_d) do\n    z_d[t] ← decoder.embedding(z_d[t]) + decoder.positional_embedding(t)\n\nfor each l in 1:length(decoder.layers) do\n        layer ← decoder.layers[l]\n\n        /* first sublayer */\n        z_d_copy ← copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← layer.layer_norm(z_d[t])\n        z_d ← layer.masked_multiheaded_attention(z_d, z_d, z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← z_d[t] + z_d_copy[t]\n\n        /* second sublayer */\n        z_d_copy ← copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← layer.layer_norm(z_d[t])\n        z_d ← layer.multiheaded_attention(z_d, z_e, z_e) \n        for each i in 1:length(z_d) do\n            z_d[t] ← z_d[t] + z_d_copy[t]\n\n        /* third sublayer */\n        z_d_copy ← copy(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← layer.layer_norm(z_d[t])\n        z_d ← layer.feedforward(z_d)\n        for each t in 1:length(z_d) do\n            z_d[t] ← z_d[t] + z_d_copy[t]\n\nz_d ← decoder.final_layer_norm(z_d)\n\noutput_distributions ← []\nfor each t in 1:length(z_d) do\n    output_distributions.append(decoder.unembed(z_d[t]))\n\nreturn output_distributions\n\n\n=== Terminology ===\nThe Transformer architecture, being modular, allows variations. Several common variations are described here.\nAn \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.\nA \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only.\nAn \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder.\nA \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form\n  \n    \n      \n        \n          M\n          \n            prefixLM\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    0\n                  \n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  \n                    0\n                  \n                \n                \n                  \n                    M\n                    \n                      causal\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{\\text{causal}}\\end{bmatrix}}}\n  \nwhere the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less \"sparsity\". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.\nThere are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.\n\n\n== Subsequent work ==\n\n\n=== Alternative activation functions ===\nThe original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU; both GPT-1 and BERT used GELU.\nAlternative activation functions are often used in combination with Gated Linear Units in the feedforward module.\n\n\n=== Alternative normalizations ===\nThe normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm which is used in the Llama series. Other examples include CapsuleNorm ScaleNorm, or FixNorm.\n\n\n=== Alternative positional encodings ===\nTransformers may use other positional encoding methods than sinusoidal.\nThe original Transformer paper reported using a learned positional encoding, but finding it not superior to the sinusoidal one. Later, found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.\n\n\n==== RoPE ====\nRoPE (rotary positional embedding), is best explained by considering a list of 2-dimensional vectors \n  \n    \n      \n        [\n        (\n        \n          x\n          \n            1\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            1\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            2\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        (\n        \n          x\n          \n            3\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            3\n          \n          \n            (\n            2\n            )\n          \n        \n        )\n        ,\n        .\n        .\n        .\n        ]\n      \n    \n    {\\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}\n  \n. Now pick some angle \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. Then RoPE encoding is\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        ,\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  cos\n                  ⁡\n                  m\n                  θ\n                \n                \n                  −\n                  sin\n                  ⁡\n                  m\n                  θ\n                \n              \n              \n                \n                  sin\n                  ⁡\n                  m\n                  θ\n                \n                \n                  cos\n                  ⁡\n                  m\n                  θ\n                \n              \n            \n            )\n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                \n              \n            \n            )\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  cos\n                  ⁡\n                  m\n                  θ\n                  −\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  sin\n                  ⁡\n                  m\n                  θ\n                \n              \n              \n                \n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      2\n                      )\n                    \n                  \n                  cos\n                  ⁡\n                  m\n                  θ\n                  +\n                  \n                    x\n                    \n                      m\n                    \n                    \n                      (\n                      1\n                      )\n                    \n                  \n                  sin\n                  ⁡\n                  m\n                  θ\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x_{m}^{(1)},x_{m}^{(2)},m{\\big )}={\\begin{pmatrix}\\cos m\\theta &-\\sin m\\theta \\\\\\sin m\\theta &\\cos m\\theta \\end{pmatrix}}{\\begin{pmatrix}x_{m}^{(1)}\\\\x_{m}^{(2)}\\\\\\end{pmatrix}}={\\begin{pmatrix}x_{m}^{(1)}\\cos m\\theta -x_{m}^{(2)}\\sin m\\theta \\\\x_{m}^{(2)}\\cos m\\theta +x_{m}^{(1)}\\sin m\\theta \\\\\\end{pmatrix}}}\n  \nEquivalently, if we write the 2-dimensional vectors as complex numbers \n  \n    \n      \n        \n          z\n          \n            m\n          \n        \n        :=\n        \n          x\n          \n            m\n          \n          \n            (\n            1\n            )\n          \n        \n        +\n        i\n        \n          x\n          \n            m\n          \n          \n            (\n            2\n            )\n          \n        \n      \n    \n    {\\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}\n  \n, then RoPE encoding is just multiplication by an angle:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        \n          z\n          \n            m\n          \n        \n        ,\n        m\n        \n          \n            )\n          \n        \n        =\n        \n          e\n          \n            i\n            m\n            θ\n          \n        \n        \n          z\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}z_{m},m{\\big )}=e^{im\\theta }z_{m}}\n  \nFor a list of \n  \n    \n      \n        2\n        n\n      \n    \n    {\\displaystyle 2n}\n  \n-dimensional vectors, a RoPE encoder is defined by a sequence of angles \n  \n    \n      \n        \n          θ\n          \n            (\n            1\n            )\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          θ\n          \n            (\n            n\n            )\n          \n        \n      \n    \n    {\\displaystyle \\theta ^{(1)},...,\\theta ^{(n)}}\n  \n. Then the RoPE encoding is applied to each pair of coordinates.\nThe benefit of RoPE is that the dot-product between two vectors depends on their relative location only:\n  \n    \n      \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        \n          \n            )\n          \n        \n        =\n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        x\n        ,\n        m\n        +\n        k\n        \n          \n            \n              )\n            \n          \n          \n            T\n          \n        \n        \n          RoPE\n        \n        \n          \n            (\n          \n        \n        y\n        ,\n        n\n        +\n        k\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\text{RoPE}}{\\big (}x,m{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n{\\big )}={\\text{RoPE}}{\\big (}x,m+k{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n+k{\\big )}}\n  \n\nfor any integer \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n.\n\n\n==== ALiBi ====\nALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    s\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+sB\\right)V\\end{aligned}}}\n  \nHere, \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is a real number (\"scalar\"), and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is the linear bias matrix defined by\n  \n    \n      \n        B\n        =\n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  3\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  −\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  −\n                  2\n                \n                \n                  −\n                  1\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  −\n                  3\n                \n                \n                  −\n                  2\n                \n                \n                  −\n                  1\n                \n                \n                  0\n                \n                \n                  ⋯\n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle B={\\begin{pmatrix}0&1&2&3&\\cdots \\\\-1&0&1&2&\\cdots \\\\-2&-1&0&1&\\cdots \\\\-3&-2&-1&0&\\cdots \\\\\\vdots &\\vdots &\\vdots &\\vdots &\\ddots \\\\\\end{pmatrix}}}\n  \nin other words, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        j\n        −\n        i\n      \n    \n    {\\displaystyle B_{i,j}=j-i}\n  \n. The idea being that the linear bias matrix is a softened mask. Just as \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n represent full attention paid, and \n  \n    \n      \n        −\n        ∞\n      \n    \n    {\\displaystyle -\\infty }\n  \n represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.\nALiBi allows pretraining on short context windows, then fine-tuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).\n\n\n==== Relative Position Encodings ====\nRelative Position Encodings is similar to ALiBi, but more generic:\n  \n    \n      \n        \n          \n            \n              \n                \n                  Attention\n                \n                (\n                Q\n                ,\n                K\n                ,\n                V\n                )\n                =\n                \n                  softmax\n                \n                \n                  (\n                  \n                    \n                      \n                        \n                          Q\n                          \n                            K\n                            \n                              \n                                T\n                              \n                            \n                          \n                        \n                        \n                          \n                            d\n                            \n                              k\n                            \n                          \n                        \n                      \n                    \n                    +\n                    B\n                  \n                  )\n                \n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+B\\right)V\\end{aligned}}}\n  \nwhere \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n is a Toeplitz matrix, that is, \n  \n    \n      \n        \n          B\n          \n            i\n            ,\n            j\n          \n        \n        =\n        \n          B\n          \n            \n              i\n              ′\n            \n            ,\n            \n              j\n              ′\n            \n          \n        \n      \n    \n    {\\displaystyle B_{i,j}=B_{i',j'}}\n  \n whenever \n  \n    \n      \n        i\n        −\n        j\n        =\n        \n          i\n          ′\n        \n        −\n        \n          j\n          ′\n        \n      \n    \n    {\\displaystyle i-j=i'-j'}\n  \n. This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\".\n\n\n=== Efficient implementation ===\nThe transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.\n\n\n==== KV caching ====\nWhen an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.\nIf a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.\n\n\n==== FlashAttention ====\nFlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow). See the page on softmax for details.\nAn improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.\nKey advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).\nBenchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.\n\n\n==== Multi-Query Attention ====\n\nMulti-Query Attention changes the multiheaded attention mechanism. Whereas normally,\n\n  \n    \n      \n        \n          MultiheadedAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            ∈\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                i\n              \n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\\right)W^{O}}\n  \nwith Multi-Query Attention, there is just one \n  \n    \n      \n        \n          W\n          \n            K\n          \n        \n        ,\n        \n          W\n          \n            V\n          \n        \n      \n    \n    {\\displaystyle W^{K},W^{V}}\n  \n, thus:\n\n  \n    \n      \n        \n          MultiQueryAttention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          \n            Concat\n          \n          \n            i\n            ∈\n            [\n            \n              n\n              \n                heads\n              \n            \n            ]\n          \n        \n        \n          (\n          \n            \n              Attention\n            \n            (\n            X\n            \n              W\n              \n                i\n              \n              \n                Q\n              \n            \n            ,\n            X\n            \n              W\n              \n                K\n              \n            \n            ,\n            X\n            \n              W\n              \n                V\n              \n            \n            )\n          \n          )\n        \n        \n          W\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle {\\text{MultiQueryAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\\right)W^{O}}\n  \n\nThis has a neutral effect on model quality and training speed, but increases inference speed.\nMore generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups.\n\nMultihead Latent Attention (MLA) is a low-rank approximation to standard MHA. Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces (\"latent space\"), one for query and one for key-value (KV vector). This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached.\n\n\n==== Speculative decoding ====\nSpeculative decoding is a method to accelerate token decoding. Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified. If the quickly computed tokens are incorrect, they are discarded and computed slowly.\nThe key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode, in the following sense.\nSuppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512. To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          x\n          \n            512\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},...,x_{512}}\n  \n, taking time \n  \n    \n      \n        512\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 512T_{\\text{GPT-3}}}\n  \n. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n is indeed the token with the largest log-likelihood in the \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n-th output.\nIn speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose we use GPT-3-small to generate four speculative tokens: \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1},{\\tilde {x}}_{2},{\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n  \n. This only takes \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3-small\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3-small}}}\n  \n. These tokens are then run through the larger GPT-3 in one go. Suppose that \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{1}}\n  \n and \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{2}}\n  \n are verified by GPT-3 as what it would have picked, then those are kept, but \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{3}}\n  \n is not, so \n  \n    \n      \n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            3\n          \n        \n        ,\n        \n          \n            \n              \n                x\n                ~\n              \n            \n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle {\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n  \n are discarded, and GPT-3 is run on those. This would take \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3-small\n          \n        \n        +\n        3\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3-small}}+3T_{\\text{GPT-3}}}\n  \n, which might be shorter than \n  \n    \n      \n        4\n        \n          T\n          \n            GPT-3\n          \n        \n      \n    \n    {\\displaystyle 4T_{\\text{GPT-3}}}\n  \n.\nFor non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.\n\nIn Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability. However, that vector can then be further processed by another Transformer block to predict the next token, and so on for arbitrarily many steps into the future. This trades off accuracy for speed, since each new token costs just one more Transformer block, rather than the entire stack.\n\n\n=== Sub-quadratic transformers ===\nTraining transformer-based architectures can be expensive, especially for long inputs. Many methods have been developed to attempt to address the issue. In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows. In the audio domain, SepTr decouples the attention in time and frequency domains. Long Range Arena (2020) is a standard benchmark for comparing the behavior of transformer architectures over long inputs.\n\n\n==== Alternative attention graphs ====\nThe standard attention graph is either all-to-all or causal, both of which scales as \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n where \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of tokens in a sequence.\nReformer (2020) reduces the computational load from \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n to \n  \n    \n      \n        O\n        (\n        N\n        ln\n        ⁡\n        N\n        )\n      \n    \n    {\\displaystyle O(N\\ln N)}\n  \n by using locality-sensitive hashing and reversible layers.\nSparse attention uses attention graphs that grows slower than \n  \n    \n      \n        O\n        (\n        \n          N\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle O(N^{2})}\n  \n. For example, BigBird (2020) uses random small-world networks which grows as \n  \n    \n      \n        O\n        (\n        N\n        )\n      \n    \n    {\\displaystyle O(N)}\n  \n.\nOrdinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.\n\n\n==== Random Feature Attention ====\nRandom Feature Attention (2021) uses Fourier random features:\n  \n    \n      \n        φ\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              D\n            \n          \n        \n        [\n        cos\n        ⁡\n        ⟨\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        ⟩\n        ,\n        sin\n        ⁡\n        ⟨\n        \n          w\n          \n            1\n          \n        \n        ,\n        x\n        ⟩\n        ,\n        ⋯\n        cos\n        ⁡\n        ⟨\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        ⟩\n        ,\n        sin\n        ⁡\n        ⟨\n        \n          w\n          \n            D\n          \n        \n        ,\n        x\n        ⟩\n        \n          ]\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (x)={\\frac {1}{\\sqrt {D}}}[\\cos \\langle w_{1},x\\rangle ,\\sin \\langle w_{1},x\\rangle ,\\cdots \\cos \\langle w_{D},x\\rangle ,\\sin \\langle w_{D},x\\rangle ]^{T}}\n  \nwhere \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are independent samples from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          σ\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n. This choice of parameters satisfy \n  \n    \n      \n        \n          E\n        \n        [\n        ⟨\n        φ\n        (\n        x\n        )\n        ,\n        φ\n        (\n        y\n        )\n        ⟩\n        ]\n        =\n        \n          e\n          \n            −\n            \n              \n                \n                  ‖\n                  x\n                  −\n                  y\n                  \n                    ‖\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {E} [\\langle \\varphi (x),\\varphi (y)\\rangle ]=e^{-{\\frac {\\|x-y\\|^{2}}{2\\sigma ^{2}}}}}\n  \n, or \n  \n    \n      \n        \n          e\n          \n            ⟨\n            x\n            ,\n            y\n            ⟩\n            \n              /\n            \n            \n              σ\n              \n                2\n              \n            \n          \n        \n        =\n        \n          E\n        \n        [\n        ⟨\n        \n          e\n          \n            ‖\n            x\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            ‖\n            y\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        y\n        )\n        ⟩\n        ]\n        ≈\n        ⟨\n        \n          e\n          \n            ‖\n            x\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        x\n        )\n        ,\n        \n          e\n          \n            ‖\n            y\n            \n              ‖\n              \n                2\n              \n            \n            \n              /\n            \n            2\n            \n              σ\n              \n                2\n              \n            \n          \n        \n        φ\n        (\n        y\n        )\n        ⟩\n      \n    \n    {\\displaystyle e^{\\langle x,y\\rangle /\\sigma ^{2}}=\\mathbb {E} [\\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle ]\\approx \\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle }\n  \nConsequently, the one-headed attention, with one query, can be written as \n  \n    \n      \n        \n          Attention\n        \n        (\n        q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        ≈\n        \n          \n            \n              φ\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                ∑\n                \n                  i\n                \n              \n              \n                e\n                \n                  ‖\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    ‖\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n              φ\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n              \n                v\n                \n                  i\n                \n                \n                  T\n                \n              \n            \n            \n              φ\n              (\n              q\n              \n                )\n                \n                  T\n                \n              \n              \n                ∑\n                \n                  i\n                \n              \n              \n                e\n                \n                  ‖\n                  \n                    k\n                    \n                      i\n                    \n                  \n                  \n                    ‖\n                    \n                      2\n                    \n                  \n                  \n                    /\n                  \n                  2\n                  \n                    σ\n                    \n                      2\n                    \n                  \n                \n              \n              φ\n              (\n              \n                k\n                \n                  i\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Attention}}(q,K,V)={\\text{softmax}}\\left({\\frac {qK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx {\\frac {\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})v_{i}^{T}}{\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})}}}\n  \nwhere \n  \n    \n      \n        σ\n        =\n        \n          d\n          \n            K\n          \n          \n            1\n            \n              /\n            \n            4\n          \n        \n      \n    \n    {\\displaystyle \\sigma =d_{K}^{1/4}}\n  \n. Similarly for multiple queries, and for multiheaded attention.\nThis approximation can be computed in linear time, as we can compute the matrix \n  \n    \n      \n        φ\n        (\n        \n          k\n          \n            i\n          \n        \n        )\n        \n          v\n          \n            i\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\varphi (k_{i})v_{i}^{T}}\n  \n first, then multiply it with the query. In essence, we have managed to obtain a more precise version of \n  \n    \n      \n        \n          Attention\n        \n        (\n        Q\n        ,\n        K\n        ,\n        V\n        )\n        =\n        \n          softmax\n        \n        \n          (\n          \n            \n              \n                Q\n                \n                  K\n                  \n                    \n                      T\n                    \n                  \n                \n              \n              \n                \n                  d\n                  \n                    k\n                  \n                \n              \n            \n          \n          )\n        \n        V\n        ≈\n        Q\n        (\n        \n          K\n          \n            T\n          \n        \n        V\n        \n          /\n        \n        \n          \n            \n              d\n              \n                k\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx Q(K^{T}V/{\\sqrt {d_{k}}})}\n  \nPerformer (2022) uses the same Random Feature Attention, but \n  \n    \n      \n        \n          w\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          w\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle w_{1},...,w_{D}}\n  \n are first independently sampled from the normal distribution \n  \n    \n      \n        N\n        (\n        0\n        ,\n        \n          σ\n          \n            2\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle N(0,\\sigma ^{2}I)}\n  \n, then they are Gram-Schmidt processed.\n\n\n=== Multimodality ===\nTransformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality.\nMultimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning. The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.\nVision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.\nConformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.\nPerceivers are a variant of Transformers designed for multimodality.\nFor image generation, notable architectures are DALL-E 1 (2021), Parti (2022), Phenaki (2023), and Muse (2023). Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image. Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image. Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted. Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.\n\n\n== Applications ==\nThe transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, Gemini, AlbertAGPT, Claude, BERT, Grok, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including:\n\nmachine translation\ntime series prediction\ndocument summarization\ndocument generation\nnamed entity recognition (NER)\nwriting computer code based on requirements expressed in natural language.\nspeech-to-text\nBeyond traditional NLP, the transformer architecture has had success in other applications, such as:\n\nbiological sequence analysis\nvideo understanding\nprotein folding (such as AlphaFold)\nevaluating chess board positions. Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level.\n\n\n== See also ==\nseq2seq – Family of machine learning approaches\nPerceiver – Variant of Transformer designed for multimodal data\nVision transformer – Machine learning model for vision processing\nLarge language model – Type of machine learning model\nBERT (language model) – Series of language models developed by Google AI\nGenerative pre-trained transformer – Type of large language model\nT5 (language model) – Series of large language models developed by Google AI\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==",
      "cleaned_text": "In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets. The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers). For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens. A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers. However, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer. The idea of encoder-decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014. A 380M-parameter model for machine translation uses two long short-term memories (LSTM). Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq. These early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation. The RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\". The relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time. In 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop. Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title \"attention is all you need\". That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical. In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs. In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks. Already in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom. In language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder-RNN-decoder model by a Transformer-encoder-RNN-decoder model. Starting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models. Since 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The vision transformer, in turn, stimulated new developments in convolutional neural networks. Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024), and Sora (2024), use Transformers to analyse input data (like text prompts) by breaking it down into \"tokens\" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data. The plain transformer architecture had difficulty converging. In the original paper the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again. A 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup. Transformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include: language modeling next-sentence prediction question answering reading comprehension sentiment analysis paraphrasing The T5 transformer report documents a large number of natural language pretraining tasks. Some examples are: restoring or repairing incomplete or corrupted text. For example, the input, \"Thank you ~~ me to your party ~~ week\", might generate the output, \"Thank you for inviting me to your party last week\". translation between natural languages (machine translation) judging the pragmatic acceptability of natural language. For example, the following sentence might be judged \"not acceptable\", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well. Note that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture. In general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\". These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer. In a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: Loss = − ∑ t ∈ masked tokens ln ⁡ ( probability of t conditional on its context ) {\\displaystyle { ext{Loss}}=-\\sum _{t\\in { ext{masked tokens}}}\\ln({ ext{probability of }}t{ ext{ conditional on its context}})} and the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task. In an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks. In a prefixLM task, the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks. Note that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model). All transformers have the same primary components: Tokenizers, which convert text into tokens. Embedding layer, which converts tokens and positions of the tokens into vector representations. Transformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. These consist of alternating attention and feedforward layers. There are two major types of transformer layers: encoder layers and decoder layers, with further variants. Un-embedding layer, which converts the final vector representations back to a probability distribution over the tokens. The following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section. By convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as x W {\\displaystyle xW} . As the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences is a tokenizer. The set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size n vocabulary {\\displaystyle n_{ ext{vocabulary}}} . When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\". Some commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece. Each token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix M {\\displaystyle M} . For example, if the input token is 3 {\\displaystyle 3} , then the one-hot representation is [ 0 , 0 , 0 , 1 , 0 , 0 , ... ] {\\displaystyle [0,0,0,1,0,0,\\dots ]} , and its embedding vector is E m b e d ( 3 ) = [ 0 , 0 , 0 , 1 , 0 , 0 , ... ] M {\\displaystyle \\mathrm {Embed} (3)=[0,0,0,1,0,0,\\dots ]M} The token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors. The number of dimensions in an embedding vector is called hidden size or embedding size and written as d emb {\\displaystyle d_{ ext{emb}}} . This size is written as d model {\\displaystyle d_{ ext{model}}} in the original Transformer paper. An un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens. The un-embedding layer is a linear-softmax layer: U n E m b e d ( x ) = s o f t m a x ( x W + b ) {\\displaystyle \\mathrm {UnEmbed} (x)=\\mathrm {softmax} (xW+b)} The matrix has shape ( d emb , n vocabulary ) {\\displaystyle (d_{ ext{emb}},n_{ ext{vocabulary}})} . The embedding matrix M {\\displaystyle M} and the un-embedding matrix W {\\displaystyle W} are sometimes required to be transposes of each other, a practice called weight tying. A positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This induces a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\". The positional encoding is defined as a function of type f : R → R d ; d ∈ Z , d > 0 {\\displaystyle f:\\mathbb {R} o \\mathbb {R} ^{d};d\\in \\mathbb {Z} ,d>0} , where d {\\displaystyle d} is a positive even integer. The full positional encoding defined in the original paper is: ( f ( t ) 2 k , f ( t ) 2 k + 1 ) = ( sin ⁡ ( θ ) , cos ⁡ ( θ ) ) ∀ k ∈ { 0 , 1 , ... , d / 2 − 1 } {\\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\\sin( heta ),\\cos( heta ))\\quad \\forall k\\in \\{0,1,\\ldots ,d/2-1\\}} where θ = t r k , r = N 2 / d {\\displaystyle heta ={\\frac {t}{r^{k}}},r=N^{2/d}} . Here, N {\\displaystyle N} is a free parameter that should be significantly larger than the biggest k {\\displaystyle k} that would be input into the positional encoding function. The original paper uses N = 10000 {\\displaystyle N=10000} . The function is in a simpler form when written as a complex function of type f : R → C d / 2 {\\displaystyle f:\\mathbb {R} o \\mathbb {C} ^{d/2}} f ( t ) = ( e i t / r k ) k = 0 , 1 , ... , d 2 − 1 {\\displaystyle f(t)=\\left(e^{it/r^{k}}\\right)_{k=0,1,\\ldots ,{\\frac {d}{2}}-1}} where r = N 2 / d {\\displaystyle r=N^{2/d}} . The main reason for using this positional encoding function is that using it, shifts are linear transformations: f ( t + Δ t ) = d i a g ( f ( Δ t ) ) f ( t ) {\\displaystyle f(t+\\Delta t)=\\mathrm {diag} (f(\\Delta t))f(t)} where Δ t ∈ R {\\displaystyle \\Delta t\\in \\mathbb {R} } is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication. By taking a linear sum, any convolution can also be implemented as linear transformations: ∑ j c j f ( t + Δ t j ) = ( ∑ j c j d i a g ( f ( Δ t j ) ) ) f ( t ) {\\displaystyle \\sum _{j}c_{j}f(t+\\Delta t_{j})=\\left(\\sum _{j}c_{j}\\,\\mathrm {diag} (f(\\Delta t_{j}))\\right)f(t)} for any constants c j {\\displaystyle c_{j}} . This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\" In typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference. Like earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far. The purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for \"mixing\" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time). Both the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps. These feed-forward layers contain most of the parameters in a Transformer model. The feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons: F F N ( x ) = ϕ ( x W ( 1 ) + b ( 1 ) ) W ( 2 ) + b ( 2 ) {\\displaystyle \\mathrm {FFN} (x)=\\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}} where W ( 1 ) {\\displaystyle W^{(1)}} and W ( 2 ) {\\displaystyle W^{(2)}} are weight matrices and b ( 1 ) {\\displaystyle b^{(1)}} and b ( 2 ) {\\displaystyle b^{(2)}} are bias vectors, and ϕ {\\displaystyle \\phi } is its activation function. The original Transformer used ReLU activation. The number of neurons in the middle layer is called intermediate size (GPT), filter size (BERT), or feedforward size (BERT). It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: d ffn = 4 d emb {\\displaystyle d_{ ext{ffn}}=4d_{ ext{emb}}} . The attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights W Q {\\displaystyle W^{Q}} , the key weights W K {\\displaystyle W^{K}} , and the value weights W V {\\displaystyle W^{V}} . The module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length ℓ seq, query {\\displaystyle \\ell _{ ext{seq, query}}} , and each entry is a vector of dimension d emb, query {\\displaystyle d_{ ext{emb, query}}} . Similarly for the key and value sequences. For each vector x i , query {\\displaystyle x_{i,{ ext{query}}}} in the query sequence, it is multiplied by a matrix W Q {\\displaystyle W^{Q}} to produce a query vector q i = x i , query W Q {\\displaystyle q_{i}=x_{i,{ ext{query}}}W^{Q}} . The matrix of all query vectors is the query matrix: Q = X query W Q {\\displaystyle Q=X_{ ext{query}}W^{Q}} Similarly, we construct the key matrix K = X key W K {\\displaystyle K=X_{ ext{key}}W^{K}} and the value matrix V = X value W V {\\displaystyle V=X_{ ext{value}}W^{V}} . It is usually the case that all W Q , W K , W V {\\displaystyle W^{Q},W^{K},W^{V}} are square matrices, meaning d emb, query = d query {\\displaystyle d_{ ext{emb, query}}=d_{ ext{query}}} , etc. Attention weights are calculated using the query and key vectors: the attention weight a i j {\\displaystyle a_{ij}} from token i {\\displaystyle i} to token j {\\displaystyle j} is the dot product between q i {\\displaystyle q_{i}} and k j {\\displaystyle k_{j}} . The attention weights are divided by the square root of the dimension of the key vectors, d k {\\displaystyle {\\sqrt {d_{k}}}} , which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that W Q {\\displaystyle W^{Q}} and W K {\\displaystyle W^{K}} are different matrices allows attention to be non-symmetric: if token i {\\displaystyle i} attends to token j {\\displaystyle j} (i.e. q i ⋅ k j {\\displaystyle q_{i}\\cdot k_{j}} is large), this does not necessarily mean that token j {\\displaystyle j} will attend to token i {\\displaystyle i} (i.e. q j ⋅ k i {\\displaystyle q_{j}\\cdot k_{i}} could be small). The output of the attention unit for token i {\\displaystyle i} is the weighted sum of the value vectors of all tokens, weighted by a i j {\\displaystyle a_{ij}} , the attention from token i {\\displaystyle i} to each token. The attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices Q {\\displaystyle Q} , K {\\displaystyle K} and V {\\displaystyle V} are defined as the matrices where the i {\\displaystyle i} th rows are vectors q i {\\displaystyle q_{i}} , k i {\\displaystyle k_{i}} , and v i {\\displaystyle v_{i}} respectively. Then we can represent the attention as Attention ( Q , K , V ) = softmax ( Q K T d k ) V {\\displaystyle {\\begin{aligned}{ ext{Attention}}(Q,K,V)={ ext{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}} where the softmax is applied over each of the rows of the matrix. The number of dimensions in a query vector is query size d query {\\displaystyle d_{ ext{query}}} and similarly for the key size d key {\\displaystyle d_{ ext{key}}} and value size d value {\\displaystyle d_{ ext{value}}} . The output dimension of an attention head is its head dimension d head {\\displaystyle d_{ ext{head}}} . The attention mechanism requires the following three equalities to hold: ℓ seq, key = ℓ seq, value , d query = d key , d value = d head {\\displaystyle \\ell _{ ext{seq, key}}=\\ell _{ ext{seq, value}},\\;d_{ ext{query}}=d_{ ext{key}},\\;d_{ ext{value}}=d_{ ext{head}}} but is otherwise unconstrained. If the attention head is used in a self-attention fashion, then X query = X key = X value {\\displaystyle X_{ ext{query}}=X_{ ext{key}}=X_{ ext{value}}} . If the attention head is used in a cross-attention fashion, then usually X query ≠ X key = X value {\\displaystyle X_{ ext{query}} eq X_{ ext{key}}=X_{ ext{value}}} . It is theoretically possible for all three to be different, but that is rarely the case in practice. One set of ( W Q , W K , W V ) {\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)} matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\". Specifically, the query and key projection matrices, W Q {\\displaystyle W^{Q}} and W K {\\displaystyle W^{K}} , which are involved in the attention score computation, defines the \"relevance\". Meanwhile, the value projection matrix W V {\\displaystyle W^{V}} , in combination with the part of the output projection matrix W O {\\displaystyle W^{O}} , determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects. The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers. Concretely, let the multiple attention heads be indexed by i {\\displaystyle i} , then we have MultiheadedAttention ( Q , K , V ) = Concat i ∈ [ n heads ] ( Attention ( X W i Q , X W i K , X W i V ) ) W O {\\displaystyle { ext{MultiheadedAttention}}(Q,K,V)={ ext{Concat}}_{i\\in [n_{ ext{heads}}]}({ ext{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V}))W^{O}} where the matrix X {\\displaystyle X} is the concatenation of word embeddings, and the matrices W i Q , W i K , W i V {\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}} are \"projection matrices\" owned by individual attention head i {\\displaystyle i} , and W O {\\displaystyle W^{O}} is a final projection matrix owned by the whole multi-headed attention head. It is theoretically possible for each attention head to have a different head dimension d head {\\displaystyle d_{ ext{head}}} , but that is rarely the case in practice. As an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions: d emb = 768 , n head = 12 , d head = 64 {\\displaystyle d_{ ext{emb}}=768,n_{ ext{head}}=12,d_{ ext{head}}=64} Since 12 × 64 = 768 {\\displaystyle 12 imes 64=768} , its output projection matrix W O ∈ R ( 12 × 64 ) × 768 {\\displaystyle W^{O}\\in \\mathbb {R} ^{(12 imes 64) imes 768}} is a square matrix. The Transformer architecture is constructed to calculate output tokens iteratively. Assuming t = 0 {\\displaystyle t=0} refers to the calculation of the first output token i = 0 {\\displaystyle i=0} , for step t > 0 {\\displaystyle t>0} , the output token i = 0 {\\displaystyle i=0} shall remain constant. This ensures properties of the model similar to autoregressive models. Therefore, at every time step t {\\displaystyle t} , the calculation for all outputs i {\\displaystyle i} should not have access to tokens at position j {\\displaystyle j} for j >= i {\\displaystyle j>=i} (as it naturally is the case for time step t = i {\\displaystyle t=i} , when tokens j > t {\\displaystyle j>t} are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix M {\\displaystyle M} that is − ∞ {\\displaystyle -\\infty } at entries where the attention link must be cut, and 0 {\\displaystyle 0} at other places: MaskedAttention ( Q , K , V ) = softmax ( M + Q K T d k ) V {\\displaystyle {\\begin{aligned}{ ext{MaskedAttention}}(Q,K,V)={ ext{softmax}}\\left(M+{\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}} The following matrix is commonly used in decoder self-attention modules, called \"causal masking\": M causal = [ 0 − ∞ − ∞ ... − ∞ 0 0 − ∞ ... − ∞ 0 0 0 ... − ∞ ⋮ ⋮ ⋮ ⋱ ⋮ 0 0 0 ... 0 ] {\\displaystyle M_{ ext{causal}}={\\begin{bmatrix}0&-\\infty &-\\infty &\\dots &-\\infty \\\\0&0&-\\infty &\\dots &-\\infty \\\\0&0&0&\\dots &-\\infty \\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix}}} In words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form P M causal P − 1 {\\displaystyle PM_{ ext{causal}}P^{-1}} , where P {\\displaystyle P} is a random permutation matrix. An encoder consists of an embedding layer, followed by multiple encoder layers. Each encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have: given input vectors h 0 , h 1 , ... combine them into a matrix H = [ h 0 h 1 ⋮ ] EncoderLayer ( H ) = [ FFN ( MultiheadedAttention ( H , H , H ) 0 ) FFN ( MultiheadedAttention ( H , H , H ) 1 ) ⋮ ] {\\displaystyle {\\begin{aligned}{ ext{given input vectors }}&h_{0},h_{1},\\dots \\\\{ ext{combine them into a matrix }}H&={\\begin{bmatrix}h_{0}\\\\h_{1}\\\\\\vdots \\end{bmatrix}}\\\\{ ext{EncoderLayer}}(H)&={\\begin{bmatrix}{ ext{FFN}}({ ext{MultiheadedAttention}}(H,H,H)_{0})\\\\{ ext{FFN}}({ ext{MultiheadedAttention}}(H,H,H)_{1})\\\\\\vdots \\end{bmatrix}}\\\\\\end{aligned}}} where FFN {\\displaystyle { ext{FFN}}} stands for \"feed-forward network\". We can more succinctly write it as EncoderLayer ( H ) = FFN ( MultiheadedAttention ( H , H , H ) ) {\\displaystyle { ext{EncoderLayer}}(H)={ ext{FFN}}({ ext{MultiheadedAttention}}(H,H,H))} with the implicit convention that the FFN {\\displaystyle { ext{FFN}}} is applied to each row of the matrix individually. The encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder. As the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking. A decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer. Each decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention. Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow. This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked. In contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism. Schematically, we have: H ′ = MaskedMultiheadedAttention ( H , H , H ) DecoderLayer ( H ) = FFN ( MultiheadedAttention ( H ′ , H E , H E ) ) {\\displaystyle {\\begin{aligned}H'&={ ext{MaskedMultiheadedAttention}}(H,H,H)\\\\{ ext{DecoderLayer}}(H)&={ ext{FFN}}({ ext{MultiheadedAttention}}(H',H^{E},H^{E}))\\end{aligned}}} where H E {\\displaystyle H^{E}} is the matrix with rows being the output vectors from the encoder. The last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text. Many large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture. Early GPT models are decoder-only models trained to predict the next token in a sequence. BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence. Each encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network. The final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence. The residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y = F(x) + x. The expression indicates that an output y is the sum of the transformation of input x (F(x)) and the input itself (x). Adding the input x can preserve the input information and avoid issues when the gradient of F(x) is close to zero. Similarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector. There are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is L a y e r N o r m ( x + S u b l a y e r ( x ) ) {\\displaystyle \\mathrm {LayerNorm} (x+\\mathrm {Sublayer} (x))} where S u b l a y e r ( x ) {\\displaystyle \\mathrm {Sublayer} (x)} is the function implemented by the sublayer itself. In the pre-LN convention, the output of each sublayer is x + S u b l a y e r ( L a y e r N o r m ( x ) ) {\\displaystyle x+\\mathrm {Sublayer} (\\mathrm {LayerNorm} (x))} The original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence. The following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from input: Encoder input t_e Decoder input t_d output: Array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence)) /* encoder */ z_e ← encoder.tokenizer(t_e) for each t in 1:length(z_e) do z_e[t] ← encoder.embedding(z_e[t]) + encoder.positional_embedding(t) for each l in 1:length(encoder.layers) do layer ← encoder.layers[l] /* first sublayer */ z_e_copy ← copy(z_e) for each t in 1:length(z_e) do z_e[t] ← layer.layer_norm(z_e[t]) z_e ← layer.multiheaded_attention(z_e, z_e, z_e) for each t in 1:length(z_e) do z_e[t] ← z_e[t] + z_e_copy[t] /* second sublayer */ z_e_copy ← copy(z_e) for each t in 1:length(z_e) do z_e[t] ← layer.layer_norm(z_e[t]) z_e ← layer.feedforward(z_e) for each t in 1:length(z_e) do z_e[t] ← z_e[t] + z_e_copy[t] for each t in 1:length(z_e) do z_e[t] ← encoder.final_layer_norm(z_e[t]) /* decoder */ z_d ← decoder.tokenizer(t_d) for each t in 1:length(z_d) do z_d[t] ← decoder.embedding(z_d[t]) + decoder.positional_embedding(t) for each l in 1:length(decoder.layers) do layer ← decoder.layers[l] /* first sublayer */ z_d_copy ← copy(z_d) for each t in 1:length(z_d) do z_d[t] ← layer.layer_norm(z_d[t]) z_d ← layer.masked_multiheaded_attention(z_d, z_d, z_d) for each t in 1:length(z_d) do z_d[t] ← z_d[t] + z_d_copy[t] /* second sublayer */ z_d_copy ← copy(z_d) for each t in 1:length(z_d) do z_d[t] ← layer.layer_norm(z_d[t]) z_d ← layer.multiheaded_attention(z_d, z_e, z_e) for each i in 1:length(z_d) do z_d[t] ← z_d[t] + z_d_copy[t] /* third sublayer */ z_d_copy ← copy(z_d) for each t in 1:length(z_d) do z_d[t] ← layer.layer_norm(z_d[t]) z_d ← layer.feedforward(z_d) for each t in 1:length(z_d) do z_d[t] ← z_d[t] + z_d_copy[t] z_d ← decoder.final_layer_norm(z_d) output_distributions ← [] for each t in 1:length(z_d) do output_distributions.append(decoder.unembed(z_d[t])) return output_distributions The Transformer architecture, being modular, allows variations. Several common variations are described here. An \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder. A \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only. An \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder. A \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form M prefixLM = [ 0 − ∞ 0 M causal ] {\\displaystyle M_{ ext{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{ ext{causal}}\\end{bmatrix}}} where the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less \"sparsity\". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons. There are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder-RNN-decoder model by a Transformer-encoder-RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively. The original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU; both GPT-1 and BERT used GELU. Alternative activation functions are often used in combination with Gated Linear Units in the feedforward module. The normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm which is used in the Llama series. Other examples include CapsuleNorm ScaleNorm, or FixNorm. Transformers may use other positional encoding methods than sinusoidal. The original Transformer paper reported using a learned positional encoding, but finding it not superior to the sinusoidal one. Later, found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module. RoPE (rotary positional embedding), is best explained by considering a list of 2-dimensional vectors [ ( x 1 ( 1 ) , x 1 ( 2 ) ) , ( x 2 ( 1 ) , x 2 ( 2 ) ) , ( x 3 ( 1 ) , x 3 ( 2 ) ) , . . . ] {\\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]} . Now pick some angle θ {\\displaystyle heta } . Then RoPE encoding is RoPE ( x m ( 1 ) , x m ( 2 ) , m ) = ( cos ⁡ m θ − sin ⁡ m θ sin ⁡ m θ cos ⁡ m θ ) ( x m ( 1 ) x m ( 2 ) ) = ( x m ( 1 ) cos ⁡ m θ − x m ( 2 ) sin ⁡ m θ x m ( 2 ) cos ⁡ m θ + x m ( 1 ) sin ⁡ m θ ) {\\displaystyle { ext{RoPE}}{\\big (}x_{m}^{(1)},x_{m}^{(2)},m{\\big )}={\\begin{pmatrix}\\cos m heta &-\\sin m heta \\\\\\sin m heta &\\cos m heta \\end{pmatrix}}{\\begin{pmatrix}x_{m}^{(1)}\\\\x_{m}^{(2)}\\\\\\end{pmatrix}}={\\begin{pmatrix}x_{m}^{(1)}\\cos m heta -x_{m}^{(2)}\\sin m heta \\\\x_{m}^{(2)}\\cos m heta +x_{m}^{(1)}\\sin m heta \\\\\\end{pmatrix}}} Equivalently, if we write the 2-dimensional vectors as complex numbers z m := x m ( 1 ) + i x m ( 2 ) {\\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}} , then RoPE encoding is just multiplication by an angle: RoPE ( z m , m ) = e i m θ z m {\\displaystyle { ext{RoPE}}{\\big (}z_{m},m{\\big )}=e^{im heta }z_{m}} For a list of 2 n {\\displaystyle 2n} -dimensional vectors, a RoPE encoder is defined by a sequence of angles θ ( 1 ) , . . . , θ ( n ) {\\displaystyle heta ^{(1)},..., heta ^{(n)}} . Then the RoPE encoding is applied to each pair of coordinates. The benefit of RoPE is that the dot-product between two vectors depends on their relative location only: RoPE ( x , m ) T RoPE ( y , n ) = RoPE ( x , m + k ) T RoPE ( y , n + k ) {\\displaystyle { ext{RoPE}}{\\big (}x,m{\\big )}^{T}{ ext{RoPE}}{\\big (}y,n{\\big )}={ ext{RoPE}}{\\big (}x,m+k{\\big )}^{T}{ ext{RoPE}}{\\big (}y,n+k{\\big )}} for any integer k {\\displaystyle k} . ALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is Attention ( Q , K , V ) = softmax ( Q K T d k + s B ) V {\\displaystyle {\\begin{aligned}{ ext{Attention}}(Q,K,V)={ ext{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+sB\\right)V\\end{aligned}}} Here, s {\\displaystyle s} is a real number (\"scalar\"), and B {\\displaystyle B} is the linear bias matrix defined by B = ( 0 1 2 3 ⋯ − 1 0 1 2 ⋯ − 2 − 1 0 1 ⋯ − 3 − 2 − 1 0 ⋯ ⋮ ⋮ ⋮ ⋮ ⋱ ) {\\displaystyle B={\\begin{pmatrix}0&1&2&3&\\cdots \\\\-1&0&1&2&\\cdots \\\\-2&-1&0&1&\\cdots \\\\-3&-2&-1&0&\\cdots \\\\\\vdots &\\vdots &\\vdots &\\vdots &\\ddots \\\\\\end{pmatrix}}} in other words, B i , j = j − i {\\displaystyle B_{i,j}=j-i} . The idea being that the linear bias matrix is a softened mask. Just as 0 {\\displaystyle 0} represent full attention paid, and − ∞ {\\displaystyle -\\infty } represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction. ALiBi allows pretraining on short context windows, then fine-tuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located). Relative Position Encodings is similar to ALiBi, but more generic: Attention ( Q , K , V ) = softmax ( Q K T d k + B ) V {\\displaystyle {\\begin{aligned}{ ext{Attention}}(Q,K,V)={ ext{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+B\\right)V\\end{aligned}}} where B {\\displaystyle B} is a Toeplitz matrix, that is, B i , j = B i ′ , j ′ {\\displaystyle B_{i,j}=B_{i',j'}} whenever i − j = i ′ − j ′ {\\displaystyle i-j=i'-j'} . This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\". The transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models. When an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching. If a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots. FlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow). See the page on softmax for details. An improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention. Key advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA). Benchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8. Multi-Query Attention changes the multiheaded attention mechanism. Whereas normally, MultiheadedAttention ( Q , K , V ) = Concat i ∈ [ n heads ] ( Attention ( X W i Q , X W i K , X W i V ) ) W O {\\displaystyle { ext{MultiheadedAttention}}(Q,K,V)={ ext{Concat}}_{i\\in [n_{ ext{heads}}]}\\left({ ext{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\\right)W^{O}} with Multi-Query Attention, there is just one W K , W V {\\displaystyle W^{K},W^{V}} , thus: MultiQueryAttention ( Q , K , V ) = Concat i ∈ [ n heads ] ( Attention ( X W i Q , X W K , X W V ) ) W O {\\displaystyle { ext{MultiQueryAttention}}(Q,K,V)={ ext{Concat}}_{i\\in [n_{ ext{heads}}]}\\left({ ext{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\\right)W^{O}} This has a neutral effect on model quality and training speed, but increases inference speed. More generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups. Multihead Latent Attention (MLA) is a low-rank approximation to standard MHA. Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces (\"latent space\"), one for query and one for key-value (KV vector). This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached. Speculative decoding is a method to accelerate token decoding. Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified. If the quickly computed tokens are incorrect, they are discarded and computed slowly. The key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode, in the following sense. Suppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512. To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token x 1 , x 2 , . . . , x 512 {\\displaystyle x_{1},x_{2},...,x_{512}} , taking time 512 T GPT-3 {\\displaystyle 512T_{ ext{GPT-3}}} . However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each x t {\\displaystyle x_{t}} is indeed the token with the largest log-likelihood in the t {\\displaystyle t} -th output. In speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose we use GPT-3-small to generate four speculative tokens: x ~ 1 , x ~ 2 , x ~ 3 , x ~ 4 {\\displaystyle { ilde {x}}_{1},{ ilde {x}}_{2},{ ilde {x}}_{3},{ ilde {x}}_{4}} . This only takes 4 T GPT-3-small {\\displaystyle 4T_{ ext{GPT-3-small}}} . These tokens are then run through the larger GPT-3 in one go. Suppose that x ~ 1 {\\displaystyle { ilde {x}}_{1}} and x ~ 2 {\\displaystyle { ilde {x}}_{2}} are verified by GPT-3 as what it would have picked, then those are kept, but x ~ 3 {\\displaystyle { ilde {x}}_{3}} is not, so x ~ 3 , x ~ 4 {\\displaystyle { ilde {x}}_{3},{ ilde {x}}_{4}} are discarded, and GPT-3 is run on those. This would take 4 T GPT-3-small + 3 T GPT-3 {\\displaystyle 4T_{ ext{GPT-3-small}}+3T_{ ext{GPT-3}}} , which might be shorter than 4 T GPT-3 {\\displaystyle 4T_{ ext{GPT-3}}} . For non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used. In Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability. However, that vector can then be further processed by another Transformer block to predict the next token, and so on for arbitrarily many steps into the future. This trades off accuracy for speed, since each new token costs just one more Transformer block, rather than the entire stack. Training transformer-based architectures can be expensive, especially for long inputs. Many methods have been developed to attempt to address the issue. In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows. In the audio domain, SepTr decouples the attention in time and frequency domains. Long Range Arena (2020) is a standard benchmark for comparing the behavior of transformer architectures over long inputs. The standard attention graph is either all-to-all or causal, both of which scales as O ( N 2 ) {\\displaystyle O(N^{2})} where N {\\displaystyle N} is the number of tokens in a sequence. Reformer (2020) reduces the computational load from O ( N 2 ) {\\displaystyle O(N^{2})} to O ( N ln ⁡ N ) {\\displaystyle O(N\\ln N)} by using locality-sensitive hashing and reversible layers. Sparse attention uses attention graphs that grows slower than O ( N 2 ) {\\displaystyle O(N^{2})} . For example, BigBird (2020) uses random small-world networks which grows as O ( N ) {\\displaystyle O(N)} . Ordinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value. Random Feature Attention (2021) uses Fourier random features: φ ( x ) = 1 D [ cos ⁡ ⟨ w 1 , x ⟩ , sin ⁡ ⟨ w 1 , x ⟩ , ⋯ cos ⁡ ⟨ w D , x ⟩ , sin ⁡ ⟨ w D , x ⟩ ] T {\\displaystyle \\varphi (x)={\\frac {1}{\\sqrt {D}}}[\\cos \\langle w_{1},x\\rangle ,\\sin \\langle w_{1},x\\rangle ,\\cdots \\cos \\langle w_{D},x\\rangle ,\\sin \\langle w_{D},x\\rangle ]^{T}} where w 1 , . . . , w D {\\displaystyle w_{1},...,w_{D}} are independent samples from the normal distribution N ( 0 , σ 2 I ) {\\displaystyle N(0,\\sigma ^{2}I)} . This choice of parameters satisfy E [ ⟨ φ ( x ) , φ ( y ) ⟩ ] = e − ‖ x − y ‖ 2 2 σ 2 {\\displaystyle \\mathbb {E} [\\langle \\varphi (x),\\varphi (y)\\rangle ]=e^{-{\\frac {\\|x-y\\|^{2}}{2\\sigma ^{2}}}}} , or e ⟨ x , y ⟩ / σ 2 = E [ ⟨ e ‖ x ‖ 2 / 2 σ 2 φ ( x ) , e ‖ y ‖ 2 / 2 σ 2 φ ( y ) ⟩ ] ≈ ⟨ e ‖ x ‖ 2 / 2 σ 2 φ ( x ) , e ‖ y ‖ 2 / 2 σ 2 φ ( y ) ⟩ {\\displaystyle e^{\\langle x,y\\rangle /\\sigma ^{2}}=\\mathbb {E} [\\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle ]\\approx \\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle } Consequently, the one-headed attention, with one query, can be written as Attention ( q , K , V ) = softmax ( q K T d k ) V ≈ φ ( q ) T ∑ i e ‖ k i ‖ 2 / 2 σ 2 φ ( k i ) v i T φ ( q ) T ∑ i e ‖ k i ‖ 2 / 2 σ 2 φ ( k i ) {\\displaystyle { ext{Attention}}(q,K,V)={ ext{softmax}}\\left({\\frac {qK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx {\\frac {\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})v_{i}^{T}}{\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})}}} where σ = d K 1 / 4 {\\displaystyle \\sigma =d_{K}^{1/4}} . Similarly for multiple queries, and for multiheaded attention. This approximation can be computed in linear time, as we can compute the matrix φ ( k i ) v i T {\\displaystyle \\varphi (k_{i})v_{i}^{T}} first, then multiply it with the query. In essence, we have managed to obtain a more precise version of Attention ( Q , K , V ) = softmax ( Q K T d k ) V ≈ Q ( K T V / d k ) {\\displaystyle { ext{Attention}}(Q,K,V)={ ext{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx Q(K^{T}V/{\\sqrt {d_{k}}})} Performer (2022) uses the same Random Feature Attention, but w 1 , . . . , w D {\\displaystyle w_{1},...,w_{D}} are first independently sampled from the normal distribution N ( 0 , σ 2 I ) {\\displaystyle N(0,\\sigma ^{2}I)} , then they are Gram-Schmidt processed. Transformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality. Multimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning. The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned. Vision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer. Conformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer. Perceivers are a variant of Transformers designed for multimodality. For image generation, notable architectures are DALL-E 1 (2021), Parti (2022), Phenaki (2023), and Muse (2023). Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image. Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image. Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted. Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video. The transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, Gemini, AlbertAGPT, Claude, BERT, Grok, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including: machine translation time series prediction document summarization document generation named entity recognition (NER) writing computer code based on requirements expressed in natural language. speech-to-text Beyond traditional NLP, the transformer architecture has had success in other applications, such as: biological sequence analysis video understanding protein folding (such as AlphaFold) evaluating chess board positions. Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level.",
      "sentences": [
        "In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.",
        "At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.",
        "Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).",
        "Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.",
        "The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google.",
        "Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since.",
        "They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess.",
        "It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).",
        "For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs).",
        "A well-cited early example was the Elman network (1990).",
        "In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.",
        "A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling.",
        "One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units.",
        "Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks.",
        "LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.",
        "However, LSTM still used sequential processing, like most other RNNs.",
        "Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.",
        "Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window.",
        "The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input.",
        "One of its two networks has \"fast weights\" or \"dynamic links\" (1981).",
        "A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries.",
        "This was later shown to be equivalent to the unnormalized linear Transformer.",
        "The idea of encoder-decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.",
        "A 380M-parameter model for machine translation uses two long short-term memories (LSTM).",
        "Its architecture consists of two parts.",
        "The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector.",
        "The decoder is another LSTM that converts the vector into a sequence of tokens.",
        "Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM.",
        "Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.",
        "These early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed.",
        "Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved.",
        "This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output.",
        "If the input is long, then the output vector would not be able to contain all relevant information, degrading the output.",
        "As evidence, reversing the input sentence improved seq2seq translation.",
        "The RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily.",
        "The name is because it \"emulates searching through a source sentence during decoding a translation\".",
        "The relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.",
        "In 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation.",
        "The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM.",
        "It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.",
        "Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs.",
        "In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs.",
        "One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title \"attention is all you need\".",
        "That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical.",
        "In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.",
        "In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper.",
        "At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance.",
        "This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence.",
        "Its parallelizability was an important factor to its widespread use in large neural networks.",
        "Already in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles.",
        "Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.",
        "In language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec.",
        "It was followed by BERT (2018), an encoder-only Transformer model.",
        "In 2019 October, Google started using BERT to process search queries.",
        "In 2020, Google Translate replaced the previous RNN-encoder-RNN-decoder model by a Transformer-encoder-RNN-decoder model.",
        "Starting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation.",
        "In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models.",
        "Since 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal.",
        "The vision transformer, in turn, stimulated new developments in convolutional neural networks.",
        "The plain transformer architecture had difficulty converging.",
        "In the original paper the authors recommended using learning rate warmup.",
        "That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.",
        "A 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.",
        "Transformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset.",
        "The pretrain dataset is typically an unlabeled large corpus, such as The Pile.",
        "Tasks for pretraining and fine-tuning commonly include: language modeling next-sentence prediction question answering reading comprehension sentiment analysis paraphrasing The T5 transformer report documents a large number of natural language pretraining tasks.",
        "Some examples are: restoring or repairing incomplete or corrupted text.",
        "For example, the input, \"Thank you ~~ me to your party ~~ week\", might generate the output, \"Thank you for inviting me to your party last week\".",
        "translation between natural languages (machine translation) judging the pragmatic acceptability of natural language.",
        "For example, the following sentence might be judged \"not acceptable\", because even though it is syntactically well-formed, it is improbable in ordinary human usage: The course is jumping well.",
        "Note that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.",
        "In general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\".",
        "These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.",
        "In a masked task, one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context.",
        "The loss function for the task is typically sum of log-perplexities for the masked-out tokens: Loss = − ∑ t ∈ masked tokens ln ⁡ ( probability of t conditional on its context ) {\\displaystyle { ext{Loss}}=-\\sum _{t\\in { ext{masked tokens}}}\\ln({ ext{probability of }}t{ ext{ conditional on its context}})} and the model is trained to minimize this loss function.",
        "The BERT series of models are trained for masked token prediction and another task.",
        "In an autoregressive task, the entire sequence is masked at first, and the model produces a probability distribution for the first token.",
        "Then the first token is revealed and the model predicts the second token, and so on.",
        "The loss function for the task is still typically the same.",
        "The GPT series of models are trained by autoregressive tasks.",
        "In a prefixLM task, the sequence is divided into two parts.",
        "The first part is presented as context, and the model predicts the first token of the second part.",
        "Then that would be revealed, and the model predicts the second token, and so on.",
        "The loss function for the task is still typically the same.",
        "The T5 series of models are trained by prefixLM tasks.",
        "Note that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model).",
        "All transformers have the same primary components: Tokenizers, which convert text into tokens.",
        "Embedding layer, which converts tokens and positions of the tokens into vector representations.",
        "Transformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information.",
        "These consist of alternating attention and feedforward layers.",
        "There are two major types of transformer layers: encoder layers and decoder layers, with further variants.",
        "Un-embedding layer, which converts the final vector representations back to a probability distribution over the tokens.",
        "The following description follows exactly the Transformer as described in the original paper.",
        "There are variants, described in the following section.",
        "By convention, we write all vectors as row vectors.",
        "This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as x W {\\displaystyle xW} .",
        "As the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens.",
        "A token is an integer that represents a character, or a short segment of characters.",
        "On the input side, the input text is parsed into a token sequence.",
        "Similarly, on the output side, the output tokens are parsed back to text.",
        "The module doing the conversion between texts and token sequences is a tokenizer.",
        "The set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size n vocabulary {\\displaystyle n_{ ext{vocabulary}}} .",
        "When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\".",
        "Some commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.",
        "Each token is converted into an embedding vector via a lookup table.",
        "Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix M {\\displaystyle M} .",
        "The number of dimensions in an embedding vector is called hidden size or embedding size and written as d emb {\\displaystyle d_{ ext{emb}}} .",
        "This size is written as d model {\\displaystyle d_{ ext{model}}} in the original Transformer paper.",
        "An un-embedding layer is almost the reverse of an embedding layer.",
        "Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.",
        "The embedding matrix M {\\displaystyle M} and the un-embedding matrix W {\\displaystyle W} are sometimes required to be transposes of each other, a practice called weight tying.",
        "A positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence.",
        "This induces a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\".",
        "The positional encoding is defined as a function of type f : R → R d ; d ∈ Z , d > 0 {\\displaystyle f:\\mathbb {R} o \\mathbb {R} ^{d};d\\in \\mathbb {Z} ,d>0} , where d {\\displaystyle d} is a positive even integer.",
        "Here, N {\\displaystyle N} is a free parameter that should be significantly larger than the biggest k {\\displaystyle k} that would be input into the positional encoding function.",
        "The original paper uses N = 10000 {\\displaystyle N=10000} .",
        "This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.",
        "This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors.",
        "This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model.",
        "In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\"",
        "In typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.",
        "Like earlier seq2seq models, the original transformer model used an encoder-decoder architecture.",
        "The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.",
        "The purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism.",
        "the tokens generated so far during inference time).",
        "Both the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps.",
        "These feed-forward layers contain most of the parameters in a Transformer model.",
        "The original Transformer used ReLU activation.",
        "It is typically larger than the embedding size.",
        "For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: d ffn = 4 d emb {\\displaystyle d_{ ext{ffn}}=4d_{ ext{emb}}} .",
        "The attention mechanism used in the Transformer architecture are scaled dot-product attention units.",
        "For each unit, the transformer model learns three weight matrices: the query weights W Q {\\displaystyle W^{Q}} , the key weights W K {\\displaystyle W^{K}} , and the value weights W V {\\displaystyle W^{V}} .",
        "The module takes three sequences, a query sequence, a key sequence, and a value sequence.",
        "The query sequence is a sequence of length ℓ seq, query {\\displaystyle \\ell _{ ext{seq, query}}} , and each entry is a vector of dimension d emb, query {\\displaystyle d_{ ext{emb, query}}} .",
        "Similarly for the key and value sequences.",
        "For each vector x i , query {\\displaystyle x_{i,{ ext{query}}}} in the query sequence, it is multiplied by a matrix W Q {\\displaystyle W^{Q}} to produce a query vector q i = x i , query W Q {\\displaystyle q_{i}=x_{i,{ ext{query}}}W^{Q}} .",
        "The matrix of all query vectors is the query matrix: Q = X query W Q {\\displaystyle Q=X_{ ext{query}}W^{Q}} Similarly, we construct the key matrix K = X key W K {\\displaystyle K=X_{ ext{key}}W^{K}} and the value matrix V = X value W V {\\displaystyle V=X_{ ext{value}}W^{V}} .",
        "It is usually the case that all W Q , W K , W V {\\displaystyle W^{Q},W^{K},W^{V}} are square matrices, meaning d emb, query = d query {\\displaystyle d_{ ext{emb, query}}=d_{ ext{query}}} , etc.",
        "Attention weights are calculated using the query and key vectors: the attention weight a i j {\\displaystyle a_{ij}} from token i {\\displaystyle i} to token j {\\displaystyle j} is the dot product between q i {\\displaystyle q_{i}} and k j {\\displaystyle k_{j}} .",
        "The attention weights are divided by the square root of the dimension of the key vectors, d k {\\displaystyle {\\sqrt {d_{k}}}} , which stabilizes gradients during training, and passed through a softmax which normalizes the weights.",
        "The fact that W Q {\\displaystyle W^{Q}} and W K {\\displaystyle W^{K}} are different matrices allows attention to be non-symmetric: if token i {\\displaystyle i} attends to token j {\\displaystyle j} (i.e.",
        "q i ⋅ k j {\\displaystyle q_{i}\\cdot k_{j}} is large), this does not necessarily mean that token j {\\displaystyle j} will attend to token i {\\displaystyle i} (i.e.",
        "q j ⋅ k i {\\displaystyle q_{j}\\cdot k_{i}} could be small).",
        "The output of the attention unit for token i {\\displaystyle i} is the weighted sum of the value vectors of all tokens, weighted by a i j {\\displaystyle a_{ij}} , the attention from token i {\\displaystyle i} to each token.",
        "The attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations.",
        "The matrices Q {\\displaystyle Q} , K {\\displaystyle K} and V {\\displaystyle V} are defined as the matrices where the i {\\displaystyle i} th rows are vectors q i {\\displaystyle q_{i}} , k i {\\displaystyle k_{i}} , and v i {\\displaystyle v_{i}} respectively.",
        "The number of dimensions in a query vector is query size d query {\\displaystyle d_{ ext{query}}} and similarly for the key size d key {\\displaystyle d_{ ext{key}}} and value size d value {\\displaystyle d_{ ext{value}}} .",
        "The output dimension of an attention head is its head dimension d head {\\displaystyle d_{ ext{head}}} .",
        "The attention mechanism requires the following three equalities to hold: ℓ seq, key = ℓ seq, value , d query = d key , d value = d head {\\displaystyle \\ell _{ ext{seq, key}}=\\ell _{ ext{seq, value}},\\;d_{ ext{query}}=d_{ ext{key}},\\;d_{ ext{value}}=d_{ ext{head}}} but is otherwise unconstrained.",
        "If the attention head is used in a self-attention fashion, then X query = X key = X value {\\displaystyle X_{ ext{query}}=X_{ ext{key}}=X_{ ext{value}}} .",
        "If the attention head is used in a cross-attention fashion, then usually X query ≠ X key = X value {\\displaystyle X_{ ext{query}} eq X_{ ext{key}}=X_{ ext{value}}} .",
        "It is theoretically possible for all three to be different, but that is rarely the case in practice.",
        "One set of ( W Q , W K , W V ) {\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)} matrices is called an attention head, and each layer in a transformer model has multiple attention heads.",
        "While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\".",
        "Specifically, the query and key projection matrices, W Q {\\displaystyle W^{Q}} and W K {\\displaystyle W^{K}} , which are involved in the attention score computation, defines the \"relevance\".",
        "Meanwhile, the value projection matrix W V {\\displaystyle W^{V}} , in combination with the part of the output projection matrix W O {\\displaystyle W^{O}} , determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits.",
        "In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers.",
        "This allows the model to capture more complex and long-range dependencies in deeper layers.",
        "Many transformer attention heads encode relevance relations that are meaningful to humans.",
        "For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects.",
        "The computations for each attention head can be performed in parallel, which allows for fast processing.",
        "The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.",
        "It is theoretically possible for each attention head to have a different head dimension d head {\\displaystyle d_{ ext{head}}} , but that is rarely the case in practice.",
        "As an example, in the smallest GPT-2 model, there are only self-attention mechanisms.",
        "It has the following dimensions: d emb = 768 , n head = 12 , d head = 64 {\\displaystyle d_{ ext{emb}}=768,n_{ ext{head}}=12,d_{ ext{head}}=64} Since 12 × 64 = 768 {\\displaystyle 12 imes 64=768} , its output projection matrix W O ∈ R ( 12 × 64 ) × 768 {\\displaystyle W^{O}\\in \\mathbb {R} ^{(12 imes 64) imes 768}} is a square matrix.",
        "The Transformer architecture is constructed to calculate output tokens iteratively.",
        "Assuming t = 0 {\\displaystyle t=0} refers to the calculation of the first output token i = 0 {\\displaystyle i=0} , for step t > 0 {\\displaystyle t>0} , the output token i = 0 {\\displaystyle i=0} shall remain constant.",
        "This ensures properties of the model similar to autoregressive models.",
        "Therefore, at every time step t {\\displaystyle t} , the calculation for all outputs i {\\displaystyle i} should not have access to tokens at position j {\\displaystyle j} for j >= i {\\displaystyle j>=i} (as it naturally is the case for time step t = i {\\displaystyle t=i} , when tokens j > t {\\displaystyle j>t} are not yet calculated).",
        "A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero.",
        "As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form P M causal P − 1 {\\displaystyle PM_{ ext{causal}}P^{-1}} , where P {\\displaystyle P} is a random permutation matrix.",
        "An encoder consists of an embedding layer, followed by multiple encoder layers.",
        "Each encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer.",
        "It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually.",
        "The encoder layers are stacked.",
        "The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors.",
        "This sequence of vectors is processed by the second encoder, and so on.",
        "The output from the final encoder layer is then used by the decoder.",
        "As the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.",
        "A decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.",
        "Each decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network.",
        "The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders.",
        "This mechanism can also be called the encoder-decoder attention.",
        "Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings.",
        "The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow.",
        "This allows for autoregressive text generation.",
        "For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated.",
        "Thus, the self-attention module in the decoder is causally masked.",
        "In contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding.",
        "Consequently, there is no need for masking in the cross-attention mechanism.",
        "The last decoder is followed by a final un-embedding layer.",
        "to produce the output probabilities over the vocabulary.",
        "Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.",
        "Many large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture.",
        "Early GPT models are decoder-only models trained to predict the next token in a sequence.",
        "BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence.",
        "Each encoder layer contains 2 sublayers: the self-attention and the feedforward network.",
        "Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.",
        "The final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence.",
        "The residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y = F(x) + x.",
        "Adding the input x can preserve the input information and avoid issues when the gradient of F(x) is close to zero.",
        "Similarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector.",
        "There are two common conventions in use: the post-LN and the pre-LN convention.",
        "It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases.",
        "The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence.",
        "Several common variations are described here.",
        "An \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text.",
        "This is usually used for text embedding and representation learning for downstream applications.",
        "BERT is encoder-only.",
        "They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.",
        "A \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to.",
        "Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network.",
        "This is usually used for text generation and instruction following.",
        "The models in the GPT series and Chinchilla series are decoder-only.",
        "An \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc.",
        "They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc.",
        "This is also usually used for text generation and instruction following.",
        "The models in the T5 series are encoder-decoder.",
        "A \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking.",
        "Specifically, it has mask of the form M prefixLM = [ 0 − ∞ 0 M causal ] {\\displaystyle M_{ ext{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{ ext{causal}}\\end{bmatrix}}} where the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix.",
        "They resemble encoder-decoder models, but has less \"sparsity\".",
        "Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.",
        "There are also mixed seq2seq models.",
        "For example, in 2020, Google Translate replaced the previous RNN-encoder-RNN-decoder model by a Transformer-encoder-RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.",
        "The original transformer uses ReLU activation function.",
        "Other activation functions were developed.",
        "The Llama series and PaLM used SwiGLU; both GPT-1 and BERT used GELU.",
        "Alternative activation functions are often used in combination with Gated Linear Units in the feedforward module.",
        "The normalization used in the Transformer can be different from LayerNorm.",
        "One example is RMSNorm which is used in the Llama series.",
        "Other examples include CapsuleNorm ScaleNorm, or FixNorm.",
        "Transformers may use other positional encoding methods than sinusoidal.",
        "The original Transformer paper reported using a learned positional encoding, but finding it not superior to the sinusoidal one.",
        "Later, found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.",
        "Now pick some angle θ {\\displaystyle heta } .",
        "Then the RoPE encoding is applied to each pair of coordinates.",
        "ALiBi (Attention with Linear Biases) is not a replacement for the positional encoder on the original transformer.",
        "Instead, it is an additional positional encoder that is directly plugged into the attention mechanism.",
        "The idea being that the linear bias matrix is a softened mask.",
        "Just as 0 {\\displaystyle 0} represent full attention paid, and − ∞ {\\displaystyle -\\infty } represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.",
        "ALiBi allows pretraining on short context windows, then fine-tuning on longer context windows.",
        "Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).",
        "This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\".",
        "The transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch.",
        "Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.",
        "When an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same.",
        "The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token.",
        "PagedAttention applies memory paging to KV caching.",
        "If a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk.",
        "The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.",
        "FlashAttention is an algorithm that implements the transformer attention mechanism efficiently on a GPU.",
        "It is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow).",
        "See the page on softmax for details.",
        "An improved version, FlashAttention-2, was developed to cater to the rising demand for language models capable of handling longer context lengths.",
        "It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.",
        "Key advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).",
        "Benchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch.",
        "Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.",
        "Multi-Query Attention changes the multiheaded attention mechanism.",
        "More generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair.",
        "MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups.",
        "Multihead Latent Attention (MLA) is a low-rank approximation to standard MHA.",
        "Specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces (\"latent space\"), one for query and one for key-value (KV vector).",
        "This design minimizes the KV cache, as only the low-dimensional KV vector needs to be cached.",
        "Speculative decoding is a method to accelerate token decoding.",
        "Similarly to speculative execution in CPUs, future tokens are computed quickly, then verified.",
        "If the quickly computed tokens are incorrect, they are discarded and computed slowly.",
        "The key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode, in the following sense.",
        "Suppose we have two transformer models like GPT-3 and GPT-3-small, both with a context window size of 512.",
        "To generate an entire context window autoregressively with greedy decoding with GPT-3, it must be run for 512 times, each time generating a token x 1 , x 2 , .",
        ", x 512 {\\displaystyle x_{1},x_{2},...,x_{512}} , taking time 512 T GPT-3 {\\displaystyle 512T_{ ext{GPT-3}}} .",
        "However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each x t {\\displaystyle x_{t}} is indeed the token with the largest log-likelihood in the t {\\displaystyle t} -th output.",
        "In speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model.",
        "For example, suppose we use GPT-3-small to generate four speculative tokens: x ~ 1 , x ~ 2 , x ~ 3 , x ~ 4 {\\displaystyle { ilde {x}}_{1},{ ilde {x}}_{2},{ ilde {x}}_{3},{ ilde {x}}_{4}} .",
        "This only takes 4 T GPT-3-small {\\displaystyle 4T_{ ext{GPT-3-small}}} .",
        "These tokens are then run through the larger GPT-3 in one go.",
        "Suppose that x ~ 1 {\\displaystyle { ilde {x}}_{1}} and x ~ 2 {\\displaystyle { ilde {x}}_{2}} are verified by GPT-3 as what it would have picked, then those are kept, but x ~ 3 {\\displaystyle { ilde {x}}_{3}} is not, so x ~ 3 , x ~ 4 {\\displaystyle { ilde {x}}_{3},{ ilde {x}}_{4}} are discarded, and GPT-3 is run on those.",
        "This would take 4 T GPT-3-small + 3 T GPT-3 {\\displaystyle 4T_{ ext{GPT-3-small}}+3T_{ ext{GPT-3}}} , which might be shorter than 4 T GPT-3 {\\displaystyle 4T_{ ext{GPT-3}}} .",
        "For non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.",
        "In Multi-Token Prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability.",
        "However, that vector can then be further processed by another Transformer block to predict the next token, and so on for arbitrarily many steps into the future.",
        "This trades off accuracy for speed, since each new token costs just one more Transformer block, rather than the entire stack.",
        "Training transformer-based architectures can be expensive, especially for long inputs.",
        "Many methods have been developed to attempt to address the issue.",
        "In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows.",
        "In the audio domain, SepTr decouples the attention in time and frequency domains.",
        "Long Range Arena (2020) is a standard benchmark for comparing the behavior of transformer architectures over long inputs.",
        "The standard attention graph is either all-to-all or causal, both of which scales as O ( N 2 ) {\\displaystyle O(N^{2})} where N {\\displaystyle N} is the number of tokens in a sequence.",
        "Sparse attention uses attention graphs that grows slower than O ( N 2 ) {\\displaystyle O(N^{2})} .",
        "Ordinary transformers require a memory size that is quadratic in the size of the context window.",
        "Attention-free transformers reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.",
        ", w D {\\displaystyle w_{1},...,w_{D}} are independent samples from the normal distribution N ( 0 , σ 2 I ) {\\displaystyle N(0,\\sigma ^{2}I)} .",
        "Similarly for multiple queries, and for multiheaded attention.",
        "This approximation can be computed in linear time, as we can compute the matrix φ ( k i ) v i T {\\displaystyle \\varphi (k_{i})v_{i}^{T}} first, then multiply it with the query.",
        ", w D {\\displaystyle w_{1},...,w_{D}} are first independently sampled from the normal distribution N ( 0 , σ 2 I ) {\\displaystyle N(0,\\sigma ^{2}I)} , then they are Gram-Schmidt processed.",
        "Transformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality.",
        "Multimodal models can either be trained from scratch, or by finetuning.",
        "A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning.",
        "The LLaVA was a vision-language model composed of a language model (Vicuna-13B) and a vision model (ViT-L/14), connected by a linear layer.",
        "Only the linear layer is finetuned.",
        "Vision transformers adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.",
        "Conformer and later Whisper follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e.",
        "broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.",
        "Perceivers are a variant of Transformers designed for multimodality.",
        "Unlike later models, DALL-E is not a diffusion model.",
        "Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image.",
        "Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image.",
        "Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens.",
        "During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted.",
        "Phenaki is a text-to-video model.",
        "It is a bidirectional masked transformer conditioned on pre-computed text tokens.",
        "The generated tokens are then decoded to a video.",
        "The transformer has had great success in natural language processing (NLP).",
        "Many large language models such as GPT-2, GPT-3, GPT-4, Gemini, AlbertAGPT, Claude, BERT, Grok, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including: machine translation time series prediction document summarization document generation named entity recognition (NER) writing computer code based on requirements expressed in natural language.",
        "speech-to-text Beyond traditional NLP, the transformer architecture has had success in other applications, such as: biological sequence analysis video understanding protein folding (such as AlphaFold) evaluating chess board positions.",
        "Using static evaluation alone (that is, with no Minimax search) transformer achieved an Elo of 2895, putting it at grandmaster level."
      ],
      "metadata": {
        "title": "Transformer (deep learning architecture)",
        "url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)",
        "word_count": 9220,
        "char_count": 57548,
        "sentence_count": 316,
        "scraped_at": "2025-08-09T14:47:02.883803",
        "language": "en",
        "processing_time": 0.0377042293548584,
        "source_hash": "8c36acffd3397af17fb0957c3bd9ab4d"
      }
    },
    {
      "title": "Deep Learning (South Park)",
      "url": "https://en.wikipedia.org/wiki/Deep_Learning_(South_Park)",
      "raw_text": "\"Deep Learning\" is the fourth episode of the twenty-sixth season of the American animated television series South Park, and the 323rd episode of the series overall. Written and directed by Trey Parker, it premiered on March 8, 2023. The episode, which parodies the use of the artificial intelligence chatbot ChatGPT (which is credited as a co-writer for the episode) for text messages, centers upon fourth-grader Stan Marsh, who comes to rely on the software for writing both school essays and romantic texts to his girlfriend Wendy Testaburger, bringing him into conflict with her, his classmates, and school officials.\n\n\n== Plot ==\nWhen fourth-grader Bebe Stevens extols the romantic texts written to her by Clyde Donovan, classmate Wendy Testaburger complains to her boyfriend, Stan Marsh, that his replies to her messages consist of merely a thumbs up. Clyde tells Stan about ChatGPT, an AI-based app he uses to write the texts, but cautions Stan not to tell anyone else about it. Stan relies on the app to text Wendy while engaging in other activities. Wendy is buoyed by Stan's more heartfelt texts, though he cannot truthfully answer her questions about them.\nStan and Clyde also use the app to write their school essays, as do their classmates Eric Cartman and Butters Stotch. Cartman complains that more students learning about it would cost them their advantage, and their teacher, Mr. Garrison, would learn they cheated. Meanwhile, Garrison laments to his partner, Rick, that he now has to work harder to grade and comment on his students' essays. Rick tells him about ChatGPT, but instead of realizing that his students used it for essays, Garrison realizes he can use it to grade them. He thanks Rick for his supportive replies to his texts.\nSchool counselor Mr. Mackey informs Stan's class that a student used OpenAI technology for schoolwork. A \"technician\" dressed as a falconer arrives with his falcon Shadowbane to analyze the students' work and identify the cheater. Stan and Garrison confess to each other that they used ChatGPT. They rationalize that it is merely akin to having a good writer's assistant, but when Garrison learns this can be used for texting, he is angered to realize that Rick used it to text him. The technician reveals Shadowbane detected chatbot writing in Wendy's cell phone, though she denies using the app. Worrying he cannot think of a way out of this, Stan instructs ChatGPT to write a story that is resolved when he convinces everyone that it is okay that he lied about using the app, and that tech companies who monetize OpenAI should not determine the ethical limits of AI. The remainder of the episode depicts this story and that resolution. Stan decides that sometimes a thumbs up from a human is better than machine-generated lies, but when Clyde asks Stan how he pulled this off, Stan simply explains, \"ChatGPT, dude.\"\nIn the closing credits, the writers of the episode are credited as both Trey Parker and ChatGPT.\n\n\n== Reception ==\nBubbleblabber contributor John Schwarz rated the episode a 7.5 out of 10, stating in his review, \"One day we're going to look back on this episode like we do when we think of the many chimps that we've sent to outer space when testing space flight capabilities and marvel at how far we've come in web3 show business production. Trey Parker's genius is still quite evident in this week's episode, in just a few seasons, we may not even need that.\"\nMax Nocerino with The Future of the Force gave the episode a B+ rating, conceding that while the episode was \"brilliant\", the show was not as \"hysterically funny as it used to be\", and cited this episode as example of that trend. Nocerino stated, \"It just doesn't split my sides anymore. Perhaps like all things, nothing lasts forever. Yet, I will continue to watch and give this episode props for understanding the double-edged sword that is ChatGPT. One of South Park's strengths is that it has its finger on the pulse of current events. And knows how to rip them to shreds.\"\nCathal Gunning, reviewing the episode for Screen Rant, wrote that in keeping with South Park's proclivity for self-satire, the ending was \"intentionally far too tidy and the scene ended the story way too slickly\", but nonetheless effective and clever. In particular, Gunning stated, \"When Stan used ChatGPT to end the episode's story, the resulting scene sounded very familiar since the sequence that was supposedly written by AI sounded like a lazy episode of South Park. From Cartman's insults to Stan's impassioned ending speech to Mr. Mackey's long-forgotten catchphrase, the final scenes of 'Deep Learning' leaned into tropes that South Park has used ad nauseam.\"\n\n\n== References ==\n\n\n== External links ==\n\"Deep Learning\" Full Episode at South Park Studios\n\"Deep Learning\" at IMDb\nVainilavičius, Justinas (March 8, 2023). \"ChatGPT, dude: viral chatbot makes it to 'South Park'\". Cybernews. Archived from the original on March 8, 2023.",
      "cleaned_text": "\"Deep Learning\" is the fourth episode of the twenty-sixth season of the American animated television series South Park, and the 323rd episode of the series overall. Written and directed by Trey Parker, it premiered on March 8, 2023. The episode, which parodies the use of the artificial intelligence chatbot ChatGPT (which is credited as a co-writer for the episode) for text messages, centers upon fourth-grader Stan Marsh, who comes to rely on the software for writing both school essays and romantic texts to his girlfriend Wendy Testaburger, bringing him into conflict with her, his classmates, and school officials. When fourth-grader Bebe Stevens extols the romantic texts written to her by Clyde Donovan, classmate Wendy Testaburger complains to her boyfriend, Stan Marsh, that his replies to her messages consist of merely a thumbs up. Clyde tells Stan about ChatGPT, an AI-based app he uses to write the texts, but cautions Stan not to tell anyone else about it. Stan relies on the app to text Wendy while engaging in other activities. Wendy is buoyed by Stan's more heartfelt texts, though he cannot truthfully answer her questions about them. Stan and Clyde also use the app to write their school essays, as do their classmates Eric Cartman and Butters Stotch. Cartman complains that more students learning about it would cost them their advantage, and their teacher, Mr. Garrison, would learn they cheated. Meanwhile, Garrison laments to his partner, Rick, that he now has to work harder to grade and comment on his students' essays. Rick tells him about ChatGPT, but instead of realizing that his students used it for essays, Garrison realizes he can use it to grade them. He thanks Rick for his supportive replies to his texts. School counselor Mr. Mackey informs Stan's class that a student used OpenAI technology for schoolwork. A \"technician\" dressed as a falconer arrives with his falcon Shadowbane to analyze the students' work and identify the cheater. Stan and Garrison confess to each other that they used ChatGPT. They rationalize that it is merely akin to having a good writer's assistant, but when Garrison learns this can be used for texting, he is angered to realize that Rick used it to text him. The technician reveals Shadowbane detected chatbot writing in Wendy's cell phone, though she denies using the app. Worrying he cannot think of a way out of this, Stan instructs ChatGPT to write a story that is resolved when he convinces everyone that it is okay that he lied about using the app, and that tech companies who monetize OpenAI should not determine the ethical limits of AI. The remainder of the episode depicts this story and that resolution. Stan decides that sometimes a thumbs up from a human is better than machine-generated lies, but when Clyde asks Stan how he pulled this off, Stan simply explains, \"ChatGPT, dude.\" In the closing credits, the writers of the episode are credited as both Trey Parker and ChatGPT. Bubbleblabber contributor John Schwarz rated the episode a 7.5 out of 10, stating in his review, \"One day we're going to look back on this episode like we do when we think of the many chimps that we've sent to outer space when testing space flight capabilities and marvel at how far we've come in web3 show business production. Trey Parker's genius is still quite evident in this week's episode, in just a few seasons, we may not even need that.\" Max Nocerino with The Future of the Force gave the episode a B+ rating, conceding that while the episode was \"brilliant\", the show was not as \"hysterically funny as it used to be\", and cited this episode as example of that trend. Nocerino stated, \"It just doesn't split my sides anymore. Perhaps like all things, nothing lasts forever. Yet, I will continue to watch and give this episode props for understanding the double-edged sword that is ChatGPT. One of South Park's strengths is that it has its finger on the pulse of current events. And knows how to rip them to shreds.\" Cathal Gunning, reviewing the episode for Screen Rant, wrote that in keeping with South Park's proclivity for self-satire, the ending was \"intentionally far too tidy and the scene ended the story way too slickly\", but nonetheless effective and clever. In particular, Gunning stated, \"When Stan used ChatGPT to end the episode's story, the resulting scene sounded very familiar since the sequence that was supposedly written by AI sounded like a lazy episode of South Park. From Cartman's insults to Stan's impassioned ending speech to Mr. Mackey's long-forgotten catchphrase, the final scenes of 'Deep Learning' leaned into tropes that South Park has used ad nauseam.\"",
      "sentences": [
        "\"Deep Learning\" is the fourth episode of the twenty-sixth season of the American animated television series South Park, and the 323rd episode of the series overall.",
        "Written and directed by Trey Parker, it premiered on March 8, 2023.",
        "The episode, which parodies the use of the artificial intelligence chatbot ChatGPT (which is credited as a co-writer for the episode) for text messages, centers upon fourth-grader Stan Marsh, who comes to rely on the software for writing both school essays and romantic texts to his girlfriend Wendy Testaburger, bringing him into conflict with her, his classmates, and school officials.",
        "When fourth-grader Bebe Stevens extols the romantic texts written to her by Clyde Donovan, classmate Wendy Testaburger complains to her boyfriend, Stan Marsh, that his replies to her messages consist of merely a thumbs up.",
        "Clyde tells Stan about ChatGPT, an AI-based app he uses to write the texts, but cautions Stan not to tell anyone else about it.",
        "Stan relies on the app to text Wendy while engaging in other activities.",
        "Wendy is buoyed by Stan's more heartfelt texts, though he cannot truthfully answer her questions about them.",
        "Stan and Clyde also use the app to write their school essays, as do their classmates Eric Cartman and Butters Stotch.",
        "Cartman complains that more students learning about it would cost them their advantage, and their teacher, Mr. Garrison, would learn they cheated.",
        "Meanwhile, Garrison laments to his partner, Rick, that he now has to work harder to grade and comment on his students' essays.",
        "Rick tells him about ChatGPT, but instead of realizing that his students used it for essays, Garrison realizes he can use it to grade them.",
        "He thanks Rick for his supportive replies to his texts.",
        "School counselor Mr. Mackey informs Stan's class that a student used OpenAI technology for schoolwork.",
        "A \"technician\" dressed as a falconer arrives with his falcon Shadowbane to analyze the students' work and identify the cheater.",
        "Stan and Garrison confess to each other that they used ChatGPT.",
        "They rationalize that it is merely akin to having a good writer's assistant, but when Garrison learns this can be used for texting, he is angered to realize that Rick used it to text him.",
        "The technician reveals Shadowbane detected chatbot writing in Wendy's cell phone, though she denies using the app.",
        "Worrying he cannot think of a way out of this, Stan instructs ChatGPT to write a story that is resolved when he convinces everyone that it is okay that he lied about using the app, and that tech companies who monetize OpenAI should not determine the ethical limits of AI.",
        "The remainder of the episode depicts this story and that resolution.",
        "Stan decides that sometimes a thumbs up from a human is better than machine-generated lies, but when Clyde asks Stan how he pulled this off, Stan simply explains, \"ChatGPT, dude.\"",
        "In the closing credits, the writers of the episode are credited as both Trey Parker and ChatGPT.",
        "Bubbleblabber contributor John Schwarz rated the episode a 7.5 out of 10, stating in his review, \"One day we're going to look back on this episode like we do when we think of the many chimps that we've sent to outer space when testing space flight capabilities and marvel at how far we've come in web3 show business production.",
        "Trey Parker's genius is still quite evident in this week's episode, in just a few seasons, we may not even need that.\"",
        "Max Nocerino with The Future of the Force gave the episode a B+ rating, conceding that while the episode was \"brilliant\", the show was not as \"hysterically funny as it used to be\", and cited this episode as example of that trend.",
        "Nocerino stated, \"It just doesn't split my sides anymore.",
        "Perhaps like all things, nothing lasts forever.",
        "Yet, I will continue to watch and give this episode props for understanding the double-edged sword that is ChatGPT.",
        "One of South Park's strengths is that it has its finger on the pulse of current events.",
        "And knows how to rip them to shreds.\"",
        "Cathal Gunning, reviewing the episode for Screen Rant, wrote that in keeping with South Park's proclivity for self-satire, the ending was \"intentionally far too tidy and the scene ended the story way too slickly\", but nonetheless effective and clever.",
        "In particular, Gunning stated, \"When Stan used ChatGPT to end the episode's story, the resulting scene sounded very familiar since the sequence that was supposedly written by AI sounded like a lazy episode of South Park.",
        "From Cartman's insults to Stan's impassioned ending speech to Mr. Mackey's long-forgotten catchphrase, the final scenes of 'Deep Learning' leaned into tropes that South Park has used ad nauseam.\""
      ],
      "metadata": {
        "title": "Deep Learning (South Park)",
        "url": "https://en.wikipedia.org/wiki/Deep_Learning_(South_Park)",
        "word_count": 780,
        "char_count": 4651,
        "sentence_count": 32,
        "scraped_at": "2025-08-09T14:47:02.884940",
        "language": "en",
        "processing_time": 0.0010120868682861328,
        "source_hash": "252433a181ce9d74fefea89fd69b9c83"
      }
    },
    {
      "title": "Deep reinforcement learning",
      "url": "https://en.wikipedia.org/wiki/Deep_reinforcement_learning",
      "raw_text": "Deep reinforcement learning (DRL) is a subfield of machine learning that combines principles of reinforcement learning (RL) and deep learning. It involves training agents to make decisions by interacting with an environment to maximize cumulative rewards, while using deep neural networks to represent policies, value functions, or environment models. This integration enables DRL systems to process high-dimensional inputs, such as images or continuous control signals, making the approach effective for solving complex tasks. Since the introduction of the deep Q-network (DQN) in 2015, DRL has achieved significant successes across domains including games, robotics, and autonomous systems, and is increasingly applied in areas such as healthcare, finance, and autonomous vehicles.\n\n\n== Deep reinforcement learning ==\n\n\n=== Introduction ===\nDeep reinforcement learning (DRL) is part of machine learning, which combines reinforcement learning (RL) and deep learning. In DRL, agents learn how  decisions are to be made by interacting with environments in order to maximize cumulative rewards, while using deep neural networks to represent policies, value functions, or models of the environment. This integration enables agents to handle high-dimensional input spaces, such as raw images or continuous control signals, making DRL a widely used approach for addressing complex tasks.\nSince the development of the deep Q-network (DQN) in 2015, DRL has led to major breakthroughs in domains such as games, robotics, and autonomous systems. Research in DRL continues to expand rapidly, with active work on challenges like sample efficiency and robustness, as well as innovations in model-based methods, transformer architectures, and open-ended learning. Applications now range from healthcare and finance to language systems and autonomous vehicles.\n\n\n=== Background ===\nReinforcement learning (RL) is a framework in which agents interact with  environments by taking actions and learning from feedback in form of rewards or penalties. Traditional RL methods, such as Q-learning and policy gradient techniques, rely on tabular representations or linear approximations, which are often not scalable to high-dimensional or continuous input spaces.\nDRL came out as solution to above limitation by integrating RL and deep neural networks. This combination enables agents to approximate complex functions and handle unstructured input data like raw images, sensor data, or natural language. The approach became widely recognized following the success of DeepMind's deep Q-network (DQN), which achieved human-level performance on several Atari video games using only pixel inputs and game scores as feedback.\nSince then, DRL has evolved to include various architectures and learning strategies, including model-based methods, actor-critic frameworks, and applications in continuous control environments. These developments have significantly expanded the applicability of DRL across domains where traditional RL was limited.\n\n\n=== Key algorithms and methods ===\nSeveral algorithmic approaches form the foundation of deep reinforcement learning, each with different strategies for learning optimal behavior.\nOne of the earliest and most influential DRL algorithms is the Deep Q-Network (DQN), which combines Q-learning with deep neural networks. DQN approximates the optimal action-value function using a convolutional neural network and introduced techniques such as experience replay and target networks which stabilize training.\n\nPolicy gradient methods directly optimize the agent’s policy by adjusting parameters in the direction that increases expected rewards. These methods are well-suited to high-dimensional or continuous action spaces and form the basis of many modern DRL algorithms.\nActor-critic algorithms combine the advantages of value-based and policy-based methods. The actor updates the policy, while the critic evaluates the current policy using a value function. Popular variants include A2C (Advantage Actor-Critic) and PPO (Proximal Policy Optimization), both of which are widely used in benchmarks and real-world applications.\nOther methods include multi-agent reinforcement learning, hierarchical RL, and approaches that integrate planning or memory mechanisms, depending on the complexity of the task and environment.\n\n\n=== Applications ===\nDRL has been applied to wide range of domains that require sequential decision-making and the ability to learn from high-dimensional input data.\nOne of the most well-known applications is in games, where DRL agents have demonstrated performance comparable to or exceeding human-level benchmarks. DeepMind's AlphaGo and AlphaStar, as well as OpenAI Five, are notable examples of DRL systems mastering complex games such as Go, StarCraft II, and Dota 2. While these systems have demonstrated high performance in constrained environments, their success often depends on extensive computational resources and may not generalize easily to tasks outside their training domains.\nIn robotics, DRL has been used to train agents for tasks such as locomotion, manipulation, and navigation in both simulated and real-world environments. By learning directly from sensory input, DRL enables robots to adapt to complex dynamics without relying on hand-crafted control rules.\nOther growing areas of application include finance (e.g., portfolio optimization), healthcare (e.g., treatment planning and medical decision-making), natural language processing (e.g., dialogue systems), and autonomous vehicles (e.g., path planning and control).All of these applications shows  how DRL deals with real-world problems like uncertainty, sequential reasoning, and high-dimensional data.\n\n\n=== Challenges and limitations ===\nDRL has several significant challenges which limit its broader deployment.\nOne of the most prominent issues is sample inefficiency. DRL algorithms often require millions of interactions with the environment to learn effective policies, which is impractical in many real-world settings where data collection is expensive or time-consuming.\nAnother challenge is sparse or delayed reward problem, where feedback signals are infrequent, which makes it difficult for agents to attribute outcomes to specific decisions. Techniques such as reward shaping and exploration strategies have been developed to address this issue.\nDRL systems also tend to be sensitive to hyperparameters and lack robustness across tasks or environments. Models that are trained in simulation fail very often when deployed in the real world due to discrepancies between simulated and real-world dynamics, a problem known as the \"reality gap.\"Bias and fairness in DRL systems have also emerged as concerns, particularly in domains like healthcare and finance where imbalanced data can lead to unequal outcomes for underrepresented groups.\nAdditionally, concerns about safety, interpretability, and reproducibility have become increasingly important, especially in high-stakes domains such as healthcare or autonomous driving. These issues remain active areas of research in the DRL community.\n\n\n=== Recent advances ===\nRecent developments in DRL have introduced new architectures and training strategies which aims to improving performance, efficiency, and generalization.\nOne key area of progress is model-based reinforcement learning, where agents learn an internal model of the environment to simulate outcomes before acting. This kind of approach improves sample efficiency and planning. An example is the Dreamer algorithm, which learns a latent space model to train agents more efficiently in complex environments.\nAnother major innovation is the use of transformer-based architectures in DRL. Unlike traditional models that rely on recurrent or convolutional networks, transformers can model long-term dependencies more effectively. The Decision Transformer and other similar models treat RL as a sequence modeling problem, enabling agents to generalize better across tasks.\nIn addition, research into open-ended learning has led to the creation of capable agents that are able to solve a range of tasks without task-specific tuning. Similar systems like the ones that are developed by OpenAI show that agents trained in diverse, evolving environments can generalize across new challenges, moving toward more adaptive and flexible intelligence.\n\n\n=== Future directions ===\nAs deep reinforcement learning continues to evolve, researchers are exploring ways to make algorithms more efficient, robust, and generalizable across a wide range of tasks. Improving sample efficiency through model-based learning, enhancing generalization with open-ended training environments, and integrating foundation models are among the current research goals.\nSimilar area of interest is safe and ethical deployment, particularly in high-risk settings like healthcare, autonomous driving, and finance. Researchers are developing frameworks for safer exploration, interpretability, and better alignment with human values. Ensuring that DRL systems promote equitable outcomes remains an ongoing challenge, especially where historical data may under‑represent marginalized populations.\nThe future of DRL may also involve more integration with other subfields of machine learning, such as unsupervised learning, transfer learning, and large language models, enabling agents that can learn from diverse data modalities and interact more naturally with human users.\n\n\n== References ==",
      "cleaned_text": "Deep reinforcement learning (DRL) is a subfield of machine learning that combines principles of reinforcement learning (RL) and deep learning. It involves training agents to make decisions by interacting with an environment to maximize cumulative rewards, while using deep neural networks to represent policies, value functions, or environment models. This integration enables DRL systems to process high-dimensional inputs, such as images or continuous control signals, making the approach effective for solving complex tasks. Since the introduction of the deep Q-network (DQN) in 2015, DRL has achieved significant successes across domains including games, robotics, and autonomous systems, and is increasingly applied in areas such as healthcare, finance, and autonomous vehicles. Deep reinforcement learning (DRL) is part of machine learning, which combines reinforcement learning (RL) and deep learning. In DRL, agents learn how decisions are to be made by interacting with environments in order to maximize cumulative rewards, while using deep neural networks to represent policies, value functions, or models of the environment. This integration enables agents to handle high-dimensional input spaces, such as raw images or continuous control signals, making DRL a widely used approach for addressing complex tasks. Since the development of the deep Q-network (DQN) in 2015, DRL has led to major breakthroughs in domains such as games, robotics, and autonomous systems. Research in DRL continues to expand rapidly, with active work on challenges like sample efficiency and robustness, as well as innovations in model-based methods, transformer architectures, and open-ended learning. Applications now range from healthcare and finance to language systems and autonomous vehicles. Reinforcement learning (RL) is a framework in which agents interact with environments by taking actions and learning from feedback in form of rewards or penalties. Traditional RL methods, such as Q-learning and policy gradient techniques, rely on tabular representations or linear approximations, which are often not scalable to high-dimensional or continuous input spaces. DRL came out as solution to above limitation by integrating RL and deep neural networks. This combination enables agents to approximate complex functions and handle unstructured input data like raw images, sensor data, or natural language. The approach became widely recognized following the success of DeepMind's deep Q-network (DQN), which achieved human-level performance on several Atari video games using only pixel inputs and game scores as feedback. Since then, DRL has evolved to include various architectures and learning strategies, including model-based methods, actor-critic frameworks, and applications in continuous control environments. These developments have significantly expanded the applicability of DRL across domains where traditional RL was limited. Several algorithmic approaches form the foundation of deep reinforcement learning, each with different strategies for learning optimal behavior. One of the earliest and most influential DRL algorithms is the Deep Q-Network (DQN), which combines Q-learning with deep neural networks. DQN approximates the optimal action-value function using a convolutional neural network and introduced techniques such as experience replay and target networks which stabilize training. Policy gradient methods directly optimize the agent’s policy by adjusting parameters in the direction that increases expected rewards. These methods are well-suited to high-dimensional or continuous action spaces and form the basis of many modern DRL algorithms. Actor-critic algorithms combine the advantages of value-based and policy-based methods. The actor updates the policy, while the critic evaluates the current policy using a value function. Popular variants include A2C (Advantage Actor-Critic) and PPO (Proximal Policy Optimization), both of which are widely used in benchmarks and real-world applications. Other methods include multi-agent reinforcement learning, hierarchical RL, and approaches that integrate planning or memory mechanisms, depending on the complexity of the task and environment. DRL has been applied to wide range of domains that require sequential decision-making and the ability to learn from high-dimensional input data. One of the most well-known applications is in games, where DRL agents have demonstrated performance comparable to or exceeding human-level benchmarks. DeepMind's AlphaGo and AlphaStar, as well as OpenAI Five, are notable examples of DRL systems mastering complex games such as Go, StarCraft II, and Dota 2. While these systems have demonstrated high performance in constrained environments, their success often depends on extensive computational resources and may not generalize easily to tasks outside their training domains. In robotics, DRL has been used to train agents for tasks such as locomotion, manipulation, and navigation in both simulated and real-world environments. By learning directly from sensory input, DRL enables robots to adapt to complex dynamics without relying on hand-crafted control rules. Other growing areas of application include finance (e.g., portfolio optimization), healthcare (e.g., treatment planning and medical decision-making), natural language processing (e.g., dialogue systems), and autonomous vehicles (e.g., path planning and control).All of these applications shows how DRL deals with real-world problems like uncertainty, sequential reasoning, and high-dimensional data. DRL has several significant challenges which limit its broader deployment. One of the most prominent issues is sample inefficiency. DRL algorithms often require millions of interactions with the environment to learn effective policies, which is impractical in many real-world settings where data collection is expensive or time-consuming. Another challenge is sparse or delayed reward problem, where feedback signals are infrequent, which makes it difficult for agents to attribute outcomes to specific decisions. Techniques such as reward shaping and exploration strategies have been developed to address this issue. DRL systems also tend to be sensitive to hyperparameters and lack robustness across tasks or environments. Models that are trained in simulation fail very often when deployed in the real world due to discrepancies between simulated and real-world dynamics, a problem known as the \"reality gap.\"Bias and fairness in DRL systems have also emerged as concerns, particularly in domains like healthcare and finance where imbalanced data can lead to unequal outcomes for underrepresented groups. Additionally, concerns about safety, interpretability, and reproducibility have become increasingly important, especially in high-stakes domains such as healthcare or autonomous driving. These issues remain active areas of research in the DRL community. Recent developments in DRL have introduced new architectures and training strategies which aims to improving performance, efficiency, and generalization. One key area of progress is model-based reinforcement learning, where agents learn an internal model of the environment to simulate outcomes before acting. This kind of approach improves sample efficiency and planning. An example is the Dreamer algorithm, which learns a latent space model to train agents more efficiently in complex environments. Another major innovation is the use of transformer-based architectures in DRL. Unlike traditional models that rely on recurrent or convolutional networks, transformers can model long-term dependencies more effectively. The Decision Transformer and other similar models treat RL as a sequence modeling problem, enabling agents to generalize better across tasks. In addition, research into open-ended learning has led to the creation of capable agents that are able to solve a range of tasks without task-specific tuning. Similar systems like the ones that are developed by OpenAI show that agents trained in diverse, evolving environments can generalize across new challenges, moving toward more adaptive and flexible intelligence. As deep reinforcement learning continues to evolve, researchers are exploring ways to make algorithms more efficient, robust, and generalizable across a wide range of tasks. Improving sample efficiency through model-based learning, enhancing generalization with open-ended training environments, and integrating foundation models are among the current research goals. Similar area of interest is safe and ethical deployment, particularly in high-risk settings like healthcare, autonomous driving, and finance. Researchers are developing frameworks for safer exploration, interpretability, and better alignment with human values. Ensuring that DRL systems promote equitable outcomes remains an ongoing challenge, especially where historical data may under‑represent marginalized populations. The future of DRL may also involve more integration with other subfields of machine learning, such as unsupervised learning, transfer learning, and large language models, enabling agents that can learn from diverse data modalities and interact more naturally with human users.",
      "sentences": [
        "Deep reinforcement learning (DRL) is a subfield of machine learning that combines principles of reinforcement learning (RL) and deep learning.",
        "It involves training agents to make decisions by interacting with an environment to maximize cumulative rewards, while using deep neural networks to represent policies, value functions, or environment models.",
        "This integration enables DRL systems to process high-dimensional inputs, such as images or continuous control signals, making the approach effective for solving complex tasks.",
        "Since the introduction of the deep Q-network (DQN) in 2015, DRL has achieved significant successes across domains including games, robotics, and autonomous systems, and is increasingly applied in areas such as healthcare, finance, and autonomous vehicles.",
        "Deep reinforcement learning (DRL) is part of machine learning, which combines reinforcement learning (RL) and deep learning.",
        "In DRL, agents learn how decisions are to be made by interacting with environments in order to maximize cumulative rewards, while using deep neural networks to represent policies, value functions, or models of the environment.",
        "This integration enables agents to handle high-dimensional input spaces, such as raw images or continuous control signals, making DRL a widely used approach for addressing complex tasks.",
        "Since the development of the deep Q-network (DQN) in 2015, DRL has led to major breakthroughs in domains such as games, robotics, and autonomous systems.",
        "Research in DRL continues to expand rapidly, with active work on challenges like sample efficiency and robustness, as well as innovations in model-based methods, transformer architectures, and open-ended learning.",
        "Applications now range from healthcare and finance to language systems and autonomous vehicles.",
        "Reinforcement learning (RL) is a framework in which agents interact with environments by taking actions and learning from feedback in form of rewards or penalties.",
        "Traditional RL methods, such as Q-learning and policy gradient techniques, rely on tabular representations or linear approximations, which are often not scalable to high-dimensional or continuous input spaces.",
        "DRL came out as solution to above limitation by integrating RL and deep neural networks.",
        "This combination enables agents to approximate complex functions and handle unstructured input data like raw images, sensor data, or natural language.",
        "The approach became widely recognized following the success of DeepMind's deep Q-network (DQN), which achieved human-level performance on several Atari video games using only pixel inputs and game scores as feedback.",
        "Since then, DRL has evolved to include various architectures and learning strategies, including model-based methods, actor-critic frameworks, and applications in continuous control environments.",
        "These developments have significantly expanded the applicability of DRL across domains where traditional RL was limited.",
        "Several algorithmic approaches form the foundation of deep reinforcement learning, each with different strategies for learning optimal behavior.",
        "One of the earliest and most influential DRL algorithms is the Deep Q-Network (DQN), which combines Q-learning with deep neural networks.",
        "DQN approximates the optimal action-value function using a convolutional neural network and introduced techniques such as experience replay and target networks which stabilize training.",
        "Policy gradient methods directly optimize the agent’s policy by adjusting parameters in the direction that increases expected rewards.",
        "These methods are well-suited to high-dimensional or continuous action spaces and form the basis of many modern DRL algorithms.",
        "Actor-critic algorithms combine the advantages of value-based and policy-based methods.",
        "The actor updates the policy, while the critic evaluates the current policy using a value function.",
        "Popular variants include A2C (Advantage Actor-Critic) and PPO (Proximal Policy Optimization), both of which are widely used in benchmarks and real-world applications.",
        "Other methods include multi-agent reinforcement learning, hierarchical RL, and approaches that integrate planning or memory mechanisms, depending on the complexity of the task and environment.",
        "DRL has been applied to wide range of domains that require sequential decision-making and the ability to learn from high-dimensional input data.",
        "One of the most well-known applications is in games, where DRL agents have demonstrated performance comparable to or exceeding human-level benchmarks.",
        "DeepMind's AlphaGo and AlphaStar, as well as OpenAI Five, are notable examples of DRL systems mastering complex games such as Go, StarCraft II, and Dota 2.",
        "While these systems have demonstrated high performance in constrained environments, their success often depends on extensive computational resources and may not generalize easily to tasks outside their training domains.",
        "In robotics, DRL has been used to train agents for tasks such as locomotion, manipulation, and navigation in both simulated and real-world environments.",
        "By learning directly from sensory input, DRL enables robots to adapt to complex dynamics without relying on hand-crafted control rules.",
        "DRL has several significant challenges which limit its broader deployment.",
        "One of the most prominent issues is sample inefficiency.",
        "DRL algorithms often require millions of interactions with the environment to learn effective policies, which is impractical in many real-world settings where data collection is expensive or time-consuming.",
        "Another challenge is sparse or delayed reward problem, where feedback signals are infrequent, which makes it difficult for agents to attribute outcomes to specific decisions.",
        "Techniques such as reward shaping and exploration strategies have been developed to address this issue.",
        "DRL systems also tend to be sensitive to hyperparameters and lack robustness across tasks or environments.",
        "Models that are trained in simulation fail very often when deployed in the real world due to discrepancies between simulated and real-world dynamics, a problem known as the \"reality gap.",
        "\"Bias and fairness in DRL systems have also emerged as concerns, particularly in domains like healthcare and finance where imbalanced data can lead to unequal outcomes for underrepresented groups.",
        "Additionally, concerns about safety, interpretability, and reproducibility have become increasingly important, especially in high-stakes domains such as healthcare or autonomous driving.",
        "These issues remain active areas of research in the DRL community.",
        "Recent developments in DRL have introduced new architectures and training strategies which aims to improving performance, efficiency, and generalization.",
        "One key area of progress is model-based reinforcement learning, where agents learn an internal model of the environment to simulate outcomes before acting.",
        "This kind of approach improves sample efficiency and planning.",
        "An example is the Dreamer algorithm, which learns a latent space model to train agents more efficiently in complex environments.",
        "Another major innovation is the use of transformer-based architectures in DRL.",
        "Unlike traditional models that rely on recurrent or convolutional networks, transformers can model long-term dependencies more effectively.",
        "The Decision Transformer and other similar models treat RL as a sequence modeling problem, enabling agents to generalize better across tasks.",
        "In addition, research into open-ended learning has led to the creation of capable agents that are able to solve a range of tasks without task-specific tuning.",
        "Similar systems like the ones that are developed by OpenAI show that agents trained in diverse, evolving environments can generalize across new challenges, moving toward more adaptive and flexible intelligence.",
        "As deep reinforcement learning continues to evolve, researchers are exploring ways to make algorithms more efficient, robust, and generalizable across a wide range of tasks.",
        "Improving sample efficiency through model-based learning, enhancing generalization with open-ended training environments, and integrating foundation models are among the current research goals.",
        "Similar area of interest is safe and ethical deployment, particularly in high-risk settings like healthcare, autonomous driving, and finance.",
        "Researchers are developing frameworks for safer exploration, interpretability, and better alignment with human values.",
        "Ensuring that DRL systems promote equitable outcomes remains an ongoing challenge, especially where historical data may under‑represent marginalized populations.",
        "The future of DRL may also involve more integration with other subfields of machine learning, such as unsupervised learning, transfer learning, and large language models, enabling agents that can learn from diverse data modalities and interact more naturally with human users."
      ],
      "metadata": {
        "title": "Deep reinforcement learning",
        "url": "https://en.wikipedia.org/wiki/Deep_reinforcement_learning",
        "word_count": 1285,
        "char_count": 9237,
        "sentence_count": 57,
        "scraped_at": "2025-08-09T14:47:02.886954",
        "language": "en",
        "processing_time": 0.0018880367279052734,
        "source_hash": "64c8e7aa5ece3d2587fadef6f2a5fb3e"
      }
    },
    {
      "title": "Neural network (machine learning)",
      "url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
      "raw_text": "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks.\nA neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\n\n\n== Training ==\nNeural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data.\n\n\n== History ==\n\n\n=== Early work ===\nToday's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.\nHistorically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.\nWarren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.\nIn the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). \nIn 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research.\nR. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\"\nThe perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.\nThe first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.\n\n\n=== Deep learning breakthroughs in the 1960s and 1970s ===\nFundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\"\nThe first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\nIn 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning.\nNevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967).\nIn 1976 transfer learning was introduced in neural networks learning.\nDeep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.\n\n\n=== Backpropagation ===\nBackpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.\n\n\n=== Convolutional neural networks ===\nKunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision.\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.\nIn 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images.\nFrom 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.\n\n\n=== Recurrent neural networks ===\nOne origin of RNN was statistical mechanics. In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning. This was popularized as the Hopfield network by John Hopfield (1982). Another origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. The McCulloch and Pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.\nIn 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a Crossbar Adaptive Array, used direct recurrent connections from the output to the supervisor (teaching) inputs. In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations. Eliminating the external supervisor, it introduced the self-learning method in neural networks.  \nIn cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on the relation between cognition and emotion. Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion. In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation. It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology.\nTwo early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. \nIn the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, Jürgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\" which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\nIn 1991, Sepp Hochreiter's diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains. This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999. It became the default choice for RNN architecture.\nDuring 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models.\n\n\n=== Deep learning ===\nBetween 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly.\nIn October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.\nIn 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".\nRadial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.\nGenerative adversarial network (GAN) (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014–2018 period. The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here, the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).\nIn 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in December 2015. ResNet behaves like an open-gated Highway Net. \n\nDuring the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need.\nIt requires computation time that is quadratic in the size of the context window. Jürgen Schmidhuber's fast weight controller (1992) scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.\nTransformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.\n\n\n== Models ==\n\nANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.\nAn artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons.\n\n\n=== Artificial neurons ===\n\nANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.\nTo find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.\n\n\n=== Organization ===\nThe neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.\n\n\n=== Hyperparameter ===\n\nA hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.\n\n\n=== Learning ===\n\nLearning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.\n\n\n==== Learning rate ====\n\nThe learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.\n\n\n==== Cost function ====\nWhile it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) because it arises from the model (e.g. in a probabilistic model, the model's posterior probability can be used as an inverse cost).\n\n\n==== Backpropagation ====\n\nBackpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backpropagation calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.\n\n\n=== Learning paradigms ===\n\nMachine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task.\n\n\n==== Supervised learning ====\nSupervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\n\n\n==== Unsupervised learning ====\nIn unsupervised learning, input data is given along with the cost function, some function of the data \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\textstyle x}\n  \n and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model \n  \n    \n      \n        \n          f\n          (\n          x\n          )\n          =\n          a\n        \n      \n    \n    {\\displaystyle \\textstyle f(x)=a}\n  \n where \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\textstyle a}\n  \n is a constant and the cost \n  \n    \n      \n        \n          C\n          =\n          E\n          [\n          (\n          x\n          −\n          f\n          (\n          x\n          )\n          \n            )\n            \n              2\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\textstyle C=E[(x-f(x))^{2}]}\n  \n. Minimizing this cost produces a value of \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\textstyle a}\n  \n that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\textstyle x}\n  \n and \n  \n    \n      \n        \n          f\n          (\n          x\n          )\n        \n      \n    \n    {\\displaystyle \\textstyle f(x)}\n  \n, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\n\n\n==== Reinforcement learning ====\n\nIn applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.\nFormally, the environment is modeled as a Markov decision process (MDP) with states \n  \n    \n      \n        \n          \n            \n              s\n              \n                1\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              s\n              \n                n\n              \n            \n          \n          ∈\n          S\n        \n      \n    \n    {\\displaystyle \\textstyle {s_{1},...,s_{n}}\\in S}\n  \n and actions \n  \n    \n      \n        \n          \n            \n              a\n              \n                1\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              a\n              \n                m\n              \n            \n          \n          ∈\n          A\n        \n      \n    \n    {\\displaystyle \\textstyle {a_{1},...,a_{m}}\\in A}\n  \n. Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution \n  \n    \n      \n        \n          P\n          (\n          \n            c\n            \n              t\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(c_{t}|s_{t})}\n  \n, the observation distribution \n  \n    \n      \n        \n          P\n          (\n          \n            x\n            \n              t\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(x_{t}|s_{t})}\n  \n and the transition distribution \n  \n    \n      \n        \n          P\n          (\n          \n            s\n            \n              t\n              +\n              1\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          ,\n          \n            a\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(s_{t+1}|s_{t},a_{t})}\n  \n, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.\nANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\n\n\n==== Self-learning ====\nSelf-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation:\n\n In situation s perform action a;\n Receive consequence situation s';\n Compute emotion of being in consequence situation v(s');\n Update crossbar memory w'(a,s) = w(a,s) + v(s').\n\nThe backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it receives initial emotions (only once) about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.\n\n\n==== Neuroevolution ====\n\nNeuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".\n\n\n=== Stochastic neural network ===\nStochastic neural networks originating from Sherrington–Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions , or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.\n\n\n=== Topological deep learning ===\nTopological deep learning, first introduced in 2017, is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and high-order data. Initially rooted in algebraic topology, TDL has since evolved into a versatile framework incorporating tools from other mathematical disciplines, such as differential topology and geometric topology. As a successful example of mathematical deep learning, TDL continues to inspire advancements in mathematical artificial intelligence, fostering a mutually beneficial relationship between AI and mathematics.  \n\n\n=== Other ===\nIn a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation–maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.\n\n\n==== Modes ====\n\nTwo modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning, weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.\n\n\n== Types ==\n\nANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\nSome of the main breakthroughs include: \n\nConvolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; where long short-term memory avoids the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads;\nCompetitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input.\n\n\n== Network design ==\nUsing artificial neural networks requires an understanding of their characteristics.\n\nChoice of model: This depends on the data representation and the application. Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc.). Overly complex models learn slowly.\nLearning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.\nRobustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.\nNeural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras. scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras.\nHyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc. The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters:\n\n\n== Applications ==\nBecause of their ability to model and reproduce nonlinear processes, artificial neural networks have found applications in many disciplines. These include:\n\nFunction approximation, or regression analysis, (including time series prediction, fitness approximation, and modeling)\nData processing (including filtering, clustering, blind source separation, and compression)\nNonlinear system identification and control (including vehicle control, trajectory prediction, adaptive control, process control, and natural resource management)\nPattern recognition (including radar systems, face identification, signal classification, novelty detection, 3D reconstruction, object recognition, and sequential decision making)\nSequence recognition (including gesture, speech, and handwritten and printed text recognition)\nSensor data analysis (including image analysis)\nRobotics (including directing manipulators and prostheses)\nData mining (including knowledge discovery in databases)\nFinance (such as ex-ante models for specific financial long-run forecasts and artificial financial markets)\nQuantum chemistry\nGeneral game playing\nGenerative AI\nData visualization\nMachine translation\nSocial network filtering\nE-mail spam filtering\nMedical diagnosis\nANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.\nANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.\nANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.\nIt is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.\nBeyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.\n\n\n== Theoretical properties ==\n\n\n=== Computational power ===\nThe multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\nA specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.\n\n\n=== Capacity ===\nA model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\nTwo notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.\n\n\n=== Convergence ===\nModels may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.\nAnother issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction.\nThe convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fit target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.\n\n\n=== Generalization and statistics ===\n\nApplications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. \nTwo approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\n\nSupervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\nBy assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.\nThe softmax activation function is:\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \n            \n              e\n              \n                \n                  x\n                  \n                    i\n                  \n                \n              \n            \n            \n              \n                ∑\n                \n                  j\n                  =\n                  1\n                \n                \n                  c\n                \n              \n              \n                e\n                \n                  \n                    x\n                    \n                      j\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}}\n  \n\n\n== Criticism ==\n\n\n=== Training ===\nA common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.\nAny learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.\nDean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right).\n\n\n=== Theory ===\nA central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a \n\nsomething-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything. One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.\nTechnology writer Roger Bridgman commented:\n\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\nIn spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\n\nAlthough it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.\nBiological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\n\n\n=== Hardware ===\nLarge and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time.\nSome argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.\nNeuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.\n\n\n=== Practical counterexamples ===\nAnalyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.\n\n\n=== Hybrid approaches ===\nAdvocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.\n\n\n=== Dataset bias ===\nNeural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement. For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field. The program would penalize any resume with the word \"woman\" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.\n\n\n== Gallery ==\n\n\n== Recent advancements and future directions ==\nArtificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.\n\n\n=== Image processing ===\nIn the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance. This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.\n\n\n=== Speech recognition ===\nBy modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques. These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.\n\n\n=== Natural language processing ===\nIn natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content. This has implications for automated customer service, content moderation, and language understanding technologies.\n\n\n=== Control systems ===\nIn the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications.\n\n\n=== Finance ===\n\nANNs are used for stock market prediction and credit scoring: \n\nIn investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions.\nIn credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process.\nANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.\n\n\n=== Medicine ===\nANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning. In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs. Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.\n\n\n=== Content creation ===\nANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry, generative models are used to create personalized advertisements for consumers. Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020. Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== External links ==\n\nA Brief Introduction to Neural Networks (D. Kriesel) – Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.\nReview of Neural Networks in Materials Science Archived 7 June 2015 at the Wayback Machine\nArtificial Neural Networks Tutorial in three languages (Univ. Politécnica de Madrid)\nAnother introduction to ANN\nNext Generation of Neural Networks Archived 24 January 2011 at the Wayback Machine – Google Tech Talks\nPerformance of Neural Networks\nNeural Networks and Information Archived 9 July 2009 at the Wayback Machine\nSanderson G (5 October 2017). \"But what is a Neural Network?\". 3Blue1Brown. Archived from the original on 7 November 2021 – via YouTube.",
      "cleaned_text": "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks. A neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers. Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information. Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. Today's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing. Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. In the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research. R. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\" The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. The first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning. Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\" The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967). In 1976 transfer learning was introduced in neural networks learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. Kunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision. The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images. From 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments. One origin of RNN was statistical mechanics. In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning. This was popularized as the Hopfield network by John Hopfield (1982). Another origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. The McCulloch and Pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past. In 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a Crossbar Adaptive Array, used direct recurrent connections from the output to the supervisor (teaching) inputs. In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations. Eliminating the external supervisor, it introduced the self-learning method in neural networks. In cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on the relation between cognition and emotion. Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion. In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation. It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology. Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. In the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, Jürgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\" which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. In 1991, Sepp Hochreiter's diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains. This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999. It became the default choice for RNN architecture. During 1985-1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models. Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly. In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\". Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications. Generative adversarial network (GAN) (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014-2018 period. The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here, the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022). In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in December 2015. ResNet behaves like an open-gated Highway Net. During the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need. It requires computation time that is quadratic in the size of the context window. Jürgen Schmidhuber's fast weight controller (1992) scales linearly and was later shown to be equivalent to the unnormalized linear Transformer. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture. ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph. An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons. ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image. To find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image. The neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks. A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers. Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation. The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change. While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) because it arises from the model (e.g. in a probabilistic model, the model's posterior probability can be used as an inverse cost). Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backpropagation calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks. Machine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task. Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. In unsupervised learning, input data is given along with the cost function, some function of the data x {\\displaystyle extstyle x} and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model f ( x ) = a {\\displaystyle extstyle f(x)=a} where a {\\displaystyle extstyle a} is a constant and the cost C = E [ ( x − f ( x ) ) 2 ] {\\displaystyle extstyle C=E[(x-f(x))^{2}]} . Minimizing this cost produces a value of a {\\displaystyle extstyle a} that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between x {\\displaystyle extstyle x} and f ( x ) {\\displaystyle extstyle f(x)} , whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering. In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly. Formally, the environment is modeled as a Markov decision process (MDP) with states s 1 , . . . , s n ∈ S {\\displaystyle extstyle {s_{1},...,s_{n}}\\in S} and actions a 1 , . . . , a m ∈ A {\\displaystyle extstyle {a_{1},...,a_{m}}\\in A} . Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution P ( c t | s t ) {\\displaystyle extstyle P(c_{t}|s_{t})} , the observation distribution P ( x t | s t ) {\\displaystyle extstyle P(x_{t}|s_{t})} and the transition distribution P ( s t + 1 | s t , a t ) {\\displaystyle extstyle P(s_{t+1}|s_{t},a_{t})} , while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC. ANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks. Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation: In situation s perform action a; Receive consequence situation s'; Compute emotion of being in consequence situation v(s'); Update crossbar memory w'(a,s) = w(a,s) + v(s'). The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it receives initial emotions (only once) about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations. Neuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\". Stochastic neural networks originating from Sherrington-Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions , or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks. Topological deep learning, first introduced in 2017, is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and high-order data. Initially rooted in algebraic topology, TDL has since evolved into a versatile framework incorporating tools from other mathematical disciplines, such as differential topology and geometric topology. As a successful example of mathematical deep learning, TDL continues to inspire advancements in mathematical artificial intelligence, fostering a mutually beneficial relationship between AI and mathematics. In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks. Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning, weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set. ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers. Some of the main breakthroughs include: Convolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; where long short-term memory avoids the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads; Competitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input. Using artificial neural networks requires an understanding of their characteristics. Choice of model: This depends on the data representation and the application. Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc.). Overly complex models learn slowly. Learning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation. Robustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust. Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras. scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras. Hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc. The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters: Because of their ability to model and reproduce nonlinear processes, artificial neural networks have found applications in many disciplines. These include: Function approximation, or regression analysis, (including time series prediction, fitness approximation, and modeling) Data processing (including filtering, clustering, blind source separation, and compression) Nonlinear system identification and control (including vehicle control, trajectory prediction, adaptive control, process control, and natural resource management) Pattern recognition (including radar systems, face identification, signal classification, novelty detection, 3D reconstruction, object recognition, and sequential decision making) Sequence recognition (including gesture, speech, and handwritten and printed text recognition) Sensor data analysis (including image analysis) Robotics (including directing manipulators and prostheses) Data mining (including knowledge discovery in databases) Finance (such as ex-ante models for specific financial long-run forecasts and artificial financial markets) Quantum chemistry General game playing Generative AI Data visualization Machine translation Social network filtering E-mail spam filtering Medical diagnosis ANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information. ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions. ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level. It is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition. Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation. The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters. A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power. A model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity. Two notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity. Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical. Another issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction. The convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fit target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions. Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting. Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is: y i = e x i ∑ j = 1 c e x j {\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} A common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation. Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC. Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns-it should not learn to always turn right). A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything. One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. Technology writer Roger Bridgman commented: Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\". In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture. Biological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies. Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons - which require enormous CPU power and time. Some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days. Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU. Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture. Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. Neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement. For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field. The program would penalize any resume with the word \"woman\" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets. Artificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine. In the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance. This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging. By modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques. These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products. In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content. This has implications for automated customer service, content moderation, and language understanding technologies. In the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications. ANNs are used for stock market prediction and credit scoring: In investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions. In credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process. ANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies. ANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning. In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs. Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine. ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry, generative models are used to create personalized advertisements for consumers. Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020. Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.",
      "sentences": [
        "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks.",
        "A neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain.",
        "Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance.",
        "These are connected by edges, which model the synapses in the brain.",
        "Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons.",
        "The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function.",
        "The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.",
        "Typically, neurons are aggregated into layers.",
        "Different layers may perform different transformations on their inputs.",
        "A network is typically called a deep neural network if it has at least two hidden layers.",
        "Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence.",
        "They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.",
        "Neural networks are typically trained through empirical risk minimization.",
        "This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset.",
        "Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network.",
        "During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function.",
        "This method allows the network to generalize to unseen data.",
        "Today's deep neural networks are based on early work in statistics over 200 years ago.",
        "The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights.",
        "The sum of the products of the weights and the inputs is calculated at each node.",
        "The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights.",
        "This technique has been known for over two centuries as the method of least squares or linear regression.",
        "It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.",
        "Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors.",
        "Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism.",
        "Unlike the von Neumann model, connectionist computing does not separate memory and processing.",
        "Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks.",
        "This model paved the way for research to split into two approaches.",
        "One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.",
        "In the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning.",
        "It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network.",
        "Farley and Clark (1954) used computational machines to simulate a Hebbian network.",
        "Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).",
        "In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research.",
        "R. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\"",
        "However, \"they dropped the subject.\"",
        "The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding.",
        "This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.",
        "The first perceptrons did not have adaptive hidden units.",
        "However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer.",
        "Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight.",
        "Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.",
        "Fundamental research was conducted on ANNs in the 1960s and 1970s.",
        "The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965).",
        "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron.",
        "A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis.",
        "Superfluous hidden units are pruned using a separate validation set.",
        "Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\"",
        "The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.",
        "In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes.",
        "Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.",
        "In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.",
        "The rectifier has become the most popular activation function for deep learning.",
        "Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit.",
        "This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967).",
        "In 1976 transfer learning was introduced in neural networks learning.",
        "Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.",
        "Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes.",
        "The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.",
        "In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970).",
        "Ostrovski et al.",
        "republished it in 1971.",
        "Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm).",
        "In 1986, David E. Rumelhart et al.",
        "popularised backpropagation but did not cite the original work.",
        "Kunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs.",
        "CNNs have become an essential tool for computer vision.",
        "The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition.",
        "It used convolutions, weight sharing, and backpropagation.",
        "In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.",
        "In 1989, Yann LeCun et al.",
        "created a CNN called LeNet for recognizing handwritten ZIP codes on mail.",
        "Training required 3 days.",
        "In 1990, Wei Zhang implemented a CNN on optical computing hardware.",
        "In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms.",
        "LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images.",
        "From 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.",
        "One origin of RNN was statistical mechanics.",
        "In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning.",
        "This was popularized as the Hopfield network by John Hopfield (1982).",
        "Another origin of RNN was neuroscience.",
        "The word \"recurrent\" is used to describe loop-like structures in anatomy.",
        "In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex.",
        "Hebb considered \"reverberating circuit\" as an explanation for short-term memory.",
        "The McCulloch and Pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.",
        "In 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a Crossbar Adaptive Array, used direct recurrent connections from the output to the supervisor (teaching) inputs.",
        "In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations.",
        "Eliminating the external supervisor, it introduced the self-learning method in neural networks.",
        "In cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on the relation between cognition and emotion.",
        "Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion.",
        "In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation.",
        "It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology.",
        "Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology.",
        "In the 1980s, backpropagation did not work well for deep RNNs.",
        "To overcome this problem, in 1991, Jürgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\" which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation.",
        "In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.",
        "In 1991, Sepp Hochreiter's diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it.",
        "He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains.",
        "This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999.",
        "It became the default choice for RNN architecture.",
        "During 1985-1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm.",
        "These were designed for unsupervised learning of deep generative models.",
        "Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition.",
        "In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.",
        "It then won more contests.",
        "They also showed how max-pooling CNNs on GPU improved performance significantly.",
        "In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods.",
        "Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.",
        "In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images.",
        "Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".",
        "Radial basis function and wavelet networks were introduced in 2013.",
        "These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.",
        "Generative adversarial network (GAN) (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014-2018 period.",
        "The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.",
        "The first network is a generative model that models a probability distribution over output patterns.",
        "The second network learns by gradient descent to predict the reactions of the environment to these patterns.",
        "Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al.",
        "Here, the GAN generator is grown from small to large scale in a pyramidal fashion.",
        "Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.",
        "In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers.",
        "Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem.",
        "In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in December 2015.",
        "ResNet behaves like an open-gated Highway Net.",
        "During the 2010s, the seq2seq model was developed, and attention mechanisms were added.",
        "It led to the modern Transformer architecture in 2017 in Attention Is All You Need.",
        "It requires computation time that is quadratic in the size of the context window.",
        "Jürgen Schmidhuber's fast weight controller (1992) scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.",
        "Transformers have increasingly become the model of choice for natural language processing.",
        "Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.",
        "ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with.",
        "They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors.",
        "ANNs have the ability to learn and model non-linearities and complex relationships.",
        "This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others.",
        "The network forms a directed, weighted graph.",
        "An artificial neural network consists of simulated neurons.",
        "Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection.",
        "All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data.",
        "Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons.",
        "ANNs are composed of artificial neurons which are conceptually derived from biological neurons.",
        "Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons.",
        "The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons.",
        "The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.",
        "To find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron.",
        "We add a bias term to this sum.",
        "This weighted sum is sometimes called the activation.",
        "This weighted sum is then passed through a (usually nonlinear) activation function to produce the output.",
        "The initial inputs are external data, such as images and documents.",
        "The ultimate outputs accomplish the task, such as recognizing an object in an image.",
        "The neurons are typically organized into multiple layers, especially in deep learning.",
        "Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers.",
        "The layer that receives external data is the input layer.",
        "The layer that produces the ultimate result is the output layer.",
        "In between them are zero or more hidden layers.",
        "Single layer and unlayered networks are also used.",
        "Between two layers, multiple connection patterns are possible.",
        "They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer.",
        "They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer.",
        "Neurons with only such connections form a directed acyclic graph and are known as feedforward networks.",
        "Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.",
        "A hyperparameter is a constant parameter whose value is set before the learning process begins.",
        "The values of parameters are derived via learning.",
        "Examples of hyperparameters include learning rate, the number of hidden layers and batch size.",
        "The values of some hyperparameters can be dependent on those of other hyperparameters.",
        "For example, the size of some layers can depend on the overall number of layers.",
        "Learning is the adaptation of the network to better handle a task by considering sample observations.",
        "Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result.",
        "This is done by minimizing the observed errors.",
        "Learning is complete when examining additional observations does not usefully reduce the error rate.",
        "Even after learning, the error rate typically does not reach 0.",
        "If after learning, the error rate is too high, the network typically must be redesigned.",
        "Practically this is done by defining a cost function that is evaluated periodically during learning.",
        "As long as its output continues to decline, learning continues.",
        "The cost is frequently defined as a statistic whose value can only be approximated.",
        "The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small.",
        "Learning attempts to reduce the total of the differences across the observations.",
        "Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.",
        "The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation.",
        "A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy.",
        "Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability.",
        "In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate.",
        "The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change.",
        "A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.",
        "While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) because it arises from the model (e.g.",
        "in a probabilistic model, the model's posterior probability can be used as an inverse cost).",
        "Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning.",
        "The error amount is effectively divided among the connections.",
        "Technically, backpropagation calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights.",
        "The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.",
        "Machine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning.",
        "Each corresponds to a particular learning task.",
        "Supervised learning uses a set of paired inputs and desired outputs.",
        "The learning task is to produce the desired output for each input.",
        "In this case, the cost function is related to eliminating incorrect deductions.",
        "A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output.",
        "Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation).",
        "Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition).",
        "This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.",
        "In unsupervised learning, input data is given along with the cost function, some function of the data x {\\displaystyle extstyle x} and the network's output.",
        "The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables).",
        "Minimizing this cost produces a value of a {\\displaystyle extstyle a} that is equal to the mean of the data.",
        "The cost function can be much more complicated.",
        "Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.",
        "In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one.",
        "The goal is to win the game, i.e., generate the most positive (lowest cost) responses.",
        "In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost.",
        "At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules.",
        "The rules and the long-term cost usually only can be estimated.",
        "At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.",
        "Formally, the environment is modeled as a Markov decision process (MDP) with states s 1 , .",
        ", s n ∈ S {\\displaystyle extstyle {s_{1},...,s_{n}}\\in S} and actions a 1 , .",
        "Taken together, the two define a Markov chain (MC).",
        "The aim is to discover the lowest-cost MC.",
        "ANNs serve as the learning component in such applications.",
        "Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems.",
        "Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.",
        "Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA).",
        "It is a system with only one input, situation s, and only one output, action (or behavior) a.",
        "It has neither external advice input nor external reinforcement input from the environment.",
        "The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations.",
        "The system is driven by the interaction between cognition and emotion.",
        "The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation.",
        "The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it receives initial emotions (only once) about to be encountered situations in the behavioral environment.",
        "Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.",
        "Neuroevolution can create neural network topologies and weights using evolutionary computation.",
        "It is competitive with sophisticated gradient descent approaches.",
        "One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".",
        "Stochastic neural networks originating from Sherrington-Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions , or by giving them stochastic weights.",
        "This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima.",
        "Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.",
        "Topological deep learning, first introduced in 2017, is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and high-order data.",
        "Initially rooted in algebraic topology, TDL has since evolved into a versatile framework incorporating tools from other mathematical disciplines, such as differential topology and geometric topology.",
        "As a successful example of mathematical deep learning, TDL continues to inspire advancements in mathematical artificial intelligence, fostering a mutually beneficial relationship between AI and mathematics.",
        "In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost.",
        "Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other learning algorithms.",
        "Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.",
        "Two modes of learning are available: stochastic and batch.",
        "In stochastic learning, each input creates a weight adjustment.",
        "In batch learning, weights are adjusted based on a batch of inputs, accumulating errors over the batch.",
        "Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima.",
        "However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error.",
        "A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.",
        "ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.",
        "The simplest types have one or more static components, including number of units, number of layers, unit weights and topology.",
        "Dynamic types allow one or more of these to evolve via learning.",
        "The latter is much more complicated but can shorten learning periods and produce better results.",
        "Some types allow/require learning to be \"supervised\" by the operator, while others operate independently.",
        "Some types operate purely in hardware, while others are purely software and run on general purpose computers.",
        "Using artificial neural networks requires an understanding of their characteristics.",
        "Choice of model: This depends on the data representation and the application.",
        "Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc.).",
        "Overly complex models learn slowly.",
        "Learning algorithm: Numerous trade-offs exist between learning algorithms.",
        "Almost any algorithm will work well with the correct hyperparameters for training on a particular data set.",
        "However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.",
        "Robustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.",
        "Neural architecture search (NAS) uses machine learning to automate ANN design.",
        "Various approaches to NAS have designed networks that compare well with hand-designed systems.",
        "The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network.",
        "Available systems include AutoML and AutoKeras.",
        "scikit-learn library provides functions to help with building a deep network from scratch.",
        "We can then implement a deep network with TensorFlow or Keras.",
        "Hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc.",
        "The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters: Because of their ability to model and reproduce nonlinear processes, artificial neural networks have found applications in many disciplines.",
        "ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements.",
        "It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff.",
        "ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology.",
        "ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones.",
        "For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk.",
        "Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.",
        "ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems.",
        "In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems.",
        "Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.",
        "It is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.",
        "Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science.",
        "For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals.",
        "This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.",
        "The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem.",
        "However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.",
        "A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections.",
        "Further, the use of irrational values for weights results in a machine with super-Turing power.",
        "A model's \"capacity\" property corresponds to its ability to model any given function.",
        "It is related to the amount of information that can be stored in the network and to the notion of complexity.",
        "Two notions of capacity are known by the community.",
        "The information capacity and the VC Dimension.",
        "The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover.",
        "The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element.",
        "The information capacity captures the functions modelable by the network given any data as input.",
        "The second notion, is the VC dimension.",
        "VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances.",
        "This is, given input data in a specific form.",
        "As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a perceptron.",
        "The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.",
        "Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model.",
        "Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum.",
        "Thirdly, for sufficiently large data or parameters, some methods become impractical.",
        "Another issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction.",
        "The convergence behavior of certain types of ANN architectures are more understood than others.",
        "When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models.",
        "Another example is when parameters are small, it is observed that ANNs often fit target functions from low to high frequencies.",
        "This behavior is referred to as the spectral bias, or frequency principle, of neural networks.",
        "This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method.",
        "Deeper neural networks have been observed to be more biased towards low frequency functions.",
        "Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training.",
        "This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters.",
        "Two approaches address over-training.",
        "The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.",
        "The second is to use some form of regularization.",
        "This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.",
        "Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model.",
        "The MSE on a validation set can be used as an estimate for variance.",
        "This value can then be used to calculate the confidence interval of network output, assuming a normal distribution.",
        "A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.",
        "By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities.",
        "This is useful in classification as it gives a certainty measure on classifications.",
        "The softmax activation function is: y i = e x i ∑ j = 1 c e x j {\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} A common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.",
        "Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases.",
        "Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.",
        "Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.",
        "), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns-it should not learn to always turn right).",
        "A central claim of ANNs is that they embody new and powerful general principles for processing information.",
        "These principles are ill-defined.",
        "It is often claimed that they are emergent from the network itself.",
        "This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition.",
        "In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are.",
        "No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything.",
        "One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.",
        "Technology writer Roger Bridgman commented: Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?)",
        "but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".",
        "In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers.",
        "An unreadable table that a useful machine could read would still be well worth having.",
        "Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network.",
        "Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks.",
        "Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful.",
        "For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.",
        "Biological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance.",
        "Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.",
        "Large and effective neural networks require considerable computing resources.",
        "While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage.",
        "Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons - which require enormous CPU power and time.",
        "Some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before.",
        "The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.",
        "Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry.",
        "Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.",
        "Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network.",
        "Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful.",
        "For example, local vs. non-local learning and shallow vs. deep architecture.",
        "Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.",
        "Neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.",
        "These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute.",
        "This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement.",
        "For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field.",
        "The program would penalize any resume with the word \"woman\" or the name of any women's college.",
        "However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.",
        "Artificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications.",
        "Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.",
        "In the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation.",
        "For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance.",
        "This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.",
        "By modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion.",
        "Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques.",
        "These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.",
        "In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation.",
        "They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content.",
        "This has implications for automated customer service, content moderation, and language understanding technologies.",
        "In the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization.",
        "For instance, deep feedforward neural networks are important in system identification and control applications.",
        "ANNs are used for stock market prediction and credit scoring: In investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions.",
        "In credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process.",
        "ANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation.",
        "Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.",
        "ANNs are able to process and analyze vast medical datasets.",
        "They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning.",
        "In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs.",
        "Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management.",
        "Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.",
        "ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries.",
        "This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions.",
        "For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user.",
        "In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck.",
        "In the marketing industry, generative models are used to create personalized advertisements for consumers.",
        "Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020.",
        "Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game."
      ],
      "metadata": {
        "title": "Neural network (machine learning)",
        "url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
        "word_count": 7880,
        "char_count": 52748,
        "sentence_count": 382,
        "scraped_at": "2025-08-09T14:47:02.899292",
        "language": "en",
        "processing_time": 0.011887311935424805,
        "source_hash": "8a0c1003edb6daf2062575dfb1014bd9"
      }
    },
    {
      "title": "Neural network",
      "url": "https://en.wikipedia.org/wiki/Neural_network",
      "raw_text": "A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or signal pathways. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural networks.\n\nIn neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses.\nIn machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems.\n\n\n== In biology ==\n\nIn the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses.\nEach neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead.\nPopulations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks, and many of these together form brains and nervous systems.\nSignals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, where they cause contraction and thereby motion.\n\n\n== In machine learning ==\n\nIn machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software.\nNeurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the hidden layers) to the final layer (the output layer).\nThe \"signal\" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.\nThe term deep neural network refers to neural networks that have more than three layers, typically including at least two hidden layers in addition to the input and output layers.\nNeural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI.\n\n\n== History ==\n\nThe theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.\nArtificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957,\nartificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts.\n\n\n== See also ==\nEmergence\nBiological cybernetics\nBiologically-inspired computing\n\n\n== References ==",
      "cleaned_text": "A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or signal pathways. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural networks. In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems - a population of nerve cells connected by synapses. In machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. In the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead. Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks, and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, where they cause contraction and thereby motion. In machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the hidden layers) to the final layer (the output layer). The \"signal\" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset. The term deep neural network refers to neural networks that have more than three layers, typically including at least two hidden layers in addition to the input and output layers. Neural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI. The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it. Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957, artificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts.",
      "sentences": [
        "A neural network is a group of interconnected units called neurons that send signals to one another.",
        "Neurons can be either biological cells or signal pathways.",
        "While individual neurons are simple, many of them together in a network can perform complex tasks.",
        "There are two main types of neural networks.",
        "In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems - a population of nerve cells connected by synapses.",
        "In machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions.",
        "Artificial neural networks are used to solve artificial intelligence problems.",
        "In the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses.",
        "A given neuron can be connected to hundreds of thousands of synapses.",
        "Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors.",
        "A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead.",
        "Populations of interconnected neurons that are smaller than neural networks are called neural circuits.",
        "Very large interconnected networks are called large scale brain networks, and many of these together form brains and nervous systems.",
        "Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, where they cause contraction and thereby motion.",
        "In machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions.",
        "While early artificial neural networks were physical machines, today they are almost always implemented in software.",
        "The \"signal\" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer.",
        "The signal each neuron outputs is calculated from this number, according to its activation function.",
        "The behavior of the network depends on the strengths (or weights) of the connections between neurons.",
        "A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.",
        "The term deep neural network refers to neural networks that have more than three layers, typically including at least two hidden layers in addition to the input and output layers.",
        "Neural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI.",
        "The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890.",
        "Both posited that human thought emerged from interactions among large numbers of neurons inside the brain.",
        "In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.",
        "Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism.",
        "However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957, artificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts."
      ],
      "metadata": {
        "title": "Neural network",
        "url": "https://en.wikipedia.org/wiki/Neural_network",
        "word_count": 576,
        "char_count": 3882,
        "sentence_count": 27,
        "scraped_at": "2025-08-09T14:47:09.025271",
        "language": "en",
        "processing_time": 0.0022840499877929688,
        "source_hash": "a9917b01fd5737dc0c2f0a813fd5d4e2"
      }
    },
    {
      "title": "Neural network (machine learning)",
      "url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
      "raw_text": "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks.\nA neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\n\n\n== Training ==\nNeural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data.\n\n\n== History ==\n\n\n=== Early work ===\nToday's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.\nHistorically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.\nWarren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.\nIn the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). \nIn 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research.\nR. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\"\nThe perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.\nThe first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.\n\n\n=== Deep learning breakthroughs in the 1960s and 1970s ===\nFundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\"\nThe first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\nIn 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning.\nNevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967).\nIn 1976 transfer learning was introduced in neural networks learning.\nDeep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.\n\n\n=== Backpropagation ===\nBackpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.\n\n\n=== Convolutional neural networks ===\nKunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision.\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.\nIn 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images.\nFrom 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.\n\n\n=== Recurrent neural networks ===\nOne origin of RNN was statistical mechanics. In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning. This was popularized as the Hopfield network by John Hopfield (1982). Another origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. The McCulloch and Pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.\nIn 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a Crossbar Adaptive Array, used direct recurrent connections from the output to the supervisor (teaching) inputs. In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations. Eliminating the external supervisor, it introduced the self-learning method in neural networks.  \nIn cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on the relation between cognition and emotion. Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion. In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation. It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology.\nTwo early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. \nIn the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, Jürgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\" which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\nIn 1991, Sepp Hochreiter's diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains. This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999. It became the default choice for RNN architecture.\nDuring 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models.\n\n\n=== Deep learning ===\nBetween 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly.\nIn October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.\nIn 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".\nRadial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.\nGenerative adversarial network (GAN) (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014–2018 period. The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here, the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).\nIn 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in December 2015. ResNet behaves like an open-gated Highway Net. \n\nDuring the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need.\nIt requires computation time that is quadratic in the size of the context window. Jürgen Schmidhuber's fast weight controller (1992) scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.\nTransformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.\n\n\n== Models ==\n\nANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.\nAn artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons.\n\n\n=== Artificial neurons ===\n\nANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.\nTo find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.\n\n\n=== Organization ===\nThe neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.\n\n\n=== Hyperparameter ===\n\nA hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.\n\n\n=== Learning ===\n\nLearning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.\n\n\n==== Learning rate ====\n\nThe learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.\n\n\n==== Cost function ====\nWhile it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) because it arises from the model (e.g. in a probabilistic model, the model's posterior probability can be used as an inverse cost).\n\n\n==== Backpropagation ====\n\nBackpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backpropagation calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.\n\n\n=== Learning paradigms ===\n\nMachine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task.\n\n\n==== Supervised learning ====\nSupervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\n\n\n==== Unsupervised learning ====\nIn unsupervised learning, input data is given along with the cost function, some function of the data \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\textstyle x}\n  \n and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model \n  \n    \n      \n        \n          f\n          (\n          x\n          )\n          =\n          a\n        \n      \n    \n    {\\displaystyle \\textstyle f(x)=a}\n  \n where \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\textstyle a}\n  \n is a constant and the cost \n  \n    \n      \n        \n          C\n          =\n          E\n          [\n          (\n          x\n          −\n          f\n          (\n          x\n          )\n          \n            )\n            \n              2\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\textstyle C=E[(x-f(x))^{2}]}\n  \n. Minimizing this cost produces a value of \n  \n    \n      \n        \n          a\n        \n      \n    \n    {\\displaystyle \\textstyle a}\n  \n that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\textstyle x}\n  \n and \n  \n    \n      \n        \n          f\n          (\n          x\n          )\n        \n      \n    \n    {\\displaystyle \\textstyle f(x)}\n  \n, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\n\n\n==== Reinforcement learning ====\n\nIn applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.\nFormally, the environment is modeled as a Markov decision process (MDP) with states \n  \n    \n      \n        \n          \n            \n              s\n              \n                1\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              s\n              \n                n\n              \n            \n          \n          ∈\n          S\n        \n      \n    \n    {\\displaystyle \\textstyle {s_{1},...,s_{n}}\\in S}\n  \n and actions \n  \n    \n      \n        \n          \n            \n              a\n              \n                1\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              a\n              \n                m\n              \n            \n          \n          ∈\n          A\n        \n      \n    \n    {\\displaystyle \\textstyle {a_{1},...,a_{m}}\\in A}\n  \n. Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution \n  \n    \n      \n        \n          P\n          (\n          \n            c\n            \n              t\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(c_{t}|s_{t})}\n  \n, the observation distribution \n  \n    \n      \n        \n          P\n          (\n          \n            x\n            \n              t\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(x_{t}|s_{t})}\n  \n and the transition distribution \n  \n    \n      \n        \n          P\n          (\n          \n            s\n            \n              t\n              +\n              1\n            \n          \n          \n            |\n          \n          \n            s\n            \n              t\n            \n          \n          ,\n          \n            a\n            \n              t\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\textstyle P(s_{t+1}|s_{t},a_{t})}\n  \n, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.\nANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\n\n\n==== Self-learning ====\nSelf-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation:\n\n In situation s perform action a;\n Receive consequence situation s';\n Compute emotion of being in consequence situation v(s');\n Update crossbar memory w'(a,s) = w(a,s) + v(s').\n\nThe backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it receives initial emotions (only once) about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.\n\n\n==== Neuroevolution ====\n\nNeuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".\n\n\n=== Stochastic neural network ===\nStochastic neural networks originating from Sherrington–Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions , or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.\n\n\n=== Topological deep learning ===\nTopological deep learning, first introduced in 2017, is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and high-order data. Initially rooted in algebraic topology, TDL has since evolved into a versatile framework incorporating tools from other mathematical disciplines, such as differential topology and geometric topology. As a successful example of mathematical deep learning, TDL continues to inspire advancements in mathematical artificial intelligence, fostering a mutually beneficial relationship between AI and mathematics.  \n\n\n=== Other ===\nIn a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation–maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.\n\n\n==== Modes ====\n\nTwo modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning, weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.\n\n\n== Types ==\n\nANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\nSome of the main breakthroughs include: \n\nConvolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; where long short-term memory avoids the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads;\nCompetitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input.\n\n\n== Network design ==\nUsing artificial neural networks requires an understanding of their characteristics.\n\nChoice of model: This depends on the data representation and the application. Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc.). Overly complex models learn slowly.\nLearning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.\nRobustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.\nNeural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras. scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras.\nHyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc. The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters:\n\n\n== Applications ==\nBecause of their ability to model and reproduce nonlinear processes, artificial neural networks have found applications in many disciplines. These include:\n\nFunction approximation, or regression analysis, (including time series prediction, fitness approximation, and modeling)\nData processing (including filtering, clustering, blind source separation, and compression)\nNonlinear system identification and control (including vehicle control, trajectory prediction, adaptive control, process control, and natural resource management)\nPattern recognition (including radar systems, face identification, signal classification, novelty detection, 3D reconstruction, object recognition, and sequential decision making)\nSequence recognition (including gesture, speech, and handwritten and printed text recognition)\nSensor data analysis (including image analysis)\nRobotics (including directing manipulators and prostheses)\nData mining (including knowledge discovery in databases)\nFinance (such as ex-ante models for specific financial long-run forecasts and artificial financial markets)\nQuantum chemistry\nGeneral game playing\nGenerative AI\nData visualization\nMachine translation\nSocial network filtering\nE-mail spam filtering\nMedical diagnosis\nANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.\nANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.\nANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.\nIt is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.\nBeyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.\n\n\n== Theoretical properties ==\n\n\n=== Computational power ===\nThe multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\nA specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.\n\n\n=== Capacity ===\nA model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\nTwo notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.\n\n\n=== Convergence ===\nModels may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.\nAnother issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction.\nThe convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fit target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.\n\n\n=== Generalization and statistics ===\n\nApplications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. \nTwo approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\n\nSupervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\nBy assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.\nThe softmax activation function is:\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \n            \n              e\n              \n                \n                  x\n                  \n                    i\n                  \n                \n              \n            \n            \n              \n                ∑\n                \n                  j\n                  =\n                  1\n                \n                \n                  c\n                \n              \n              \n                e\n                \n                  \n                    x\n                    \n                      j\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}}\n  \n\n\n== Criticism ==\n\n\n=== Training ===\nA common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.\nAny learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.\nDean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right).\n\n\n=== Theory ===\nA central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a \n\nsomething-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything. One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.\nTechnology writer Roger Bridgman commented:\n\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\nIn spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\n\nAlthough it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.\nBiological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\n\n\n=== Hardware ===\nLarge and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time.\nSome argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.\nNeuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.\n\n\n=== Practical counterexamples ===\nAnalyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.\n\n\n=== Hybrid approaches ===\nAdvocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.\n\n\n=== Dataset bias ===\nNeural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement. For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field. The program would penalize any resume with the word \"woman\" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.\n\n\n== Gallery ==\n\n\n== Recent advancements and future directions ==\nArtificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.\n\n\n=== Image processing ===\nIn the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance. This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.\n\n\n=== Speech recognition ===\nBy modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques. These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.\n\n\n=== Natural language processing ===\nIn natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content. This has implications for automated customer service, content moderation, and language understanding technologies.\n\n\n=== Control systems ===\nIn the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications.\n\n\n=== Finance ===\n\nANNs are used for stock market prediction and credit scoring: \n\nIn investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions.\nIn credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process.\nANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.\n\n\n=== Medicine ===\nANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning. In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs. Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.\n\n\n=== Content creation ===\nANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry, generative models are used to create personalized advertisements for consumers. Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020. Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== External links ==\n\nA Brief Introduction to Neural Networks (D. Kriesel) – Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.\nReview of Neural Networks in Materials Science Archived 7 June 2015 at the Wayback Machine\nArtificial Neural Networks Tutorial in three languages (Univ. Politécnica de Madrid)\nAnother introduction to ANN\nNext Generation of Neural Networks Archived 24 January 2011 at the Wayback Machine – Google Tech Talks\nPerformance of Neural Networks\nNeural Networks and Information Archived 9 July 2009 at the Wayback Machine\nSanderson G (5 October 2017). \"But what is a Neural Network?\". 3Blue1Brown. Archived from the original on 7 November 2021 – via YouTube.",
      "cleaned_text": "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks. A neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers. Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information. Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. Today's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing. Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. In the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research. R. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\" The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. The first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning. Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\" The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967). In 1976 transfer learning was introduced in neural networks learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. Kunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision. The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images. From 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments. One origin of RNN was statistical mechanics. In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning. This was popularized as the Hopfield network by John Hopfield (1982). Another origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. The McCulloch and Pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past. In 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a Crossbar Adaptive Array, used direct recurrent connections from the output to the supervisor (teaching) inputs. In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations. Eliminating the external supervisor, it introduced the self-learning method in neural networks. In cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on the relation between cognition and emotion. Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion. In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation. It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology. Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. In the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, Jürgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\" which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. In 1991, Sepp Hochreiter's diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains. This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999. It became the default choice for RNN architecture. During 1985-1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models. Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly. In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\". Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications. Generative adversarial network (GAN) (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014-2018 period. The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here, the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022). In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in December 2015. ResNet behaves like an open-gated Highway Net. During the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need. It requires computation time that is quadratic in the size of the context window. Jürgen Schmidhuber's fast weight controller (1992) scales linearly and was later shown to be equivalent to the unnormalized linear Transformer. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture. ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph. An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons. ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image. To find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image. The neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks. A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers. Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation. The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change. While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) because it arises from the model (e.g. in a probabilistic model, the model's posterior probability can be used as an inverse cost). Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backpropagation calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks. Machine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task. Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. In unsupervised learning, input data is given along with the cost function, some function of the data x {\\displaystyle extstyle x} and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model f ( x ) = a {\\displaystyle extstyle f(x)=a} where a {\\displaystyle extstyle a} is a constant and the cost C = E [ ( x − f ( x ) ) 2 ] {\\displaystyle extstyle C=E[(x-f(x))^{2}]} . Minimizing this cost produces a value of a {\\displaystyle extstyle a} that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between x {\\displaystyle extstyle x} and f ( x ) {\\displaystyle extstyle f(x)} , whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering. In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly. Formally, the environment is modeled as a Markov decision process (MDP) with states s 1 , . . . , s n ∈ S {\\displaystyle extstyle {s_{1},...,s_{n}}\\in S} and actions a 1 , . . . , a m ∈ A {\\displaystyle extstyle {a_{1},...,a_{m}}\\in A} . Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution P ( c t | s t ) {\\displaystyle extstyle P(c_{t}|s_{t})} , the observation distribution P ( x t | s t ) {\\displaystyle extstyle P(x_{t}|s_{t})} and the transition distribution P ( s t + 1 | s t , a t ) {\\displaystyle extstyle P(s_{t+1}|s_{t},a_{t})} , while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC. ANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks. Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation: In situation s perform action a; Receive consequence situation s'; Compute emotion of being in consequence situation v(s'); Update crossbar memory w'(a,s) = w(a,s) + v(s'). The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it receives initial emotions (only once) about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations. Neuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\". Stochastic neural networks originating from Sherrington-Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions , or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks. Topological deep learning, first introduced in 2017, is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and high-order data. Initially rooted in algebraic topology, TDL has since evolved into a versatile framework incorporating tools from other mathematical disciplines, such as differential topology and geometric topology. As a successful example of mathematical deep learning, TDL continues to inspire advancements in mathematical artificial intelligence, fostering a mutually beneficial relationship between AI and mathematics. In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks. Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning, weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set. ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers. Some of the main breakthroughs include: Convolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; where long short-term memory avoids the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads; Competitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input. Using artificial neural networks requires an understanding of their characteristics. Choice of model: This depends on the data representation and the application. Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc.). Overly complex models learn slowly. Learning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation. Robustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust. Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras. scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras. Hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc. The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters: Because of their ability to model and reproduce nonlinear processes, artificial neural networks have found applications in many disciplines. These include: Function approximation, or regression analysis, (including time series prediction, fitness approximation, and modeling) Data processing (including filtering, clustering, blind source separation, and compression) Nonlinear system identification and control (including vehicle control, trajectory prediction, adaptive control, process control, and natural resource management) Pattern recognition (including radar systems, face identification, signal classification, novelty detection, 3D reconstruction, object recognition, and sequential decision making) Sequence recognition (including gesture, speech, and handwritten and printed text recognition) Sensor data analysis (including image analysis) Robotics (including directing manipulators and prostheses) Data mining (including knowledge discovery in databases) Finance (such as ex-ante models for specific financial long-run forecasts and artificial financial markets) Quantum chemistry General game playing Generative AI Data visualization Machine translation Social network filtering E-mail spam filtering Medical diagnosis ANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information. ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions. ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level. It is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition. Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation. The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters. A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power. A model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity. Two notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity. Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical. Another issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction. The convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fit target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions. Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting. Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is: y i = e x i ∑ j = 1 c e x j {\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} A common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation. Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC. Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns-it should not learn to always turn right). A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything. One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. Technology writer Roger Bridgman commented: Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\". In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture. Biological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies. Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons - which require enormous CPU power and time. Some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days. Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU. Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture. Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. Neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement. For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field. The program would penalize any resume with the word \"woman\" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets. Artificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine. In the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance. This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging. By modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques. These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products. In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content. This has implications for automated customer service, content moderation, and language understanding technologies. In the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications. ANNs are used for stock market prediction and credit scoring: In investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions. In credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process. ANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies. ANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning. In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs. Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine. ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry, generative models are used to create personalized advertisements for consumers. Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020. Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.",
      "sentences": [
        "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks.",
        "A neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain.",
        "Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance.",
        "These are connected by edges, which model the synapses in the brain.",
        "Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons.",
        "The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function.",
        "The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.",
        "Typically, neurons are aggregated into layers.",
        "Different layers may perform different transformations on their inputs.",
        "A network is typically called a deep neural network if it has at least two hidden layers.",
        "Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence.",
        "They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.",
        "Neural networks are typically trained through empirical risk minimization.",
        "This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset.",
        "Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network.",
        "During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function.",
        "This method allows the network to generalize to unseen data.",
        "Today's deep neural networks are based on early work in statistics over 200 years ago.",
        "The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights.",
        "The sum of the products of the weights and the inputs is calculated at each node.",
        "The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights.",
        "This technique has been known for over two centuries as the method of least squares or linear regression.",
        "It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.",
        "Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors.",
        "Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism.",
        "Unlike the von Neumann model, connectionist computing does not separate memory and processing.",
        "Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks.",
        "This model paved the way for research to split into two approaches.",
        "One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.",
        "In the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning.",
        "It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network.",
        "Farley and Clark (1954) used computational machines to simulate a Hebbian network.",
        "Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).",
        "In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research.",
        "R. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\"",
        "However, \"they dropped the subject.\"",
        "The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding.",
        "This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.",
        "The first perceptrons did not have adaptive hidden units.",
        "However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer.",
        "Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight.",
        "Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.",
        "Fundamental research was conducted on ANNs in the 1960s and 1970s.",
        "The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965).",
        "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron.",
        "A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis.",
        "Superfluous hidden units are pruned using a separate validation set.",
        "Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\"",
        "The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.",
        "In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes.",
        "Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.",
        "In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.",
        "The rectifier has become the most popular activation function for deep learning.",
        "Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit.",
        "This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967).",
        "In 1976 transfer learning was introduced in neural networks learning.",
        "Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.",
        "Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes.",
        "The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.",
        "In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970).",
        "Ostrovski et al.",
        "republished it in 1971.",
        "Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm).",
        "In 1986, David E. Rumelhart et al.",
        "popularised backpropagation but did not cite the original work.",
        "Kunihiko Fukushima's convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs.",
        "CNNs have become an essential tool for computer vision.",
        "The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition.",
        "It used convolutions, weight sharing, and backpropagation.",
        "In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.",
        "In 1989, Yann LeCun et al.",
        "created a CNN called LeNet for recognizing handwritten ZIP codes on mail.",
        "Training required 3 days.",
        "In 1990, Wei Zhang implemented a CNN on optical computing hardware.",
        "In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms.",
        "LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images.",
        "From 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.",
        "One origin of RNN was statistical mechanics.",
        "In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning.",
        "This was popularized as the Hopfield network by John Hopfield (1982).",
        "Another origin of RNN was neuroscience.",
        "The word \"recurrent\" is used to describe loop-like structures in anatomy.",
        "In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex.",
        "Hebb considered \"reverberating circuit\" as an explanation for short-term memory.",
        "The McCulloch and Pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.",
        "In 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a Crossbar Adaptive Array, used direct recurrent connections from the output to the supervisor (teaching) inputs.",
        "In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations.",
        "Eliminating the external supervisor, it introduced the self-learning method in neural networks.",
        "In cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on the relation between cognition and emotion.",
        "Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion.",
        "In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation.",
        "It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology.",
        "Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology.",
        "In the 1980s, backpropagation did not work well for deep RNNs.",
        "To overcome this problem, in 1991, Jürgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\" which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation.",
        "In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.",
        "In 1991, Sepp Hochreiter's diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it.",
        "He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains.",
        "This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999.",
        "It became the default choice for RNN architecture.",
        "During 1985-1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm.",
        "These were designed for unsupervised learning of deep generative models.",
        "Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition.",
        "In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.",
        "It then won more contests.",
        "They also showed how max-pooling CNNs on GPU improved performance significantly.",
        "In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods.",
        "Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.",
        "In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images.",
        "Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".",
        "Radial basis function and wavelet networks were introduced in 2013.",
        "These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.",
        "Generative adversarial network (GAN) (Ian Goodfellow et al., 2014) became state of the art in generative modeling during 2014-2018 period.",
        "The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.",
        "The first network is a generative model that models a probability distribution over output patterns.",
        "The second network learns by gradient descent to predict the reactions of the environment to these patterns.",
        "Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al.",
        "Here, the GAN generator is grown from small to large scale in a pyramidal fashion.",
        "Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.",
        "In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers.",
        "Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem.",
        "In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in December 2015.",
        "ResNet behaves like an open-gated Highway Net.",
        "During the 2010s, the seq2seq model was developed, and attention mechanisms were added.",
        "It led to the modern Transformer architecture in 2017 in Attention Is All You Need.",
        "It requires computation time that is quadratic in the size of the context window.",
        "Jürgen Schmidhuber's fast weight controller (1992) scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.",
        "Transformers have increasingly become the model of choice for natural language processing.",
        "Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.",
        "ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with.",
        "They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors.",
        "ANNs have the ability to learn and model non-linearities and complex relationships.",
        "This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others.",
        "The network forms a directed, weighted graph.",
        "An artificial neural network consists of simulated neurons.",
        "Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection.",
        "All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data.",
        "Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons.",
        "ANNs are composed of artificial neurons which are conceptually derived from biological neurons.",
        "Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons.",
        "The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons.",
        "The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.",
        "To find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron.",
        "We add a bias term to this sum.",
        "This weighted sum is sometimes called the activation.",
        "This weighted sum is then passed through a (usually nonlinear) activation function to produce the output.",
        "The initial inputs are external data, such as images and documents.",
        "The ultimate outputs accomplish the task, such as recognizing an object in an image.",
        "The neurons are typically organized into multiple layers, especially in deep learning.",
        "Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers.",
        "The layer that receives external data is the input layer.",
        "The layer that produces the ultimate result is the output layer.",
        "In between them are zero or more hidden layers.",
        "Single layer and unlayered networks are also used.",
        "Between two layers, multiple connection patterns are possible.",
        "They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer.",
        "They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer.",
        "Neurons with only such connections form a directed acyclic graph and are known as feedforward networks.",
        "Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.",
        "A hyperparameter is a constant parameter whose value is set before the learning process begins.",
        "The values of parameters are derived via learning.",
        "Examples of hyperparameters include learning rate, the number of hidden layers and batch size.",
        "The values of some hyperparameters can be dependent on those of other hyperparameters.",
        "For example, the size of some layers can depend on the overall number of layers.",
        "Learning is the adaptation of the network to better handle a task by considering sample observations.",
        "Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result.",
        "This is done by minimizing the observed errors.",
        "Learning is complete when examining additional observations does not usefully reduce the error rate.",
        "Even after learning, the error rate typically does not reach 0.",
        "If after learning, the error rate is too high, the network typically must be redesigned.",
        "Practically this is done by defining a cost function that is evaluated periodically during learning.",
        "As long as its output continues to decline, learning continues.",
        "The cost is frequently defined as a statistic whose value can only be approximated.",
        "The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small.",
        "Learning attempts to reduce the total of the differences across the observations.",
        "Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.",
        "The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation.",
        "A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy.",
        "Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability.",
        "In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate.",
        "The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change.",
        "A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.",
        "While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) because it arises from the model (e.g.",
        "in a probabilistic model, the model's posterior probability can be used as an inverse cost).",
        "Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning.",
        "The error amount is effectively divided among the connections.",
        "Technically, backpropagation calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights.",
        "The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.",
        "Machine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning.",
        "Each corresponds to a particular learning task.",
        "Supervised learning uses a set of paired inputs and desired outputs.",
        "The learning task is to produce the desired output for each input.",
        "In this case, the cost function is related to eliminating incorrect deductions.",
        "A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output.",
        "Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation).",
        "Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition).",
        "This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.",
        "In unsupervised learning, input data is given along with the cost function, some function of the data x {\\displaystyle extstyle x} and the network's output.",
        "The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables).",
        "Minimizing this cost produces a value of a {\\displaystyle extstyle a} that is equal to the mean of the data.",
        "The cost function can be much more complicated.",
        "Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.",
        "In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one.",
        "The goal is to win the game, i.e., generate the most positive (lowest cost) responses.",
        "In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost.",
        "At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules.",
        "The rules and the long-term cost usually only can be estimated.",
        "At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.",
        "Formally, the environment is modeled as a Markov decision process (MDP) with states s 1 , .",
        ", s n ∈ S {\\displaystyle extstyle {s_{1},...,s_{n}}\\in S} and actions a 1 , .",
        "Taken together, the two define a Markov chain (MC).",
        "The aim is to discover the lowest-cost MC.",
        "ANNs serve as the learning component in such applications.",
        "Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems.",
        "Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.",
        "Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA).",
        "It is a system with only one input, situation s, and only one output, action (or behavior) a.",
        "It has neither external advice input nor external reinforcement input from the environment.",
        "The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations.",
        "The system is driven by the interaction between cognition and emotion.",
        "The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation.",
        "The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it receives initial emotions (only once) about to be encountered situations in the behavioral environment.",
        "Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.",
        "Neuroevolution can create neural network topologies and weights using evolutionary computation.",
        "It is competitive with sophisticated gradient descent approaches.",
        "One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".",
        "Stochastic neural networks originating from Sherrington-Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions , or by giving them stochastic weights.",
        "This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima.",
        "Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.",
        "Topological deep learning, first introduced in 2017, is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and high-order data.",
        "Initially rooted in algebraic topology, TDL has since evolved into a versatile framework incorporating tools from other mathematical disciplines, such as differential topology and geometric topology.",
        "As a successful example of mathematical deep learning, TDL continues to inspire advancements in mathematical artificial intelligence, fostering a mutually beneficial relationship between AI and mathematics.",
        "In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost.",
        "Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other learning algorithms.",
        "Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.",
        "Two modes of learning are available: stochastic and batch.",
        "In stochastic learning, each input creates a weight adjustment.",
        "In batch learning, weights are adjusted based on a batch of inputs, accumulating errors over the batch.",
        "Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima.",
        "However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error.",
        "A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.",
        "ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.",
        "The simplest types have one or more static components, including number of units, number of layers, unit weights and topology.",
        "Dynamic types allow one or more of these to evolve via learning.",
        "The latter is much more complicated but can shorten learning periods and produce better results.",
        "Some types allow/require learning to be \"supervised\" by the operator, while others operate independently.",
        "Some types operate purely in hardware, while others are purely software and run on general purpose computers.",
        "Using artificial neural networks requires an understanding of their characteristics.",
        "Choice of model: This depends on the data representation and the application.",
        "Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc.).",
        "Overly complex models learn slowly.",
        "Learning algorithm: Numerous trade-offs exist between learning algorithms.",
        "Almost any algorithm will work well with the correct hyperparameters for training on a particular data set.",
        "However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.",
        "Robustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.",
        "Neural architecture search (NAS) uses machine learning to automate ANN design.",
        "Various approaches to NAS have designed networks that compare well with hand-designed systems.",
        "The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network.",
        "Available systems include AutoML and AutoKeras.",
        "scikit-learn library provides functions to help with building a deep network from scratch.",
        "We can then implement a deep network with TensorFlow or Keras.",
        "Hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc.",
        "The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters: Because of their ability to model and reproduce nonlinear processes, artificial neural networks have found applications in many disciplines.",
        "ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements.",
        "It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff.",
        "ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology.",
        "ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones.",
        "For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk.",
        "Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.",
        "ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems.",
        "In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems.",
        "Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.",
        "It is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.",
        "Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science.",
        "For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals.",
        "This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.",
        "The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem.",
        "However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.",
        "A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections.",
        "Further, the use of irrational values for weights results in a machine with super-Turing power.",
        "A model's \"capacity\" property corresponds to its ability to model any given function.",
        "It is related to the amount of information that can be stored in the network and to the notion of complexity.",
        "Two notions of capacity are known by the community.",
        "The information capacity and the VC Dimension.",
        "The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover.",
        "The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element.",
        "The information capacity captures the functions modelable by the network given any data as input.",
        "The second notion, is the VC dimension.",
        "VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances.",
        "This is, given input data in a specific form.",
        "As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a perceptron.",
        "The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.",
        "Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model.",
        "Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum.",
        "Thirdly, for sufficiently large data or parameters, some methods become impractical.",
        "Another issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction.",
        "The convergence behavior of certain types of ANN architectures are more understood than others.",
        "When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models.",
        "Another example is when parameters are small, it is observed that ANNs often fit target functions from low to high frequencies.",
        "This behavior is referred to as the spectral bias, or frequency principle, of neural networks.",
        "This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method.",
        "Deeper neural networks have been observed to be more biased towards low frequency functions.",
        "Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training.",
        "This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters.",
        "Two approaches address over-training.",
        "The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.",
        "The second is to use some form of regularization.",
        "This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.",
        "Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model.",
        "The MSE on a validation set can be used as an estimate for variance.",
        "This value can then be used to calculate the confidence interval of network output, assuming a normal distribution.",
        "A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.",
        "By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities.",
        "This is useful in classification as it gives a certainty measure on classifications.",
        "The softmax activation function is: y i = e x i ∑ j = 1 c e x j {\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} A common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.",
        "Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases.",
        "Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.",
        "Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.",
        "), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns-it should not learn to always turn right).",
        "A central claim of ANNs is that they embody new and powerful general principles for processing information.",
        "These principles are ill-defined.",
        "It is often claimed that they are emergent from the network itself.",
        "This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition.",
        "In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are.",
        "No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything.",
        "One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.",
        "Technology writer Roger Bridgman commented: Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?)",
        "but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".",
        "In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers.",
        "An unreadable table that a useful machine could read would still be well worth having.",
        "Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network.",
        "Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks.",
        "Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful.",
        "For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.",
        "Biological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance.",
        "Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.",
        "Large and effective neural networks require considerable computing resources.",
        "While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage.",
        "Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons - which require enormous CPU power and time.",
        "Some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before.",
        "The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.",
        "Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry.",
        "Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.",
        "Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network.",
        "Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful.",
        "For example, local vs. non-local learning and shallow vs. deep architecture.",
        "Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.",
        "Neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.",
        "These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute.",
        "This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement.",
        "For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field.",
        "The program would penalize any resume with the word \"woman\" or the name of any women's college.",
        "However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.",
        "Artificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications.",
        "Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.",
        "In the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation.",
        "For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance.",
        "This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.",
        "By modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion.",
        "Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques.",
        "These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.",
        "In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation.",
        "They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content.",
        "This has implications for automated customer service, content moderation, and language understanding technologies.",
        "In the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization.",
        "For instance, deep feedforward neural networks are important in system identification and control applications.",
        "ANNs are used for stock market prediction and credit scoring: In investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions.",
        "In credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process.",
        "ANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation.",
        "Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.",
        "ANNs are able to process and analyze vast medical datasets.",
        "They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning.",
        "In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs.",
        "Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management.",
        "Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.",
        "ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries.",
        "This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions.",
        "For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user.",
        "In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck.",
        "In the marketing industry, generative models are used to create personalized advertisements for consumers.",
        "Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020.",
        "Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game."
      ],
      "metadata": {
        "title": "Neural network (machine learning)",
        "url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
        "word_count": 7880,
        "char_count": 52748,
        "sentence_count": 382,
        "scraped_at": "2025-08-09T14:47:09.045880",
        "language": "en",
        "processing_time": 0.019947052001953125,
        "source_hash": "8a0c1003edb6daf2062575dfb1014bd9"
      }
    },
    {
      "title": "Convolutional neural network",
      "url": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
      "raw_text": "A convolutional neural network (CNN) is a type of feedforward neural network that learns features via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced—in some cases—by newer deep learning architectures such as the transformer.\nVanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections. For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, only 25 weights for each convolutional layer are required to process 5x5-sized tiles. Higher-layer features are extracted from wider context windows, compared to lower-layer features.\nSome applications of CNNs include: \n\nimage and video recognition,\nrecommender systems,\nimage classification,\nimage segmentation,\nmedical image analysis,\nnatural language processing,\nbrain–computer interfaces, and\nfinancial time series.\nCNNs are also known as shift invariant or space invariant artificial neural networks, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.\nFeedforward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.\nConvolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This simplifies and automates the process, enhancing efficiency and scalability overcoming human-intervention bottlenecks.\n\n\n== Architecture ==\n\nA convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.\nHere it should be noted how close a convolutional neural network is to a matched filter.\n\n\n=== Convolutional layers ===\nIn a CNN, the input is a tensor with shape:\n(number of inputs) × (input height) × (input width) × (input channels)\nAfter passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape:\n(number of inputs) × (feature map height) × (feature map width) × (feature map channels).\nConvolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. \n\nAlthough fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 × 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper. For example, using a 5 × 5 tiling region, each with the same shared weights, requires only 25 neurons. Using shared weights means there are many fewer parameters, which helps avoid the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.\nTo speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers, which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of \n  \n    \n      \n        1\n        ×\n        1\n      \n    \n    {\\displaystyle 1\\times 1}\n  \n kernels.\n\n\n=== Pooling layers ===\nConvolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map. There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map, while average pooling takes the average value.\n\n\n=== Fully connected layers ===\nFully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\n\n\n=== Receptive field ===\nIn neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.\nTo manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios, thus having a variable receptive field size.\n\n\n=== Weights ===\nEach neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights.\nThe vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.\n\n\n=== Deconvolutional ===\n\nA deconvolutional neural network is essentially the reverse of a CNN. It consists of deconvolutional layers and unpooling layers. \nA deconvolutional layer is the transpose of a convolutional layer. Specifically, a convolutional layer can be written as a multiplication with a matrix, and a deconvolutional layer is multiplication with the transpose of that matrix.\nAn unpooling layer expands the layer. The max-unpooling layer is the simplest, as it simply copies each entry multiple times. For example, a 2-by-2 max-unpooling layer is \n  \n    \n      \n        [\n        x\n        ]\n        ↦\n        \n          \n            [\n            \n              \n                \n                  x\n                \n                \n                  x\n                \n              \n              \n                \n                  x\n                \n                \n                  x\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle [x]\\mapsto {\\begin{bmatrix}x&x\\\\x&x\\end{bmatrix}}}\n  \n.\nDeconvolution layers are used in image generators. By default, it creates periodic checkerboard artifact, which can be fixed by upscale-then-convolve.\n\n\n== History ==\nCNN are often compared to the way the brain achieves vision processing in living organisms.\n\n\n=== Receptive fields in the visual cortex ===\n\nWork by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.\nTheir 1968 paper identified two basic visual cell types in the brain:\n\nsimple cells, whose output is maximized by straight edges having particular orientations within their receptive field\ncomplex cells, which have larger receptive fields, whose output is insensitive to the exact position of the edges in the field.\nHubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\n\n\n=== Fukushima's analog threshold elements in a vision model ===\nIn 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\"  This is the essential core of a convolutional network, but the weights were not trained.  In the same paper, Fukushima also introduced the ReLU (rectified linear unit) activation function. \n\n\n=== Neocognitron, origin of the trainable CNN architecture ===\nThe \"neocognitron\" was introduced by Fukushima in 1980.  The neocognitron introduced the two basic types of layers:\n\n\"S-layer\": a shared-weights receptive-field layer, later known as a convolutional layer, which contains units whose receptive fields cover a patch of the previous layer. A shared-weights receptive-field group (a \"plane\" in neocognitron terminology) is often called a filter, and a layer typically has several such filters.\n\"C-layer\": a downsampling layer that contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes a weighted average of the activations of the units in its patch, and applies inhibition (divisive normalization) pooled from a somewhat larger patch and across different filters in a layer, and applies a saturating activation function. The patch weights are nonnegative and are not trainable in the original neocognitron. The downsampling and competitive inhibition help to classify features and objects in visual scenes even when the objects are shifted.\nSeveral supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today, however, the CNN architecture is usually trained through backpropagation.\nFukushima's ReLU activation function was not used in his neocognitron since all the weights were nonnegative; lateral inhibition was used instead. The rectifier has become a very popular activation function for CNNs and deep neural networks in general.\n\n\n=== Convolution in time ===\nThe term \"convolution\" first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987. Their paper replaced multiplication with convolution in time, inherently providing shift invariance, motivated by and connecting more directly to the signal-processing concept of a filter, and demonstrated it on a speech recognition task. They also pointed out that as a data-trainable system, convolution is essentially equivalent to correlation since reversal of the weights does not affect the final learned function (\"For convenience, we denote * as correlation instead of convolution. Note that convolving a(t) with b(t) is equivalent to correlating a(-t) with b(t).\"). Modern CNN implementations typically do correlation and call it convolution, for convenience, as they did here.\n\n\n=== Time delay neural networks ===\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was an early convolutional network exhibiting shift-invariance. A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.\nTDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution. Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts, as with images processed by a neocognitron.\nTDNNs improved the performance of far-distance speech recognition.\n\n\n=== Image recognition with CNNs trained by gradient descent ===\nDenker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers. However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.\nFollowing the advances in the training of 1-D CNNs by Waibel et al. (1987), Yann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. \nWei Zhang et al. (1988) used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called shift-invariant pattern recognition neural network before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991) and breast cancer detection in mammograms (1994).\nThis approach became a foundation of modern computer vision.\n\n\n==== Max pooling ====\nIn 1990 Yamaguchi et al. introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system. In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.\nIn a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging with inhibition and saturation, J. Weng et al. in 1993 used max pooling, where a downsampling unit computes the maximum of the activations of the units in its patch, introducing this method into the vision field. \nMax pooling is often used in modern CNNs.\n\n\n==== LeNet-5 ====\n\nLeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1995, classifies hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.\nIt was superior than other commercial courtesy amount reading systems (as of 1995). The system was integrated in NCR's check reading systems, and fielded in several American banks since June 1996, reading millions of checks per day.\n\n\n=== Shift-invariant neural network ===\nA shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988. It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer. The model was trained with back-propagation. The training algorithm was further improved in 1991 to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991) and automatic detection of breast cancer in mammograms (1994).\nA different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.\n\n\n=== GPU implementations ===\nAlthough CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).\nIn 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU. In 2005, another paper also emphasised the value of GPGPU for machine learning.\nThe first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU. In the same period, GPUs were also used for unsupervised training of deep belief networks.\nIn 2010, Dan Ciresan et al. at IDSIA trained deep feedforward networks on GPUs. In 2011, they extended this to CNNs, accelerating by 60 compared to training CPU. In 2011, the network won an image recognition contest where they achieved superhuman performance for the first time. Then they won more competitions and achieved state of the art on several benchmarks.\nSubsequently, AlexNet, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012. It was an early catalytic event for the AI boom.\nCompared to the training of CNNs using GPUs, not much attention was given to CPU. (Viebke et al 2019) parallelizes CNN by thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.\n\n\n== Distinguishing features ==\nIn the past, traditional multilayer perceptron (MLP) models were used for image recognition. However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher-resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale.\n\nFor example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\nAlso, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\nConvolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\n\n3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth. Where each neuron inside a convolutional layer is connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.\nLocal connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned \"filters\" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to nonlinear filters that become increasingly global (i.e. responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas.\nShared weights: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for the resulting activation map to be equivariant under shifts of the locations of input features in the visual field, i.e. they grant translational equivariance—given that the layer has a stride of one.\nPooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. In addition to reducing the sizes of feature maps, the pooling operation grants a degree of local translational invariance to the features contained therein, allowing the CNN to be more robust to variations in their positions.\nTogether, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\n\n\n== Building blocks ==\nA CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.\n\n\n=== Convolutional layer ===\n\nThe convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.\nStacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter.\nSelf-supervised learning has been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer.\n\n\n==== Local connectivity ====\n\nWhen dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.\nThe extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learned filters produce the strongest response to a spatially local input pattern.\n\n\n==== Spatial arrangement ====\nThree hyperparameters control the size of the output volume of the convolutional layer: the depth, stride, and padding size:\n\nThe depth of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color.\nStride controls how depth columns around the width and height are allocated. If the stride is 1, then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns, and to large output volumes. For any integer \n  \n    \n      \n        S\n        >\n        0\n        ,\n      \n    \n    {\\textstyle S>0,}\n  \n a stride S means that the filter is translated S units at a time per output. In practice, \n  \n    \n      \n        S\n        ≥\n        3\n      \n    \n    {\\textstyle S\\geq 3}\n  \n is rare. A greater stride means smaller overlap of receptive fields and smaller spatial dimensions of the output volume.\nSometimes, it is convenient to pad the input with zeros (or other values, such as the average of the region) on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume's spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume, this is commonly referred to as \"same\" padding.\n\nThe spatial size of the output volume is a function of the input volume size \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n, the kernel field size \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n of the convolutional layer neurons, the stride \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n, and the amount of zero padding \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n on the border. The number of neurons that \"fit\" in a given volume is then:\n\n  \n    \n      \n        \n          \n            \n              W\n              −\n              K\n              +\n              2\n              P\n            \n            S\n          \n        \n        +\n        1.\n      \n    \n    {\\displaystyle {\\frac {W-K+2P}{S}}+1.}\n  \n\nIf this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be \n  \n    \n      \n        P\n        =\n        (\n        K\n        −\n        1\n        )\n        \n          /\n        \n        2\n      \n    \n    {\\textstyle P=(K-1)/2}\n  \n when the stride is \n  \n    \n      \n        S\n        =\n        1\n      \n    \n    {\\displaystyle S=1}\n  \n ensures that the input volume and output volume will have the same size spatially. However, it is not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.\n\n\n==== Parameter sharing ====\nA parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias.\nSince all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume. Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.\nSometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\".\n\n\n=== Pooling layer ===\n\nAnother important concept of CNNs is pooling, which is used as a form of non-linear down-sampling. Pooling provides downsampling because it reduces the spatial dimensions (height and width) of the input feature maps while retaining the most important information. There are several non-linear functions to implement pooling, where max pooling and average pooling are the most common. Pooling aggregates information from small regions of the input creating partitions of the input feature map, typically using a fixed-size window (like 2x2) and applying a stride (often 2) to move the window across the input. Note that without using a stride greater than 1, pooling would not perform downsampling, as it would simply move the pooling window across the input one step at a time, without reducing the size of the feature map. In other words, the stride is what actually causes the downsampling by determining how much the pooling window moves over the input.\nIntuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture. While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used. The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2×2, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations:\n  \n    \n      \n        \n          f\n          \n            X\n            ,\n            Y\n          \n        \n        (\n        S\n        )\n        =\n        \n          max\n          \n            a\n            ,\n            b\n            =\n            0\n          \n          \n            1\n          \n        \n        \n          S\n          \n            2\n            X\n            +\n            a\n            ,\n            2\n            Y\n            +\n            b\n          \n        \n        .\n      \n    \n    {\\displaystyle f_{X,Y}(S)=\\max _{a,b=0}^{1}S_{2X+a,2Y+b}.}\n  \n\nIn this case, every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well).\nIn addition to max pooling, pooling units can use other functions, such as average pooling or ℓ2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice.\nDue to the effects of fast spatial reduction of the size of the representation, there is a recent trend towards using smaller filters or discarding pooling layers altogether.\n\n\n==== Channel max pooling ====\nA channel max pooling (CMP) operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features. Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F ∈ R(C×M×N) and C ∈ R(c×M×N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed, which is different from the MP operation.\nSee  for reviews for pooling methods.\n\n\n=== ReLU layer ===\nReLU is the abbreviation of rectified linear unit. It was proposed by Alston Householder in 1941, and used in CNN by Kunihiko Fukushima in 1969. ReLU applies the non-saturating activation function \n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        max\n        (\n        0\n        ,\n        x\n        )\n      \n    \n    {\\textstyle f(x)=\\max(0,x)}\n  \n. It effectively removes negative values from an activation map by setting them to zero. It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers.\nIn 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks, compared to widely used activation functions prior to 2011.\nOther functions can also be used to increase nonlinearity, for example the saturating hyperbolic tangent \n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        tanh\n        ⁡\n        (\n        x\n        )\n      \n    \n    {\\displaystyle f(x)=\\tanh(x)}\n  \n, \n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          |\n        \n        tanh\n        ⁡\n        (\n        x\n        )\n        \n          |\n        \n      \n    \n    {\\displaystyle f(x)=|\\tanh(x)|}\n  \n, and the sigmoid function \n  \n    \n      \n        σ\n        (\n        x\n        )\n        =\n        (\n        1\n        +\n        \n          e\n          \n            −\n            x\n          \n        \n        \n          )\n          \n            −\n            1\n          \n        \n      \n    \n    {\\textstyle \\sigma (x)=(1+e^{-x})^{-1}}\n  \n. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.\n\n\n=== Fully connected layer ===\nAfter several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).\n\n\n=== Loss layer ===\n\nThe \"loss layer\", or \"loss function\", exemplifies how training penalizes the deviation between the predicted output of the network, and the true data labels (during supervised learning). Various loss functions can be used, depending on the specific task.\nThe Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n  \n. Euclidean loss is used for regressing to real-valued labels \n  \n    \n      \n        (\n        −\n        ∞\n        ,\n        ∞\n        )\n      \n    \n    {\\displaystyle (-\\infty ,\\infty )}\n  \n.\n\n\n== Hyperparameters ==\n\nHyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).\n\n\n=== Padding ===\nPadding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image.\n\n\n=== Stride ===\nThe stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor.\n\n\n=== Number of filters ===\nSince feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\nThe number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.\n\n\n=== Filter (or kernel) size ===\nCommon filter sizes found in the literature vary greatly, and are usually chosen based on the data set. Typical filter sizes range from 1x1 to 7x7. As two famous examples, AlexNet used 3x3, 5x5, and 11x11. Inceptionv3 used 1x1, 3x3, and 5x5.\nThe challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and without overfitting.\n\n\n=== Pooling type and size ===\nMax pooling is typically used, often with a 2x2 dimension. This implies that the input is drastically downsampled, reducing processing cost.\nGreater pooling reduces the dimension of the signal, and may result in unacceptable information loss. Often, non-overlapping pooling windows perform best.\n\n\n=== Dilation ===\nDilation involves ignoring pixels within a kernel. This reduces processing memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5, while still processing 9 (evenly spaced) pixels. Specifically, the processed pixels after the dilation are the cells (1,1), (1,3), (1,5), (3,1), (3,3), (3,5), (5,1), (5,3), (5,5), where (i,j) denotes the cell of the i-th row and j-th column in the expanded 5x5 kernel. Accordingly, dilation of 4 expands the kernel to 7x7.\n\n\n== Translation equivariance and aliasing ==\nIt is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input. However, layers with a stride greater than one ignore the Nyquist–Shannon sampling theorem and might lead to aliasing of the input signal While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happen in practice, and therefore yield models that are not equivariant to translations.\nFurthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input. One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer. Additionally, several other partial solutions have been proposed, such as anti-aliasing before downsampling operations, spatial transformer networks, data augmentation, subsampling combined with pooling, and capsule neural networks.\n\n\n== Evaluation ==\nThe accuracy of the final model is typically estimated on a sub-part of the dataset set apart at the start, often called a test set. Alternatively, methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction.\n\n\n== Regularization methods ==\n\nRegularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.\n\n\n=== Empirical ===\n\n\n==== Dropout ====\nBecause networks have so many parameters, they are prone to overfitting. One method to reduce overfitting is dropout, introduced in 2014. At each training stage, individual nodes are either \"dropped out\" of the net (ignored) with probability \n  \n    \n      \n        1\n        −\n        p\n      \n    \n    {\\displaystyle 1-p}\n  \n or kept with probability \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\nIn the training stages, \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored.\nAt testing time after training has finished, we would ideally like to find a sample average of all possible \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n dropped-out networks; unfortunately this is unfeasible for large values of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n. However, we can find an approximation by using the full network with each node's output weighted by a factor of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\nBy avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.\n\n\n==== DropConnect ====\nDropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability \n  \n    \n      \n        1\n        −\n        p\n      \n    \n    {\\displaystyle 1-p}\n  \n. Each unit thus receives input from a random subset of units in the previous layer.\nDropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.\n\n\n==== Stochastic pooling ====\nA major drawback to dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\nEven before dropout, in 2013 a technique called stochastic pooling, the conventional deterministic pooling operations were replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.\nAn alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\n\n\n==== Artificial data ====\n\nBecause the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.\n\n\n=== Explicit ===\n\n\n==== Early stopping ====\n\nOne of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\n\n\n==== Number of parameters ====\nAnother simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\".\n\n\n==== Weight decay ====\nA simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter), thus increasing the penalty for large weight vectors.\nL2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.\nL1 regularization is also common. It makes the weight vectors sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization.\n\n\n==== Max norm constraints ====\nAnother form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector \n  \n    \n      \n        \n          \n            \n              w\n              →\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {w}}}\n  \n of every neuron to satisfy \n  \n    \n      \n        ‖\n        \n          \n            \n              w\n              →\n            \n          \n        \n        \n          ‖\n          \n            2\n          \n        \n        <\n        c\n      \n    \n    {\\displaystyle \\|{\\vec {w}}\\|_{2}<c}\n  \n. Typical values of \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n are order of 3–4. Some papers report improvements when using this form of regularization.\n\n\n== Hierarchical coordinate frames ==\nPooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.\nAn earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.\nThus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.\n\n\n== Applications ==\n\n\n=== Image recognition ===\nCNNs are often used in image recognition systems. In 2012, an error rate of 0.23% on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database. Subsequently, a similar CNN called AlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012.\nWhen applied to facial recognition, CNNs achieved a large decrease in error rate. Another paper reported a 97.6% recognition rate on \"5,600 still images of more than 10 subjects\". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.\nThe ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.\nIn 2015, a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.\n\n\n=== Video analysis ===\nCompared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream. Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis. Its application can be seen in text-to-video model.\n\n\n=== Natural language processing ===\nCNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks.\nCompared to traditional language processing methods such as recurrent neural networks, CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption, while RNNs are better suitable when classical time series modeling is required.\n\n\n=== Anomaly detection ===\nA CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.\n\n\n=== Drug discovery ===\nCNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp3 carbons, and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis.\n\n\n=== Checkers game ===\nCNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checkers using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its \"expert\" level of play.\n\n\n=== Go ===\nCNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.\nA couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.\n\n\n=== Time series forecasting ===\nRecurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better. Dilated convolutions might enable one-dimensional convolutional neural networks to effectively learn time series dependences. Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients. Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from. CNNs can also be applied to further tasks in time series analysis (e.g., time series classification or quantile forecasting).\n\n\n=== Cultural heritage and 3D-datasets ===\nAs archaeological findings such as clay tablets with cuneiform writing are increasingly acquired using 3D scanners, benchmark datasets are becoming available, including HeiCuBeDa providing almost 2000 normalized 2-D and 3-D datasets prepared with the GigaMesh Software Framework. So curvature-based measures are used in conjunction with geometric neural networks (GNNs), e.g. for period classification of those clay tablets being among the oldest documents of human history.\n\n\n== Fine-tuning ==\nFor many applications, training data is not very available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.\n\n\n== Human interpretable explanations ==\nEnd-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience, spatial attention, and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.\n\n\n== Related architectures ==\n\n\n=== Deep Q-networks ===\nA deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.\nPreliminary results were presented in 2014, with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.\n\n\n=== Deep belief networks ===\n\nConvolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs.\n\n\n=== Neural abstraction pyramid ===\nThe feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.\n\n\n== Notable libraries ==\nCaffe: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.\nDeeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka.\nDlib: A toolkit for making real world machine learning and data analysis applications in C++.\nMicrosoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java.\nTensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU, Google's proprietary tensor processing unit (TPU), and mobile devices.\nTheano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.\nTorch: A scientific computing framework with wide support for machine learning algorithms, written in C and Lua.\n\n\n== See also ==\nAttention (machine learning)\nConvolution\nDeep learning\nNatural-language processing\nNeocognitron\nScale-invariant feature transform\nTime delay neural network\nVision processing unit\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nCS231n: Convolutional Neural Networks for Visual Recognition — Andrej Karpathy's Stanford computer science course on CNNs in computer vision\nvdumoulin/conv_arithmetic: A technical report on convolution arithmetic in the context of deep learning. Animations of convolutions.",
      "cleaned_text": "A convolutional neural network (CNN) is a type of feedforward neural network that learns features via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced-in some cases-by newer deep learning architectures such as the transformer. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections. For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, only 25 weights for each convolutional layer are required to process 5x5-sized tiles. Higher-layer features are extracted from wider context windows, compared to lower-layer features. Some applications of CNNs include: image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain-computer interfaces, and financial time series. CNNs are also known as shift invariant or space invariant artificial neural networks, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input. Feedforward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set. Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This simplifies and automates the process, enhancing efficiency and scalability overcoming human-intervention bottlenecks. A convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers. Here it should be noted how close a convolutional neural network is to a matched filter. In a CNN, the input is a tensor with shape: (number of inputs) × (input height) × (input width) × (input channels) After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: (number of inputs) × (feature map height) × (feature map width) × (feature map channels). Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 × 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper. For example, using a 5 × 5 tiling region, each with the same shared weights, requires only 25 neurons. Using shared weights means there are many fewer parameters, which helps avoid the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks. To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers, which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of 1 × 1 {\\displaystyle 1 imes 1} kernels. Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map. There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map, while average pooling takes the average value. Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images. In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers. To manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios, thus having a variable receptive field size. Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights. The vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting. A deconvolutional neural network is essentially the reverse of a CNN. It consists of deconvolutional layers and unpooling layers. A deconvolutional layer is the transpose of a convolutional layer. Specifically, a convolutional layer can be written as a multiplication with a matrix, and a deconvolutional layer is multiplication with the transpose of that matrix. An unpooling layer expands the layer. The max-unpooling layer is the simplest, as it simply copies each entry multiple times. For example, a 2-by-2 max-unpooling layer is [ x ] ↦ [ x x x x ] {\\displaystyle [x]\\mapsto {\\begin{bmatrix}x&x\\\\x&x\\end{bmatrix}}} . Deconvolution layers are used in image generators. By default, it creates periodic checkerboard artifact, which can be fixed by upscale-then-convolve. CNN are often compared to the way the brain achieves vision processing in living organisms. Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field. Their 1968 paper identified two basic visual cell types in the brain: simple cells, whose output is maximized by straight edges having particular orientations within their receptive field complex cells, which have larger receptive fields, whose output is insensitive to the exact position of the edges in the field. Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks. In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\" This is the essential core of a convolutional network, but the weights were not trained. In the same paper, Fukushima also introduced the ReLU (rectified linear unit) activation function. The \"neocognitron\" was introduced by Fukushima in 1980. The neocognitron introduced the two basic types of layers: \"S-layer\": a shared-weights receptive-field layer, later known as a convolutional layer, which contains units whose receptive fields cover a patch of the previous layer. A shared-weights receptive-field group (a \"plane\" in neocognitron terminology) is often called a filter, and a layer typically has several such filters. \"C-layer\": a downsampling layer that contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes a weighted average of the activations of the units in its patch, and applies inhibition (divisive normalization) pooled from a somewhat larger patch and across different filters in a layer, and applies a saturating activation function. The patch weights are nonnegative and are not trainable in the original neocognitron. The downsampling and competitive inhibition help to classify features and objects in visual scenes even when the objects are shifted. Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today, however, the CNN architecture is usually trained through backpropagation. Fukushima's ReLU activation function was not used in his neocognitron since all the weights were nonnegative; lateral inhibition was used instead. The rectifier has become a very popular activation function for CNNs and deep neural networks in general. The term \"convolution\" first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987. Their paper replaced multiplication with convolution in time, inherently providing shift invariance, motivated by and connecting more directly to the signal-processing concept of a filter, and demonstrated it on a speech recognition task. They also pointed out that as a data-trainable system, convolution is essentially equivalent to correlation since reversal of the weights does not affect the final learned function (\"For convenience, we denote * as correlation instead of convolution. Note that convolving a(t) with b(t) is equivalent to correlating a(-t) with b(t).\"). Modern CNN implementations typically do correlation and call it convolution, for convenience, as they did here. The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was an early convolutional network exhibiting shift-invariance. A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one. TDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution. Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts, as with images processed by a neocognitron. TDNNs improved the performance of far-distance speech recognition. Denker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers. However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed. Following the advances in the training of 1-D CNNs by Waibel et al. (1987), Yann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. Wei Zhang et al. (1988) used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called shift-invariant pattern recognition neural network before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991) and breast cancer detection in mammograms (1994). This approach became a foundation of modern computer vision. In 1990 Yamaguchi et al. introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system. In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging with inhibition and saturation, J. Weng et al. in 1993 used max pooling, where a downsampling unit computes the maximum of the activations of the units in its patch, introducing this method into the vision field. Max pooling is often used in modern CNNs. LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1995, classifies hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources. It was superior than other commercial courtesy amount reading systems (as of 1995). The system was integrated in NCR's check reading systems, and fielded in several American banks since June 1996, reading millions of checks per day. A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988. It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer. The model was trained with back-propagation. The training algorithm was further improved in 1991 to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991) and automatic detection of breast cancer in mammograms (1994). A different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs. Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs). In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU. In 2005, another paper also emphasised the value of GPGPU for machine learning. The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU. In the same period, GPUs were also used for unsupervised training of deep belief networks. In 2010, Dan Ciresan et al. at IDSIA trained deep feedforward networks on GPUs. In 2011, they extended this to CNNs, accelerating by 60 compared to training CPU. In 2011, the network won an image recognition contest where they achieved superhuman performance for the first time. Then they won more competitions and achieved state of the art on several benchmarks. Subsequently, AlexNet, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012. It was an early catalytic event for the AI boom. Compared to the training of CNNs using GPUs, not much attention was given to CPU. (Viebke et al 2019) parallelizes CNN by thread- and SIMD-level parallelism that is available on the Intel Xeon Phi. In the past, traditional multilayer perceptron (MLP) models were used for image recognition. However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher-resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale. For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights. Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns. Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features: 3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth. Where each neuron inside a convolutional layer is connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture. Local connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned \"filters\" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to nonlinear filters that become increasingly global (i.e. responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas. Shared weights: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for the resulting activation map to be equivariant under shifts of the locations of input features in the visual field, i.e. they grant translational equivariance-given that the layer has a stride of one. Pooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. In addition to reducing the sizes of feature maps, the pooling operation grants a degree of local translational invariance to the features contained therein, allowing the CNN to be more robust to variations in their positions. Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks. A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below. The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input. Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter. Self-supervised learning has been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer. When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume. The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learned filters produce the strongest response to a spatially local input pattern. Three hyperparameters control the size of the output volume of the convolutional layer: the depth, stride, and padding size: The depth of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color. Stride controls how depth columns around the width and height are allocated. If the stride is 1, then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns, and to large output volumes. For any integer S > 0 , { extstyle S>0,} a stride S means that the filter is translated S units at a time per output. In practice, S ≥ 3 { extstyle S\\geq 3} is rare. A greater stride means smaller overlap of receptive fields and smaller spatial dimensions of the output volume. Sometimes, it is convenient to pad the input with zeros (or other values, such as the average of the region) on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume's spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume, this is commonly referred to as \"same\" padding. The spatial size of the output volume is a function of the input volume size W {\\displaystyle W} , the kernel field size K {\\displaystyle K} of the convolutional layer neurons, the stride S {\\displaystyle S} , and the amount of zero padding P {\\displaystyle P} on the border. The number of neurons that \"fit\" in a given volume is then: W − K + 2 P S + 1. {\\displaystyle {\\frac {W-K+2P}{S}}+1.} If this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be P = ( K − 1 ) / 2 { extstyle P=(K-1)/2} when the stride is S = 1 {\\displaystyle S=1} ensures that the input volume and output volume will have the same size spatially. However, it is not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding. A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias. Since all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume. Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture. Sometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\". Another important concept of CNNs is pooling, which is used as a form of non-linear down-sampling. Pooling provides downsampling because it reduces the spatial dimensions (height and width) of the input feature maps while retaining the most important information. There are several non-linear functions to implement pooling, where max pooling and average pooling are the most common. Pooling aggregates information from small regions of the input creating partitions of the input feature map, typically using a fixed-size window (like 2x2) and applying a stride (often 2) to move the window across the input. Note that without using a stride greater than 1, pooling would not perform downsampling, as it would simply move the pooling window across the input one step at a time, without reducing the size of the feature map. In other words, the stride is what actually causes the downsampling by determining how much the pooling window moves over the input. Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture. While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used. The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2×2, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations: f X , Y ( S ) = max a , b = 0 1 S 2 X + a , 2 Y + b . {\\displaystyle f_{X,Y}(S)=\\max _{a,b=0}^{1}S_{2X+a,2Y+b}.} In this case, every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well). In addition to max pooling, pooling units can use other functions, such as average pooling or ℓ2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice. Due to the effects of fast spatial reduction of the size of the representation, there is a recent trend towards using smaller filters or discarding pooling layers altogether. A channel max pooling (CMP) operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features. Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F ∈ R(C×M×N) and C ∈ R(c×M×N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed, which is different from the MP operation. See for reviews for pooling methods. ReLU is the abbreviation of rectified linear unit. It was proposed by Alston Householder in 1941, and used in CNN by Kunihiko Fukushima in 1969. ReLU applies the non-saturating activation function f ( x ) = max ( 0 , x ) { extstyle f(x)=\\max(0,x)} . It effectively removes negative values from an activation map by setting them to zero. It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers. In 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks, compared to widely used activation functions prior to 2011. Other functions can also be used to increase nonlinearity, for example the saturating hyperbolic tangent f ( x ) = tanh ⁡ ( x ) {\\displaystyle f(x)= anh(x)} , f ( x ) = | tanh ⁡ ( x ) | {\\displaystyle f(x)=| anh(x)|} , and the sigmoid function σ ( x ) = ( 1 + e − x ) − 1 { extstyle \\sigma (x)=(1+e^{-x})^{-1}} . ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy. After several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term). The \"loss layer\", or \"loss function\", exemplifies how training penalizes the deviation between the predicted output of the network, and the true data labels (during supervised learning). Various loss functions can be used, depending on the specific task. The Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in {\\displaystyle } . Euclidean loss is used for regressing to real-valued labels ( − ∞ , ∞ ) {\\displaystyle (-\\infty ,\\infty )} . Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP). Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image. The stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor. Since feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next. The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity. Common filter sizes found in the literature vary greatly, and are usually chosen based on the data set. Typical filter sizes range from 1x1 to 7x7. As two famous examples, AlexNet used 3x3, 5x5, and 11x11. Inceptionv3 used 1x1, 3x3, and 5x5. The challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and without overfitting. Max pooling is typically used, often with a 2x2 dimension. This implies that the input is drastically downsampled, reducing processing cost. Greater pooling reduces the dimension of the signal, and may result in unacceptable information loss. Often, non-overlapping pooling windows perform best. Dilation involves ignoring pixels within a kernel. This reduces processing memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5, while still processing 9 (evenly spaced) pixels. Specifically, the processed pixels after the dilation are the cells (1,1), (1,3), (1,5), (3,1), (3,3), (3,5), (5,1), (5,3), (5,5), where (i,j) denotes the cell of the i-th row and j-th column in the expanded 5x5 kernel. Accordingly, dilation of 4 expands the kernel to 7x7. It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input. However, layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happen in practice, and therefore yield models that are not equivariant to translations. Furthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input. One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer. Additionally, several other partial solutions have been proposed, such as anti-aliasing before downsampling operations, spatial transformer networks, data augmentation, subsampling combined with pooling, and capsule neural networks. The accuracy of the final model is typically estimated on a sub-part of the dataset set apart at the start, often called a test set. Alternatively, methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction. Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization. Because networks have so many parameters, they are prone to overfitting. One method to reduce overfitting is dropout, introduced in 2014. At each training stage, individual nodes are either \"dropped out\" of the net (ignored) with probability 1 − p {\\displaystyle 1-p} or kept with probability p {\\displaystyle p} , so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights. In the training stages, p {\\displaystyle p} is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored. At testing time after training has finished, we would ideally like to find a sample average of all possible 2 n {\\displaystyle 2^{n}} dropped-out networks; unfortunately this is unfeasible for large values of n {\\displaystyle n} . However, we can find an approximation by using the full network with each node's output weighted by a factor of p {\\displaystyle p} , so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates 2 n {\\displaystyle 2^{n}} neural nets, and as such allows for model combination, at test time only a single network needs to be tested. By avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data. DropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability 1 − p {\\displaystyle 1-p} . Each unit thus receives input from a random subset of units in the previous layer. DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage. A major drawback to dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected. Even before dropout, in 2013 a technique called stochastic pooling, the conventional deterministic pooling operations were replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation. An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below. Because the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set. One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted. Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\". A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter), thus increasing the penalty for large weight vectors. L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization. Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector w → {\\displaystyle {\\vec {w}}} of every neuron to satisfy ‖ w → ‖ 2 < c {\\displaystyle \\|{\\vec {w}}\\|_{2}<c} . Typical values of c {\\displaystyle c} are order of 3-4. Some papers report improvements when using this form of regularization. Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint. An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame. Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes. CNNs are often used in image recognition systems. In 2012, an error rate of 0.23% on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database. Subsequently, a similar CNN called AlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012. When applied to facial recognition, CNNs achieved a large decrease in error rate. Another paper reported a 97.6% recognition rate on \"5,600 still images of more than 10 subjects\". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error. The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this. In 2015, a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations. Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream. Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis. Its application can be seen in text-to-video model. CNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks. Compared to traditional language processing methods such as recurrent neural networks, CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption, while RNNs are better suitable when classical time series modeling is required. A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain. CNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp3 carbons, and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis. CNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checkers using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its \"expert\" level of play. CNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move. A couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time. Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better. Dilated convolutions might enable one-dimensional convolutional neural networks to effectively learn time series dependences. Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients. Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from. CNNs can also be applied to further tasks in time series analysis (e.g., time series classification or quantile forecasting). As archaeological findings such as clay tablets with cuneiform writing are increasingly acquired using 3D scanners, benchmark datasets are becoming available, including HeiCuBeDa providing almost 2000 normalized 2-D and 3-D datasets prepared with the GigaMesh Software Framework. So curvature-based measures are used in conjunction with geometric neural networks (GNNs), e.g. for period classification of those clay tablets being among the oldest documents of human history. For many applications, training data is not very available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets. End-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience, spatial attention, and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions. A deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning. Preliminary results were presented in 2014, with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it. Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs. The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks. Caffe: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers. Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka. Dlib: A toolkit for making real world machine learning and data analysis applications in C++. Microsoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java. TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU, Google's proprietary tensor processing unit (TPU), and mobile devices. Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation. Torch: A scientific computing framework with wide support for machine learning algorithms, written in C and Lua.",
      "sentences": [
        "A convolutional neural network (CNN) is a type of feedforward neural network that learns features via filter (or kernel) optimization.",
        "This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio.",
        "Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced-in some cases-by newer deep learning architectures such as the transformer.",
        "Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.",
        "For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels.",
        "However, applying cascaded convolution (or cross-correlation) kernels, only 25 weights for each convolutional layer are required to process 5x5-sized tiles.",
        "Higher-layer features are extracted from wider context windows, compared to lower-layer features.",
        "Some applications of CNNs include: image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain-computer interfaces, and financial time series.",
        "CNNs are also known as shift invariant or space invariant artificial neural networks, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps.",
        "Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.",
        "Feedforward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer.",
        "The \"full connectivity\" of these networks makes them prone to overfitting data.",
        "Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.)",
        "Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.",
        "Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex.",
        "Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field.",
        "The receptive fields of different neurons partially overlap such that they cover the entire visual field.",
        "CNNs use relatively little pre-processing compared to other image classification algorithms.",
        "This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered.",
        "This simplifies and automates the process, enhancing efficiency and scalability overcoming human-intervention bottlenecks.",
        "A convolutional neural network consists of an input layer, hidden layers and an output layer.",
        "In a convolutional neural network, the hidden layers include one or more layers that perform convolutions.",
        "Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix.",
        "This product is usually the Frobenius inner product, and its activation function is commonly ReLU.",
        "As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer.",
        "This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.",
        "Here it should be noted how close a convolutional neural network is to a matched filter.",
        "Convolutional layers convolve the input and pass its result to the next layer.",
        "This is similar to the response of a neuron in the visual cortex to a specific stimulus.",
        "Each convolutional neuron processes data only for its receptive field.",
        "Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature.",
        "A fully connected layer for an image of size 100 × 100 has 10,000 weights for each neuron in the second layer.",
        "Convolution reduces the number of free parameters, allowing the network to be deeper.",
        "For example, using a 5 × 5 tiling region, each with the same shared weights, requires only 25 neurons.",
        "Using shared weights means there are many fewer parameters, which helps avoid the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.",
        "To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers, which are based on a depthwise convolution followed by a pointwise convolution.",
        "The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of 1 × 1 {\\displaystyle 1 imes 1} kernels.",
        "Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers.",
        "Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer.",
        "Local pooling combines small clusters, tiling sizes such as 2 × 2 are commonly used.",
        "Global pooling acts on all the neurons of the feature map.",
        "There are two common types of pooling in popular use: max and average.",
        "Max pooling uses the maximum value of each local cluster of neurons in the feature map, while average pooling takes the average value.",
        "Fully connected layers connect every neuron in one layer to every neuron in another layer.",
        "It is the same as a traditional multilayer perceptron neural network (MLP).",
        "The flattened matrix goes through a fully connected layer to classify the images.",
        "In neural networks, each neuron receives input from some number of locations in the previous layer.",
        "In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field.",
        "Typically the area is a square (e.g.",
        "5 by 5 neurons).",
        "Whereas, in a fully connected layer, the receptive field is the entire previous layer.",
        "Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers.",
        "This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels.",
        "When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.",
        "To manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer.",
        "For example, atrous or dilated convolution expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions.",
        "Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios, thus having a variable receptive field size.",
        "Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer.",
        "The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers).",
        "Learning consists of iteratively adjusting these biases and weights.",
        "The vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape).",
        "A distinguishing feature of CNNs is that many neurons can share the same filter.",
        "This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.",
        "A deconvolutional neural network is essentially the reverse of a CNN.",
        "It consists of deconvolutional layers and unpooling layers.",
        "A deconvolutional layer is the transpose of a convolutional layer.",
        "Specifically, a convolutional layer can be written as a multiplication with a matrix, and a deconvolutional layer is multiplication with the transpose of that matrix.",
        "An unpooling layer expands the layer.",
        "The max-unpooling layer is the simplest, as it simply copies each entry multiple times.",
        "Deconvolution layers are used in image generators.",
        "By default, it creates periodic checkerboard artifact, which can be fixed by upscale-then-convolve.",
        "CNN are often compared to the way the brain achieves vision processing in living organisms.",
        "Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field.",
        "Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field.",
        "Neighboring cells have similar and overlapping receptive fields.",
        "Receptive field size and location varies systematically across the cortex to form a complete map of visual space.",
        "The cortex in each hemisphere represents the contralateral visual field.",
        "Their 1968 paper identified two basic visual cell types in the brain: simple cells, whose output is maximized by straight edges having particular orientations within their receptive field complex cells, which have larger receptive fields, whose output is insensitive to the exact position of the edges in the field.",
        "Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.",
        "In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\"",
        "This is the essential core of a convolutional network, but the weights were not trained.",
        "In the same paper, Fukushima also introduced the ReLU (rectified linear unit) activation function.",
        "The \"neocognitron\" was introduced by Fukushima in 1980.",
        "The neocognitron introduced the two basic types of layers: \"S-layer\": a shared-weights receptive-field layer, later known as a convolutional layer, which contains units whose receptive fields cover a patch of the previous layer.",
        "A shared-weights receptive-field group (a \"plane\" in neocognitron terminology) is often called a filter, and a layer typically has several such filters.",
        "\"C-layer\": a downsampling layer that contain units whose receptive fields cover patches of previous convolutional layers.",
        "Such a unit typically computes a weighted average of the activations of the units in its patch, and applies inhibition (divisive normalization) pooled from a somewhat larger patch and across different filters in a layer, and applies a saturating activation function.",
        "The patch weights are nonnegative and are not trainable in the original neocognitron.",
        "The downsampling and competitive inhibition help to classify features and objects in visual scenes even when the objects are shifted.",
        "Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron.",
        "Today, however, the CNN architecture is usually trained through backpropagation.",
        "Fukushima's ReLU activation function was not used in his neocognitron since all the weights were nonnegative; lateral inhibition was used instead.",
        "The rectifier has become a very popular activation function for CNNs and deep neural networks in general.",
        "The term \"convolution\" first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987.",
        "Their paper replaced multiplication with convolution in time, inherently providing shift invariance, motivated by and connecting more directly to the signal-processing concept of a filter, and demonstrated it on a speech recognition task.",
        "They also pointed out that as a data-trainable system, convolution is essentially equivalent to correlation since reversal of the weights does not affect the final learned function (\"For convenience, we denote * as correlation instead of convolution.",
        "Modern CNN implementations typically do correlation and call it convolution, for convenience, as they did here.",
        "The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al.",
        "for phoneme recognition and was an early convolutional network exhibiting shift-invariance.",
        "A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data.",
        "It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation.",
        "Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.",
        "TDNNs are convolutional networks that share weights along the temporal dimension.",
        "They allow speech signals to be processed time-invariantly.",
        "In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution.",
        "Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts, as with images processed by a neocognitron.",
        "TDNNs improved the performance of far-distance speech recognition.",
        "Denker et al.",
        "(1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers.",
        "However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.",
        "Following the advances in the training of 1-D CNNs by Waibel et al.",
        "(1987), Yann LeCun et al.",
        "(1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers.",
        "Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.",
        "Wei Zhang et al.",
        "(1988) used back-propagation to train the convolution kernels of a CNN for alphabets recognition.",
        "The model was called shift-invariant pattern recognition neural network before the name CNN was coined later in the early 1990s.",
        "Wei Zhang et al.",
        "also applied the same CNN without the last fully connected layer for medical image object segmentation (1991) and breast cancer detection in mammograms (1994).",
        "This approach became a foundation of modern computer vision.",
        "In 1990 Yamaguchi et al.",
        "introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region.",
        "They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system.",
        "In their system they used several TDNNs per word, one for each syllable.",
        "The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.",
        "In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging with inhibition and saturation, J. Weng et al.",
        "in 1993 used max pooling, where a downsampling unit computes the maximum of the activations of the units in its patch, introducing this method into the vision field.",
        "Max pooling is often used in modern CNNs.",
        "LeNet-5, a pioneering 7-level convolutional network by LeCun et al.",
        "in 1995, classifies hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images.",
        "The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.",
        "It was superior than other commercial courtesy amount reading systems (as of 1995).",
        "The system was integrated in NCR's check reading systems, and fielded in several American banks since June 1996, reading millions of checks per day.",
        "A shift-invariant neural network was proposed by Wei Zhang et al.",
        "for image character recognition in 1988.",
        "It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.",
        "The model was trained with back-propagation.",
        "The training algorithm was further improved in 1991 to improve its generalization ability.",
        "The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991) and automatic detection of breast cancer in mammograms (1994).",
        "A different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution.",
        "This design was modified in 1989 to other de-convolution-based designs.",
        "Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).",
        "In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs.",
        "Their implementation was 20 times faster than an equivalent implementation on CPU.",
        "In 2005, another paper also emphasised the value of GPGPU for machine learning.",
        "The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al.",
        "Their implementation was 4 times faster than an equivalent implementation on CPU.",
        "In the same period, GPUs were also used for unsupervised training of deep belief networks.",
        "In 2010, Dan Ciresan et al.",
        "at IDSIA trained deep feedforward networks on GPUs.",
        "In 2011, they extended this to CNNs, accelerating by 60 compared to training CPU.",
        "In 2011, the network won an image recognition contest where they achieved superhuman performance for the first time.",
        "Then they won more competitions and achieved state of the art on several benchmarks.",
        "Subsequently, AlexNet, a similar GPU-based CNN by Alex Krizhevsky et al.",
        "won the ImageNet Large Scale Visual Recognition Challenge 2012.",
        "It was an early catalytic event for the AI boom.",
        "Compared to the training of CNNs using GPUs, not much attention was given to CPU.",
        "(Viebke et al 2019) parallelizes CNN by thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.",
        "In the past, traditional multilayer perceptron (MLP) models were used for image recognition.",
        "However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher-resolution images.",
        "A 1000×1000-pixel image with RGB color channels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale.",
        "For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights.",
        "A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.",
        "Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together.",
        "This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically.",
        "Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.",
        "Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex.",
        "These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images.",
        "As opposed to MLPs, CNNs have the following distinguishing features: 3D volumes of neurons.",
        "The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth.",
        "Where each neuron inside a convolutional layer is connected to only a small region of the layer before it, called a receptive field.",
        "Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.",
        "Local connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers.",
        "The architecture thus ensures that the learned \"filters\" produce the strongest response to a spatially local input pattern.",
        "Stacking many such layers leads to nonlinear filters that become increasingly global (i.e.",
        "responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas.",
        "Shared weights: In CNNs, each filter is replicated across the entire visual field.",
        "These replicated units share the same parameterization (weight vector and bias) and form a feature map.",
        "This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field.",
        "Replicating units in this way allows for the resulting activation map to be equivariant under shifts of the locations of input features in the visual field, i.e.",
        "they grant translational equivariance-given that the layer has a stride of one.",
        "Pooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value.",
        "In addition to reducing the sizes of feature maps, the pooling operation grants a degree of local translational invariance to the features contained therein, allowing the CNN to be more robust to variations in their positions.",
        "Together, these properties allow CNNs to achieve better generalization on vision problems.",
        "Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.",
        "A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g.",
        "holding the class scores) through a differentiable function.",
        "A few distinct types of layers are commonly used.",
        "These are further discussed below.",
        "The convolutional layer is the core building block of a CNN.",
        "The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume.",
        "During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter.",
        "As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.",
        "Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer.",
        "Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input.",
        "Each entry in an activation map use the same set of parameters that define the filter.",
        "Self-supervised learning has been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer.",
        "When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account.",
        "Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.",
        "The extent of this connectivity is a hyperparameter called the receptive field of the neuron.",
        "The connections are local in space (along width and height), but always extend along the entire depth of the input volume.",
        "Such an architecture ensures that the learned filters produce the strongest response to a spatially local input pattern.",
        "Three hyperparameters control the size of the output volume of the convolutional layer: the depth, stride, and padding size: The depth of the output volume controls the number of neurons in a layer that connect to the same region of the input volume.",
        "These neurons learn to activate for different features in the input.",
        "For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color.",
        "Stride controls how depth columns around the width and height are allocated.",
        "If the stride is 1, then we move the filters one pixel at a time.",
        "This leads to heavily overlapping receptive fields between the columns, and to large output volumes.",
        "For any integer S > 0 , { extstyle S>0,} a stride S means that the filter is translated S units at a time per output.",
        "In practice, S ≥ 3 { extstyle S\\geq 3} is rare.",
        "A greater stride means smaller overlap of receptive fields and smaller spatial dimensions of the output volume.",
        "Sometimes, it is convenient to pad the input with zeros (or other values, such as the average of the region) on the border of the input volume.",
        "The size of this padding is a third hyperparameter.",
        "Padding provides control of the output volume's spatial size.",
        "In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume, this is commonly referred to as \"same\" padding.",
        "The spatial size of the output volume is a function of the input volume size W {\\displaystyle W} , the kernel field size K {\\displaystyle K} of the convolutional layer neurons, the stride S {\\displaystyle S} , and the amount of zero padding P {\\displaystyle P} on the border.",
        "The number of neurons that \"fit\" in a given volume is then: W − K + 2 P S + 1.",
        "If this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way.",
        "In general, setting zero padding to be P = ( K − 1 ) / 2 { extstyle P=(K-1)/2} when the stride is S = 1 {\\displaystyle S=1} ensures that the input volume and output volume will have the same size spatially.",
        "However, it is not always completely necessary to use all of the neurons of the previous layer.",
        "For example, a neural network designer may decide to use just a portion of padding.",
        "A parameter sharing scheme is used in convolutional layers to control the number of free parameters.",
        "It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions.",
        "Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias.",
        "Since all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume.",
        "Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input.",
        "The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume.",
        "Parameter sharing contributes to the translation invariance of the CNN architecture.",
        "Sometimes, the parameter sharing assumption may not make sense.",
        "This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations.",
        "One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image.",
        "In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\".",
        "Another important concept of CNNs is pooling, which is used as a form of non-linear down-sampling.",
        "Pooling provides downsampling because it reduces the spatial dimensions (height and width) of the input feature maps while retaining the most important information.",
        "There are several non-linear functions to implement pooling, where max pooling and average pooling are the most common.",
        "Pooling aggregates information from small regions of the input creating partitions of the input feature map, typically using a fixed-size window (like 2x2) and applying a stride (often 2) to move the window across the input.",
        "Note that without using a stride greater than 1, pooling would not perform downsampling, as it would simply move the pooling window across the input one step at a time, without reducing the size of the feature map.",
        "In other words, the stride is what actually causes the downsampling by determining how much the pooling window moves over the input.",
        "Intuitively, the exact location of a feature is less important than its rough location relative to other features.",
        "This is the idea behind the use of pooling in convolutional neural networks.",
        "The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting.",
        "This is known as down-sampling.",
        "It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture.",
        "While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used.",
        "The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially.",
        "A very common form of max pooling is a layer with filters of size 2×2, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations: f X , Y ( S ) = max a , b = 0 1 S 2 X + a , 2 Y + b .",
        "In this case, every max operation is over 4 numbers.",
        "The depth dimension remains unchanged (this is true for other forms of pooling as well).",
        "In addition to max pooling, pooling units can use other functions, such as average pooling or ℓ2-norm pooling.",
        "Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice.",
        "Due to the effects of fast spatial reduction of the size of the representation, there is a recent trend towards using smaller filters or discarding pooling layers altogether.",
        "A channel max pooling (CMP) operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination.",
        "The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features.",
        "Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer.",
        "Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F ∈ R(C×M×N) and C ∈ R(c×M×N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively.",
        "Note that the CMP operation only changes the channel number of the feature maps.",
        "The width and the height of the feature maps are not changed, which is different from the MP operation.",
        "See for reviews for pooling methods.",
        "ReLU is the abbreviation of rectified linear unit.",
        "It was proposed by Alston Householder in 1941, and used in CNN by Kunihiko Fukushima in 1969.",
        "It effectively removes negative values from an activation map by setting them to zero.",
        "It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers.",
        "In 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks, compared to widely used activation functions prior to 2011.",
        "ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.",
        "After several convolutional and max pooling layers, the final classification is done via fully connected layers.",
        "Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks.",
        "Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).",
        "The \"loss layer\", or \"loss function\", exemplifies how training penalizes the deviation between the predicted output of the network, and the true data labels (during supervised learning).",
        "Various loss functions can be used, depending on the specific task.",
        "The Softmax loss function is used for predicting a single class of K mutually exclusive classes.",
        "Sigmoid cross-entropy loss is used for predicting K independent probability values in {\\displaystyle } .",
        "Euclidean loss is used for regressing to real-valued labels ( − ∞ , ∞ ) {\\displaystyle (-\\infty ,\\infty )} .",
        "Hyperparameters are various settings that are used to control the learning process.",
        "CNNs use more hyperparameters than a standard multilayer perceptron (MLP).",
        "Padding is the addition of (typically) 0-valued pixels on the borders of an image.",
        "This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance.",
        "The padding applied is typically one less than the corresponding kernel dimension.",
        "For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image.",
        "The stride is the number of pixels that the analysis window moves on each iteration.",
        "A stride of 2 means that each kernel is offset by 2 pixels from its predecessor.",
        "Since feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more.",
        "To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers.",
        "Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.",
        "The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.",
        "Common filter sizes found in the literature vary greatly, and are usually chosen based on the data set.",
        "Typical filter sizes range from 1x1 to 7x7.",
        "As two famous examples, AlexNet used 3x3, 5x5, and 11x11.",
        "Inceptionv3 used 1x1, 3x3, and 5x5.",
        "The challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and without overfitting.",
        "Max pooling is typically used, often with a 2x2 dimension.",
        "This implies that the input is drastically downsampled, reducing processing cost.",
        "Greater pooling reduces the dimension of the signal, and may result in unacceptable information loss.",
        "Often, non-overlapping pooling windows perform best.",
        "Dilation involves ignoring pixels within a kernel.",
        "This reduces processing memory potentially without significant signal loss.",
        "A dilation of 2 on a 3x3 kernel expands the kernel to 5x5, while still processing 9 (evenly spaced) pixels.",
        "Accordingly, dilation of 4 expands the kernel to 7x7.",
        "It is commonly assumed that CNNs are invariant to shifts of the input.",
        "Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input.",
        "However, layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happen in practice, and therefore yield models that are not equivariant to translations.",
        "Furthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input.",
        "One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer.",
        "Additionally, several other partial solutions have been proposed, such as anti-aliasing before downsampling operations, spatial transformer networks, data augmentation, subsampling combined with pooling, and capsule neural networks.",
        "The accuracy of the final model is typically estimated on a sub-part of the dataset set apart at the start, often called a test set.",
        "Alternatively, methods such as k-fold cross-validation are applied.",
        "Other strategies include using conformal prediction.",
        "Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting.",
        "CNNs use various types of regularization.",
        "Because networks have so many parameters, they are prone to overfitting.",
        "One method to reduce overfitting is dropout, introduced in 2014.",
        "At each training stage, individual nodes are either \"dropped out\" of the net (ignored) with probability 1 − p {\\displaystyle 1-p} or kept with probability p {\\displaystyle p} , so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.",
        "Only the reduced network is trained on the data in that stage.",
        "The removed nodes are then reinserted into the network with their original weights.",
        "In the training stages, p {\\displaystyle p} is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored.",
        "At testing time after training has finished, we would ideally like to find a sample average of all possible 2 n {\\displaystyle 2^{n}} dropped-out networks; unfortunately this is unfeasible for large values of n {\\displaystyle n} .",
        "However, we can find an approximation by using the full network with each node's output weighted by a factor of p {\\displaystyle p} , so the expected value of the output of any node is the same as in the training stages.",
        "This is the biggest contribution of the dropout method: although it effectively generates 2 n {\\displaystyle 2^{n}} neural nets, and as such allows for model combination, at test time only a single network needs to be tested.",
        "By avoiding training all nodes on all training data, dropout decreases overfitting.",
        "The method also significantly improves training speed.",
        "This makes the model combination practical, even for deep neural networks.",
        "The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.",
        "DropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability 1 − p {\\displaystyle 1-p} .",
        "Each unit thus receives input from a random subset of units in the previous layer.",
        "DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer.",
        "In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.",
        "A major drawback to dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.",
        "Even before dropout, in 2013 a technique called stochastic pooling, the conventional deterministic pooling operations were replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region.",
        "This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.",
        "An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations.",
        "This is similar to explicit elastic deformations of the input images, which delivers excellent performance on the MNIST data set.",
        "Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.",
        "Because the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting.",
        "Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones.",
        "The latter one is used since mid-1990s.",
        "For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.",
        "One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur.",
        "It comes with the disadvantage that the learning process is halted.",
        "Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth.",
        "For convolutional networks, the filter size also affects the number of parameters.",
        "Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting.",
        "This is equivalent to a \"zero norm\".",
        "A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node.",
        "The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter), thus increasing the penalty for large weight vectors.",
        "L2 regularization is the most common form of regularization.",
        "It can be implemented by penalizing the squared magnitude of all parameters directly in the objective.",
        "The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors.",
        "Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.",
        "L1 regularization is also common.",
        "It makes the weight vectors sparse during optimization.",
        "In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs.",
        "L1 with L2 regularization can be combined; this is called elastic net regularization.",
        "Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint.",
        "In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector w → {\\displaystyle {\\vec {w}}} of every neuron to satisfy ‖ w → ‖ 2 < c {\\displaystyle \\|{\\vec {w}}\\|_{2}<c} .",
        "Typical values of c {\\displaystyle c} are order of 3-4.",
        "Some papers report improvements when using this form of regularization.",
        "Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image).",
        "These relationships are needed for identity recognition.",
        "Overlapping the pools so that each feature occurs in multiple pools, helps retain the information.",
        "Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale.",
        "On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.",
        "An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc.",
        "so that the network can cope with these variations.",
        "This is computationally intensive for large data-sets.",
        "The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina.",
        "The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.",
        "Thus, one way to represent something is to embed the coordinate frame within it.",
        "This allows large features to be recognized by using the consistency of the poses of their parts (e.g.",
        "nose and mouth poses make a consistent prediction of the pose of the whole face).",
        "This approach ensures that the higher-level entity (e.g.",
        "face) is present when the lower-level (e.g.",
        "nose and mouth) agree on its prediction of the pose.",
        "The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints.",
        "This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.",
        "CNNs are often used in image recognition systems.",
        "In 2012, an error rate of 0.23% on the MNIST database was reported.",
        "Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.",
        "Subsequently, a similar CNN called AlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012.",
        "When applied to facial recognition, CNNs achieved a large decrease in error rate.",
        "Another paper reported a 97.6% recognition rate on \"5,600 still images of more than 10 subjects\".",
        "CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.",
        "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes.",
        "In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework.",
        "The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date.",
        "Its network applied more than 30 layers.",
        "That performance of convolutional neural networks on the ImageNet tests was close to that of humans.",
        "The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand.",
        "They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras.",
        "By contrast, those kinds of images rarely trouble humans.",
        "Humans, however, tend to have trouble with other issues.",
        "For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.",
        "In 2015, a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance.",
        "The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces.",
        "They used batches of 128 images over 50,000 iterations.",
        "Compared to image data domains, there is relatively little work on applying CNNs to video classification.",
        "Video is more complex than images since it has another (temporal) dimension.",
        "However, some extensions of CNNs into the video domain have been explored.",
        "One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.",
        "Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream.",
        "Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies.",
        "Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis.",
        "Its application can be seen in text-to-video model.",
        "CNNs have also been explored for natural language processing.",
        "CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks.",
        "Compared to traditional language processing methods such as recurrent neural networks, CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption, while RNNs are better suitable when classical time series modeling is required.",
        "A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.",
        "CNNs have been used in drug discovery.",
        "Predicting the interaction between molecules and biological proteins can identify potential treatments.",
        "In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based drug design.",
        "The system trains directly on 3-dimensional representations of chemical interactions.",
        "Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp3 carbons, and hydrogen bonding.",
        "Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis.",
        "CNNs have been used in the game of checkers.",
        "From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checkers using co-evolution.",
        "The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides.",
        "Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%.",
        "It also earned a win against the program Chinook at its \"expert\" level of play.",
        "CNNs have been used in computer Go.",
        "In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play.",
        "Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player.",
        "When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.",
        "A couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.",
        "Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better.",
        "Dilated convolutions might enable one-dimensional convolutional neural networks to effectively learn time series dependences.",
        "Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients.",
        "Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from.",
        "CNNs can also be applied to further tasks in time series analysis (e.g., time series classification or quantile forecasting).",
        "As archaeological findings such as clay tablets with cuneiform writing are increasingly acquired using 3D scanners, benchmark datasets are becoming available, including HeiCuBeDa providing almost 2000 normalized 2-D and 3-D datasets prepared with the GigaMesh Software Framework.",
        "So curvature-based measures are used in conjunction with geometric neural networks (GNNs), e.g.",
        "for period classification of those clay tablets being among the oldest documents of human history.",
        "For many applications, training data is not very available.",
        "Convolutional neural networks usually require a large amount of training data in order to avoid overfitting.",
        "A common technique is to train the network on a larger data set from a related domain.",
        "Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning.",
        "Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.",
        "End-to-end training and prediction are common practice in computer vision.",
        "However, human interpretable explanations are required for critical systems such as a self-driving cars.",
        "With recent advances in visual salience, spatial attention, and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.",
        "A deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning.",
        "Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.",
        "Preliminary results were presented in 2014, with an accompanying paper in February 2015.",
        "The research described an application to Atari 2600 gaming.",
        "Other deep reinforcement learning models preceded it.",
        "Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks.",
        "Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks.",
        "They provide a generic structure that can be used in many image and signal processing tasks.",
        "Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs.",
        "The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections.",
        "The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities.",
        "In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.",
        "Caffe: A library for convolutional neural networks.",
        "Created by the Berkeley Vision and Learning Center (BVLC).",
        "It supports both CPU and GPU.",
        "Developed in C++, and has Python and MATLAB wrappers.",
        "Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark.",
        "A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine.",
        "Allows the creation of custom layers.",
        "Integrates with Hadoop and Kafka.",
        "Dlib: A toolkit for making real world machine learning and data analysis applications in C++.",
        "Microsoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes.",
        "It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java.",
        "TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU, Google's proprietary tensor processing unit (TPU), and mobile devices.",
        "Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library.",
        "Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation.",
        "These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.",
        "Torch: A scientific computing framework with wide support for machine learning algorithms, written in C and Lua."
      ],
      "metadata": {
        "title": "Convolutional neural network",
        "url": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
        "word_count": 9243,
        "char_count": 59160,
        "sentence_count": 465,
        "scraped_at": "2025-08-09T14:47:09.064080",
        "language": "en",
        "processing_time": 0.01752185821533203,
        "source_hash": "d7af42160caec0782a017fbc934c20bd"
      }
    },
    {
      "title": "Recurrent neural network",
      "url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
      "raw_text": "In artificial neural networks, recurrent neural networks (RNNs) are designed for processing sequential data, such as text, speech, and time series, where the order of elements is important. Unlike feedforward neural networks, which process inputs independently, RNNs utilize recurrent connections, where the output of a neuron at one time step is fed back as input to the network at the next time step. This enables RNNs to capture temporal dependencies and patterns within sequences.\nThe fundamental building block of RNN is the recurrent unit, which maintains a hidden state—a form of memory that is updated at each time step based on the current input and the previous hidden state. This feedback mechanism allows the network to learn from past inputs and incorporate that knowledge into its current processing. RNNs have been successfully applied to tasks such as unsegmented, connected handwriting recognition, speech recognition, natural language processing, and neural machine translation.\nHowever, traditional RNNs suffer from the vanishing gradient problem, which limits their ability to learn long-range dependencies. This issue was addressed by the development of the long short-term memory (LSTM) architecture in 1997, making it the standard RNN variant for handling long-term dependencies. Later, gated recurrent units (GRUs) were introduced as a more computationally efficient alternative.\nIn recent years, transformers, which rely on self-attention mechanisms instead of recurrence, have become the dominant architecture for many sequence-processing tasks, particularly in natural language processing, due to their superior handling of long-range dependencies and greater parallelizability. Nevertheless, RNNs remain relevant for applications where computational efficiency, real-time processing, or the inherent sequential nature of data is crucial.\n\n\n== History ==\n\n\n=== Before modern ===\nOne origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex formed by parallel fiber, Purkinje cells, and granule cells. In 1933, Lorente de Nó discovered \"recurrent, reciprocal connections\" by Golgi's method, and proposed that excitatory loops explain certain aspects of the vestibulo-ocular reflex. During 1940s, multiple people proposed the existence of feedback in the brain, which was a contrast to the previous understanding of the neural system as a purely feedforward structure. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles. The current activity of such networks can be affected by activity indefinitely far in the past. They were both interested in closed loops as possible explanations for e.g. epilepsy and causalgia. Recurrent inhibition was proposed in 1946 as a negative feedback mechanism in motor control. Neural feedback loops were a common topic of discussion at the Macy conferences. See  for an extensive review of recurrent neural network models in neuroscience.\nFrank Rosenblatt in 1960 published \"close-loop cross-coupled perceptrons\", which are 3-layered perceptron networks whose middle layer contains recurrent connections that change by a Hebbian learning rule. Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network.\nSimilar networks were published by Kaoru Nakano in 1971,Shun'ichi Amari in 1972, and William A. Little in 1974, who was acknowledged by Hopfield in his 1982 paper.\nAnother origin of RNN was statistical mechanics. The Ising model was developed by Wilhelm Lenz and Ernst Ising in the 1920s as a simple statistical mechanical model of magnets at equilibrium. Glauber in 1963 studied the Ising model evolving in time, as a process towards equilibrium (Glauber dynamics), adding in the component of time.\nThe Sherrington–Kirkpatrick model of spin glass, published in 1975, is the Hopfield network with random initialization. Sherrington and Kirkpatrick found that it is highly likely for the energy function of the SK model to have many local minima. In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions. In a 1984 paper he extended this to continuous activation functions. It became a standard model for the study of neural networks through statistical mechanics.\n\n\n=== Modern ===\nModern RNN networks are mainly based on two architectures: LSTM and BRNN.\nAt the resurgence of neural networks in the 1980s, recurrent networks were studied again. They were sometimes called \"iterated nets\". Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\nLong short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. It became the default choice for RNN architecture.\nBidirectional recurrent neural networks (BRNN) uses two RNN that processes the same input in opposite directions. These two are often combined, giving the bidirectional LSTM architecture.\nAround 2006, bidirectional LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications. They also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google voice search, and dictation on Android devices. They broke records for improved machine translation, language modeling and Multilingual Language Processing. Also, LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s. The papers most commonly cited as the originators that produced seq2seq are two papers from 2014. A seq2seq architecture employs two RNN, typically LSTM, an \"encoder\" and a \"decoder\", for sequence transduction, such as machine translation. They became state of the art in machine translation, and was instrumental in the development of attention mechanisms and transformers.\n\n\n== Configurations ==\n\nAn RNN-based model can be factored into two parts: configuration and architecture. Multiple RNNs can be combined in a data flow, and the data flow itself is the configuration. Each RNN itself may have any architecture, including LSTM, GRU, etc.\n\n\n=== Standard ===\n\nRNNs come in many variants. Abstractly speaking, an RNN is a function \n  \n    \n      \n        \n          f\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle f_{\\theta }}\n  \n of type \n  \n    \n      \n        (\n        \n          x\n          \n            t\n          \n        \n        ,\n        \n          h\n          \n            t\n          \n        \n        )\n        ↦\n        (\n        \n          y\n          \n            t\n          \n        \n        ,\n        \n          h\n          \n            t\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{t},h_{t})\\mapsto (y_{t},h_{t+1})}\n  \n, where\n\n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n: input vector;\n\n  \n    \n      \n        \n          h\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle h_{t}}\n  \n: hidden vector;\n\n  \n    \n      \n        \n          y\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle y_{t}}\n  \n: output vector;\n\n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n: neural network parameters.\nIn words, it is a neural network that maps an input \n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n into an output \n  \n    \n      \n        \n          y\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle y_{t}}\n  \n, with the hidden vector \n  \n    \n      \n        \n          h\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle h_{t}}\n  \n playing the role of \"memory\", a partial record of all previous input-output pairs. At each step, it transforms input to an output, and modifies its \"memory\" to help it to better perform future processing.\nThe illustration to the right may be misleading to many because practical neural network topologies are frequently organized in \"layers\" and the drawing gives that appearance. However, what appears to be layers are, in fact, different steps in time, \"unfolded\" to produce the appearance of layers.\n\n\n=== Stacked RNN ===\nA stacked RNN, or deep RNN, is composed of multiple RNNs stacked one above the other. Abstractly, it is structured as follows\nLayer 1 has hidden vector \n  \n    \n      \n        \n          h\n          \n            1\n            ,\n            t\n          \n        \n      \n    \n    {\\displaystyle h_{1,t}}\n  \n, parameters \n  \n    \n      \n        \n          θ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\theta _{1}}\n  \n, and maps \n  \n    \n      \n        \n          f\n          \n            \n              θ\n              \n                1\n              \n            \n          \n        \n        :\n        (\n        \n          x\n          \n            0\n            ,\n            t\n          \n        \n        ,\n        \n          h\n          \n            1\n            ,\n            t\n          \n        \n        )\n        ↦\n        (\n        \n          x\n          \n            1\n            ,\n            t\n          \n        \n        ,\n        \n          h\n          \n            1\n            ,\n            t\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle f_{\\theta _{1}}:(x_{0,t},h_{1,t})\\mapsto (x_{1,t},h_{1,t+1})}\n  \n.\nLayer 2 has hidden vector \n  \n    \n      \n        \n          h\n          \n            2\n            ,\n            t\n          \n        \n      \n    \n    {\\displaystyle h_{2,t}}\n  \n, parameters \n  \n    \n      \n        \n          θ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\theta _{2}}\n  \n, and maps \n  \n    \n      \n        \n          f\n          \n            \n              θ\n              \n                2\n              \n            \n          \n        \n        :\n        (\n        \n          x\n          \n            1\n            ,\n            t\n          \n        \n        ,\n        \n          h\n          \n            2\n            ,\n            t\n          \n        \n        )\n        ↦\n        (\n        \n          x\n          \n            2\n            ,\n            t\n          \n        \n        ,\n        \n          h\n          \n            2\n            ,\n            t\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle f_{\\theta _{2}}:(x_{1,t},h_{2,t})\\mapsto (x_{2,t},h_{2,t+1})}\n  \n.\n...\nLayer \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n has hidden vector \n  \n    \n      \n        \n          h\n          \n            n\n            ,\n            t\n          \n        \n      \n    \n    {\\displaystyle h_{n,t}}\n  \n, parameters \n  \n    \n      \n        \n          θ\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\theta _{n}}\n  \n, and maps \n  \n    \n      \n        \n          f\n          \n            \n              θ\n              \n                n\n              \n            \n          \n        \n        :\n        (\n        \n          x\n          \n            n\n            −\n            1\n            ,\n            t\n          \n        \n        ,\n        \n          h\n          \n            n\n            ,\n            t\n          \n        \n        )\n        ↦\n        (\n        \n          x\n          \n            n\n            ,\n            t\n          \n        \n        ,\n        \n          h\n          \n            n\n            ,\n            t\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle f_{\\theta _{n}}:(x_{n-1,t},h_{n,t})\\mapsto (x_{n,t},h_{n,t+1})}\n  \n.\nEach layer operates as a stand-alone RNN, and each layer's output sequence is used as the input sequence to the layer above. There is no conceptual limit to the depth of stacked RNN.\n\n\n=== Bidirectional ===\n\nA bidirectional RNN (biRNN) is composed of two RNNs, one processing the input sequence in one direction, and another in the opposite direction. Abstractly, it is structured as follows:\nThe forward RNN processes in one direction: \n  \n    \n      \n        \n          f\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            0\n          \n        \n        )\n        =\n        (\n        \n          y\n          \n            0\n          \n        \n        ,\n        \n          h\n          \n            1\n          \n        \n        )\n        ,\n        \n          f\n          \n            θ\n          \n        \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          h\n          \n            1\n          \n        \n        )\n        =\n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \n          h\n          \n            2\n          \n        \n        )\n        ,\n        …\n      \n    \n    {\\displaystyle f_{\\theta }(x_{0},h_{0})=(y_{0},h_{1}),f_{\\theta }(x_{1},h_{1})=(y_{1},h_{2}),\\dots }\n  \n\nThe backward RNN processes in the opposite direction:\n  \n    \n      \n        \n          f\n          \n            \n              θ\n              ′\n            \n          \n          ′\n        \n        (\n        \n          x\n          \n            N\n          \n        \n        ,\n        \n          h\n          \n            N\n          \n          ′\n        \n        )\n        =\n        (\n        \n          y\n          \n            N\n          \n          ′\n        \n        ,\n        \n          h\n          \n            N\n            −\n            1\n          \n          ′\n        \n        )\n        ,\n        \n          f\n          \n            \n              θ\n              ′\n            \n          \n          ′\n        \n        (\n        \n          x\n          \n            N\n            −\n            1\n          \n        \n        ,\n        \n          h\n          \n            N\n            −\n            1\n          \n          ′\n        \n        )\n        =\n        (\n        \n          y\n          \n            N\n            −\n            1\n          \n          ′\n        \n        ,\n        \n          h\n          \n            N\n            −\n            2\n          \n          ′\n        \n        )\n        ,\n        …\n      \n    \n    {\\displaystyle f'_{\\theta '}(x_{N},h_{N}')=(y'_{N},h_{N-1}'),f'_{\\theta '}(x_{N-1},h_{N-1}')=(y'_{N-1},h_{N-2}'),\\dots }\n  \n\nThe two output sequences are then concatenated to give the total output: \n  \n    \n      \n        (\n        (\n        \n          y\n          \n            0\n          \n        \n        ,\n        \n          y\n          \n            0\n          \n          ′\n        \n        )\n        ,\n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            1\n          \n          ′\n        \n        )\n        ,\n        …\n        ,\n        (\n        \n          y\n          \n            N\n          \n        \n        ,\n        \n          y\n          \n            N\n          \n          ′\n        \n        )\n        )\n      \n    \n    {\\displaystyle ((y_{0},y_{0}'),(y_{1},y_{1}'),\\dots ,(y_{N},y_{N}'))}\n  \n.\nBidirectional RNN allows the model to process a token both in the context of what came before it and what came after it. By stacking multiple bidirectional RNNs together, the model can process a token increasingly contextually. The ELMo model (2018) is a stacked bidirectional LSTM which takes character-level as inputs and produces word-level embeddings.\n\n\n=== Encoder-decoder ===\n\nTwo RNNs can be run front-to-back in an encoder-decoder configuration. The encoder RNN processes an input sequence into a sequence of hidden vectors, and the decoder RNN processes the sequence of hidden vectors to an output sequence, with an optional attention mechanism. This was used to construct state of the art neural machine translators during the 2014–2017 period. This was an instrumental step towards the development of transformers.\n\n\n=== PixelRNN ===\nAn RNN may process data with more than one dimension. PixelRNN processes two-dimensional data, with many possible directions. For example, the row-by-row direction processes an \n  \n    \n      \n        n\n        ×\n        n\n      \n    \n    {\\displaystyle n\\times n}\n  \n grid of vectors \n  \n    \n      \n        \n          x\n          \n            i\n            ,\n            j\n          \n        \n      \n    \n    {\\displaystyle x_{i,j}}\n  \n in the following order: \n  \n    \n      \n        \n          x\n          \n            1\n            ,\n            1\n          \n        \n        ,\n        \n          x\n          \n            1\n            ,\n            2\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            1\n            ,\n            n\n          \n        \n        ,\n        \n          x\n          \n            2\n            ,\n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n            ,\n            2\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            2\n            ,\n            n\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n            ,\n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1,1},x_{1,2},\\dots ,x_{1,n},x_{2,1},x_{2,2},\\dots ,x_{2,n},\\dots ,x_{n,n}}\n  \nThe diagonal BiLSTM uses two LSTMs to process the same grid. One processes it from the top-left corner to the bottom-right, such that it processes \n  \n    \n      \n        \n          x\n          \n            i\n            ,\n            j\n          \n        \n      \n    \n    {\\displaystyle x_{i,j}}\n  \n depending on its hidden state and cell state on the top and the left side: \n  \n    \n      \n        \n          h\n          \n            i\n            −\n            1\n            ,\n            j\n          \n        \n        ,\n        \n          c\n          \n            i\n            −\n            1\n            ,\n            j\n          \n        \n      \n    \n    {\\displaystyle h_{i-1,j},c_{i-1,j}}\n  \n and \n  \n    \n      \n        \n          h\n          \n            i\n            ,\n            j\n            −\n            1\n          \n        \n        ,\n        \n          c\n          \n            i\n            ,\n            j\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle h_{i,j-1},c_{i,j-1}}\n  \n. The other processes it from the top-right corner to the bottom-left.\n\n\n== Architectures ==\n\n\n=== Fully recurrent ===\n\nFully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. In other words, it is a fully connected network. This is the most general neural network topology, because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons.\n\n\n=== Hopfield ===\n\nThe Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. However, it guarantees that it will converge. If the connections are trained using Hebbian learning, then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.\n\n\n=== Elman networks and Jordan networks ===\n\nAn Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step, the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform tasks such as sequence-prediction that are beyond the power of a standard multilayer perceptron.\nJordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.\nElman and Jordan networks are also known as \"Simple recurrent networks\" (SRN).\n\nElman network\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  h\n                  \n                    t\n                  \n                \n              \n              \n                \n                =\n                \n                  σ\n                  \n                    h\n                  \n                \n                (\n                \n                  W\n                  \n                    h\n                  \n                \n                \n                  x\n                  \n                    t\n                  \n                \n                +\n                \n                  U\n                  \n                    h\n                  \n                \n                \n                  h\n                  \n                    t\n                    −\n                    1\n                  \n                \n                +\n                \n                  b\n                  \n                    h\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  y\n                  \n                    t\n                  \n                \n              \n              \n                \n                =\n                \n                  σ\n                  \n                    y\n                  \n                \n                (\n                \n                  W\n                  \n                    y\n                  \n                \n                \n                  h\n                  \n                    t\n                  \n                \n                +\n                \n                  b\n                  \n                    y\n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}}\n  \n\nJordan network\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  h\n                  \n                    t\n                  \n                \n              \n              \n                \n                =\n                \n                  σ\n                  \n                    h\n                  \n                \n                (\n                \n                  W\n                  \n                    h\n                  \n                \n                \n                  x\n                  \n                    t\n                  \n                \n                +\n                \n                  U\n                  \n                    h\n                  \n                \n                \n                  s\n                  \n                    t\n                  \n                \n                +\n                \n                  b\n                  \n                    h\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  y\n                  \n                    t\n                  \n                \n              \n              \n                \n                =\n                \n                  σ\n                  \n                    y\n                  \n                \n                (\n                \n                  W\n                  \n                    y\n                  \n                \n                \n                  h\n                  \n                    t\n                  \n                \n                +\n                \n                  b\n                  \n                    y\n                  \n                \n                )\n              \n            \n            \n              \n                \n                  s\n                  \n                    t\n                  \n                \n              \n              \n                \n                =\n                \n                  σ\n                  \n                    s\n                  \n                \n                (\n                \n                  W\n                  \n                    s\n                    ,\n                    s\n                  \n                \n                \n                  s\n                  \n                    t\n                    −\n                    1\n                  \n                \n                +\n                \n                  W\n                  \n                    s\n                    ,\n                    y\n                  \n                \n                \n                  y\n                  \n                    t\n                    −\n                    1\n                  \n                \n                +\n                \n                  b\n                  \n                    s\n                  \n                \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}s_{t}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\\\s_{t}&=\\sigma _{s}(W_{s,s}s_{t-1}+W_{s,y}y_{t-1}+b_{s})\\end{aligned}}}\n  \n\nVariables and functions\n\n  \n    \n      \n        \n          x\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle x_{t}}\n  \n: input vector\n\n  \n    \n      \n        \n          h\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle h_{t}}\n  \n: hidden layer vector\n\n  \n    \n      \n        \n          s\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle s_{t}}\n  \n: \"state\" vector,\n\n  \n    \n      \n        \n          y\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle y_{t}}\n  \n: output vector\n\n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n, \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n: parameter matrices and vector\n\n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n: Activation functions\n\n\n=== Long short-term memory ===\n\nLong short-term memory (LSTM) is the most widely used RNN architecture. It was designed to solve the vanishing gradient problem. LSTM is normally augmented by recurrent gates called \"forget gates\". LSTM prevents backpropagated errors from vanishing or exploding. Instead, errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components.\nMany applications use stacks of LSTMs, for which it is called \"deep LSTM\". LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.\n\n\n=== Gated recurrent unit ===\n\nGated recurrent unit (GRU), introduced in 2014, was designed as a simplification of LSTM. They are used in the full form and several further simplified variants. They have fewer parameters than LSTM, as they lack an output gate.\nTheir performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. There does not appear to be particular performance difference between LSTM and GRU.\n\n\n==== Bidirectional associative memory ====\n\nIntroduced by Bart Kosko, a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bidirectionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping are optimized for increased network stability and relevance to real-world applications.\nA BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.\n\n\n=== Echo state ===\n\nEcho state networks (ESN) have a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine.\n\n\n=== Recursive ===\n\nA recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The recursive neural tensor network uses a tensor-based composition function for all nodes in the tree.\n\n\n=== Neural Turing machines ===\n\nNeural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources with which they interact. The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.\nDifferentiable neural computers (DNCs) are an extension of neural Turing machines, allowing for the usage of fuzzy amounts of each memory address and a record of chronology.\nNeural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analog stacks that are differentiable and trained. In this way, they are similar in complexity to recognizers of context free grammars (CFGs).\nRecurrent neural networks are Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.\n\n\n== Training ==\n\n\n=== Teacher forcing ===\nAn RNN can be trained into a conditionally generative model of sequences, aka autoregression.\nConcretely, let us consider the problem of machine translation, that is, given a sequence \n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{1},x_{2},\\dots ,x_{n})}\n  \n of English words, the model is to produce a sequence \n  \n    \n      \n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle (y_{1},\\dots ,y_{m})}\n  \n of French words. It is to be solved by a seq2seq model.\nNow, during training, the encoder half of the model would first ingest \n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{1},x_{2},\\dots ,x_{n})}\n  \n, then the decoder half would start generating a sequence \n  \n    \n      \n        (\n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            1\n          \n        \n        ,\n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            l\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\hat {y}}_{1},{\\hat {y}}_{2},\\dots ,{\\hat {y}}_{l})}\n  \n. The problem is that if the model makes a mistake early on, say at \n  \n    \n      \n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\hat {y}}_{2}}\n  \n, then subsequent tokens are likely to also be mistakes. This makes it inefficient for the model to obtain a learning signal, since the model would mostly learn to shift \n  \n    \n      \n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\hat {y}}_{2}}\n  \n towards \n  \n    \n      \n        \n          y\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle y_{2}}\n  \n, but not the others.\nTeacher forcing makes it so that the decoder uses the correct output sequence for generating the next entry in the sequence. So for example, it would see \n  \n    \n      \n        (\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            k\n          \n        \n        )\n      \n    \n    {\\displaystyle (y_{1},\\dots ,y_{k})}\n  \n in order to generate \n  \n    \n      \n        \n          \n            \n              \n                y\n                ^\n              \n            \n          \n          \n            k\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle {\\hat {y}}_{k+1}}\n  \n.\n\n\n=== Gradient descent ===\n\nGradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non-linear activation functions are differentiable.\nThe standard method for training RNN by gradient descent is the \"backpropagation through time\" (BPTT) algorithm, which is a special case of the general algorithm of backpropagation. A more computationally expensive online variant is called \"Real-Time Recurrent Learning\" or RTRL, which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT, this algorithm is local in time but not local in space.\nIn this context, local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector. Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT. Biological neural networks appear to be local with respect to both time and space.\nFor recursively computing the partial derivatives, RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices, while BPTT only takes O(number of weights) per time step, at the cost of storing all forward activations within the given time horizon. An online hybrid between BPTT and RTRL with intermediate complexity exists, along with variants for continuous time.\nA major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events. LSTM combined with a BPTT/RTRL hybrid learning method attempts to overcome these problems. This problem is also solved in the independently recurrent neural network (IndRNN) by reducing the context of a neuron to its own past state and the cross-neuron information can then be explored in the following layers. Memories of different ranges including long-term memory can be learned without the gradient vanishing and exploding problems.\nThe online algorithm called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally recurrent networks. It works with the most general locally recurrent networks. The CRBP algorithm can minimize the global error term. This fact improves the stability of the algorithm, providing a unifying view of gradient calculation techniques for recurrent networks with local feedback.\nOne approach to gradient information computation in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation. It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations. It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza.\n\n\n=== Connectionist temporal classification ===\nThe connectionist temporal classification (CTC) is a specialized loss function for training RNNs for sequence modeling problems where the timing is variable.\n\n\n=== Global optimization methods ===\nTraining the weights in a neural network can be modeled as a non-linear global optimization problem. A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector. Next, the network is evaluated against the training sequence. Typically, the sum-squared difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector. Arbitrary global optimization techniques may then be used to minimize this target function.\nThe most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks.\nInitially, the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link. The whole network is represented as a single chromosome. The fitness function is evaluated as follows:\n\nEach weight encoded in the chromosome is assigned to the respective weight link of the network.\nThe training set is presented to the network which propagates the input signals forward.\nThe mean-squared error is returned to the fitness function.\nThis function drives the genetic selection process.\nMany chromosomes make up the population; therefore, many different neural networks are evolved until a stopping criterion is satisfied. A common stopping scheme can be: \n\nWhen the neural network has learned a certain percentage of the training data.\nWhen the minimum value of the mean-squared-error is satisfied.\nWhen the maximum number of training generations has been reached.\nThe fitness function evaluates the stopping criterion as it receives the mean-squared error reciprocal from each network during training. Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared error.\nOther global (and/or evolutionary) optimization techniques may be used to seek a good set of weights, such as simulated annealing or particle swarm optimization.\n\n\n== Other architectures ==\n\n\n=== Independently RNN (IndRNN) ===\nThe independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections.\n\n\n=== Neural history compressor ===\nThe neural history compressor is an unsupervised stack of RNNs. At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.\nThe system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.\nIt is possible to distill the RNN hierarchy into two RNNs: the \"conscious\" chunker (higher level) and the \"subconscious\" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn, this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.\nA generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\n\n\n=== Second order RNNs ===\nSecond-order RNNs use higher order weights \n  \n    \n      \n        w\n        \n          \n\n          \n          \n            i\n            j\n            k\n          \n        \n      \n    \n    {\\displaystyle w{}_{ijk}}\n  \n instead of the standard \n  \n    \n      \n        w\n        \n          \n\n          \n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w{}_{ij}}\n  \n weights, and states can be a product. This allows a direct mapping to a finite-state machine both in training, and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability.\n\n\n=== Hierarchical recurrent neural network ===\nHierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms. Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson, whose philosophical views have inspired hierarchical models.\nHierarchical recurrent neural networks are useful in forecasting, helping to predict disaggregated inflation components of the consumer price index (CPI). The HRNN model leverages information from higher levels in the CPI hierarchy to enhance lower-level predictions. Evaluation of a substantial dataset from the US CPI-U index demonstrates the superior performance of the HRNN model compared to various established inflation prediction methods.\n\n\n=== Recurrent multilayer perceptron network ===\nGenerally, a recurrent multilayer perceptron network (RMLP network) consists of cascaded subnetworks, each containing multiple layers of nodes. Each subnetwork is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed-forward connections.\n\n\n=== Multiple timescales model ===\nA multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization depending on the spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties. With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book On Intelligence. Such a hierarchy also agrees with theories of memory posited by philosopher Henri Bergson, which have been incorporated into an MTRNN model.\n\n\n=== Memristive networks ===\nGreg Snider of HP Labs describes a system of cortical computing with memristive nanodevices. The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures that may be based on memristive systems.\nMemristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model. In this sense, the dynamics of a memristive circuit have the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology.\nThe evolution of these networks can be studied analytically using variations of the Caravelli-Traversa-Di Ventra equation.\n\n\n=== Continuous-time ===\nA continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs. They are typically analyzed by dynamical systems theory. Many RNN models in neuroscience are continuous-time.\nFor a neuron \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n in the network with activation \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  \n, the rate of change of activation is given by:\n\n  \n    \n      \n        \n          τ\n          \n            i\n          \n        \n        \n          \n            \n              \n                y\n                ˙\n              \n            \n          \n          \n            i\n          \n        \n        =\n        −\n        \n          y\n          \n            i\n          \n        \n        +\n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          w\n          \n            j\n            i\n          \n        \n        σ\n        (\n        \n          y\n          \n            j\n          \n        \n        −\n        \n          Θ\n          \n            j\n          \n        \n        )\n        +\n        \n          I\n          \n            i\n          \n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle \\tau _{i}{\\dot {y}}_{i}=-y_{i}+\\sum _{j=1}^{n}w_{ji}\\sigma (y_{j}-\\Theta _{j})+I_{i}(t)}\n  \n\nWhere:\n\n  \n    \n      \n        \n          τ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\tau _{i}}\n  \n : Time constant of postsynaptic node\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  \n : Activation of postsynaptic node\n\n  \n    \n      \n        \n          \n            \n              \n                y\n                ˙\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\dot {y}}_{i}}\n  \n : Rate of change of activation of postsynaptic node\n\n  \n    \n      \n        w\n        \n          \n\n          \n          \n            j\n            i\n          \n        \n      \n    \n    {\\displaystyle w{}_{ji}}\n  \n : Weight of connection from pre to postsynaptic node\n\n  \n    \n      \n        σ\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\sigma (x)}\n  \n : Sigmoid of x e.g. \n  \n    \n      \n        σ\n        (\n        x\n        )\n        =\n        1\n        \n          /\n        \n        (\n        1\n        +\n        \n          e\n          \n            −\n            x\n          \n        \n        )\n      \n    \n    {\\displaystyle \\sigma (x)=1/(1+e^{-x})}\n  \n.\n\n  \n    \n      \n        \n          y\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle y_{j}}\n  \n : Activation of presynaptic node\n\n  \n    \n      \n        \n          Θ\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\Theta _{j}}\n  \n : Bias of presynaptic node\n\n  \n    \n      \n        \n          I\n          \n            i\n          \n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle I_{i}(t)}\n  \n : Input (if any) to node\nCTRNNs have been applied to evolutionary robotics where they have been used to address vision, co-operation, and minimal cognitive behaviour.\nNote that, by the Shannon sampling theorem, discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        t\n        )\n      \n    \n    {\\displaystyle y_{i}(t)}\n  \n have been low-pass filtered but prior to sampling.\nThey are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.\nFrom a time-series perspective, RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX). RNN has infinite impulse response whereas convolutional neural networks have finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that cannot be unrolled.\nThe effect of memory-based learning for the recognition of sequences can also be implemented by a more biological-based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity.\nAdditional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN).\n\n\n== Libraries ==\nModern libraries provide runtime-optimized implementations of the above functionality or allow to speed up the slow loop by just-in-time compilation.\n\nApache Singa\nCaffe: Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.\nChainer: Fully in Python, production support for CPU, GPU, distributed training.\nDeeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark.\nFlux: includes interfaces for RNNs, including GRUs and LSTMs, written in Julia.\nKeras: High-level API, providing a wrapper to many other deep learning libraries.\nMicrosoft Cognitive Toolkit\nMXNet: an open-source deep learning framework used to train and deploy deep neural networks.\nPyTorch: Tensors and Dynamic neural networks in Python with GPU acceleration.\nTensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU and Google's proprietary TPU, mobile\nTheano: A deep-learning library for Python with an API largely compatible with the NumPy library.\nTorch: A scientific computing framework with support for machine learning algorithms, written in C and Lua.\n\n\n== Applications ==\nApplications of recurrent neural networks include:\n\nMachine translation\nRobot control\nTime series prediction\nSpeech recognition\nSpeech synthesis\nBrain–computer interfaces\nTime series anomaly detection\nText-to-Video model\nRhythm learning\nMusic composition\nGrammar learning\nHandwriting recognition\nHuman action recognition\nProtein homology detection\nPredicting subcellular localization of proteins\nSeveral prediction tasks in the area of business process management\nPrediction in medical care pathways\nPredictions of fusion plasma disruptions in reactors (Fusion Recurrent Neural Network (FRNN) code) \n\n\n== References ==\n\n\n== Further reading ==\nMandic, Danilo P.; Chambers, Jonathon A. (2001). Recurrent Neural Networks for Prediction: Learning Algorithms, Architectures and Stability. Wiley. ISBN 978-0-471-49517-8.\nGrossberg, Stephen (2013-02-22). \"Recurrent Neural Networks\". Scholarpedia. 8 (2): 1888. Bibcode:2013SchpJ...8.1888G. doi:10.4249/scholarpedia.1888. ISSN 1941-6016.\nRecurrent Neural Networks. List of RNN papers by Jürgen Schmidhuber's group at Dalle Molle Institute for Artificial Intelligence Research.",
      "cleaned_text": "In artificial neural networks, recurrent neural networks (RNNs) are designed for processing sequential data, such as text, speech, and time series, where the order of elements is important. Unlike feedforward neural networks, which process inputs independently, RNNs utilize recurrent connections, where the output of a neuron at one time step is fed back as input to the network at the next time step. This enables RNNs to capture temporal dependencies and patterns within sequences. The fundamental building block of RNN is the recurrent unit, which maintains a hidden state-a form of memory that is updated at each time step based on the current input and the previous hidden state. This feedback mechanism allows the network to learn from past inputs and incorporate that knowledge into its current processing. RNNs have been successfully applied to tasks such as unsegmented, connected handwriting recognition, speech recognition, natural language processing, and neural machine translation. However, traditional RNNs suffer from the vanishing gradient problem, which limits their ability to learn long-range dependencies. This issue was addressed by the development of the long short-term memory (LSTM) architecture in 1997, making it the standard RNN variant for handling long-term dependencies. Later, gated recurrent units (GRUs) were introduced as a more computationally efficient alternative. In recent years, transformers, which rely on self-attention mechanisms instead of recurrence, have become the dominant architecture for many sequence-processing tasks, particularly in natural language processing, due to their superior handling of long-range dependencies and greater parallelizability. Nevertheless, RNNs remain relevant for applications where computational efficiency, real-time processing, or the inherent sequential nature of data is crucial. One origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex formed by parallel fiber, Purkinje cells, and granule cells. In 1933, Lorente de Nó discovered \"recurrent, reciprocal connections\" by Golgi's method, and proposed that excitatory loops explain certain aspects of the vestibulo-ocular reflex. During 1940s, multiple people proposed the existence of feedback in the brain, which was a contrast to the previous understanding of the neural system as a purely feedforward structure. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles. The current activity of such networks can be affected by activity indefinitely far in the past. They were both interested in closed loops as possible explanations for e.g. epilepsy and causalgia. Recurrent inhibition was proposed in 1946 as a negative feedback mechanism in motor control. Neural feedback loops were a common topic of discussion at the Macy conferences. See for an extensive review of recurrent neural network models in neuroscience. Frank Rosenblatt in 1960 published \"close-loop cross-coupled perceptrons\", which are 3-layered perceptron networks whose middle layer contains recurrent connections that change by a Hebbian learning rule. Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network. Similar networks were published by Kaoru Nakano in 1971,Shun'ichi Amari in 1972, and William A. Little in 1974, who was acknowledged by Hopfield in his 1982 paper. Another origin of RNN was statistical mechanics. The Ising model was developed by Wilhelm Lenz and Ernst Ising in the 1920s as a simple statistical mechanical model of magnets at equilibrium. Glauber in 1963 studied the Ising model evolving in time, as a process towards equilibrium (Glauber dynamics), adding in the component of time. The Sherrington-Kirkpatrick model of spin glass, published in 1975, is the Hopfield network with random initialization. Sherrington and Kirkpatrick found that it is highly likely for the energy function of the SK model to have many local minima. In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions. In a 1984 paper he extended this to continuous activation functions. It became a standard model for the study of neural networks through statistical mechanics. Modern RNN networks are mainly based on two architectures: LSTM and BRNN. At the resurgence of neural networks in the 1980s, recurrent networks were studied again. They were sometimes called \"iterated nets\". Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. It became the default choice for RNN architecture. Bidirectional recurrent neural networks (BRNN) uses two RNN that processes the same input in opposite directions. These two are often combined, giving the bidirectional LSTM architecture. Around 2006, bidirectional LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications. They also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google voice search, and dictation on Android devices. They broke records for improved machine translation, language modeling and Multilingual Language Processing. Also, LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. The idea of encoder-decoder sequence transduction had been developed in the early 2010s. The papers most commonly cited as the originators that produced seq2seq are two papers from 2014. A seq2seq architecture employs two RNN, typically LSTM, an \"encoder\" and a \"decoder\", for sequence transduction, such as machine translation. They became state of the art in machine translation, and was instrumental in the development of attention mechanisms and transformers. An RNN-based model can be factored into two parts: configuration and architecture. Multiple RNNs can be combined in a data flow, and the data flow itself is the configuration. Each RNN itself may have any architecture, including LSTM, GRU, etc. RNNs come in many variants. Abstractly speaking, an RNN is a function f θ {\\displaystyle f_{ heta }} of type ( x t , h t ) ↦ ( y t , h t + 1 ) {\\displaystyle (x_{t},h_{t})\\mapsto (y_{t},h_{t+1})} , where x t {\\displaystyle x_{t}} : input vector; h t {\\displaystyle h_{t}} : hidden vector; y t {\\displaystyle y_{t}} : output vector; θ {\\displaystyle heta } : neural network parameters. In words, it is a neural network that maps an input x t {\\displaystyle x_{t}} into an output y t {\\displaystyle y_{t}} , with the hidden vector h t {\\displaystyle h_{t}} playing the role of \"memory\", a partial record of all previous input-output pairs. At each step, it transforms input to an output, and modifies its \"memory\" to help it to better perform future processing. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in \"layers\" and the drawing gives that appearance. However, what appears to be layers are, in fact, different steps in time, \"unfolded\" to produce the appearance of layers. A stacked RNN, or deep RNN, is composed of multiple RNNs stacked one above the other. Abstractly, it is structured as follows Layer 1 has hidden vector h 1 , t {\\displaystyle h_{1,t}} , parameters θ 1 {\\displaystyle heta _{1}} , and maps f θ 1 : ( x 0 , t , h 1 , t ) ↦ ( x 1 , t , h 1 , t + 1 ) {\\displaystyle f_{ heta _{1}}:(x_{0,t},h_{1,t})\\mapsto (x_{1,t},h_{1,t+1})} . Layer 2 has hidden vector h 2 , t {\\displaystyle h_{2,t}} , parameters θ 2 {\\displaystyle heta _{2}} , and maps f θ 2 : ( x 1 , t , h 2 , t ) ↦ ( x 2 , t , h 2 , t + 1 ) {\\displaystyle f_{ heta _{2}}:(x_{1,t},h_{2,t})\\mapsto (x_{2,t},h_{2,t+1})} . ... Layer n {\\displaystyle n} has hidden vector h n , t {\\displaystyle h_{n,t}} , parameters θ n {\\displaystyle heta _{n}} , and maps f θ n : ( x n − 1 , t , h n , t ) ↦ ( x n , t , h n , t + 1 ) {\\displaystyle f_{ heta _{n}}:(x_{n-1,t},h_{n,t})\\mapsto (x_{n,t},h_{n,t+1})} . Each layer operates as a stand-alone RNN, and each layer's output sequence is used as the input sequence to the layer above. There is no conceptual limit to the depth of stacked RNN. A bidirectional RNN (biRNN) is composed of two RNNs, one processing the input sequence in one direction, and another in the opposite direction. Abstractly, it is structured as follows: The forward RNN processes in one direction: f θ ( x 0 , h 0 ) = ( y 0 , h 1 ) , f θ ( x 1 , h 1 ) = ( y 1 , h 2 ) , ... {\\displaystyle f_{ heta }(x_{0},h_{0})=(y_{0},h_{1}),f_{ heta }(x_{1},h_{1})=(y_{1},h_{2}),\\dots } The backward RNN processes in the opposite direction: f θ ′ ′ ( x N , h N ′ ) = ( y N ′ , h N − 1 ′ ) , f θ ′ ′ ( x N − 1 , h N − 1 ′ ) = ( y N − 1 ′ , h N − 2 ′ ) , ... {\\displaystyle f'_{ heta '}(x_{N},h_{N}')=(y'_{N},h_{N-1}'),f'_{ heta '}(x_{N-1},h_{N-1}')=(y'_{N-1},h_{N-2}'),\\dots } The two output sequences are then concatenated to give the total output: ( ( y 0 , y 0 ′ ) , ( y 1 , y 1 ′ ) , ... , ( y N , y N ′ ) ) {\\displaystyle ((y_{0},y_{0}'),(y_{1},y_{1}'),\\dots ,(y_{N},y_{N}'))} . Bidirectional RNN allows the model to process a token both in the context of what came before it and what came after it. By stacking multiple bidirectional RNNs together, the model can process a token increasingly contextually. The ELMo model (2018) is a stacked bidirectional LSTM which takes character-level as inputs and produces word-level embeddings. Two RNNs can be run front-to-back in an encoder-decoder configuration. The encoder RNN processes an input sequence into a sequence of hidden vectors, and the decoder RNN processes the sequence of hidden vectors to an output sequence, with an optional attention mechanism. This was used to construct state of the art neural machine translators during the 2014-2017 period. This was an instrumental step towards the development of transformers. An RNN may process data with more than one dimension. PixelRNN processes two-dimensional data, with many possible directions. For example, the row-by-row direction processes an n × n {\\displaystyle n imes n} grid of vectors x i , j {\\displaystyle x_{i,j}} in the following order: x 1 , 1 , x 1 , 2 , ... , x 1 , n , x 2 , 1 , x 2 , 2 , ... , x 2 , n , ... , x n , n {\\displaystyle x_{1,1},x_{1,2},\\dots ,x_{1,n},x_{2,1},x_{2,2},\\dots ,x_{2,n},\\dots ,x_{n,n}} The diagonal BiLSTM uses two LSTMs to process the same grid. One processes it from the top-left corner to the bottom-right, such that it processes x i , j {\\displaystyle x_{i,j}} depending on its hidden state and cell state on the top and the left side: h i − 1 , j , c i − 1 , j {\\displaystyle h_{i-1,j},c_{i-1,j}} and h i , j − 1 , c i , j − 1 {\\displaystyle h_{i,j-1},c_{i,j-1}} . The other processes it from the top-right corner to the bottom-left. Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. In other words, it is a fully connected network. This is the most general neural network topology, because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. However, it guarantees that it will converge. If the connections are trained using Hebbian learning, then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration. An Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step, the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform tasks such as sequence-prediction that are beyond the power of a standard multilayer perceptron. Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves. Elman and Jordan networks are also known as \"Simple recurrent networks\" (SRN). Elman network h t = σ h ( W h x t + U h h t − 1 + b h ) y t = σ y ( W y h t + b y ) {\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}} Jordan network h t = σ h ( W h x t + U h s t + b h ) y t = σ y ( W y h t + b y ) s t = σ s ( W s , s s t − 1 + W s , y y t − 1 + b s ) {\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}s_{t}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\\\s_{t}&=\\sigma _{s}(W_{s,s}s_{t-1}+W_{s,y}y_{t-1}+b_{s})\\end{aligned}}} Variables and functions x t {\\displaystyle x_{t}} : input vector h t {\\displaystyle h_{t}} : hidden layer vector s t {\\displaystyle s_{t}} : \"state\" vector, y t {\\displaystyle y_{t}} : output vector W {\\displaystyle W} , U {\\displaystyle U} and b {\\displaystyle b} : parameter matrices and vector σ {\\displaystyle \\sigma } : Activation functions Long short-term memory (LSTM) is the most widely used RNN architecture. It was designed to solve the vanishing gradient problem. LSTM is normally augmented by recurrent gates called \"forget gates\". LSTM prevents backpropagated errors from vanishing or exploding. Instead, errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components. Many applications use stacks of LSTMs, for which it is called \"deep LSTM\". LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts. Gated recurrent unit (GRU), introduced in 2014, was designed as a simplification of LSTM. They are used in the full form and several further simplified variants. They have fewer parameters than LSTM, as they lack an output gate. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. There does not appear to be particular performance difference between LSTM and GRU. Introduced by Bart Kosko, a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bidirectionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping are optimized for increased network stability and relevance to real-world applications. A BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer. Echo state networks (ESN) have a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The recursive neural tensor network uses a tensor-based composition function for all nodes in the tree. Neural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources with which they interact. The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Differentiable neural computers (DNCs) are an extension of neural Turing machines, allowing for the usage of fuzzy amounts of each memory address and a record of chronology. Neural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analog stacks that are differentiable and trained. In this way, they are similar in complexity to recognizers of context free grammars (CFGs). Recurrent neural networks are Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. An RNN can be trained into a conditionally generative model of sequences, aka autoregression. Concretely, let us consider the problem of machine translation, that is, given a sequence ( x 1 , x 2 , ... , x n ) {\\displaystyle (x_{1},x_{2},\\dots ,x_{n})} of English words, the model is to produce a sequence ( y 1 , ... , y m ) {\\displaystyle (y_{1},\\dots ,y_{m})} of French words. It is to be solved by a seq2seq model. Now, during training, the encoder half of the model would first ingest ( x 1 , x 2 , ... , x n ) {\\displaystyle (x_{1},x_{2},\\dots ,x_{n})} , then the decoder half would start generating a sequence ( y ^ 1 , y ^ 2 , ... , y ^ l ) {\\displaystyle ({\\hat {y}}_{1},{\\hat {y}}_{2},\\dots ,{\\hat {y}}_{l})} . The problem is that if the model makes a mistake early on, say at y ^ 2 {\\displaystyle {\\hat {y}}_{2}} , then subsequent tokens are likely to also be mistakes. This makes it inefficient for the model to obtain a learning signal, since the model would mostly learn to shift y ^ 2 {\\displaystyle {\\hat {y}}_{2}} towards y 2 {\\displaystyle y_{2}} , but not the others. Teacher forcing makes it so that the decoder uses the correct output sequence for generating the next entry in the sequence. So for example, it would see ( y 1 , ... , y k ) {\\displaystyle (y_{1},\\dots ,y_{k})} in order to generate y ^ k + 1 {\\displaystyle {\\hat {y}}_{k+1}} . Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non-linear activation functions are differentiable. The standard method for training RNN by gradient descent is the \"backpropagation through time\" (BPTT) algorithm, which is a special case of the general algorithm of backpropagation. A more computationally expensive online variant is called \"Real-Time Recurrent Learning\" or RTRL, which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT, this algorithm is local in time but not local in space. In this context, local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector. Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT. Biological neural networks appear to be local with respect to both time and space. For recursively computing the partial derivatives, RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices, while BPTT only takes O(number of weights) per time step, at the cost of storing all forward activations within the given time horizon. An online hybrid between BPTT and RTRL with intermediate complexity exists, along with variants for continuous time. A major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events. LSTM combined with a BPTT/RTRL hybrid learning method attempts to overcome these problems. This problem is also solved in the independently recurrent neural network (IndRNN) by reducing the context of a neuron to its own past state and the cross-neuron information can then be explored in the following layers. Memories of different ranges including long-term memory can be learned without the gradient vanishing and exploding problems. The online algorithm called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally recurrent networks. It works with the most general locally recurrent networks. The CRBP algorithm can minimize the global error term. This fact improves the stability of the algorithm, providing a unifying view of gradient calculation techniques for recurrent networks with local feedback. One approach to gradient information computation in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation. It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations. It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza. The connectionist temporal classification (CTC) is a specialized loss function for training RNNs for sequence modeling problems where the timing is variable. Training the weights in a neural network can be modeled as a non-linear global optimization problem. A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector. Next, the network is evaluated against the training sequence. Typically, the sum-squared difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector. Arbitrary global optimization techniques may then be used to minimize this target function. The most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks. Initially, the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link. The whole network is represented as a single chromosome. The fitness function is evaluated as follows: Each weight encoded in the chromosome is assigned to the respective weight link of the network. The training set is presented to the network which propagates the input signals forward. The mean-squared error is returned to the fitness function. This function drives the genetic selection process. Many chromosomes make up the population; therefore, many different neural networks are evolved until a stopping criterion is satisfied. A common stopping scheme can be: When the neural network has learned a certain percentage of the training data. When the minimum value of the mean-squared-error is satisfied. When the maximum number of training generations has been reached. The fitness function evaluates the stopping criterion as it receives the mean-squared error reciprocal from each network during training. Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared error. Other global (and/or evolutionary) optimization techniques may be used to seek a good set of weights, such as simulated annealing or particle swarm optimization. The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. The neural history compressor is an unsupervised stack of RNNs. At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level. The system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events. It is possible to distill the RNN hierarchy into two RNNs: the \"conscious\" chunker (higher level) and the \"subconscious\" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn, this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events. A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. Second-order RNNs use higher order weights w i j k {\\displaystyle w{}_{ijk}} instead of the standard w i j {\\displaystyle w{}_{ij}} weights, and states can be a product. This allows a direct mapping to a finite-state machine both in training, and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability. Hierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms. Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson, whose philosophical views have inspired hierarchical models. Hierarchical recurrent neural networks are useful in forecasting, helping to predict disaggregated inflation components of the consumer price index (CPI). The HRNN model leverages information from higher levels in the CPI hierarchy to enhance lower-level predictions. Evaluation of a substantial dataset from the US CPI-U index demonstrates the superior performance of the HRNN model compared to various established inflation prediction methods. Generally, a recurrent multilayer perceptron network (RMLP network) consists of cascaded subnetworks, each containing multiple layers of nodes. Each subnetwork is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed-forward connections. A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization depending on the spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties. With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book On Intelligence. Such a hierarchy also agrees with theories of memory posited by philosopher Henri Bergson, which have been incorporated into an MTRNN model. Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices. The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures that may be based on memristive systems. Memristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model. In this sense, the dynamics of a memristive circuit have the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology. The evolution of these networks can be studied analytically using variations of the Caravelli-Traversa-Di Ventra equation. A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs. They are typically analyzed by dynamical systems theory. Many RNN models in neuroscience are continuous-time. For a neuron i {\\displaystyle i} in the network with activation y i {\\displaystyle y_{i}} , the rate of change of activation is given by: τ i y ˙ i = − y i + ∑ j = 1 n w j i σ ( y j − Θ j ) + I i ( t ) {\\displaystyle au _{i}{\\dot {y}}_{i}=-y_{i}+\\sum _{j=1}^{n}w_{ji}\\sigma (y_{j}-\\Theta _{j})+I_{i}(t)} Where: τ i {\\displaystyle au _{i}} : Time constant of postsynaptic node y i {\\displaystyle y_{i}} : Activation of postsynaptic node y ˙ i {\\displaystyle {\\dot {y}}_{i}} : Rate of change of activation of postsynaptic node w j i {\\displaystyle w{}_{ji}} : Weight of connection from pre to postsynaptic node σ ( x ) {\\displaystyle \\sigma (x)} : Sigmoid of x e.g. σ ( x ) = 1 / ( 1 + e − x ) {\\displaystyle \\sigma (x)=1/(1+e^{-x})} . y j {\\displaystyle y_{j}} : Activation of presynaptic node Θ j {\\displaystyle \\Theta _{j}} : Bias of presynaptic node I i ( t ) {\\displaystyle I_{i}(t)} : Input (if any) to node CTRNNs have been applied to evolutionary robotics where they have been used to address vision, co-operation, and minimal cognitive behaviour. Note that, by the Shannon sampling theorem, discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions y i ( t ) {\\displaystyle y_{i}(t)} have been low-pass filtered but prior to sampling. They are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step. From a time-series perspective, RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX). RNN has infinite impulse response whereas convolutional neural networks have finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that cannot be unrolled. The effect of memory-based learning for the recognition of sequences can also be implemented by a more biological-based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity. Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN). Modern libraries provide runtime-optimized implementations of the above functionality or allow to speed up the slow loop by just-in-time compilation. Apache Singa Caffe: Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers. Chainer: Fully in Python, production support for CPU, GPU, distributed training. Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. Flux: includes interfaces for RNNs, including GRUs and LSTMs, written in Julia. Keras: High-level API, providing a wrapper to many other deep learning libraries. Microsoft Cognitive Toolkit MXNet: an open-source deep learning framework used to train and deploy deep neural networks. PyTorch: Tensors and Dynamic neural networks in Python with GPU acceleration. TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU and Google's proprietary TPU, mobile Theano: A deep-learning library for Python with an API largely compatible with the NumPy library. Torch: A scientific computing framework with support for machine learning algorithms, written in C and Lua. Applications of recurrent neural networks include: Machine translation Robot control Time series prediction Speech recognition Speech synthesis Brain-computer interfaces Time series anomaly detection Text-to-Video model Rhythm learning Music composition Grammar learning Handwriting recognition Human action recognition Protein homology detection Predicting subcellular localization of proteins Several prediction tasks in the area of business process management Prediction in medical care pathways Predictions of fusion plasma disruptions in reactors (Fusion Recurrent Neural Network (FRNN) code)",
      "sentences": [
        "In artificial neural networks, recurrent neural networks (RNNs) are designed for processing sequential data, such as text, speech, and time series, where the order of elements is important.",
        "Unlike feedforward neural networks, which process inputs independently, RNNs utilize recurrent connections, where the output of a neuron at one time step is fed back as input to the network at the next time step.",
        "This enables RNNs to capture temporal dependencies and patterns within sequences.",
        "The fundamental building block of RNN is the recurrent unit, which maintains a hidden state-a form of memory that is updated at each time step based on the current input and the previous hidden state.",
        "This feedback mechanism allows the network to learn from past inputs and incorporate that knowledge into its current processing.",
        "RNNs have been successfully applied to tasks such as unsegmented, connected handwriting recognition, speech recognition, natural language processing, and neural machine translation.",
        "However, traditional RNNs suffer from the vanishing gradient problem, which limits their ability to learn long-range dependencies.",
        "This issue was addressed by the development of the long short-term memory (LSTM) architecture in 1997, making it the standard RNN variant for handling long-term dependencies.",
        "Later, gated recurrent units (GRUs) were introduced as a more computationally efficient alternative.",
        "In recent years, transformers, which rely on self-attention mechanisms instead of recurrence, have become the dominant architecture for many sequence-processing tasks, particularly in natural language processing, due to their superior handling of long-range dependencies and greater parallelizability.",
        "Nevertheless, RNNs remain relevant for applications where computational efficiency, real-time processing, or the inherent sequential nature of data is crucial.",
        "One origin of RNN was neuroscience.",
        "The word \"recurrent\" is used to describe loop-like structures in anatomy.",
        "In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex formed by parallel fiber, Purkinje cells, and granule cells.",
        "In 1933, Lorente de Nó discovered \"recurrent, reciprocal connections\" by Golgi's method, and proposed that excitatory loops explain certain aspects of the vestibulo-ocular reflex.",
        "During 1940s, multiple people proposed the existence of feedback in the brain, which was a contrast to the previous understanding of the neural system as a purely feedforward structure.",
        "Hebb considered \"reverberating circuit\" as an explanation for short-term memory.",
        "The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.",
        "The current activity of such networks can be affected by activity indefinitely far in the past.",
        "They were both interested in closed loops as possible explanations for e.g.",
        "epilepsy and causalgia.",
        "Recurrent inhibition was proposed in 1946 as a negative feedback mechanism in motor control.",
        "Neural feedback loops were a common topic of discussion at the Macy conferences.",
        "See for an extensive review of recurrent neural network models in neuroscience.",
        "Frank Rosenblatt in 1960 published \"close-loop cross-coupled perceptrons\", which are 3-layered perceptron networks whose middle layer contains recurrent connections that change by a Hebbian learning rule.",
        "Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network.",
        "Similar networks were published by Kaoru Nakano in 1971,Shun'ichi Amari in 1972, and William A.",
        "Little in 1974, who was acknowledged by Hopfield in his 1982 paper.",
        "Another origin of RNN was statistical mechanics.",
        "The Ising model was developed by Wilhelm Lenz and Ernst Ising in the 1920s as a simple statistical mechanical model of magnets at equilibrium.",
        "Glauber in 1963 studied the Ising model evolving in time, as a process towards equilibrium (Glauber dynamics), adding in the component of time.",
        "The Sherrington-Kirkpatrick model of spin glass, published in 1975, is the Hopfield network with random initialization.",
        "Sherrington and Kirkpatrick found that it is highly likely for the energy function of the SK model to have many local minima.",
        "In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions.",
        "In a 1984 paper he extended this to continuous activation functions.",
        "It became a standard model for the study of neural networks through statistical mechanics.",
        "Modern RNN networks are mainly based on two architectures: LSTM and BRNN.",
        "At the resurgence of neural networks in the 1980s, recurrent networks were studied again.",
        "They were sometimes called \"iterated nets\".",
        "Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology.",
        "In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.",
        "Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.",
        "It became the default choice for RNN architecture.",
        "Bidirectional recurrent neural networks (BRNN) uses two RNN that processes the same input in opposite directions.",
        "These two are often combined, giving the bidirectional LSTM architecture.",
        "Around 2006, bidirectional LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications.",
        "They also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google voice search, and dictation on Android devices.",
        "They broke records for improved machine translation, language modeling and Multilingual Language Processing.",
        "Also, LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.",
        "The idea of encoder-decoder sequence transduction had been developed in the early 2010s.",
        "The papers most commonly cited as the originators that produced seq2seq are two papers from 2014.",
        "A seq2seq architecture employs two RNN, typically LSTM, an \"encoder\" and a \"decoder\", for sequence transduction, such as machine translation.",
        "They became state of the art in machine translation, and was instrumental in the development of attention mechanisms and transformers.",
        "An RNN-based model can be factored into two parts: configuration and architecture.",
        "Multiple RNNs can be combined in a data flow, and the data flow itself is the configuration.",
        "Each RNN itself may have any architecture, including LSTM, GRU, etc.",
        "RNNs come in many variants.",
        "In words, it is a neural network that maps an input x t {\\displaystyle x_{t}} into an output y t {\\displaystyle y_{t}} , with the hidden vector h t {\\displaystyle h_{t}} playing the role of \"memory\", a partial record of all previous input-output pairs.",
        "At each step, it transforms input to an output, and modifies its \"memory\" to help it to better perform future processing.",
        "The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in \"layers\" and the drawing gives that appearance.",
        "However, what appears to be layers are, in fact, different steps in time, \"unfolded\" to produce the appearance of layers.",
        "A stacked RNN, or deep RNN, is composed of multiple RNNs stacked one above the other.",
        "Each layer operates as a stand-alone RNN, and each layer's output sequence is used as the input sequence to the layer above.",
        "There is no conceptual limit to the depth of stacked RNN.",
        "A bidirectional RNN (biRNN) is composed of two RNNs, one processing the input sequence in one direction, and another in the opposite direction.",
        "Bidirectional RNN allows the model to process a token both in the context of what came before it and what came after it.",
        "By stacking multiple bidirectional RNNs together, the model can process a token increasingly contextually.",
        "The ELMo model (2018) is a stacked bidirectional LSTM which takes character-level as inputs and produces word-level embeddings.",
        "Two RNNs can be run front-to-back in an encoder-decoder configuration.",
        "The encoder RNN processes an input sequence into a sequence of hidden vectors, and the decoder RNN processes the sequence of hidden vectors to an output sequence, with an optional attention mechanism.",
        "This was used to construct state of the art neural machine translators during the 2014-2017 period.",
        "This was an instrumental step towards the development of transformers.",
        "An RNN may process data with more than one dimension.",
        "PixelRNN processes two-dimensional data, with many possible directions.",
        "For example, the row-by-row direction processes an n × n {\\displaystyle n imes n} grid of vectors x i , j {\\displaystyle x_{i,j}} in the following order: x 1 , 1 , x 1 , 2 , ... , x 1 , n , x 2 , 1 , x 2 , 2 , ... , x 2 , n , ... , x n , n {\\displaystyle x_{1,1},x_{1,2},\\dots ,x_{1,n},x_{2,1},x_{2,2},\\dots ,x_{2,n},\\dots ,x_{n,n}} The diagonal BiLSTM uses two LSTMs to process the same grid.",
        "One processes it from the top-left corner to the bottom-right, such that it processes x i , j {\\displaystyle x_{i,j}} depending on its hidden state and cell state on the top and the left side: h i − 1 , j , c i − 1 , j {\\displaystyle h_{i-1,j},c_{i-1,j}} and h i , j − 1 , c i , j − 1 {\\displaystyle h_{i,j-1},c_{i,j-1}} .",
        "The other processes it from the top-right corner to the bottom-left.",
        "Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons.",
        "In other words, it is a fully connected network.",
        "This is the most general neural network topology, because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons.",
        "The Hopfield network is an RNN in which all connections across layers are equally sized.",
        "It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns.",
        "However, it guarantees that it will converge.",
        "If the connections are trained using Hebbian learning, then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.",
        "An Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of context units (u in the illustration).",
        "The middle (hidden) layer is connected to these context units fixed with a weight of one.",
        "At each time step, the input is fed forward and a learning rule is applied.",
        "The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied).",
        "Thus the network can maintain a sort of state, allowing it to perform tasks such as sequence-prediction that are beyond the power of a standard multilayer perceptron.",
        "Jordan networks are similar to Elman networks.",
        "The context units are fed from the output layer instead of the hidden layer.",
        "The context units in a Jordan network are also called the state layer.",
        "They have a recurrent connection to themselves.",
        "Elman and Jordan networks are also known as \"Simple recurrent networks\" (SRN).",
        "It was designed to solve the vanishing gradient problem.",
        "LSTM is normally augmented by recurrent gates called \"forget gates\".",
        "LSTM prevents backpropagated errors from vanishing or exploding.",
        "Instead, errors can flow backward through unlimited numbers of virtual layers unfolded in space.",
        "That is, LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier.",
        "Problem-specific LSTM-like topologies can be evolved.",
        "LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components.",
        "Many applications use stacks of LSTMs, for which it is called \"deep LSTM\".",
        "LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.",
        "Gated recurrent unit (GRU), introduced in 2014, was designed as a simplification of LSTM.",
        "They are used in the full form and several further simplified variants.",
        "They have fewer parameters than LSTM, as they lack an output gate.",
        "Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory.",
        "There does not appear to be particular performance difference between LSTM and GRU.",
        "Introduced by Bart Kosko, a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector.",
        "The bidirectionality comes from passing information through a matrix and its transpose.",
        "Typically, bipolar encoding is preferred to binary encoding of the associative pairs.",
        "Recently, stochastic BAM models using Markov stepping are optimized for increased network stability and relevance to real-world applications.",
        "A BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.",
        "Echo state networks (ESN) have a sparsely connected random hidden layer.",
        "The weights of output neurons are the only part of the network that can change (be trained).",
        "ESNs are good at reproducing certain time series.",
        "A variant for spiking neurons is known as a liquid state machine.",
        "A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order.",
        "Such networks are typically also trained by the reverse mode of automatic differentiation.",
        "They can process distributed representations of structure, such as logical terms.",
        "A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain.",
        "Recursive neural networks have been applied to natural language processing.",
        "The recursive neural tensor network uses a tensor-based composition function for all nodes in the tree.",
        "Neural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources with which they interact.",
        "The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.",
        "Differentiable neural computers (DNCs) are an extension of neural Turing machines, allowing for the usage of fuzzy amounts of each memory address and a record of chronology.",
        "Neural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analog stacks that are differentiable and trained.",
        "In this way, they are similar in complexity to recognizers of context free grammars (CFGs).",
        "Recurrent neural networks are Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.",
        "An RNN can be trained into a conditionally generative model of sequences, aka autoregression.",
        "It is to be solved by a seq2seq model.",
        "The problem is that if the model makes a mistake early on, say at y ^ 2 {\\displaystyle {\\hat {y}}_{2}} , then subsequent tokens are likely to also be mistakes.",
        "This makes it inefficient for the model to obtain a learning signal, since the model would mostly learn to shift y ^ 2 {\\displaystyle {\\hat {y}}_{2}} towards y 2 {\\displaystyle y_{2}} , but not the others.",
        "Teacher forcing makes it so that the decoder uses the correct output sequence for generating the next entry in the sequence.",
        "So for example, it would see ( y 1 , ... , y k ) {\\displaystyle (y_{1},\\dots ,y_{k})} in order to generate y ^ k + 1 {\\displaystyle {\\hat {y}}_{k+1}} .",
        "Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function.",
        "In neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non-linear activation functions are differentiable.",
        "The standard method for training RNN by gradient descent is the \"backpropagation through time\" (BPTT) algorithm, which is a special case of the general algorithm of backpropagation.",
        "A more computationally expensive online variant is called \"Real-Time Recurrent Learning\" or RTRL, which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors.",
        "Unlike BPTT, this algorithm is local in time but not local in space.",
        "In this context, local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector.",
        "Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT.",
        "Biological neural networks appear to be local with respect to both time and space.",
        "For recursively computing the partial derivatives, RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices, while BPTT only takes O(number of weights) per time step, at the cost of storing all forward activations within the given time horizon.",
        "An online hybrid between BPTT and RTRL with intermediate complexity exists, along with variants for continuous time.",
        "A major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events.",
        "LSTM combined with a BPTT/RTRL hybrid learning method attempts to overcome these problems.",
        "This problem is also solved in the independently recurrent neural network (IndRNN) by reducing the context of a neuron to its own past state and the cross-neuron information can then be explored in the following layers.",
        "Memories of different ranges including long-term memory can be learned without the gradient vanishing and exploding problems.",
        "The online algorithm called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally recurrent networks.",
        "It works with the most general locally recurrent networks.",
        "The CRBP algorithm can minimize the global error term.",
        "This fact improves the stability of the algorithm, providing a unifying view of gradient calculation techniques for recurrent networks with local feedback.",
        "One approach to gradient information computation in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation.",
        "It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations.",
        "It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza.",
        "The connectionist temporal classification (CTC) is a specialized loss function for training RNNs for sequence modeling problems where the timing is variable.",
        "Training the weights in a neural network can be modeled as a non-linear global optimization problem.",
        "A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector.",
        "Next, the network is evaluated against the training sequence.",
        "Typically, the sum-squared difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector.",
        "Arbitrary global optimization techniques may then be used to minimize this target function.",
        "The most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks.",
        "Initially, the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link.",
        "The whole network is represented as a single chromosome.",
        "The fitness function is evaluated as follows: Each weight encoded in the chromosome is assigned to the respective weight link of the network.",
        "The training set is presented to the network which propagates the input signals forward.",
        "The mean-squared error is returned to the fitness function.",
        "This function drives the genetic selection process.",
        "Many chromosomes make up the population; therefore, many different neural networks are evolved until a stopping criterion is satisfied.",
        "A common stopping scheme can be: When the neural network has learned a certain percentage of the training data.",
        "When the minimum value of the mean-squared-error is satisfied.",
        "When the maximum number of training generations has been reached.",
        "The fitness function evaluates the stopping criterion as it receives the mean-squared error reciprocal from each network during training.",
        "Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared error.",
        "Other global (and/or evolutionary) optimization techniques may be used to seek a good set of weights, such as simulated annealing or particle swarm optimization.",
        "The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN.",
        "Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history.",
        "The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory.",
        "The cross-neuron information is explored in the next layers.",
        "IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU.",
        "Deep networks can be trained using skip connections.",
        "The neural history compressor is an unsupervised stack of RNNs.",
        "At the input level, it learns to predict its next input from the previous inputs.",
        "Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely.",
        "Each higher level RNN thus studies a compressed representation of the information in the RNN below.",
        "This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.",
        "The system effectively minimizes the description length or the negative logarithm of the probability of the data.",
        "Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.",
        "It is possible to distill the RNN hierarchy into two RNNs: the \"conscious\" chunker (higher level) and the \"subconscious\" automatizer (lower level).",
        "Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker.",
        "This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals.",
        "In turn, this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.",
        "A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992.",
        "In 1993, such a system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.",
        "Second-order RNNs use higher order weights w i j k {\\displaystyle w{}_{ijk}} instead of the standard w i j {\\displaystyle w{}_{ij}} weights, and states can be a product.",
        "This allows a direct mapping to a finite-state machine both in training, and representation.",
        "Long short-term memory is an example of this but has no such formal mappings or proof of stability.",
        "Hierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms.",
        "Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson, whose philosophical views have inspired hierarchical models.",
        "Hierarchical recurrent neural networks are useful in forecasting, helping to predict disaggregated inflation components of the consumer price index (CPI).",
        "The HRNN model leverages information from higher levels in the CPI hierarchy to enhance lower-level predictions.",
        "Evaluation of a substantial dataset from the US CPI-U index demonstrates the superior performance of the HRNN model compared to various established inflation prediction methods.",
        "Generally, a recurrent multilayer perceptron network (RMLP network) consists of cascaded subnetworks, each containing multiple layers of nodes.",
        "Each subnetwork is feed-forward except for the last layer, which can have feedback connections.",
        "Each of these subnets is connected only by feed-forward connections.",
        "A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization depending on the spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties.",
        "With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors.",
        "The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book On Intelligence.",
        "Such a hierarchy also agrees with theories of memory posited by philosopher Henri Bergson, which have been incorporated into an MTRNN model.",
        "Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices.",
        "The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film.",
        "DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures that may be based on memristive systems.",
        "Memristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model.",
        "In this sense, the dynamics of a memristive circuit have the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior.",
        "From this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology.",
        "The evolution of these networks can be studied analytically using variations of the Caravelli-Traversa-Di Ventra equation.",
        "A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs.",
        "They are typically analyzed by dynamical systems theory.",
        "Many RNN models in neuroscience are continuous-time.",
        "Note that, by the Shannon sampling theorem, discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations.",
        "This transformation can be thought of as occurring after the post-synaptic node activation functions y i ( t ) {\\displaystyle y_{i}(t)} have been low-pass filtered but prior to sampling.",
        "They are in fact recursive neural networks with a particular structure: that of a linear chain.",
        "Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.",
        "From a time-series perspective, RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX).",
        "RNN has infinite impulse response whereas convolutional neural networks have finite impulse response.",
        "Both classes of networks exhibit temporal dynamic behavior.",
        "A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that cannot be unrolled.",
        "The effect of memory-based learning for the recognition of sequences can also be implemented by a more biological-based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity.",
        "Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks.",
        "Another network or graph can also replace the storage if that incorporates time delays or has feedback loops.",
        "Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units.",
        "This is also called Feedback Neural Network (FNN).",
        "Modern libraries provide runtime-optimized implementations of the above functionality or allow to speed up the slow loop by just-in-time compilation.",
        "Apache Singa Caffe: Created by the Berkeley Vision and Learning Center (BVLC).",
        "It supports both CPU and GPU.",
        "Developed in C++, and has Python and MATLAB wrappers.",
        "Chainer: Fully in Python, production support for CPU, GPU, distributed training.",
        "Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark.",
        "Flux: includes interfaces for RNNs, including GRUs and LSTMs, written in Julia.",
        "Keras: High-level API, providing a wrapper to many other deep learning libraries.",
        "Microsoft Cognitive Toolkit MXNet: an open-source deep learning framework used to train and deploy deep neural networks.",
        "PyTorch: Tensors and Dynamic neural networks in Python with GPU acceleration.",
        "TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU and Google's proprietary TPU, mobile Theano: A deep-learning library for Python with an API largely compatible with the NumPy library.",
        "Torch: A scientific computing framework with support for machine learning algorithms, written in C and Lua."
      ],
      "metadata": {
        "title": "Recurrent neural network",
        "url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
        "word_count": 5663,
        "char_count": 35864,
        "sentence_count": 245,
        "scraped_at": "2025-08-09T14:47:09.078820",
        "language": "en",
        "processing_time": 0.014329195022583008,
        "source_hash": "a2e60cf73c5015b4faff264bc5bd9149"
      }
    },
    {
      "title": "Feedforward neural network",
      "url": "https://en.wikipedia.org/wiki/Feedforward_neural_network",
      "raw_text": "Feedforward refers to recognition-inference architecture of neural networks. Artificial neural network architectures are based on inputs multiplied by weights to obtain outputs (inputs-to-output): feedforward. Recurrent neural networks, or neural networks with loops allow information from later processing stages to feed back to earlier stages for sequence processing. However, at every stage of inference a feedforward multiplication remains the core, essential for backpropagation or backpropagation through time. Thus neural networks cannot contain feedback like negative feedback or positive feedback where the outputs feed back to the very same inputs and modify them, because this forms an infinite loop which is not possible to rewind in time to generate an error signal through backpropagation. This issue and nomenclature appear to be a point of confusion between some computer scientists and scientists in other fields studying brain networks.\n\n\n== Mathematical foundations ==\n\n\n=== Activation function ===\nThe two historically common activation functions are both sigmoids, and are described by\n\n  \n    \n      \n        y\n        (\n        \n          v\n          \n            i\n          \n        \n        )\n        =\n        tanh\n        ⁡\n        (\n        \n          v\n          \n            i\n          \n        \n        )\n         \n         \n        \n          and\n        \n         \n         \n        y\n        (\n        \n          v\n          \n            i\n          \n        \n        )\n        =\n        (\n        1\n        +\n        \n          e\n          \n            −\n            \n              v\n              \n                i\n              \n            \n          \n        \n        \n          )\n          \n            −\n            1\n          \n        \n        .\n      \n    \n    {\\displaystyle y(v_{i})=\\tanh(v_{i})~~{\\text{and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}.}\n  \n\nThe first is a hyperbolic tangent that ranges from -1 to 1, while the other is the logistic function, which is similar in shape but ranges from 0 to 1. Here \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  \n is the output of the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n-th node (neuron) and \n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle v_{i}}\n  \n is the weighted sum of the input connections. Alternative activation functions have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models).\nIn recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids.\n\n\n=== Learning ===\nLearning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation.\nWe can represent the degree of error in an output node \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n in the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-th data point (training example) by \n  \n    \n      \n        \n          e\n          \n            j\n          \n        \n        (\n        n\n        )\n        =\n        \n          d\n          \n            j\n          \n        \n        (\n        n\n        )\n        −\n        \n          y\n          \n            j\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}\n  \n, where \n  \n    \n      \n        \n          d\n          \n            j\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle d_{j}(n)}\n  \n is the desired target value for \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-th data point at node \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n, and \n  \n    \n      \n        \n          y\n          \n            j\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle y_{j}(n)}\n  \n is the value produced at node \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n when the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-th data point is given as an input.\nThe node weights can then be adjusted based on corrections that minimize the error in the entire output for the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-th data point, given by\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        (\n        n\n        )\n        =\n        \n          \n            1\n            2\n          \n        \n        \n          ∑\n          \n            \n              output node \n            \n            j\n          \n        \n        \n          e\n          \n            j\n          \n          \n            2\n          \n        \n        (\n        n\n        )\n        .\n      \n    \n    {\\displaystyle {\\mathcal {E}}(n)={\\frac {1}{2}}\\sum _{{\\text{output node }}j}e_{j}^{2}(n).}\n  \n\nUsing gradient descent, the change in each weight \n  \n    \n      \n        \n          w\n          \n            i\n            j\n          \n        \n      \n    \n    {\\displaystyle w_{ij}}\n  \n is\n\n  \n    \n      \n        Δ\n        \n          w\n          \n            j\n            i\n          \n        \n        (\n        n\n        )\n        =\n        −\n        η\n        \n          \n            \n              ∂\n              \n                \n                  E\n                \n              \n              (\n              n\n              )\n            \n            \n              ∂\n              \n                v\n                \n                  j\n                \n              \n              (\n              n\n              )\n            \n          \n        \n        \n          y\n          \n            i\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle \\Delta w_{ji}(n)=-\\eta {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}y_{i}(n)}\n  \n\nwhere \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle y_{i}(n)}\n  \n is the output of the previous neuron \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, and \n  \n    \n      \n        η\n      \n    \n    {\\displaystyle \\eta }\n  \n is the learning rate, which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression, \n  \n    \n      \n        \n          \n            \n              ∂\n              \n                \n                  E\n                \n              \n              (\n              n\n              )\n            \n            \n              ∂\n              \n                v\n                \n                  j\n                \n              \n              (\n              n\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}}\n  \n denotes the partial derivative of the error \n  \n    \n      \n        \n          \n            E\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle {\\mathcal {E}}(n)}\n  \n according to the weighted sum \n  \n    \n      \n        \n          v\n          \n            j\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle v_{j}(n)}\n  \n of the input connections of neuron \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n.\nThe derivative to be calculated depends on the induced local field \n  \n    \n      \n        \n          v\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle v_{j}}\n  \n, which itself varies. It is easy to prove that for an output node this derivative can be simplified to\n\n  \n    \n      \n        −\n        \n          \n            \n              ∂\n              \n                \n                  E\n                \n              \n              (\n              n\n              )\n            \n            \n              ∂\n              \n                v\n                \n                  j\n                \n              \n              (\n              n\n              )\n            \n          \n        \n        =\n        \n          e\n          \n            j\n          \n        \n        (\n        n\n        )\n        \n          ϕ\n          \n            ′\n          \n        \n        (\n        \n          v\n          \n            j\n          \n        \n        (\n        n\n        )\n        )\n      \n    \n    {\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=e_{j}(n)\\phi ^{\\prime }(v_{j}(n))}\n  \n\nwhere \n  \n    \n      \n        \n          ϕ\n          \n            ′\n          \n        \n      \n    \n    {\\displaystyle \\phi ^{\\prime }}\n  \n is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is\n\n  \n    \n      \n        −\n        \n          \n            \n              ∂\n              \n                \n                  E\n                \n              \n              (\n              n\n              )\n            \n            \n              ∂\n              \n                v\n                \n                  j\n                \n              \n              (\n              n\n              )\n            \n          \n        \n        =\n        \n          ϕ\n          \n            ′\n          \n        \n        (\n        \n          v\n          \n            j\n          \n        \n        (\n        n\n        )\n        )\n        \n          ∑\n          \n            k\n          \n        \n        −\n        \n          \n            \n              ∂\n              \n                \n                  E\n                \n              \n              (\n              n\n              )\n            \n            \n              ∂\n              \n                v\n                \n                  k\n                \n              \n              (\n              n\n              )\n            \n          \n        \n        \n          w\n          \n            k\n            j\n          \n        \n        (\n        n\n        )\n        .\n      \n    \n    {\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n).}\n  \n\nThis depends on the change in weights of the \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \nth nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function.\n\n\n== History ==\n\n\n=== Timeline ===\nCirca 1800, Legendre (1805) and Gauss (1795) created the simplest feedforward network which consists of a single weight layer with linear activation functions. It was trained by the least squares method for minimising mean squared error, also known as linear regression. Legendre and Gauss used it for the prediction of planetary movement from training data.\nIn 1943, Warren McCulloch and Walter Pitts proposed the binary artificial neuron as a logical model of biological neural networks.\nIn 1958, Frank Rosenblatt proposed the multilayered perceptron model, consisting of an input layer, a hidden layer with randomized weights that did not learn, and an output layer with learnable connections. R. D. Joseph (1960) mentions an even earlier perceptron-like device: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\"\nIn 1960, Joseph also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.\nIn 1965, Alexey Grigorevich Ivakhnenko and Valentin Lapa published Group Method of Data Handling, the first working deep learning algorithm, a method to train arbitrarily deep neural networks. It is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\" It was used to train an eight-layer neural net in 1971.\nIn 1967, Shun'ichi Amari reported  the first multilayered neural network trained by stochastic gradient descent, which was able to classify non-linearily separable pattern classes. Amari's student Saito conducted the computer experiments, using a five-layered feedforward network with two learning layers.\nIn 1970, Seppo Linnainmaa published the modern form of backpropagation in his master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.\nIn 2003, interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.\n\n\n=== Linear regression ===\n\n\n=== Perceptron ===\n\nIf using a threshold, i.e. a linear activation function,  the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel non-linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function.\n\nPerceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of gradient descent.\n\n\n=== Multilayer perceptron ===\nA multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network, consisting of fully connected neurons (hence the synonym sometimes used of fully connected network (FCN)), often with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not linearly separable.\n\n\n== Other feedforward networks ==\n\nExamples of other feedforward networks include convolutional neural networks and radial basis function networks, which use a different activation function.\n\n\n== See also ==\nFeed forward (control)\nHopfield network\nRprop\n\n\n== References ==\n\n\n== External links ==\nFeedforward neural networks tutorial\nFeedforward Neural Network: Example\nFeedforward Neural Networks: An Introduction",
      "cleaned_text": "Feedforward refers to recognition-inference architecture of neural networks. Artificial neural network architectures are based on inputs multiplied by weights to obtain outputs (inputs-to-output): feedforward. Recurrent neural networks, or neural networks with loops allow information from later processing stages to feed back to earlier stages for sequence processing. However, at every stage of inference a feedforward multiplication remains the core, essential for backpropagation or backpropagation through time. Thus neural networks cannot contain feedback like negative feedback or positive feedback where the outputs feed back to the very same inputs and modify them, because this forms an infinite loop which is not possible to rewind in time to generate an error signal through backpropagation. This issue and nomenclature appear to be a point of confusion between some computer scientists and scientists in other fields studying brain networks. The two historically common activation functions are both sigmoids, and are described by y ( v i ) = tanh ⁡ ( v i ) and y ( v i ) = ( 1 + e − v i ) − 1 . {\\displaystyle y(v_{i})= anh(v_{i})~~{ ext{and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}.} The first is a hyperbolic tangent that ranges from -1 to 1, while the other is the logistic function, which is similar in shape but ranges from 0 to 1. Here y i {\\displaystyle y_{i}} is the output of the i {\\displaystyle i} -th node (neuron) and v i {\\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models). In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation. We can represent the degree of error in an output node j {\\displaystyle j} in the n {\\displaystyle n} -th data point (training example) by e j ( n ) = d j ( n ) − y j ( n ) {\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)} , where d j ( n ) {\\displaystyle d_{j}(n)} is the desired target value for n {\\displaystyle n} -th data point at node j {\\displaystyle j} , and y j ( n ) {\\displaystyle y_{j}(n)} is the value produced at node j {\\displaystyle j} when the n {\\displaystyle n} -th data point is given as an input. The node weights can then be adjusted based on corrections that minimize the error in the entire output for the n {\\displaystyle n} -th data point, given by E ( n ) = 1 2 ∑ output node j e j 2 ( n ) . {\\displaystyle {\\mathcal {E}}(n)={\\frac {1}{2}}\\sum _{{ ext{output node }}j}e_{j}^{2}(n).} Using gradient descent, the change in each weight w i j {\\displaystyle w_{ij}} is Δ w j i ( n ) = − η ∂ E ( n ) ∂ v j ( n ) y i ( n ) {\\displaystyle \\Delta w_{ji}(n)=-\\eta {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}y_{i}(n)} where y i ( n ) {\\displaystyle y_{i}(n)} is the output of the previous neuron i {\\displaystyle i} , and η {\\displaystyle \\eta } is the learning rate, which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression, ∂ E ( n ) ∂ v j ( n ) {\\displaystyle {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}} denotes the partial derivative of the error E ( n ) {\\displaystyle {\\mathcal {E}}(n)} according to the weighted sum v j ( n ) {\\displaystyle v_{j}(n)} of the input connections of neuron i {\\displaystyle i} . The derivative to be calculated depends on the induced local field v j {\\displaystyle v_{j}} , which itself varies. It is easy to prove that for an output node this derivative can be simplified to − ∂ E ( n ) ∂ v j ( n ) = e j ( n ) ϕ ′ ( v j ( n ) ) {\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=e_{j}(n)\\phi ^{\\prime }(v_{j}(n))} where ϕ ′ {\\displaystyle \\phi ^{\\prime }} is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is − ∂ E ( n ) ∂ v j ( n ) = ϕ ′ ( v j ( n ) ) ∑ k − ∂ E ( n ) ∂ v k ( n ) w k j ( n ) . {\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n).} This depends on the change in weights of the k {\\displaystyle k} th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. Circa 1800, Legendre (1805) and Gauss (1795) created the simplest feedforward network which consists of a single weight layer with linear activation functions. It was trained by the least squares method for minimising mean squared error, also known as linear regression. Legendre and Gauss used it for the prediction of planetary movement from training data. In 1943, Warren McCulloch and Walter Pitts proposed the binary artificial neuron as a logical model of biological neural networks. In 1958, Frank Rosenblatt proposed the multilayered perceptron model, consisting of an input layer, a hidden layer with randomized weights that did not learn, and an output layer with learnable connections. R. D. Joseph (1960) mentions an even earlier perceptron-like device: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\" In 1960, Joseph also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning. In 1965, Alexey Grigorevich Ivakhnenko and Valentin Lapa published Group Method of Data Handling, the first working deep learning algorithm, a method to train arbitrarily deep neural networks. It is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\" It was used to train an eight-layer neural net in 1971. In 1967, Shun'ichi Amari reported the first multilayered neural network trained by stochastic gradient descent, which was able to classify non-linearily separable pattern classes. Amari's student Saito conducted the computer experiments, using a five-layered feedforward network with two learning layers. In 1970, Seppo Linnainmaa published the modern form of backpropagation in his master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. In 2003, interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors. If using a threshold, i.e. a linear activation function, the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel non-linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function. Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of gradient descent. A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network, consisting of fully connected neurons (hence the synonym sometimes used of fully connected network (FCN)), often with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not linearly separable. Examples of other feedforward networks include convolutional neural networks and radial basis function networks, which use a different activation function.",
      "sentences": [
        "Feedforward refers to recognition-inference architecture of neural networks.",
        "Artificial neural network architectures are based on inputs multiplied by weights to obtain outputs (inputs-to-output): feedforward.",
        "Recurrent neural networks, or neural networks with loops allow information from later processing stages to feed back to earlier stages for sequence processing.",
        "However, at every stage of inference a feedforward multiplication remains the core, essential for backpropagation or backpropagation through time.",
        "Thus neural networks cannot contain feedback like negative feedback or positive feedback where the outputs feed back to the very same inputs and modify them, because this forms an infinite loop which is not possible to rewind in time to generate an error signal through backpropagation.",
        "This issue and nomenclature appear to be a point of confusion between some computer scientists and scientists in other fields studying brain networks.",
        "The first is a hyperbolic tangent that ranges from -1 to 1, while the other is the logistic function, which is similar in shape but ranges from 0 to 1.",
        "Here y i {\\displaystyle y_{i}} is the output of the i {\\displaystyle i} -th node (neuron) and v i {\\displaystyle v_{i}} is the weighted sum of the input connections.",
        "Alternative activation functions have been proposed, including the rectifier and softplus functions.",
        "More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models).",
        "In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids.",
        "Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result.",
        "This is an example of supervised learning, and is carried out through backpropagation.",
        "The node weights can then be adjusted based on corrections that minimize the error in the entire output for the n {\\displaystyle n} -th data point, given by E ( n ) = 1 2 ∑ output node j e j 2 ( n ) .",
        "The derivative to be calculated depends on the induced local field v j {\\displaystyle v_{j}} , which itself varies.",
        "This depends on the change in weights of the k {\\displaystyle k} th nodes, which represent the output layer.",
        "So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function.",
        "Circa 1800, Legendre (1805) and Gauss (1795) created the simplest feedforward network which consists of a single weight layer with linear activation functions.",
        "It was trained by the least squares method for minimising mean squared error, also known as linear regression.",
        "Legendre and Gauss used it for the prediction of planetary movement from training data.",
        "In 1943, Warren McCulloch and Walter Pitts proposed the binary artificial neuron as a logical model of biological neural networks.",
        "In 1958, Frank Rosenblatt proposed the multilayered perceptron model, consisting of an input layer, a hidden layer with randomized weights that did not learn, and an output layer with learnable connections.",
        "R. D. Joseph (1960) mentions an even earlier perceptron-like device: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\"",
        "However, \"they dropped the subject.\"",
        "In 1960, Joseph also discussed multilayer perceptrons with an adaptive hidden layer.",
        "Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight.",
        "Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.",
        "In 1965, Alexey Grigorevich Ivakhnenko and Valentin Lapa published Group Method of Data Handling, the first working deep learning algorithm, a method to train arbitrarily deep neural networks.",
        "It is based on layer by layer training through regression analysis.",
        "Superfluous hidden units are pruned using a separate validation set.",
        "Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\"",
        "It was used to train an eight-layer neural net in 1971.",
        "In 1967, Shun'ichi Amari reported the first multilayered neural network trained by stochastic gradient descent, which was able to classify non-linearily separable pattern classes.",
        "Amari's student Saito conducted the computer experiments, using a five-layered feedforward network with two learning layers.",
        "In 1970, Seppo Linnainmaa published the modern form of backpropagation in his master thesis (1970).",
        "Ostrovski et al.",
        "republished it in 1971.",
        "Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm).",
        "In 1986, David E. Rumelhart et al.",
        "popularised backpropagation but did not cite the original work.",
        "In 2003, interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.",
        "If using a threshold, i.e.",
        "a linear activation function, the resulting linear threshold unit is called a perceptron.",
        "(Often the term is used to denote just one of these units.)",
        "Multiple parallel non-linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1,1] despite the limited computational power of single unit with a linear threshold function.",
        "Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule.",
        "It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of gradient descent.",
        "Examples of other feedforward networks include convolutional neural networks and radial basis function networks, which use a different activation function."
      ],
      "metadata": {
        "title": "Feedforward neural network",
        "url": "https://en.wikipedia.org/wiki/Feedforward_neural_network",
        "word_count": 1447,
        "char_count": 8852,
        "sentence_count": 48,
        "scraped_at": "2025-08-09T14:47:09.082801",
        "language": "en",
        "processing_time": 0.003831148147583008,
        "source_hash": "efdbca1f48687a800bbcece23c0260c6"
      }
    },
    {
      "title": "Data science",
      "url": "https://en.wikipedia.org/wiki/Data_science",
      "raw_text": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data. \nData science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.\nData science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.\nA data scientist is a professional who creates programming code and combines it with statistical knowledge to summarize data.\n\n\n== Foundations ==\nData science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business.\nVasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a non-essential part of data science. Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.\n\n\n== Etymology ==\n\n\n=== Early usage ===\nIn 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.\nThe term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science. In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data. In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.\n\n\n=== Modern usage ===\nIn 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\", a catchphrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe. A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".\nThe modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.\nThe professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.\n\n\n== Data science and data analysis ==\n\nData analysis typically involves working with structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning and data visualization to summarize data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data.\nData science involves working with larger datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models. Data science often uses statistical analysis, data preprocessing, and supervised learning.\n\n\n== Cloud computing for data science ==\n\nCloud computing can offer access to large amounts of computational power and storage. In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.\nSome distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reduce processing times.\n\n\n== Ethical consideration in data science ==\nData science involves collecting, processing, and analyzing data which often includes personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts.\nMachine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.\n\n\n== See also ==\nPython (programming language)\nR (programming language)\nData engineering\nBig data\nMachine learning\nArtificial intelligence\nBioinformatics\nAstroinformatics\nTopological data analysis\nList of open-source data science software\n\n\n== References ==",
      "cleaned_text": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data. Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession. Data science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge. A data scientist is a professional who creates programming code and combines it with statistical knowledge to summarize data. Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a non-essential part of data science. Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics. In 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing. The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science. In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data. In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis. In 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\", a catchphrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe. A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\". The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science. The professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection. Data analysis typically involves working with structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning and data visualization to summarize data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data. Data science involves working with larger datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models. Data science often uses statistical analysis, data preprocessing, and supervised learning. Cloud computing can offer access to large amounts of computational power and storage. In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks. Some distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reduce processing times. Data science involves collecting, processing, and analyzing data which often includes personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts. Machine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.",
      "sentences": [
        "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data.",
        "Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).",
        "Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.",
        "Data science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data.",
        "It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.",
        "However, data science is different from computer science and information science.",
        "Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.",
        "A data scientist is a professional who creates programming code and combines it with statistical knowledge to summarize data.",
        "Data science is an interdisciplinary field focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains.",
        "The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings.",
        "As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business.",
        "Vasant Dhar writes that statistics emphasizes quantitative data and description.",
        "In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.)",
        "and emphasizes prediction and action.",
        "Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.",
        "Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program.",
        "He describes data science as an applied field growing out of traditional statistics.",
        "In 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science.",
        "In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics.",
        "Later, attendees at a 1992 statistics symposium at the University of Montpellier II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.",
        "The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science.",
        "In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic.",
        "However, the definition was still in flux.",
        "After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science.",
        "He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data.",
        "In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.",
        "In 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\", a catchphrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe.",
        "A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".",
        "The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland.",
        "In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.",
        "The professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008.",
        "Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.",
        "Data analysis typically involves working with structured datasets to answer specific questions or solve specific problems.",
        "This can involve tasks such as data cleaning and data visualization to summarize data and develop hypotheses about relationships between variables.",
        "Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data.",
        "Data science involves working with larger datasets that often require advanced computational and statistical methods to analyze.",
        "Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models.",
        "Data science often uses statistical analysis, data preprocessing, and supervised learning.",
        "Cloud computing can offer access to large amounts of computational power and storage.",
        "In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.",
        "Some distributed computing frameworks are designed to handle big data workloads.",
        "These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reduce processing times.",
        "Data science involves collecting, processing, and analyzing data which often includes personal and sensitive information.",
        "Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts.",
        "Machine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes."
      ],
      "metadata": {
        "title": "Data science",
        "url": "https://en.wikipedia.org/wiki/Data_science",
        "word_count": 935,
        "char_count": 6443,
        "sentence_count": 45,
        "scraped_at": "2025-08-09T14:47:14.293368",
        "language": "en",
        "processing_time": 0.003712892532348633,
        "source_hash": "d677bec44e7941839c623eefe324bc39"
      }
    },
    {
      "title": "Data",
      "url": "https://en.wikipedia.org/wiki/Data",
      "raw_text": "Data ( DAY-tə, US also  DAT-ə) are a collection of discrete or continuous values that convey information, describing the quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted formally.  A datum is an individual value in a collection of data. Data are usually organized into structures such as tables that provide additional context and meaning, and may themselves be used as data in larger structures. Data may be used as variables in a computational process. Data may represent abstract ideas or concrete measurements.\nData are commonly used in scientific research, economics, and virtually every other form of human organizational activity.  Examples of data sets include price indices (such as the consumer price index), unemployment rates, literacy rates, and census data. In this context, data represent the raw facts and figures from which useful information can be extracted.\nData are collected using techniques such as measurement, observation, query, or analysis, and are typically represented as numbers or characters that may be further processed. Field data are data that are collected in an uncontrolled, in-situ environment. Experimental data are data that are generated in the course of a controlled scientific experiment.  Data are analyzed using techniques such as calculation, reasoning, discussion, presentation, visualization, or other forms of post-analysis. Prior to analysis, raw data (or unprocessed data) is typically cleaned: Outliers are removed, and obvious instrument or data entry errors are corrected.\nData can be seen as the smallest units of factual information that can be used as a basis for calculation, reasoning, or discussion. Data can range from abstract ideas to concrete measurements, including, but not limited to, statistics. Thematically connected data presented in some relevant context can be viewed as information. Contextually connected pieces of information can then be described as data insights or intelligence. The stock of insights and intelligence that accumulate over time resulting from the synthesis of data into information, can then be described as knowledge. Data has been described as \"the new oil of the digital economy\". Data, as a general concept, refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing.\nAdvances in computing technologies have led to the advent of big data, which usually refers to very large quantities of data, usually at the petabyte scale. Using traditional data analysis methods and computing, working with such large (and growing) datasets is difficult, even impossible. (Theoretically speaking, infinite data would yield infinite information, which would render extracting insights or intelligence impossible.) In response, the relatively new field of data science uses machine learning (and other artificial intelligence) methods that allow for efficient applications of analytic methods to big data.\n\n\n== Etymology and terminology ==\n\nThe Latin word data is the plural of datum, \"(thing) given,\" and the neuter past participle of dare, \"to give\". The first English use of the word \"data\" is from the 1640s. The word \"data\" was first used to mean \"transmissible and storable computer information\" in 1946. The expression \"data processing\" was first used in 1954.\nWhen \"data\" is used more generally as a synonym for \"information\", it is treated as a mass noun in singular form.  This usage is common in everyday language and in technical and scientific fields such as software development and computer science. One example of this usage is the term \"big data\". When used more specifically to refer to the processing and analysis of sets of data, the term retains its plural form. This usage is common in the natural sciences, life sciences, social sciences, software development and computer science, and grew in popularity in the 20th and 21st centuries.  Some style guides do not recognize the different meanings of the term and simply recommend the form that best suits the target audience of the guide.  For example, APA style as of the 7th edition requires \"data\" to be treated as a plural form.\n\n\n== Meaning ==\n\nData, information, knowledge, and wisdom are closely related concepts, but each has its role concerning the other, and each term has its meaning. According to a common view, data is collected and analyzed; data only becomes information suitable for making decisions once it has been analyzed in some fashion. One can say that the extent to which a set of data is informative to someone depends on the extent to which it is unexpected by that person. The amount of information contained in a data stream may be characterized by its Shannon entropy.\nKnowledge is the awareness of its environment that some entity possesses, whereas data merely communicates that knowledge. For example, the entry in a database specifying the height of Mount Everest is a datum that communicates a precisely measured value. This measurement may be included in a book along with other data on Mount Everest to describe the mountain in a manner useful for those who wish to decide on the best method to climb it. Awareness of the characteristics represented by this data is knowledge.\nData are often assumed to be the least abstract concept, information the next least, and knowledge the most abstract. In this view, data becomes information by interpretation; e.g., the height of Mount Everest is generally considered \"data\", a book on Mount Everest geological characteristics may be considered \"information\", and a climber's guidebook containing practical information on the best way to reach Mount Everest's peak may be considered \"knowledge\". \"Information\" bears a diversity of meanings that range from everyday usage to technical use. This view, however, has also been argued to reverse how data emerges from information, and information from knowledge. Generally speaking, the concept of information is closely related to notions of constraint, communication, control, data, form, instruction, knowledge, meaning, mental stimulus, pattern, perception, and representation. Beynon-Davies uses the concept of a sign to differentiate between data and information; data is a series of symbols, while information occurs when the symbols are used to refer to something.\nBefore the development of computing devices and machines, people had to manually collect data and impose patterns on it. With the development of computing devices and machines, these devices can also collect data. In the 2010s, computers were widely used in many fields to collect data and sort or process it, in disciplines ranging from marketing, analysis of social service usage by citizens to scientific research. These patterns in the data are seen as information that can be used to enhance knowledge. These patterns may be interpreted as \"truth\" (though \"truth\" can be a subjective concept) and may be authorized as aesthetic and ethical criteria in some disciplines or cultures. Events that leave behind perceivable physical or virtual remains can be traced back through data. Marks are no longer considered data once the link between the mark and observation is broken.\nMechanical computing devices are classified according to how they represent data. An analog computer represents a datum as a voltage, distance, position, or other physical quantity. A digital computer represents a piece of data as a sequence of symbols drawn from a fixed alphabet. The most common digital computers use a binary alphabet, that is, an alphabet of two characters typically denoted \"0\" and \"1\". More familiar representations, such as numbers or letters, are then constructed from the binary alphabet. Some special forms of data are distinguished. A computer program is a collection of data, that can be interpreted as instructions. Most computer languages make a distinction between programs and the other data on which programs operate, but in some languages, notably Lisp and similar languages, programs are essentially indistinguishable from other data. It is also useful to distinguish metadata, that is, a description of other data. A similar yet earlier term for metadata is \"ancillary data.\"  The prototypical example of metadata is the library catalog, which is a description of the contents of books.\n\n\n== Data sources ==\nWith respect to ownership of data collected in the course of marketing or other corporate collection, data has been characterized according to \"party\" depending on how close the data is to the source or if it has been generated through additional processing. \"Zero-party data\" refers to data that customers \"intentionally and proactively shares\". This kind of data can come from a variety of sources, including: subscriptions, preference centers, quizzes, surveys, pop-up forms, and interactive digital experiences. \"First-party data\" may be collected by a company directly from its customers. The secure exchange of first-party data among companies can be done using data clean rooms. \"Second-party data\" refers to data obtained from other organizations or partners, through purchase or other means and has been described as \"another organization's first-party data\". \"Third-party data\" is data collected by other organizations and subsequently aggregated from different sources, websites, and platforms.\n\n\"No-party\" data can sometimes refer to synthetic data that is generated based on patterns from original data.\n\n\n== Data documents ==\n\nWhenever data needs to be registered, data exists in the form of a data document. Kinds of data documents include:\n\ndata repository\ndata study\ndata set\nsoftware\ndata paper\ndatabase\ndata handbook\ndata journal\nSome of these data documents (data repositories, data studies, data sets, and software) are indexed in Data Citation Indexes, while data papers are indexed in traditional bibliographic databases, e.g., Science Citation Index.\n\n\n=== Data collection ===\nGathering data can be accomplished through a primary source (the researcher is the first person to obtain the data) or a secondary source (the researcher obtains the data that has already been collected by other sources, such as data disseminated in a scientific journal). Data analysis methodologies vary and include data triangulation and data percolation. The latter offers an articulate method of collecting, classifying, and analyzing data using five possible angles of analysis (at least three) to maximize the research's objectivity and permit an understanding of the phenomena under investigation as complete as possible: qualitative and quantitative methods, literature reviews (including scholarly articles), interviews with experts, and computer simulation. The data is thereafter \"percolated\" using a series of pre-determined steps so as to extract the most relevant information.\n\n\n== Data longevity and accessibility ==\nAn important field in computer science, technology, and library science is the longevity of data. Scientific research generates huge amounts of data, especially in genomics and astronomy, but also in the medical sciences, e.g. in medical imaging. In the past, scientific data has been published in papers and books, stored in libraries, but more recently practically all data is stored on hard drives or optical discs. However, in contrast to paper, these storage devices may become unreadable after a few decades. Scientific publishers and libraries have been struggling with this problem for a few decades, and there is still no satisfactory solution for the long-term storage of data over centuries or even for eternity.\nData accessibility. Another problem is that much scientific data is never published or deposited in data repositories such as databases. In a recent survey, data was requested from 516 studies that were published between 2 and 22 years earlier, but less than one out of five of these studies were able or willing to provide the requested data. Overall, the likelihood of retrieving data dropped by 17% each year after publication. Similarly, a survey of 100 datasets in Dryad found that more than half lacked the details to reproduce the research results from these studies. This shows the dire situation of access to scientific data that is not published or does not have enough details to be reproduced.\nA solution to the problem of reproducibility is the attempt to require FAIR data, that is, data that is Findable, Accessible, Interoperable, and Reusable. Data that fulfills these requirements can be used in subsequent research and thus advances science and technology.\n\n\n== In other fields ==\nAlthough data is also increasingly used in other fields, it has been suggested that their highly interpretive nature might be at odds with the ethos of data as \"given\". Peter Checkland introduced the term capta (from the Latin capere, \"to take\") to distinguish between an immense number of possible data and a sub-set of them, to which attention is oriented. Johanna Drucker has argued that since the humanities affirm knowledge production as \"situated, partial, and constitutive,\" using data may introduce assumptions that are counterproductive, for example, that phenomena are discrete or are observer-independent. The term capta, which emphasizes the act of observation as constitutive, is offered as an alternative to data for visual representations in the humanities.\nThe term data-driven is a neologism applied to an activity which is primarily compelled by data over all other factors. Data-driven applications include data-driven programming and data-driven journalism.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nData is a singular noun (a detailed assessment)",
      "cleaned_text": "Data ( DAY-tə, US also DAT-ə) are a collection of discrete or continuous values that convey information, describing the quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted formally. A datum is an individual value in a collection of data. Data are usually organized into structures such as tables that provide additional context and meaning, and may themselves be used as data in larger structures. Data may be used as variables in a computational process. Data may represent abstract ideas or concrete measurements. Data are commonly used in scientific research, economics, and virtually every other form of human organizational activity. Examples of data sets include price indices (such as the consumer price index), unemployment rates, literacy rates, and census data. In this context, data represent the raw facts and figures from which useful information can be extracted. Data are collected using techniques such as measurement, observation, query, or analysis, and are typically represented as numbers or characters that may be further processed. Field data are data that are collected in an uncontrolled, in-situ environment. Experimental data are data that are generated in the course of a controlled scientific experiment. Data are analyzed using techniques such as calculation, reasoning, discussion, presentation, visualization, or other forms of post-analysis. Prior to analysis, raw data (or unprocessed data) is typically cleaned: Outliers are removed, and obvious instrument or data entry errors are corrected. Data can be seen as the smallest units of factual information that can be used as a basis for calculation, reasoning, or discussion. Data can range from abstract ideas to concrete measurements, including, but not limited to, statistics. Thematically connected data presented in some relevant context can be viewed as information. Contextually connected pieces of information can then be described as data insights or intelligence. The stock of insights and intelligence that accumulate over time resulting from the synthesis of data into information, can then be described as knowledge. Data has been described as \"the new oil of the digital economy\". Data, as a general concept, refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing. Advances in computing technologies have led to the advent of big data, which usually refers to very large quantities of data, usually at the petabyte scale. Using traditional data analysis methods and computing, working with such large (and growing) datasets is difficult, even impossible. (Theoretically speaking, infinite data would yield infinite information, which would render extracting insights or intelligence impossible.) In response, the relatively new field of data science uses machine learning (and other artificial intelligence) methods that allow for efficient applications of analytic methods to big data. The Latin word data is the plural of datum, \"(thing) given,\" and the neuter past participle of dare, \"to give\". The first English use of the word \"data\" is from the 1640s. The word \"data\" was first used to mean \"transmissible and storable computer information\" in 1946. The expression \"data processing\" was first used in 1954. When \"data\" is used more generally as a synonym for \"information\", it is treated as a mass noun in singular form. This usage is common in everyday language and in technical and scientific fields such as software development and computer science. One example of this usage is the term \"big data\". When used more specifically to refer to the processing and analysis of sets of data, the term retains its plural form. This usage is common in the natural sciences, life sciences, social sciences, software development and computer science, and grew in popularity in the 20th and 21st centuries. Some style guides do not recognize the different meanings of the term and simply recommend the form that best suits the target audience of the guide. For example, APA style as of the 7th edition requires \"data\" to be treated as a plural form. Data, information, knowledge, and wisdom are closely related concepts, but each has its role concerning the other, and each term has its meaning. According to a common view, data is collected and analyzed; data only becomes information suitable for making decisions once it has been analyzed in some fashion. One can say that the extent to which a set of data is informative to someone depends on the extent to which it is unexpected by that person. The amount of information contained in a data stream may be characterized by its Shannon entropy. Knowledge is the awareness of its environment that some entity possesses, whereas data merely communicates that knowledge. For example, the entry in a database specifying the height of Mount Everest is a datum that communicates a precisely measured value. This measurement may be included in a book along with other data on Mount Everest to describe the mountain in a manner useful for those who wish to decide on the best method to climb it. Awareness of the characteristics represented by this data is knowledge. Data are often assumed to be the least abstract concept, information the next least, and knowledge the most abstract. In this view, data becomes information by interpretation; e.g., the height of Mount Everest is generally considered \"data\", a book on Mount Everest geological characteristics may be considered \"information\", and a climber's guidebook containing practical information on the best way to reach Mount Everest's peak may be considered \"knowledge\". \"Information\" bears a diversity of meanings that range from everyday usage to technical use. This view, however, has also been argued to reverse how data emerges from information, and information from knowledge. Generally speaking, the concept of information is closely related to notions of constraint, communication, control, data, form, instruction, knowledge, meaning, mental stimulus, pattern, perception, and representation. Beynon-Davies uses the concept of a sign to differentiate between data and information; data is a series of symbols, while information occurs when the symbols are used to refer to something. Before the development of computing devices and machines, people had to manually collect data and impose patterns on it. With the development of computing devices and machines, these devices can also collect data. In the 2010s, computers were widely used in many fields to collect data and sort or process it, in disciplines ranging from marketing, analysis of social service usage by citizens to scientific research. These patterns in the data are seen as information that can be used to enhance knowledge. These patterns may be interpreted as \"truth\" (though \"truth\" can be a subjective concept) and may be authorized as aesthetic and ethical criteria in some disciplines or cultures. Events that leave behind perceivable physical or virtual remains can be traced back through data. Marks are no longer considered data once the link between the mark and observation is broken. Mechanical computing devices are classified according to how they represent data. An analog computer represents a datum as a voltage, distance, position, or other physical quantity. A digital computer represents a piece of data as a sequence of symbols drawn from a fixed alphabet. The most common digital computers use a binary alphabet, that is, an alphabet of two characters typically denoted \"0\" and \"1\". More familiar representations, such as numbers or letters, are then constructed from the binary alphabet. Some special forms of data are distinguished. A computer program is a collection of data, that can be interpreted as instructions. Most computer languages make a distinction between programs and the other data on which programs operate, but in some languages, notably Lisp and similar languages, programs are essentially indistinguishable from other data. It is also useful to distinguish metadata, that is, a description of other data. A similar yet earlier term for metadata is \"ancillary data.\" The prototypical example of metadata is the library catalog, which is a description of the contents of books. With respect to ownership of data collected in the course of marketing or other corporate collection, data has been characterized according to \"party\" depending on how close the data is to the source or if it has been generated through additional processing. \"Zero-party data\" refers to data that customers \"intentionally and proactively shares\". This kind of data can come from a variety of sources, including: subscriptions, preference centers, quizzes, surveys, pop-up forms, and interactive digital experiences. \"First-party data\" may be collected by a company directly from its customers. The secure exchange of first-party data among companies can be done using data clean rooms. \"Second-party data\" refers to data obtained from other organizations or partners, through purchase or other means and has been described as \"another organization's first-party data\". \"Third-party data\" is data collected by other organizations and subsequently aggregated from different sources, websites, and platforms. \"No-party\" data can sometimes refer to synthetic data that is generated based on patterns from original data. Whenever data needs to be registered, data exists in the form of a data document. Kinds of data documents include: data repository data study data set software data paper database data handbook data journal Some of these data documents (data repositories, data studies, data sets, and software) are indexed in Data Citation Indexes, while data papers are indexed in traditional bibliographic databases, e.g., Science Citation Index. Gathering data can be accomplished through a primary source (the researcher is the first person to obtain the data) or a secondary source (the researcher obtains the data that has already been collected by other sources, such as data disseminated in a scientific journal). Data analysis methodologies vary and include data triangulation and data percolation. The latter offers an articulate method of collecting, classifying, and analyzing data using five possible angles of analysis (at least three) to maximize the research's objectivity and permit an understanding of the phenomena under investigation as complete as possible: qualitative and quantitative methods, literature reviews (including scholarly articles), interviews with experts, and computer simulation. The data is thereafter \"percolated\" using a series of pre-determined steps so as to extract the most relevant information. An important field in computer science, technology, and library science is the longevity of data. Scientific research generates huge amounts of data, especially in genomics and astronomy, but also in the medical sciences, e.g. in medical imaging. In the past, scientific data has been published in papers and books, stored in libraries, but more recently practically all data is stored on hard drives or optical discs. However, in contrast to paper, these storage devices may become unreadable after a few decades. Scientific publishers and libraries have been struggling with this problem for a few decades, and there is still no satisfactory solution for the long-term storage of data over centuries or even for eternity. Data accessibility. Another problem is that much scientific data is never published or deposited in data repositories such as databases. In a recent survey, data was requested from 516 studies that were published between 2 and 22 years earlier, but less than one out of five of these studies were able or willing to provide the requested data. Overall, the likelihood of retrieving data dropped by 17% each year after publication. Similarly, a survey of 100 datasets in Dryad found that more than half lacked the details to reproduce the research results from these studies. This shows the dire situation of access to scientific data that is not published or does not have enough details to be reproduced. A solution to the problem of reproducibility is the attempt to require FAIR data, that is, data that is Findable, Accessible, Interoperable, and Reusable. Data that fulfills these requirements can be used in subsequent research and thus advances science and technology. Although data is also increasingly used in other fields, it has been suggested that their highly interpretive nature might be at odds with the ethos of data as \"given\". Peter Checkland introduced the term capta (from the Latin capere, \"to take\") to distinguish between an immense number of possible data and a sub-set of them, to which attention is oriented. Johanna Drucker has argued that since the humanities affirm knowledge production as \"situated, partial, and constitutive,\" using data may introduce assumptions that are counterproductive, for example, that phenomena are discrete or are observer-independent. The term capta, which emphasizes the act of observation as constitutive, is offered as an alternative to data for visual representations in the humanities. The term data-driven is a neologism applied to an activity which is primarily compelled by data over all other factors. Data-driven applications include data-driven programming and data-driven journalism.",
      "sentences": [
        "Data ( DAY-tə, US also DAT-ə) are a collection of discrete or continuous values that convey information, describing the quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted formally.",
        "A datum is an individual value in a collection of data.",
        "Data are usually organized into structures such as tables that provide additional context and meaning, and may themselves be used as data in larger structures.",
        "Data may be used as variables in a computational process.",
        "Data may represent abstract ideas or concrete measurements.",
        "Data are commonly used in scientific research, economics, and virtually every other form of human organizational activity.",
        "Examples of data sets include price indices (such as the consumer price index), unemployment rates, literacy rates, and census data.",
        "In this context, data represent the raw facts and figures from which useful information can be extracted.",
        "Data are collected using techniques such as measurement, observation, query, or analysis, and are typically represented as numbers or characters that may be further processed.",
        "Field data are data that are collected in an uncontrolled, in-situ environment.",
        "Experimental data are data that are generated in the course of a controlled scientific experiment.",
        "Data are analyzed using techniques such as calculation, reasoning, discussion, presentation, visualization, or other forms of post-analysis.",
        "Prior to analysis, raw data (or unprocessed data) is typically cleaned: Outliers are removed, and obvious instrument or data entry errors are corrected.",
        "Data can be seen as the smallest units of factual information that can be used as a basis for calculation, reasoning, or discussion.",
        "Data can range from abstract ideas to concrete measurements, including, but not limited to, statistics.",
        "Thematically connected data presented in some relevant context can be viewed as information.",
        "Contextually connected pieces of information can then be described as data insights or intelligence.",
        "The stock of insights and intelligence that accumulate over time resulting from the synthesis of data into information, can then be described as knowledge.",
        "Data has been described as \"the new oil of the digital economy\".",
        "Data, as a general concept, refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing.",
        "Advances in computing technologies have led to the advent of big data, which usually refers to very large quantities of data, usually at the petabyte scale.",
        "Using traditional data analysis methods and computing, working with such large (and growing) datasets is difficult, even impossible.",
        "(Theoretically speaking, infinite data would yield infinite information, which would render extracting insights or intelligence impossible.)",
        "In response, the relatively new field of data science uses machine learning (and other artificial intelligence) methods that allow for efficient applications of analytic methods to big data.",
        "The Latin word data is the plural of datum, \"(thing) given,\" and the neuter past participle of dare, \"to give\".",
        "The first English use of the word \"data\" is from the 1640s.",
        "The word \"data\" was first used to mean \"transmissible and storable computer information\" in 1946.",
        "The expression \"data processing\" was first used in 1954.",
        "When \"data\" is used more generally as a synonym for \"information\", it is treated as a mass noun in singular form.",
        "This usage is common in everyday language and in technical and scientific fields such as software development and computer science.",
        "One example of this usage is the term \"big data\".",
        "When used more specifically to refer to the processing and analysis of sets of data, the term retains its plural form.",
        "This usage is common in the natural sciences, life sciences, social sciences, software development and computer science, and grew in popularity in the 20th and 21st centuries.",
        "Some style guides do not recognize the different meanings of the term and simply recommend the form that best suits the target audience of the guide.",
        "For example, APA style as of the 7th edition requires \"data\" to be treated as a plural form.",
        "Data, information, knowledge, and wisdom are closely related concepts, but each has its role concerning the other, and each term has its meaning.",
        "According to a common view, data is collected and analyzed; data only becomes information suitable for making decisions once it has been analyzed in some fashion.",
        "One can say that the extent to which a set of data is informative to someone depends on the extent to which it is unexpected by that person.",
        "The amount of information contained in a data stream may be characterized by its Shannon entropy.",
        "Knowledge is the awareness of its environment that some entity possesses, whereas data merely communicates that knowledge.",
        "For example, the entry in a database specifying the height of Mount Everest is a datum that communicates a precisely measured value.",
        "This measurement may be included in a book along with other data on Mount Everest to describe the mountain in a manner useful for those who wish to decide on the best method to climb it.",
        "Awareness of the characteristics represented by this data is knowledge.",
        "Data are often assumed to be the least abstract concept, information the next least, and knowledge the most abstract.",
        "In this view, data becomes information by interpretation; e.g., the height of Mount Everest is generally considered \"data\", a book on Mount Everest geological characteristics may be considered \"information\", and a climber's guidebook containing practical information on the best way to reach Mount Everest's peak may be considered \"knowledge\".",
        "\"Information\" bears a diversity of meanings that range from everyday usage to technical use.",
        "This view, however, has also been argued to reverse how data emerges from information, and information from knowledge.",
        "Generally speaking, the concept of information is closely related to notions of constraint, communication, control, data, form, instruction, knowledge, meaning, mental stimulus, pattern, perception, and representation.",
        "Beynon-Davies uses the concept of a sign to differentiate between data and information; data is a series of symbols, while information occurs when the symbols are used to refer to something.",
        "Before the development of computing devices and machines, people had to manually collect data and impose patterns on it.",
        "With the development of computing devices and machines, these devices can also collect data.",
        "In the 2010s, computers were widely used in many fields to collect data and sort or process it, in disciplines ranging from marketing, analysis of social service usage by citizens to scientific research.",
        "These patterns in the data are seen as information that can be used to enhance knowledge.",
        "These patterns may be interpreted as \"truth\" (though \"truth\" can be a subjective concept) and may be authorized as aesthetic and ethical criteria in some disciplines or cultures.",
        "Events that leave behind perceivable physical or virtual remains can be traced back through data.",
        "Marks are no longer considered data once the link between the mark and observation is broken.",
        "Mechanical computing devices are classified according to how they represent data.",
        "An analog computer represents a datum as a voltage, distance, position, or other physical quantity.",
        "A digital computer represents a piece of data as a sequence of symbols drawn from a fixed alphabet.",
        "The most common digital computers use a binary alphabet, that is, an alphabet of two characters typically denoted \"0\" and \"1\".",
        "More familiar representations, such as numbers or letters, are then constructed from the binary alphabet.",
        "Some special forms of data are distinguished.",
        "A computer program is a collection of data, that can be interpreted as instructions.",
        "Most computer languages make a distinction between programs and the other data on which programs operate, but in some languages, notably Lisp and similar languages, programs are essentially indistinguishable from other data.",
        "It is also useful to distinguish metadata, that is, a description of other data.",
        "A similar yet earlier term for metadata is \"ancillary data.\"",
        "The prototypical example of metadata is the library catalog, which is a description of the contents of books.",
        "With respect to ownership of data collected in the course of marketing or other corporate collection, data has been characterized according to \"party\" depending on how close the data is to the source or if it has been generated through additional processing.",
        "\"Zero-party data\" refers to data that customers \"intentionally and proactively shares\".",
        "This kind of data can come from a variety of sources, including: subscriptions, preference centers, quizzes, surveys, pop-up forms, and interactive digital experiences.",
        "\"First-party data\" may be collected by a company directly from its customers.",
        "The secure exchange of first-party data among companies can be done using data clean rooms.",
        "\"Second-party data\" refers to data obtained from other organizations or partners, through purchase or other means and has been described as \"another organization's first-party data\".",
        "\"Third-party data\" is data collected by other organizations and subsequently aggregated from different sources, websites, and platforms.",
        "\"No-party\" data can sometimes refer to synthetic data that is generated based on patterns from original data.",
        "Whenever data needs to be registered, data exists in the form of a data document.",
        "Kinds of data documents include: data repository data study data set software data paper database data handbook data journal Some of these data documents (data repositories, data studies, data sets, and software) are indexed in Data Citation Indexes, while data papers are indexed in traditional bibliographic databases, e.g., Science Citation Index.",
        "Gathering data can be accomplished through a primary source (the researcher is the first person to obtain the data) or a secondary source (the researcher obtains the data that has already been collected by other sources, such as data disseminated in a scientific journal).",
        "Data analysis methodologies vary and include data triangulation and data percolation.",
        "The latter offers an articulate method of collecting, classifying, and analyzing data using five possible angles of analysis (at least three) to maximize the research's objectivity and permit an understanding of the phenomena under investigation as complete as possible: qualitative and quantitative methods, literature reviews (including scholarly articles), interviews with experts, and computer simulation.",
        "The data is thereafter \"percolated\" using a series of pre-determined steps so as to extract the most relevant information.",
        "An important field in computer science, technology, and library science is the longevity of data.",
        "Scientific research generates huge amounts of data, especially in genomics and astronomy, but also in the medical sciences, e.g.",
        "in medical imaging.",
        "In the past, scientific data has been published in papers and books, stored in libraries, but more recently practically all data is stored on hard drives or optical discs.",
        "However, in contrast to paper, these storage devices may become unreadable after a few decades.",
        "Scientific publishers and libraries have been struggling with this problem for a few decades, and there is still no satisfactory solution for the long-term storage of data over centuries or even for eternity.",
        "Data accessibility.",
        "Another problem is that much scientific data is never published or deposited in data repositories such as databases.",
        "In a recent survey, data was requested from 516 studies that were published between 2 and 22 years earlier, but less than one out of five of these studies were able or willing to provide the requested data.",
        "Overall, the likelihood of retrieving data dropped by 17% each year after publication.",
        "Similarly, a survey of 100 datasets in Dryad found that more than half lacked the details to reproduce the research results from these studies.",
        "This shows the dire situation of access to scientific data that is not published or does not have enough details to be reproduced.",
        "A solution to the problem of reproducibility is the attempt to require FAIR data, that is, data that is Findable, Accessible, Interoperable, and Reusable.",
        "Data that fulfills these requirements can be used in subsequent research and thus advances science and technology.",
        "Although data is also increasingly used in other fields, it has been suggested that their highly interpretive nature might be at odds with the ethos of data as \"given\".",
        "Peter Checkland introduced the term capta (from the Latin capere, \"to take\") to distinguish between an immense number of possible data and a sub-set of them, to which attention is oriented.",
        "Johanna Drucker has argued that since the humanities affirm knowledge production as \"situated, partial, and constitutive,\" using data may introduce assumptions that are counterproductive, for example, that phenomena are discrete or are observer-independent.",
        "The term capta, which emphasizes the act of observation as constitutive, is offered as an alternative to data for visual representations in the humanities.",
        "The term data-driven is a neologism applied to an activity which is primarily compelled by data over all other factors.",
        "Data-driven applications include data-driven programming and data-driven journalism."
      ],
      "metadata": {
        "title": "Data",
        "url": "https://en.wikipedia.org/wiki/Data",
        "word_count": 2080,
        "char_count": 13476,
        "sentence_count": 101,
        "scraped_at": "2025-08-09T14:47:14.296568",
        "language": "en",
        "processing_time": 0.002847909927368164,
        "source_hash": "c10dba94d857adf26cd92f412a826692"
      }
    },
    {
      "title": "Biomedical data science",
      "url": "https://en.wikipedia.org/wiki/Biomedical_data_science",
      "raw_text": "Biomedical data science is a multidisciplinary field which leverages large volumes of data to promote biomedical innovation and discovery. Biomedical data science draws from various fields including Biostatistics, Biomedical informatics, and machine learning, with the goal of understanding biological and medical data. It can be viewed as the study and application of data science to solve biomedical problems. Modern biomedical datasets often have specific features which make their analyses difficult, including:\n\nLarge numbers of feature (sometimes billions), typically far larger than the number of samples (typically tens or hundreds)\nNoisy and missing data\nPrivacy concerns (e.g., electronic health record confidentiality)\nRequirement of interpretability from decision makers and regulatory bodies\nMany biomedical data science projects apply machine learning to such datasets. These characteristics, while also present in many data science applications more generally, make biomedical data science a specific field. Examples of biomedical data science research include: \n\nComputational genomics\nComputational imaging\nElectronic health records data mining\nBiomedical network science\n\n\n== Training in Biomedical Data Science ==\nThe National Library of Medicine of the US National Institutes of Health (NIH) identified key biomedical data scientist attributes in an NIH-wide review: general biomedical subject matter knowledge; programming language expertise; predictive analytics, modeling, and machine learning; team science and communication; and responsible data stewardship.\n\n\n=== University Departments and Programs ===\nJohns Hopkins University’s Department of Biomedical Engineering offers biomedical data science training at the undergraduate, master's, and PhD levels. They were the first university to offer programs at both undergraduate and graduate levels.\nDartmouth College's Geisel School of Medicine houses the Department of Biomedical Data Science where Quantitative Biomedical Sciences programs are available at the master's and PhD levels.\nImperial College London’s Faculty of Medicine and Data Science Institute offer an MRes in Biomedical Research (Data Science).\nMount Sinai’s Icahn School of Medicine offers a Master of Science in Biomedical Data Science.\nStanford University’s Department of Biomedical Data Science offers multiple biomedical informatics graduate programs (MS, PhD, and MD/PhD).\nThe University of Exeter’s College of Healthcare and Medicine offers an MSc in Health Data Science.\n\n\n== Biomedical Data Science Research in Academia ==\n\n\n=== Scholarly Journals ===\nThe first journal dedicated to biomedical data science appeared in 2018 – Annual Review of Biomedical Data Science. “The Annual Review of Biomedical Data Science provides comprehensive expert reviews in biomedical data science, focusing on advanced methods to store, retrieve, analyze, and organize biomedical data and knowledge. The scope of the journal encompasses informatics, computational, and statistical approaches to biomedical data, including the sub-fields of bioinformatics, computational biology, biomedical informatics, clinical and clinical research informatics, biostatistics, and imaging informatics. The mission of the journal is to identify both emerging and established areas of biomedical data science, and the leaders in these fields.” \nOther journals have a more general scope than biomedical data science, but regularly publish biomedical data science research such as Health Data Science and Nature Machine Intelligence. Data science would not exist without curated datasets and the field has seen the rise of journals that are dedicated to describing and validating such datasets, some of which are useful for biomedical applications, including Scientific Data, Biomedical Data, and Data.\n\n\n== Example ==\nThe Human Genome Project (HGP), which uncovered the DNA sequences that compose human genes, would not have been possible without biomedical data science. Significant computational resources were required to process the data in the HGP, as the human genome contains over 6 billion DNA base pairs. Scientists constructed the genome by piecing together small fragments of DNA, and computing overlaps between these sequences alone required over 10,000 CPU hours. At this massive data scale, scientists relied on advanced algorithms to perform data processing steps such as sequence assembly and sequence alignment for quality control. Some of these algorithms, such as BLAST, are still used in modern bioinformatics. Scientists in the HGP also had to address complexities often associated with biomedical data including noisy data, such as DNA read errors, and privacy rights of the research subjects. The HGP, completed in 2004, has had immense impact both biologically, shedding light on human evolution, and medically, launching the field of bioinformatics and leading to technologies such as genetic screening and gene therapy.\n\n\n== References ==",
      "cleaned_text": "Biomedical data science is a multidisciplinary field which leverages large volumes of data to promote biomedical innovation and discovery. Biomedical data science draws from various fields including Biostatistics, Biomedical informatics, and machine learning, with the goal of understanding biological and medical data. It can be viewed as the study and application of data science to solve biomedical problems. Modern biomedical datasets often have specific features which make their analyses difficult, including: Large numbers of feature (sometimes billions), typically far larger than the number of samples (typically tens or hundreds) Noisy and missing data Privacy concerns (e.g., electronic health record confidentiality) Requirement of interpretability from decision makers and regulatory bodies Many biomedical data science projects apply machine learning to such datasets. These characteristics, while also present in many data science applications more generally, make biomedical data science a specific field. Examples of biomedical data science research include: Computational genomics Computational imaging Electronic health records data mining Biomedical network science The National Library of Medicine of the US National Institutes of Health (NIH) identified key biomedical data scientist attributes in an NIH-wide review: general biomedical subject matter knowledge; programming language expertise; predictive analytics, modeling, and machine learning; team science and communication; and responsible data stewardship. Johns Hopkins University’s Department of Biomedical Engineering offers biomedical data science training at the undergraduate, master's, and PhD levels. They were the first university to offer programs at both undergraduate and graduate levels. Dartmouth College's Geisel School of Medicine houses the Department of Biomedical Data Science where Quantitative Biomedical Sciences programs are available at the master's and PhD levels. Imperial College London’s Faculty of Medicine and Data Science Institute offer an MRes in Biomedical Research (Data Science). Mount Sinai’s Icahn School of Medicine offers a Master of Science in Biomedical Data Science. Stanford University’s Department of Biomedical Data Science offers multiple biomedical informatics graduate programs (MS, PhD, and MD/PhD). The University of Exeter’s College of Healthcare and Medicine offers an MSc in Health Data Science. The first journal dedicated to biomedical data science appeared in 2018 - Annual Review of Biomedical Data Science. “The Annual Review of Biomedical Data Science provides comprehensive expert reviews in biomedical data science, focusing on advanced methods to store, retrieve, analyze, and organize biomedical data and knowledge. The scope of the journal encompasses informatics, computational, and statistical approaches to biomedical data, including the sub-fields of bioinformatics, computational biology, biomedical informatics, clinical and clinical research informatics, biostatistics, and imaging informatics. The mission of the journal is to identify both emerging and established areas of biomedical data science, and the leaders in these fields.” Other journals have a more general scope than biomedical data science, but regularly publish biomedical data science research such as Health Data Science and Nature Machine Intelligence. Data science would not exist without curated datasets and the field has seen the rise of journals that are dedicated to describing and validating such datasets, some of which are useful for biomedical applications, including Scientific Data, Biomedical Data, and Data. The Human Genome Project (HGP), which uncovered the DNA sequences that compose human genes, would not have been possible without biomedical data science. Significant computational resources were required to process the data in the HGP, as the human genome contains over 6 billion DNA base pairs. Scientists constructed the genome by piecing together small fragments of DNA, and computing overlaps between these sequences alone required over 10,000 CPU hours. At this massive data scale, scientists relied on advanced algorithms to perform data processing steps such as sequence assembly and sequence alignment for quality control. Some of these algorithms, such as BLAST, are still used in modern bioinformatics. Scientists in the HGP also had to address complexities often associated with biomedical data including noisy data, such as DNA read errors, and privacy rights of the research subjects. The HGP, completed in 2004, has had immense impact both biologically, shedding light on human evolution, and medically, launching the field of bioinformatics and leading to technologies such as genetic screening and gene therapy.",
      "sentences": [
        "Biomedical data science is a multidisciplinary field which leverages large volumes of data to promote biomedical innovation and discovery.",
        "Biomedical data science draws from various fields including Biostatistics, Biomedical informatics, and machine learning, with the goal of understanding biological and medical data.",
        "It can be viewed as the study and application of data science to solve biomedical problems.",
        "These characteristics, while also present in many data science applications more generally, make biomedical data science a specific field.",
        "Johns Hopkins University’s Department of Biomedical Engineering offers biomedical data science training at the undergraduate, master's, and PhD levels.",
        "They were the first university to offer programs at both undergraduate and graduate levels.",
        "Dartmouth College's Geisel School of Medicine houses the Department of Biomedical Data Science where Quantitative Biomedical Sciences programs are available at the master's and PhD levels.",
        "Imperial College London’s Faculty of Medicine and Data Science Institute offer an MRes in Biomedical Research (Data Science).",
        "Mount Sinai’s Icahn School of Medicine offers a Master of Science in Biomedical Data Science.",
        "Stanford University’s Department of Biomedical Data Science offers multiple biomedical informatics graduate programs (MS, PhD, and MD/PhD).",
        "The University of Exeter’s College of Healthcare and Medicine offers an MSc in Health Data Science.",
        "The first journal dedicated to biomedical data science appeared in 2018 - Annual Review of Biomedical Data Science.",
        "“The Annual Review of Biomedical Data Science provides comprehensive expert reviews in biomedical data science, focusing on advanced methods to store, retrieve, analyze, and organize biomedical data and knowledge.",
        "The scope of the journal encompasses informatics, computational, and statistical approaches to biomedical data, including the sub-fields of bioinformatics, computational biology, biomedical informatics, clinical and clinical research informatics, biostatistics, and imaging informatics.",
        "The mission of the journal is to identify both emerging and established areas of biomedical data science, and the leaders in these fields.” Other journals have a more general scope than biomedical data science, but regularly publish biomedical data science research such as Health Data Science and Nature Machine Intelligence.",
        "Data science would not exist without curated datasets and the field has seen the rise of journals that are dedicated to describing and validating such datasets, some of which are useful for biomedical applications, including Scientific Data, Biomedical Data, and Data.",
        "The Human Genome Project (HGP), which uncovered the DNA sequences that compose human genes, would not have been possible without biomedical data science.",
        "Significant computational resources were required to process the data in the HGP, as the human genome contains over 6 billion DNA base pairs.",
        "Scientists constructed the genome by piecing together small fragments of DNA, and computing overlaps between these sequences alone required over 10,000 CPU hours.",
        "At this massive data scale, scientists relied on advanced algorithms to perform data processing steps such as sequence assembly and sequence alignment for quality control.",
        "Some of these algorithms, such as BLAST, are still used in modern bioinformatics.",
        "Scientists in the HGP also had to address complexities often associated with biomedical data including noisy data, such as DNA read errors, and privacy rights of the research subjects.",
        "The HGP, completed in 2004, has had immense impact both biologically, shedding light on human evolution, and medically, launching the field of bioinformatics and leading to technologies such as genetic screening and gene therapy."
      ],
      "metadata": {
        "title": "Biomedical data science",
        "url": "https://en.wikipedia.org/wiki/Biomedical_data_science",
        "word_count": 668,
        "char_count": 4770,
        "sentence_count": 23,
        "scraped_at": "2025-08-09T14:47:14.297456",
        "language": "en",
        "processing_time": 0.0008089542388916016,
        "source_hash": "5bc482f420490ea542d8a983a47a3fa1"
      }
    },
    {
      "title": "Data (computer science)",
      "url": "https://en.wikipedia.org/wiki/Data_(computer_science)",
      "raw_text": "In computer science, data (treated as singular, plural, or as a mass noun) is any sequence of one or more symbols; datum is a single symbol of data. Data requires interpretation  to become information. Digital data is data that is represented using the binary number system of ones (1) and zeros (0), instead of analog representation. In modern (post-1960) computer systems, all data is digital.\nData exists in three states: data at rest, data in transit and data in use. Data within a computer, in most cases, moves as parallel data. Data moving to or from a computer, in most cases, moves as serial data. Data sourced from an analog device, such as a temperature sensor, may be converted to digital using an analog-to-digital converter. Data representing quantities, characters, or symbols on which operations are performed by a computer are stored and recorded on magnetic, optical, electronic, or mechanical recording media, and transmitted in the form of digital electrical or optical signals. Data pass in and out of computers via peripheral devices.\nPhysical computer memory elements consist of an address and a byte/word of data storage. Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs. Data can be organized in many different types of data structures, including arrays, graphs, and objects. Data structures can store data of many different types, including numbers, strings and even other data structures.\n\n\n== Characteristics ==\nMetadata helps translate data to information. Metadata is data about the data. Metadata may be implied, specified or given.  \nData relating to physical events or processes will have a temporal component. This temporal component may be implied. This is the case when a device such as a temperature logger receives data from a temperature sensor. When the temperature is received it is assumed that the data has a temporal reference of now. So the device records the date, time and temperature together. When the data logger communicates temperatures, it must also report the date and time as metadata for each temperature reading.\nFundamentally, computers follow a sequence of instructions they are given in the form of data. A set of instructions to perform a given task (or tasks) is called a program. A program is data in the form of coded instructions to control the operation of a computer or other machine. In the nominal case, the program, as executed by the computer, will consist of machine code. The elements of storage manipulated by the program, but not actually executed by the central processing unit (CPU), are also data. At its most essential, a single datum is a value stored at a specific location. Therefore, it is possible for computer programs to operate on other computer programs, by manipulating their programmatic data.\nTo store data bytes in a file, they have to be serialized in a file format. Typically, programs are stored in special file types, different from those used for other data. Executable files contain programs; all other files are also data files. However, executable files may also contain data used by the program which is built into the program. In particular, some executable files have a data segment, which nominally contains constants and initial values for variables, both of which can be considered data.\nThe line between program and data can become blurry. An interpreter, for example, is a program. The input data to an interpreter is itself a program, just not one expressed in native machine language. In many cases, the interpreted program will be a human-readable text file, which is manipulated with a text editor program. Metaprogramming similarly involves programs manipulating other programs as data. Programs like compilers, linkers, debuggers, program updaters, virus scanners and such use other programs as their data.\nFor example, a user might first instruct the operating system to load a word processor program from one file, and then use the running program to open and edit a document stored in another file. In this example, the document would be considered data. If the word processor also features a spell checker, then the dictionary (word list) for the spell checker would also be considered data. The algorithms used by the spell checker to suggest corrections would be either machine code data or text in some interpretable programming language.\nIn an alternate usage, binary files (which are not human-readable) are sometimes called data as distinguished from human-readable text. \nThe total amount of digital data in 2007 was estimated to be 281 billion gigabytes (281 exabytes).\n\n\n== Data keys and values, structures and persistence ==\nKeys in data provide the context for values. Regardless of the structure of data, there is always a key component present. Keys in data and data-structures are essential for giving meaning to data values. Without a key that is directly or indirectly associated with a value, or collection of values in a structure, the values become meaningless and cease to be data. That is to say, there has to be a key component linked to a value component in order for it to be considered data.\nData can be represented in computers in multiple ways, as per the following examples:\n\n\n=== RAM ===\nRandom access memory (RAM) holds data that the CPU has direct access to. A CPU may only manipulate data within its processor registers or memory. This is as opposed to data storage, where the CPU must direct the transfer of data between the storage device (disk, tape...) and memory. RAM is an array of linear contiguous locations that a processor may read or write by providing an address for the read or write operation. The processor may operate on any location in memory at any time in any order. In RAM the smallest element of data is the binary bit. The capabilities and limitations of accessing RAM are processor specific. In general main memory is arranged as an array of locations beginning at address 0 (hexadecimal 0). Each location can store usually 8 or 32 bits depending on the computer architecture.\n\n\n=== Keys ===\nData keys need not be a direct hardware address in memory. Indirect, abstract and logical keys codes can be stored in association with values to form a data structure. Data structures have predetermined offsets (or links or paths) from the start of the structure, in which data values are stored. Therefore, the data key consists of the key to the structure plus the offset (or links or paths) into the structure. When such a structure is repeated, storing variations of the data values and the data keys within the same repeating structure, the result can be considered to resemble a table, in which each element of the repeating structure is considered to be a column and each repetition of the structure is considered as a row of the table. In such an organization of data, the data key is usually a value in one (or a composite of the values in several) of the columns.\n\n\n=== Organised recurring data structures ===\nThe tabular view of repeating data structures is only one of many possibilities. Repeating data structures can be organised hierarchically, such that nodes are linked to each other in a cascade of parent-child relationships. Values and potentially more complex data-structures are linked to the nodes. Thus the nodal hierarchy provides the key for addressing the data structures associated with the nodes. This representation can be thought of as an inverted tree. Modern computer operating system file systems are a common example; and XML is another.\n\n\n=== Sorted or ordered data ===\nData has some inherent features when it is sorted on a key. All the values for subsets of the key appear together. When passing sequentially through groups of the data with the same key, or a subset of the key changes, this is referred to in data processing circles as a break, or a control break. It particularly facilitates the aggregation of data values on subsets of a key.\n\n\n=== Peripheral storage ===\nUntil the advent of bulk non-volatile memory like flash, persistent data storage was traditionally achieved by writing the data to external block devices like magnetic tape and disk drives. These devices typically seek to a location on the magnetic media and then read or write blocks of data of a predetermined size. In this case, the seek location on the media, is the data key and the blocks are the data values. Early used raw disk data file-systems or disc operating systems reserved contiguous blocks on the disc drive for data files. In those systems, the files could be filled up, running out of data space before all the data had been written to them. Thus much unused data space was reserved unproductively to ensure adequate free space for each file. Later file-systems introduced partitions. They reserved blocks of disc data space for partitions and used the allocated blocks more economically, by dynamically assigning blocks of a partition to a file as needed. To achieve this, the file system had to keep track of which blocks were used or unused by data files in a catalog or file allocation table. Though this made better use of the disc data space, it resulted in fragmentation of files across the disc, and a concomitant performance overhead due additional seek time to read the data. Modern file systems reorganize fragmented files dynamically to optimize file access times. Further developments in file systems resulted in virtualization of disc drives i.e. where a logical drive can be defined as partitions from a number of physical drives.\n\n\n=== Indexed data ===\nRetrieving a small subset of data from a much larger set may imply inefficiently searching through the data sequentially. Indexes are a way to copy out keys and location addresses from data structures in files, tables and data sets, then organize them using inverted tree structures to reduce the time taken to retrieve a subset of the original data. In order to do this, the key of the subset of data to be retrieved must be known before retrieval begins. The most popular indexes are the B-tree and the dynamic hash key indexing methods. Indexing is overhead for filing and retrieving data. There are other ways of organizing indexes, e.g. sorting the keys and using a binary search algorithm.\n\n\n=== Abstraction and indirection ===\nObject-oriented programming uses two basic concepts for understanding data and software:\nThe taxonomic rank-structure of classes, which is an example of a hierarchical data structure; and\nat run time, the creation of references to in-memory data-structures of objects that have been instantiated from a class library.\nIt is only after instantiation that an object of a specified class exists. After an object's reference is cleared, the object also ceases to exist. The memory locations where the object's data was stored are garbage and are reclassified as unused memory available for reuse.\n\n\n=== Database data ===\nThe advent of databases introduced a further layer of abstraction for persistent data storage. Databases use metadata, and a structured query language protocol between client and server systems, communicating over a computer network, using a two phase commit logging system to ensure transactional completeness, when saving data.\n\n\n=== Parallel distributed data processing ===\nModern scalable and high-performance data persistence technologies, such as Apache Hadoop, rely on massively parallel distributed data processing across many commodity computers on a high bandwidth network. In such systems, the data is distributed across multiple computers and therefore any particular computer in the system must be represented in the key of the data, either directly, or indirectly. This enables the differentiation between two identical sets of data, each being processed on a different computer at the same time.\n\n\n== See also ==\n\n\n== References ==",
      "cleaned_text": "In computer science, data (treated as singular, plural, or as a mass noun) is any sequence of one or more symbols; datum is a single symbol of data. Data requires interpretation to become information. Digital data is data that is represented using the binary number system of ones (1) and zeros (0), instead of analog representation. In modern (post-1960) computer systems, all data is digital. Data exists in three states: data at rest, data in transit and data in use. Data within a computer, in most cases, moves as parallel data. Data moving to or from a computer, in most cases, moves as serial data. Data sourced from an analog device, such as a temperature sensor, may be converted to digital using an analog-to-digital converter. Data representing quantities, characters, or symbols on which operations are performed by a computer are stored and recorded on magnetic, optical, electronic, or mechanical recording media, and transmitted in the form of digital electrical or optical signals. Data pass in and out of computers via peripheral devices. Physical computer memory elements consist of an address and a byte/word of data storage. Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs. Data can be organized in many different types of data structures, including arrays, graphs, and objects. Data structures can store data of many different types, including numbers, strings and even other data structures. Metadata helps translate data to information. Metadata is data about the data. Metadata may be implied, specified or given. Data relating to physical events or processes will have a temporal component. This temporal component may be implied. This is the case when a device such as a temperature logger receives data from a temperature sensor. When the temperature is received it is assumed that the data has a temporal reference of now. So the device records the date, time and temperature together. When the data logger communicates temperatures, it must also report the date and time as metadata for each temperature reading. Fundamentally, computers follow a sequence of instructions they are given in the form of data. A set of instructions to perform a given task (or tasks) is called a program. A program is data in the form of coded instructions to control the operation of a computer or other machine. In the nominal case, the program, as executed by the computer, will consist of machine code. The elements of storage manipulated by the program, but not actually executed by the central processing unit (CPU), are also data. At its most essential, a single datum is a value stored at a specific location. Therefore, it is possible for computer programs to operate on other computer programs, by manipulating their programmatic data. To store data bytes in a file, they have to be serialized in a file format. Typically, programs are stored in special file types, different from those used for other data. Executable files contain programs; all other files are also data files. However, executable files may also contain data used by the program which is built into the program. In particular, some executable files have a data segment, which nominally contains constants and initial values for variables, both of which can be considered data. The line between program and data can become blurry. An interpreter, for example, is a program. The input data to an interpreter is itself a program, just not one expressed in native machine language. In many cases, the interpreted program will be a human-readable text file, which is manipulated with a text editor program. Metaprogramming similarly involves programs manipulating other programs as data. Programs like compilers, linkers, debuggers, program updaters, virus scanners and such use other programs as their data. For example, a user might first instruct the operating system to load a word processor program from one file, and then use the running program to open and edit a document stored in another file. In this example, the document would be considered data. If the word processor also features a spell checker, then the dictionary (word list) for the spell checker would also be considered data. The algorithms used by the spell checker to suggest corrections would be either machine code data or text in some interpretable programming language. In an alternate usage, binary files (which are not human-readable) are sometimes called data as distinguished from human-readable text. The total amount of digital data in 2007 was estimated to be 281 billion gigabytes (281 exabytes). Keys in data provide the context for values. Regardless of the structure of data, there is always a key component present. Keys in data and data-structures are essential for giving meaning to data values. Without a key that is directly or indirectly associated with a value, or collection of values in a structure, the values become meaningless and cease to be data. That is to say, there has to be a key component linked to a value component in order for it to be considered data. Data can be represented in computers in multiple ways, as per the following examples: Random access memory (RAM) holds data that the CPU has direct access to. A CPU may only manipulate data within its processor registers or memory. This is as opposed to data storage, where the CPU must direct the transfer of data between the storage device (disk, tape...) and memory. RAM is an array of linear contiguous locations that a processor may read or write by providing an address for the read or write operation. The processor may operate on any location in memory at any time in any order. In RAM the smallest element of data is the binary bit. The capabilities and limitations of accessing RAM are processor specific. In general main memory is arranged as an array of locations beginning at address 0 (hexadecimal 0). Each location can store usually 8 or 32 bits depending on the computer architecture. Data keys need not be a direct hardware address in memory. Indirect, abstract and logical keys codes can be stored in association with values to form a data structure. Data structures have predetermined offsets (or links or paths) from the start of the structure, in which data values are stored. Therefore, the data key consists of the key to the structure plus the offset (or links or paths) into the structure. When such a structure is repeated, storing variations of the data values and the data keys within the same repeating structure, the result can be considered to resemble a table, in which each element of the repeating structure is considered to be a column and each repetition of the structure is considered as a row of the table. In such an organization of data, the data key is usually a value in one (or a composite of the values in several) of the columns. The tabular view of repeating data structures is only one of many possibilities. Repeating data structures can be organised hierarchically, such that nodes are linked to each other in a cascade of parent-child relationships. Values and potentially more complex data-structures are linked to the nodes. Thus the nodal hierarchy provides the key for addressing the data structures associated with the nodes. This representation can be thought of as an inverted tree. Modern computer operating system file systems are a common example; and XML is another. Data has some inherent features when it is sorted on a key. All the values for subsets of the key appear together. When passing sequentially through groups of the data with the same key, or a subset of the key changes, this is referred to in data processing circles as a break, or a control break. It particularly facilitates the aggregation of data values on subsets of a key. Until the advent of bulk non-volatile memory like flash, persistent data storage was traditionally achieved by writing the data to external block devices like magnetic tape and disk drives. These devices typically seek to a location on the magnetic media and then read or write blocks of data of a predetermined size. In this case, the seek location on the media, is the data key and the blocks are the data values. Early used raw disk data file-systems or disc operating systems reserved contiguous blocks on the disc drive for data files. In those systems, the files could be filled up, running out of data space before all the data had been written to them. Thus much unused data space was reserved unproductively to ensure adequate free space for each file. Later file-systems introduced partitions. They reserved blocks of disc data space for partitions and used the allocated blocks more economically, by dynamically assigning blocks of a partition to a file as needed. To achieve this, the file system had to keep track of which blocks were used or unused by data files in a catalog or file allocation table. Though this made better use of the disc data space, it resulted in fragmentation of files across the disc, and a concomitant performance overhead due additional seek time to read the data. Modern file systems reorganize fragmented files dynamically to optimize file access times. Further developments in file systems resulted in virtualization of disc drives i.e. where a logical drive can be defined as partitions from a number of physical drives. Retrieving a small subset of data from a much larger set may imply inefficiently searching through the data sequentially. Indexes are a way to copy out keys and location addresses from data structures in files, tables and data sets, then organize them using inverted tree structures to reduce the time taken to retrieve a subset of the original data. In order to do this, the key of the subset of data to be retrieved must be known before retrieval begins. The most popular indexes are the B-tree and the dynamic hash key indexing methods. Indexing is overhead for filing and retrieving data. There are other ways of organizing indexes, e.g. sorting the keys and using a binary search algorithm. Object-oriented programming uses two basic concepts for understanding data and software: The taxonomic rank-structure of classes, which is an example of a hierarchical data structure; and at run time, the creation of references to in-memory data-structures of objects that have been instantiated from a class library. It is only after instantiation that an object of a specified class exists. After an object's reference is cleared, the object also ceases to exist. The memory locations where the object's data was stored are garbage and are reclassified as unused memory available for reuse. The advent of databases introduced a further layer of abstraction for persistent data storage. Databases use metadata, and a structured query language protocol between client and server systems, communicating over a computer network, using a two phase commit logging system to ensure transactional completeness, when saving data. Modern scalable and high-performance data persistence technologies, such as Apache Hadoop, rely on massively parallel distributed data processing across many commodity computers on a high bandwidth network. In such systems, the data is distributed across multiple computers and therefore any particular computer in the system must be represented in the key of the data, either directly, or indirectly. This enables the differentiation between two identical sets of data, each being processed on a different computer at the same time.",
      "sentences": [
        "In computer science, data (treated as singular, plural, or as a mass noun) is any sequence of one or more symbols; datum is a single symbol of data.",
        "Data requires interpretation to become information.",
        "Digital data is data that is represented using the binary number system of ones (1) and zeros (0), instead of analog representation.",
        "In modern (post-1960) computer systems, all data is digital.",
        "Data exists in three states: data at rest, data in transit and data in use.",
        "Data within a computer, in most cases, moves as parallel data.",
        "Data moving to or from a computer, in most cases, moves as serial data.",
        "Data sourced from an analog device, such as a temperature sensor, may be converted to digital using an analog-to-digital converter.",
        "Data representing quantities, characters, or symbols on which operations are performed by a computer are stored and recorded on magnetic, optical, electronic, or mechanical recording media, and transmitted in the form of digital electrical or optical signals.",
        "Data pass in and out of computers via peripheral devices.",
        "Physical computer memory elements consist of an address and a byte/word of data storage.",
        "Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs.",
        "Data can be organized in many different types of data structures, including arrays, graphs, and objects.",
        "Data structures can store data of many different types, including numbers, strings and even other data structures.",
        "Metadata helps translate data to information.",
        "Metadata is data about the data.",
        "Metadata may be implied, specified or given.",
        "Data relating to physical events or processes will have a temporal component.",
        "This temporal component may be implied.",
        "This is the case when a device such as a temperature logger receives data from a temperature sensor.",
        "When the temperature is received it is assumed that the data has a temporal reference of now.",
        "So the device records the date, time and temperature together.",
        "When the data logger communicates temperatures, it must also report the date and time as metadata for each temperature reading.",
        "Fundamentally, computers follow a sequence of instructions they are given in the form of data.",
        "A set of instructions to perform a given task (or tasks) is called a program.",
        "A program is data in the form of coded instructions to control the operation of a computer or other machine.",
        "In the nominal case, the program, as executed by the computer, will consist of machine code.",
        "The elements of storage manipulated by the program, but not actually executed by the central processing unit (CPU), are also data.",
        "At its most essential, a single datum is a value stored at a specific location.",
        "Therefore, it is possible for computer programs to operate on other computer programs, by manipulating their programmatic data.",
        "To store data bytes in a file, they have to be serialized in a file format.",
        "Typically, programs are stored in special file types, different from those used for other data.",
        "Executable files contain programs; all other files are also data files.",
        "However, executable files may also contain data used by the program which is built into the program.",
        "In particular, some executable files have a data segment, which nominally contains constants and initial values for variables, both of which can be considered data.",
        "The line between program and data can become blurry.",
        "An interpreter, for example, is a program.",
        "The input data to an interpreter is itself a program, just not one expressed in native machine language.",
        "In many cases, the interpreted program will be a human-readable text file, which is manipulated with a text editor program.",
        "Metaprogramming similarly involves programs manipulating other programs as data.",
        "Programs like compilers, linkers, debuggers, program updaters, virus scanners and such use other programs as their data.",
        "For example, a user might first instruct the operating system to load a word processor program from one file, and then use the running program to open and edit a document stored in another file.",
        "In this example, the document would be considered data.",
        "If the word processor also features a spell checker, then the dictionary (word list) for the spell checker would also be considered data.",
        "The algorithms used by the spell checker to suggest corrections would be either machine code data or text in some interpretable programming language.",
        "In an alternate usage, binary files (which are not human-readable) are sometimes called data as distinguished from human-readable text.",
        "The total amount of digital data in 2007 was estimated to be 281 billion gigabytes (281 exabytes).",
        "Keys in data provide the context for values.",
        "Regardless of the structure of data, there is always a key component present.",
        "Keys in data and data-structures are essential for giving meaning to data values.",
        "Without a key that is directly or indirectly associated with a value, or collection of values in a structure, the values become meaningless and cease to be data.",
        "That is to say, there has to be a key component linked to a value component in order for it to be considered data.",
        "Data can be represented in computers in multiple ways, as per the following examples: Random access memory (RAM) holds data that the CPU has direct access to.",
        "A CPU may only manipulate data within its processor registers or memory.",
        "This is as opposed to data storage, where the CPU must direct the transfer of data between the storage device (disk, tape...) and memory.",
        "RAM is an array of linear contiguous locations that a processor may read or write by providing an address for the read or write operation.",
        "The processor may operate on any location in memory at any time in any order.",
        "In RAM the smallest element of data is the binary bit.",
        "The capabilities and limitations of accessing RAM are processor specific.",
        "In general main memory is arranged as an array of locations beginning at address 0 (hexadecimal 0).",
        "Each location can store usually 8 or 32 bits depending on the computer architecture.",
        "Data keys need not be a direct hardware address in memory.",
        "Indirect, abstract and logical keys codes can be stored in association with values to form a data structure.",
        "Data structures have predetermined offsets (or links or paths) from the start of the structure, in which data values are stored.",
        "Therefore, the data key consists of the key to the structure plus the offset (or links or paths) into the structure.",
        "When such a structure is repeated, storing variations of the data values and the data keys within the same repeating structure, the result can be considered to resemble a table, in which each element of the repeating structure is considered to be a column and each repetition of the structure is considered as a row of the table.",
        "In such an organization of data, the data key is usually a value in one (or a composite of the values in several) of the columns.",
        "The tabular view of repeating data structures is only one of many possibilities.",
        "Repeating data structures can be organised hierarchically, such that nodes are linked to each other in a cascade of parent-child relationships.",
        "Values and potentially more complex data-structures are linked to the nodes.",
        "Thus the nodal hierarchy provides the key for addressing the data structures associated with the nodes.",
        "This representation can be thought of as an inverted tree.",
        "Modern computer operating system file systems are a common example; and XML is another.",
        "Data has some inherent features when it is sorted on a key.",
        "All the values for subsets of the key appear together.",
        "When passing sequentially through groups of the data with the same key, or a subset of the key changes, this is referred to in data processing circles as a break, or a control break.",
        "It particularly facilitates the aggregation of data values on subsets of a key.",
        "Until the advent of bulk non-volatile memory like flash, persistent data storage was traditionally achieved by writing the data to external block devices like magnetic tape and disk drives.",
        "These devices typically seek to a location on the magnetic media and then read or write blocks of data of a predetermined size.",
        "In this case, the seek location on the media, is the data key and the blocks are the data values.",
        "Early used raw disk data file-systems or disc operating systems reserved contiguous blocks on the disc drive for data files.",
        "In those systems, the files could be filled up, running out of data space before all the data had been written to them.",
        "Thus much unused data space was reserved unproductively to ensure adequate free space for each file.",
        "Later file-systems introduced partitions.",
        "They reserved blocks of disc data space for partitions and used the allocated blocks more economically, by dynamically assigning blocks of a partition to a file as needed.",
        "To achieve this, the file system had to keep track of which blocks were used or unused by data files in a catalog or file allocation table.",
        "Though this made better use of the disc data space, it resulted in fragmentation of files across the disc, and a concomitant performance overhead due additional seek time to read the data.",
        "Modern file systems reorganize fragmented files dynamically to optimize file access times.",
        "Further developments in file systems resulted in virtualization of disc drives i.e.",
        "where a logical drive can be defined as partitions from a number of physical drives.",
        "Retrieving a small subset of data from a much larger set may imply inefficiently searching through the data sequentially.",
        "Indexes are a way to copy out keys and location addresses from data structures in files, tables and data sets, then organize them using inverted tree structures to reduce the time taken to retrieve a subset of the original data.",
        "In order to do this, the key of the subset of data to be retrieved must be known before retrieval begins.",
        "The most popular indexes are the B-tree and the dynamic hash key indexing methods.",
        "Indexing is overhead for filing and retrieving data.",
        "There are other ways of organizing indexes, e.g.",
        "sorting the keys and using a binary search algorithm.",
        "Object-oriented programming uses two basic concepts for understanding data and software: The taxonomic rank-structure of classes, which is an example of a hierarchical data structure; and at run time, the creation of references to in-memory data-structures of objects that have been instantiated from a class library.",
        "It is only after instantiation that an object of a specified class exists.",
        "After an object's reference is cleared, the object also ceases to exist.",
        "The memory locations where the object's data was stored are garbage and are reclassified as unused memory available for reuse.",
        "The advent of databases introduced a further layer of abstraction for persistent data storage.",
        "Databases use metadata, and a structured query language protocol between client and server systems, communicating over a computer network, using a two phase commit logging system to ensure transactional completeness, when saving data.",
        "Modern scalable and high-performance data persistence technologies, such as Apache Hadoop, rely on massively parallel distributed data processing across many commodity computers on a high bandwidth network.",
        "In such systems, the data is distributed across multiple computers and therefore any particular computer in the system must be represented in the key of the data, either directly, or indirectly.",
        "This enables the differentiation between two identical sets of data, each being processed on a different computer at the same time."
      ],
      "metadata": {
        "title": "Data (computer science)",
        "url": "https://en.wikipedia.org/wiki/Data_(computer_science)",
        "word_count": 1900,
        "char_count": 11570,
        "sentence_count": 106,
        "scraped_at": "2025-08-09T14:47:14.299678",
        "language": "en",
        "processing_time": 0.0021140575408935547,
        "source_hash": "595ada87bf48f7950963a0b2722ecbc7"
      }
    },
    {
      "title": "Data type",
      "url": "https://en.wikipedia.org/wiki/Data_type",
      "raw_text": "In computer science and computer programming, a data type (or simply type) is a collection or grouping of data values, usually specified by a set of possible values, a set of allowed operations on these values, and/or a representation of these values as machine types. A data type specification in a program constrains the possible values that an expression, such as a variable or a function call, might take. On literal data, it tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans.\n\n\n== Concept ==\nA data type may be specified for many reasons: similarity, convenience, or to focus the attention. It is frequently a matter of good organization\nthat aids the understanding of complex definitions. Almost all programming languages explicitly include the notion of data type, though the possible data types are often restricted by considerations of simplicity, computability, or regularity. An explicit data type declaration typically allows the compiler to choose an efficient machine representation, but the conceptual organization offered by data types should not be discounted.\nDifferent languages may use different data types or similar types with different semantics. For example, in the Python programming language, int represents an arbitrary-precision integer which has the traditional numeric operations such as addition, subtraction, and multiplication. However, in the Java programming language, the type int represents the set of 32-bit integers ranging in value from −2,147,483,648 to 2,147,483,647, with arithmetic operations that wrap on overflow. In Rust this 32-bit integer type is denoted i32 and panics on overflow in debug mode.\nMost programming languages also allow the programmer to define additional data types, usually by combining multiple elements of other types and defining the valid operations of the new data type.  For example, a programmer might create a new data type named \"complex number\" that would include real and imaginary parts, or a color data type represented by three bytes denoting the amounts each of red, green, and blue, and a string representing the color's name.\nData types are used within type systems, which offer various ways of defining, implementing, and using them. In a type system, a data type represents a constraint placed upon the interpretation of data, describing representation, interpretation and structure of values or objects stored in computer memory. The type system uses data type information to check correctness of computer programs that access or manipulate the data. A compiler may use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).\n\n\n== Definition ==\nParnas, Shore & Weiss (1976) identified five definitions of a \"type\" that were used—sometimes implicitly—in the literature:\n\nSyntactic\nA type is a purely syntactic label associated with a variable when it is declared. Although useful for advanced type systems such as substructural type systems, such definitions provide no intuitive meaning of the types.\nRepresentation\nA type is defined in terms of a composition of more primitive types—often machine types.\nRepresentation and behaviour\nA type is defined as its representation and a set of operators manipulating these representations.\nValue space\nA type is a set of possible values which a variable can possess. Such definitions make it possible to speak about (disjoint) unions or Cartesian products of types.\nValue space and behaviour\nA type is a set of values which a variable can possess and a set of functions that one can apply to these values.\nThe definition in terms of a representation was often done in imperative languages such as ALGOL and Pascal, while the definition in terms of a value space and behaviour was used in higher-level languages such as Simula and CLU. Types including behavior align more closely with object-oriented models, whereas a structured programming model would tend to not include code, and are called plain old data structures.\n\n\n== Classification ==\nData types may be categorized according to several factors:\n\nPrimitive data types or built-in data types are types that are built-in to a language implementation. User-defined data types are non-primitive types. For example, Java's numeric types are primitive, while classes are user-defined.\nA value of an atomic type is a single data item that cannot be broken into component parts. A value of a composite type  or aggregate type is a collection of data items that can be accessed individually. For example, an integer is generally considered atomic, although it consists of a sequence of bits, while an array of integers is certainly composite.\nBasic data types or fundamental data types are defined axiomatically from fundamental notions or by enumeration of their elements. Generated data types or derived data types are specified, and partly defined, in terms of other data types. All basic types are atomic. For example, integers are a basic type defined in mathematics, while an array of integers is the result of applying an array type generator to the integer type.\nThe terminology varies - in the literature, primitive, built-in, basic, atomic, and fundamental may be used interchangeably.\n\n\n== Examples ==\n\n\n=== Machine data types ===\nAll data in computers based on digital electronics is represented as bits (alternatives 0 and 1) on the lowest level. The smallest addressable unit of data is usually a group of bits called a byte (usually an octet, which is 8 bits). The unit processed by machine code instructions is called a word (as of 2025, typically 64 bits).\nMachine data types expose or make available fine-grained control over hardware, but this can also expose implementation details that make code less portable. Hence machine types are mainly used in systems programming or low-level programming languages. In higher-level languages most data types are abstracted in that they do not have a language-defined machine representation. The C programming language, for instance, supplies types such as Booleans, integers, floating-point numbers, etc., but the precise bit representations of these types are implementation-defined. The only C type with a precise machine representation is the char type that represents a byte.\n\n\n=== Boolean type ===\nThe Boolean type represents the values true and false. Although only two values are possible, they are more often represented as a byte or word rather as a single bit as it requires more machine instructions to store and retrieve an individual bit. Many programming languages do not have an explicit Boolean type, instead using an integer type and interpreting (for instance) 0 as false and other values as true.\nBoolean data refers to the logical structure of how the language is interpreted to the machine language. In this case a Boolean 0 refers to the logic False. True is always a non zero, especially a one which is known as Boolean 1.\n\n\n=== Numeric types ===\nAlmost all programming languages supply one or more integer data types. They may either supply a small number of predefined subtypes restricted to certain ranges (such as short and long and their corresponding unsigned variants in C/C++); or allow users to freely define subranges such as 1..12 (e.g. Pascal/Ada). If a corresponding native type does not exist on the target platform, the compiler will break them down into code using types that do exist. For instance, if a 32-bit integer is requested on a 16 bit platform, the compiler will tacitly treat it as an array of two 16 bit integers.\nFloating point data types represent certain fractional values (rational numbers, mathematically). Although they have predefined limits on both their maximum values and their precision, they are sometimes misleadingly called reals (evocative of mathematical real numbers). They are typically stored internally in the form a × 2b (where a and b are integers), but displayed in familiar decimal form.\nFixed point data types are convenient for representing monetary values. They are often implemented internally as integers, leading to predefined limits.\nFor independence from architecture details, a Bignum or arbitrary precision numeric type might be supplied. This represents an integer or rational to a precision limited only by the available memory and computational resources on the system. Bignum implementations of arithmetic operations on machine-sized values are significantly slower than the corresponding machine operations.\n\n\n=== Enumerations ===\nThe enumerated type has distinct values, which can be compared and assigned, but which do not necessarily have any particular concrete representation in the computer's memory; compilers and interpreters can represent them arbitrarily. For example, the four suits in a deck of playing cards may be four enumerators named CLUB, DIAMOND, HEART, SPADE, belonging to an enumerated type named suit.  If a variable V is declared having suit as its data type, one can assign any of those four values to it. Some implementations allow programmers to assign integer values to the enumeration values, or even treat them as type-equivalent to integers.\n\n\n=== String and text types ===\nStrings are a sequence of characters used to store words or plain text, most often textual markup languages representing formatted text. Characters may be a letter of some alphabet, a digit, a blank space, a punctuation mark, etc. Characters are drawn from a character set such as ASCII or Unicode. Character and string types can have different subtypes according to the character encoding. The original 7-bit wide ASCII was found to be limited, and superseded by 8, 16 and 32-bit sets, which can encode a wide variety of non-Latin alphabets (such as Hebrew and Chinese) and other symbols. Strings may be of either variable length or fixed length, and some programming languages have both types. They may also be subtyped by their maximum size.\nSince most character sets include the digits, it is possible to have a numeric string, such as \"1234\". These numeric strings are usually considered distinct from numeric values such as 1234, although some languages automatically convert between them.\n\n\n=== Union types ===\n\nA union type definition will specify which of a number of permitted subtypes may be stored in its instances, e.g. \"float or long integer\". In contrast with a record, which could be defined to contain a float and an integer, a union may only contain one subtype at a time.\nA tagged union (also called a variant, variant record, discriminated union, or disjoint union) contains an additional field indicating its current type for enhanced type safety.\n\n\n=== Algebraic data types ===\n\nAn algebraic data type (ADT) is a possibly recursive sum type of product types. A value of an ADT consists of a constructor tag together with zero or more field values, with the number and type of the field values fixed by the constructor. The set of all possible values of an ADT is the set-theoretic disjoint union (sum), of the sets of all possible values of its variants (product of fields). Values of algebraic types are analyzed with pattern matching, which identifies a value's constructor and extracts the fields it contains.\nIf there is only one constructor, then the ADT corresponds to a product type similar to a tuple or record. A constructor with no fields corresponds to the empty product (unit type). If all constructors have no fields then the ADT corresponds to an enumerated type.\nOne common ADT is the option type, defined in Haskell as data Maybe a = Nothing | Just a.\n\n\n=== Data structures ===\nSome types are very useful for storing and retrieving data and are called data structures. Common data structures include:\n\nAn array (also called vector, list, or sequence) stores a number of elements and provides random access to individual elements. The elements of an array are typically (but not in all contexts) required to be of the same type. Arrays may be fixed-length or expandable. Indices into an array are typically required to be integers (if not, one may stress this relaxation by speaking about an associative array) from a specific range (if not all indices in that range correspond to elements, it may be a sparse array).\nRecord (also called tuple or struct) Records are among the simplest data structures. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called fields or members.\nAn object contains a number of data fields, like a record, and also offers a number of subroutines for accessing or modifying them, called methods.\nthe singly linked list, which can be used to implement a queue and is defined in Haskell as the ADT data List a = Nil | Cons a (List a), and\nthe binary tree, which allows fast searching, and can be defined in Haskell as the ADT data BTree a = Nil | Node (BTree a) a (BTree a)\n\n\n=== Abstract data types ===\n\nAn abstract data type is a data type that does not specify the concrete representation of the data.  Instead, a formal specification based on the data type's operations is used to describe it. Any implementation of a specification must fulfill the rules given. For example, a stack has push/pop operations that follow a Last-In-First-Out rule, and can be concretely implemented using either a list or an array. Abstract data types are used in formal semantics and program verification and, less strictly, in design.\n\n\n=== Pointers and references ===\n\nThe main non-composite, derived type is the pointer, a data type whose value refers directly to (or \"points to\") another value stored elsewhere in the computer memory using its address. It is a primitive kind of reference. (In everyday terms, a page number in a book could be considered a piece of data that refers to another one). Pointers are often stored in a format similar to an integer; however, attempting to dereference or \"look up\" a pointer whose value was never a valid memory address would cause a program to crash. To ameliorate this potential problem, a pointer type is typically considered distinct from the corresponding integer type, even if the underlying representation is the same.\n\n\n=== Function types ===\n\nFunctional programming languages treat functions as a distinct datatype and allow values of this type to be stored in variables and passed to functions. Some multi-paradigm languages such as JavaScript also have mechanisms for treating functions as data. Most contemporary type systems go beyond JavaScript's simple type \"function object\" and have a family of function types differentiated by argument and return types, such as the type Int -> Bool denoting functions taking an integer and returning a Boolean. In C, a function is not a first-class data type but function pointers can be manipulated by the program. Java and C++ originally did not have function values but have added them in C++11 and Java 8.\n\n\n=== Type constructors ===\n\nA type constructor builds new types from old ones, and can be thought of as an operator taking zero or more types as arguments and producing a type. Product types, function types, power types and list types can be made into type constructors.\n\n\n=== Quantified types ===\nUniversally-quantified and existentially-quantified types are based on predicate logic. Universal quantification is written as \n  \n    \n      \n        ∀\n        x\n        .\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\forall x.f(x)}\n  \n or forall x. f x and is the intersection over all types x of the body f x, i.e. the value is of type f x for every x. Existential quantification written as \n  \n    \n      \n        ∃\n        x\n        .\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\exists x.f(x)}\n  \n or exists x. f x and is the union over all types x of the body f x, i.e. the value is of type f x for some x.\nIn Haskell, universal quantification is commonly used, but existential types must be encoded by transforming exists a. f a to forall r. (forall a. f a -> r) -> r or a similar type.\n\n\n=== Refinement types ===\n\nA refinement type is a type endowed with a predicate which is assumed to hold for any element of the refined type. For instance, the type of natural numbers greater than 5 may be written as \n  \n    \n      \n        {\n        n\n        ∈\n        \n          N\n        \n        \n        \n          |\n        \n        \n        n\n        >\n        5\n        }\n      \n    \n    {\\displaystyle \\{n\\in \\mathbb {N} \\,|\\,n>5\\}}\n  \n\n\n=== Dependent types ===\n\nA dependent type is a type whose definition depends on a value. Two common examples of dependent types are dependent functions and dependent pairs. The return type of a dependent function may depend on the value (not just type) of one of its arguments. A dependent pair may have a second value of which the type depends on the first value.\n\n\n=== Intersection types ===\n\nAn intersection type is a type containing those values that are members of two specified types. For example, in Java the class Boolean implements both the Serializable and the Comparable interfaces. Therefore, an object of type Boolean is a member of the type Serializable & Comparable. Considering types as sets of values, the intersection type \n  \n    \n      \n        σ\n        ∩\n        τ\n      \n    \n    {\\displaystyle \\sigma \\cap \\tau }\n  \n is the set-theoretic intersection of \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n and \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n. It is also possible to define a dependent intersection type, denoted \n  \n    \n      \n        (\n        x\n        :\n        σ\n        )\n        ∩\n        τ\n      \n    \n    {\\displaystyle (x:\\sigma )\\cap \\tau }\n  \n, where the type \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n may depend on the term variable \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n.\n\n\n=== Meta types ===\n\nSome programming languages represent the type information as data, enabling type introspection and reflective programming (reflection). In contrast, higher order type systems, while allowing types to be constructed from other types and passed to functions as values, typically avoid basing computational decisions on them.\n\n\n=== Convenience types ===\nFor convenience, high-level languages and databases may supply ready-made \"real world\" data types, for instance times, dates, and monetary values (currency). These may be built-in to the language or implemented as composite types in a library.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nParnas, David L.; Shore, John E.; Weiss, David (1976). \"Abstract types defined as classes of variables\". Proceedings of the 1976 conference on Data : Abstraction, definition and structure -. pp. 149–154. doi:10.1145/800237.807133. S2CID 14448258.\nCardelli, Luca; Wegner, Peter (December 1985). \"On Understanding Types, Data Abstraction, and Polymorphism\" (PDF). ACM Computing Surveys. 17 (4): 471–523. CiteSeerX 10.1.1.117.695. doi:10.1145/6041.6042. ISSN 0360-0300. S2CID 2921816. Archived (PDF) from the original on 2008-12-03.\nCleaveland, J. Craig (1986). An Introduction to Data Types. Addison-Wesley. ISBN 978-0201119404.\n\n\n== External links ==\n Media related to Data types at Wikimedia Commons",
      "cleaned_text": "In computer science and computer programming, a data type (or simply type) is a collection or grouping of data values, usually specified by a set of possible values, a set of allowed operations on these values, and/or a representation of these values as machine types. A data type specification in a program constrains the possible values that an expression, such as a variable or a function call, might take. On literal data, it tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans. A data type may be specified for many reasons: similarity, convenience, or to focus the attention. It is frequently a matter of good organization that aids the understanding of complex definitions. Almost all programming languages explicitly include the notion of data type, though the possible data types are often restricted by considerations of simplicity, computability, or regularity. An explicit data type declaration typically allows the compiler to choose an efficient machine representation, but the conceptual organization offered by data types should not be discounted. Different languages may use different data types or similar types with different semantics. For example, in the Python programming language, int represents an arbitrary-precision integer which has the traditional numeric operations such as addition, subtraction, and multiplication. However, in the Java programming language, the type int represents the set of 32-bit integers ranging in value from −2,147,483,648 to 2,147,483,647, with arithmetic operations that wrap on overflow. In Rust this 32-bit integer type is denoted i32 and panics on overflow in debug mode. Most programming languages also allow the programmer to define additional data types, usually by combining multiple elements of other types and defining the valid operations of the new data type. For example, a programmer might create a new data type named \"complex number\" that would include real and imaginary parts, or a color data type represented by three bytes denoting the amounts each of red, green, and blue, and a string representing the color's name. Data types are used within type systems, which offer various ways of defining, implementing, and using them. In a type system, a data type represents a constraint placed upon the interpretation of data, describing representation, interpretation and structure of values or objects stored in computer memory. The type system uses data type information to check correctness of computer programs that access or manipulate the data. A compiler may use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.). Parnas, Shore & Weiss (1976) identified five definitions of a \"type\" that were used-sometimes implicitly-in the literature: Syntactic A type is a purely syntactic label associated with a variable when it is declared. Although useful for advanced type systems such as substructural type systems, such definitions provide no intuitive meaning of the types. Representation A type is defined in terms of a composition of more primitive types-often machine types. Representation and behaviour A type is defined as its representation and a set of operators manipulating these representations. Value space A type is a set of possible values which a variable can possess. Such definitions make it possible to speak about (disjoint) unions or Cartesian products of types. Value space and behaviour A type is a set of values which a variable can possess and a set of functions that one can apply to these values. The definition in terms of a representation was often done in imperative languages such as ALGOL and Pascal, while the definition in terms of a value space and behaviour was used in higher-level languages such as Simula and CLU. Types including behavior align more closely with object-oriented models, whereas a structured programming model would tend to not include code, and are called plain old data structures. Data types may be categorized according to several factors: Primitive data types or built-in data types are types that are built-in to a language implementation. User-defined data types are non-primitive types. For example, Java's numeric types are primitive, while classes are user-defined. A value of an atomic type is a single data item that cannot be broken into component parts. A value of a composite type or aggregate type is a collection of data items that can be accessed individually. For example, an integer is generally considered atomic, although it consists of a sequence of bits, while an array of integers is certainly composite. Basic data types or fundamental data types are defined axiomatically from fundamental notions or by enumeration of their elements. Generated data types or derived data types are specified, and partly defined, in terms of other data types. All basic types are atomic. For example, integers are a basic type defined in mathematics, while an array of integers is the result of applying an array type generator to the integer type. The terminology varies - in the literature, primitive, built-in, basic, atomic, and fundamental may be used interchangeably. All data in computers based on digital electronics is represented as bits (alternatives 0 and 1) on the lowest level. The smallest addressable unit of data is usually a group of bits called a byte (usually an octet, which is 8 bits). The unit processed by machine code instructions is called a word (as of 2025, typically 64 bits). Machine data types expose or make available fine-grained control over hardware, but this can also expose implementation details that make code less portable. Hence machine types are mainly used in systems programming or low-level programming languages. In higher-level languages most data types are abstracted in that they do not have a language-defined machine representation. The C programming language, for instance, supplies types such as Booleans, integers, floating-point numbers, etc., but the precise bit representations of these types are implementation-defined. The only C type with a precise machine representation is the char type that represents a byte. The Boolean type represents the values true and false. Although only two values are possible, they are more often represented as a byte or word rather as a single bit as it requires more machine instructions to store and retrieve an individual bit. Many programming languages do not have an explicit Boolean type, instead using an integer type and interpreting (for instance) 0 as false and other values as true. Boolean data refers to the logical structure of how the language is interpreted to the machine language. In this case a Boolean 0 refers to the logic False. True is always a non zero, especially a one which is known as Boolean 1. Almost all programming languages supply one or more integer data types. They may either supply a small number of predefined subtypes restricted to certain ranges (such as short and long and their corresponding unsigned variants in C/C++); or allow users to freely define subranges such as 1..12 (e.g. Pascal/Ada). If a corresponding native type does not exist on the target platform, the compiler will break them down into code using types that do exist. For instance, if a 32-bit integer is requested on a 16 bit platform, the compiler will tacitly treat it as an array of two 16 bit integers. Floating point data types represent certain fractional values (rational numbers, mathematically). Although they have predefined limits on both their maximum values and their precision, they are sometimes misleadingly called reals (evocative of mathematical real numbers). They are typically stored internally in the form a × 2b (where a and b are integers), but displayed in familiar decimal form. Fixed point data types are convenient for representing monetary values. They are often implemented internally as integers, leading to predefined limits. For independence from architecture details, a Bignum or arbitrary precision numeric type might be supplied. This represents an integer or rational to a precision limited only by the available memory and computational resources on the system. Bignum implementations of arithmetic operations on machine-sized values are significantly slower than the corresponding machine operations. The enumerated type has distinct values, which can be compared and assigned, but which do not necessarily have any particular concrete representation in the computer's memory; compilers and interpreters can represent them arbitrarily. For example, the four suits in a deck of playing cards may be four enumerators named CLUB, DIAMOND, HEART, SPADE, belonging to an enumerated type named suit. If a variable V is declared having suit as its data type, one can assign any of those four values to it. Some implementations allow programmers to assign integer values to the enumeration values, or even treat them as type-equivalent to integers. Strings are a sequence of characters used to store words or plain text, most often textual markup languages representing formatted text. Characters may be a letter of some alphabet, a digit, a blank space, a punctuation mark, etc. Characters are drawn from a character set such as ASCII or Unicode. Character and string types can have different subtypes according to the character encoding. The original 7-bit wide ASCII was found to be limited, and superseded by 8, 16 and 32-bit sets, which can encode a wide variety of non-Latin alphabets (such as Hebrew and Chinese) and other symbols. Strings may be of either variable length or fixed length, and some programming languages have both types. They may also be subtyped by their maximum size. Since most character sets include the digits, it is possible to have a numeric string, such as \"1234\". These numeric strings are usually considered distinct from numeric values such as 1234, although some languages automatically convert between them. A union type definition will specify which of a number of permitted subtypes may be stored in its instances, e.g. \"float or long integer\". In contrast with a record, which could be defined to contain a float and an integer, a union may only contain one subtype at a time. A tagged union (also called a variant, variant record, discriminated union, or disjoint union) contains an additional field indicating its current type for enhanced type safety. An algebraic data type (ADT) is a possibly recursive sum type of product types. A value of an ADT consists of a constructor tag together with zero or more field values, with the number and type of the field values fixed by the constructor. The set of all possible values of an ADT is the set-theoretic disjoint union (sum), of the sets of all possible values of its variants (product of fields). Values of algebraic types are analyzed with pattern matching, which identifies a value's constructor and extracts the fields it contains. If there is only one constructor, then the ADT corresponds to a product type similar to a tuple or record. A constructor with no fields corresponds to the empty product (unit type). If all constructors have no fields then the ADT corresponds to an enumerated type. One common ADT is the option type, defined in Haskell as data Maybe a = Nothing | Just a. Some types are very useful for storing and retrieving data and are called data structures. Common data structures include: An array (also called vector, list, or sequence) stores a number of elements and provides random access to individual elements. The elements of an array are typically (but not in all contexts) required to be of the same type. Arrays may be fixed-length or expandable. Indices into an array are typically required to be integers (if not, one may stress this relaxation by speaking about an associative array) from a specific range (if not all indices in that range correspond to elements, it may be a sparse array). Record (also called tuple or struct) Records are among the simplest data structures. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called fields or members. An object contains a number of data fields, like a record, and also offers a number of subroutines for accessing or modifying them, called methods. the singly linked list, which can be used to implement a queue and is defined in Haskell as the ADT data List a = Nil | Cons a (List a), and the binary tree, which allows fast searching, and can be defined in Haskell as the ADT data BTree a = Nil | Node (BTree a) a (BTree a) An abstract data type is a data type that does not specify the concrete representation of the data. Instead, a formal specification based on the data type's operations is used to describe it. Any implementation of a specification must fulfill the rules given. For example, a stack has push/pop operations that follow a Last-In-First-Out rule, and can be concretely implemented using either a list or an array. Abstract data types are used in formal semantics and program verification and, less strictly, in design. The main non-composite, derived type is the pointer, a data type whose value refers directly to (or \"points to\") another value stored elsewhere in the computer memory using its address. It is a primitive kind of reference. (In everyday terms, a page number in a book could be considered a piece of data that refers to another one). Pointers are often stored in a format similar to an integer; however, attempting to dereference or \"look up\" a pointer whose value was never a valid memory address would cause a program to crash. To ameliorate this potential problem, a pointer type is typically considered distinct from the corresponding integer type, even if the underlying representation is the same. Functional programming languages treat functions as a distinct datatype and allow values of this type to be stored in variables and passed to functions. Some multi-paradigm languages such as JavaScript also have mechanisms for treating functions as data. Most contemporary type systems go beyond JavaScript's simple type \"function object\" and have a family of function types differentiated by argument and return types, such as the type Int -> Bool denoting functions taking an integer and returning a Boolean. In C, a function is not a first-class data type but function pointers can be manipulated by the program. Java and C++ originally did not have function values but have added them in C++11 and Java 8. A type constructor builds new types from old ones, and can be thought of as an operator taking zero or more types as arguments and producing a type. Product types, function types, power types and list types can be made into type constructors. Universally-quantified and existentially-quantified types are based on predicate logic. Universal quantification is written as ∀ x . f ( x ) {\\displaystyle \\forall x.f(x)} or forall x. f x and is the intersection over all types x of the body f x, i.e. the value is of type f x for every x. Existential quantification written as ∃ x . f ( x ) {\\displaystyle \\exists x.f(x)} or exists x. f x and is the union over all types x of the body f x, i.e. the value is of type f x for some x. In Haskell, universal quantification is commonly used, but existential types must be encoded by transforming exists a. f a to forall r. (forall a. f a -> r) -> r or a similar type. A refinement type is a type endowed with a predicate which is assumed to hold for any element of the refined type. For instance, the type of natural numbers greater than 5 may be written as { n ∈ N | n > 5 } {\\displaystyle \\{n\\in \\mathbb {N} \\,|\\,n>5\\}} A dependent type is a type whose definition depends on a value. Two common examples of dependent types are dependent functions and dependent pairs. The return type of a dependent function may depend on the value (not just type) of one of its arguments. A dependent pair may have a second value of which the type depends on the first value. An intersection type is a type containing those values that are members of two specified types. For example, in Java the class Boolean implements both the Serializable and the Comparable interfaces. Therefore, an object of type Boolean is a member of the type Serializable & Comparable. Considering types as sets of values, the intersection type σ ∩ τ {\\displaystyle \\sigma \\cap au } is the set-theoretic intersection of σ {\\displaystyle \\sigma } and τ {\\displaystyle au } . It is also possible to define a dependent intersection type, denoted ( x : σ ) ∩ τ {\\displaystyle (x:\\sigma )\\cap au } , where the type τ {\\displaystyle au } may depend on the term variable x {\\displaystyle x} . Some programming languages represent the type information as data, enabling type introspection and reflective programming (reflection). In contrast, higher order type systems, while allowing types to be constructed from other types and passed to functions as values, typically avoid basing computational decisions on them. For convenience, high-level languages and databases may supply ready-made \"real world\" data types, for instance times, dates, and monetary values (currency). These may be built-in to the language or implemented as composite types in a library.",
      "sentences": [
        "In computer science and computer programming, a data type (or simply type) is a collection or grouping of data values, usually specified by a set of possible values, a set of allowed operations on these values, and/or a representation of these values as machine types.",
        "A data type specification in a program constrains the possible values that an expression, such as a variable or a function call, might take.",
        "On literal data, it tells the compiler or interpreter how the programmer intends to use the data.",
        "Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans.",
        "A data type may be specified for many reasons: similarity, convenience, or to focus the attention.",
        "It is frequently a matter of good organization that aids the understanding of complex definitions.",
        "Almost all programming languages explicitly include the notion of data type, though the possible data types are often restricted by considerations of simplicity, computability, or regularity.",
        "An explicit data type declaration typically allows the compiler to choose an efficient machine representation, but the conceptual organization offered by data types should not be discounted.",
        "Different languages may use different data types or similar types with different semantics.",
        "For example, in the Python programming language, int represents an arbitrary-precision integer which has the traditional numeric operations such as addition, subtraction, and multiplication.",
        "However, in the Java programming language, the type int represents the set of 32-bit integers ranging in value from −2,147,483,648 to 2,147,483,647, with arithmetic operations that wrap on overflow.",
        "In Rust this 32-bit integer type is denoted i32 and panics on overflow in debug mode.",
        "Most programming languages also allow the programmer to define additional data types, usually by combining multiple elements of other types and defining the valid operations of the new data type.",
        "For example, a programmer might create a new data type named \"complex number\" that would include real and imaginary parts, or a color data type represented by three bytes denoting the amounts each of red, green, and blue, and a string representing the color's name.",
        "Data types are used within type systems, which offer various ways of defining, implementing, and using them.",
        "In a type system, a data type represents a constraint placed upon the interpretation of data, describing representation, interpretation and structure of values or objects stored in computer memory.",
        "The type system uses data type information to check correctness of computer programs that access or manipulate the data.",
        "A compiler may use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value.",
        "In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers.",
        "They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).",
        "Parnas, Shore & Weiss (1976) identified five definitions of a \"type\" that were used-sometimes implicitly-in the literature: Syntactic A type is a purely syntactic label associated with a variable when it is declared.",
        "Although useful for advanced type systems such as substructural type systems, such definitions provide no intuitive meaning of the types.",
        "Representation A type is defined in terms of a composition of more primitive types-often machine types.",
        "Representation and behaviour A type is defined as its representation and a set of operators manipulating these representations.",
        "Value space A type is a set of possible values which a variable can possess.",
        "Such definitions make it possible to speak about (disjoint) unions or Cartesian products of types.",
        "Value space and behaviour A type is a set of values which a variable can possess and a set of functions that one can apply to these values.",
        "The definition in terms of a representation was often done in imperative languages such as ALGOL and Pascal, while the definition in terms of a value space and behaviour was used in higher-level languages such as Simula and CLU.",
        "Types including behavior align more closely with object-oriented models, whereas a structured programming model would tend to not include code, and are called plain old data structures.",
        "Data types may be categorized according to several factors: Primitive data types or built-in data types are types that are built-in to a language implementation.",
        "User-defined data types are non-primitive types.",
        "For example, Java's numeric types are primitive, while classes are user-defined.",
        "A value of an atomic type is a single data item that cannot be broken into component parts.",
        "A value of a composite type or aggregate type is a collection of data items that can be accessed individually.",
        "For example, an integer is generally considered atomic, although it consists of a sequence of bits, while an array of integers is certainly composite.",
        "Basic data types or fundamental data types are defined axiomatically from fundamental notions or by enumeration of their elements.",
        "Generated data types or derived data types are specified, and partly defined, in terms of other data types.",
        "All basic types are atomic.",
        "For example, integers are a basic type defined in mathematics, while an array of integers is the result of applying an array type generator to the integer type.",
        "The terminology varies - in the literature, primitive, built-in, basic, atomic, and fundamental may be used interchangeably.",
        "All data in computers based on digital electronics is represented as bits (alternatives 0 and 1) on the lowest level.",
        "The smallest addressable unit of data is usually a group of bits called a byte (usually an octet, which is 8 bits).",
        "The unit processed by machine code instructions is called a word (as of 2025, typically 64 bits).",
        "Machine data types expose or make available fine-grained control over hardware, but this can also expose implementation details that make code less portable.",
        "Hence machine types are mainly used in systems programming or low-level programming languages.",
        "In higher-level languages most data types are abstracted in that they do not have a language-defined machine representation.",
        "The C programming language, for instance, supplies types such as Booleans, integers, floating-point numbers, etc., but the precise bit representations of these types are implementation-defined.",
        "The only C type with a precise machine representation is the char type that represents a byte.",
        "The Boolean type represents the values true and false.",
        "Although only two values are possible, they are more often represented as a byte or word rather as a single bit as it requires more machine instructions to store and retrieve an individual bit.",
        "Many programming languages do not have an explicit Boolean type, instead using an integer type and interpreting (for instance) 0 as false and other values as true.",
        "Boolean data refers to the logical structure of how the language is interpreted to the machine language.",
        "In this case a Boolean 0 refers to the logic False.",
        "True is always a non zero, especially a one which is known as Boolean 1.",
        "Almost all programming languages supply one or more integer data types.",
        "They may either supply a small number of predefined subtypes restricted to certain ranges (such as short and long and their corresponding unsigned variants in C/C++); or allow users to freely define subranges such as 1..12 (e.g.",
        "Pascal/Ada).",
        "If a corresponding native type does not exist on the target platform, the compiler will break them down into code using types that do exist.",
        "For instance, if a 32-bit integer is requested on a 16 bit platform, the compiler will tacitly treat it as an array of two 16 bit integers.",
        "Floating point data types represent certain fractional values (rational numbers, mathematically).",
        "Although they have predefined limits on both their maximum values and their precision, they are sometimes misleadingly called reals (evocative of mathematical real numbers).",
        "They are typically stored internally in the form a × 2b (where a and b are integers), but displayed in familiar decimal form.",
        "Fixed point data types are convenient for representing monetary values.",
        "They are often implemented internally as integers, leading to predefined limits.",
        "For independence from architecture details, a Bignum or arbitrary precision numeric type might be supplied.",
        "This represents an integer or rational to a precision limited only by the available memory and computational resources on the system.",
        "Bignum implementations of arithmetic operations on machine-sized values are significantly slower than the corresponding machine operations.",
        "The enumerated type has distinct values, which can be compared and assigned, but which do not necessarily have any particular concrete representation in the computer's memory; compilers and interpreters can represent them arbitrarily.",
        "For example, the four suits in a deck of playing cards may be four enumerators named CLUB, DIAMOND, HEART, SPADE, belonging to an enumerated type named suit.",
        "If a variable V is declared having suit as its data type, one can assign any of those four values to it.",
        "Some implementations allow programmers to assign integer values to the enumeration values, or even treat them as type-equivalent to integers.",
        "Strings are a sequence of characters used to store words or plain text, most often textual markup languages representing formatted text.",
        "Characters may be a letter of some alphabet, a digit, a blank space, a punctuation mark, etc.",
        "Characters are drawn from a character set such as ASCII or Unicode.",
        "Character and string types can have different subtypes according to the character encoding.",
        "The original 7-bit wide ASCII was found to be limited, and superseded by 8, 16 and 32-bit sets, which can encode a wide variety of non-Latin alphabets (such as Hebrew and Chinese) and other symbols.",
        "Strings may be of either variable length or fixed length, and some programming languages have both types.",
        "They may also be subtyped by their maximum size.",
        "Since most character sets include the digits, it is possible to have a numeric string, such as \"1234\".",
        "These numeric strings are usually considered distinct from numeric values such as 1234, although some languages automatically convert between them.",
        "A union type definition will specify which of a number of permitted subtypes may be stored in its instances, e.g.",
        "\"float or long integer\".",
        "In contrast with a record, which could be defined to contain a float and an integer, a union may only contain one subtype at a time.",
        "A tagged union (also called a variant, variant record, discriminated union, or disjoint union) contains an additional field indicating its current type for enhanced type safety.",
        "An algebraic data type (ADT) is a possibly recursive sum type of product types.",
        "A value of an ADT consists of a constructor tag together with zero or more field values, with the number and type of the field values fixed by the constructor.",
        "The set of all possible values of an ADT is the set-theoretic disjoint union (sum), of the sets of all possible values of its variants (product of fields).",
        "Values of algebraic types are analyzed with pattern matching, which identifies a value's constructor and extracts the fields it contains.",
        "If there is only one constructor, then the ADT corresponds to a product type similar to a tuple or record.",
        "A constructor with no fields corresponds to the empty product (unit type).",
        "If all constructors have no fields then the ADT corresponds to an enumerated type.",
        "One common ADT is the option type, defined in Haskell as data Maybe a = Nothing | Just a.",
        "Some types are very useful for storing and retrieving data and are called data structures.",
        "Common data structures include: An array (also called vector, list, or sequence) stores a number of elements and provides random access to individual elements.",
        "The elements of an array are typically (but not in all contexts) required to be of the same type.",
        "Arrays may be fixed-length or expandable.",
        "Indices into an array are typically required to be integers (if not, one may stress this relaxation by speaking about an associative array) from a specific range (if not all indices in that range correspond to elements, it may be a sparse array).",
        "Record (also called tuple or struct) Records are among the simplest data structures.",
        "A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names.",
        "The elements of records are usually called fields or members.",
        "An object contains a number of data fields, like a record, and also offers a number of subroutines for accessing or modifying them, called methods.",
        "Instead, a formal specification based on the data type's operations is used to describe it.",
        "Any implementation of a specification must fulfill the rules given.",
        "For example, a stack has push/pop operations that follow a Last-In-First-Out rule, and can be concretely implemented using either a list or an array.",
        "Abstract data types are used in formal semantics and program verification and, less strictly, in design.",
        "The main non-composite, derived type is the pointer, a data type whose value refers directly to (or \"points to\") another value stored elsewhere in the computer memory using its address.",
        "It is a primitive kind of reference.",
        "(In everyday terms, a page number in a book could be considered a piece of data that refers to another one).",
        "Pointers are often stored in a format similar to an integer; however, attempting to dereference or \"look up\" a pointer whose value was never a valid memory address would cause a program to crash.",
        "To ameliorate this potential problem, a pointer type is typically considered distinct from the corresponding integer type, even if the underlying representation is the same.",
        "Functional programming languages treat functions as a distinct datatype and allow values of this type to be stored in variables and passed to functions.",
        "Some multi-paradigm languages such as JavaScript also have mechanisms for treating functions as data.",
        "Most contemporary type systems go beyond JavaScript's simple type \"function object\" and have a family of function types differentiated by argument and return types, such as the type Int -> Bool denoting functions taking an integer and returning a Boolean.",
        "In C, a function is not a first-class data type but function pointers can be manipulated by the program.",
        "Java and C++ originally did not have function values but have added them in C++11 and Java 8.",
        "A type constructor builds new types from old ones, and can be thought of as an operator taking zero or more types as arguments and producing a type.",
        "Product types, function types, power types and list types can be made into type constructors.",
        "Universally-quantified and existentially-quantified types are based on predicate logic.",
        "Universal quantification is written as ∀ x .",
        "f ( x ) {\\displaystyle \\forall x.f(x)} or forall x. f x and is the intersection over all types x of the body f x, i.e.",
        "the value is of type f x for every x. Existential quantification written as ∃ x .",
        "f ( x ) {\\displaystyle \\exists x.f(x)} or exists x. f x and is the union over all types x of the body f x, i.e.",
        "the value is of type f x for some x.",
        "In Haskell, universal quantification is commonly used, but existential types must be encoded by transforming exists a. f a to forall r. (forall a. f a -> r) -> r or a similar type.",
        "A refinement type is a type endowed with a predicate which is assumed to hold for any element of the refined type.",
        "For instance, the type of natural numbers greater than 5 may be written as { n ∈ N | n > 5 } {\\displaystyle \\{n\\in \\mathbb {N} \\,|\\,n>5\\}} A dependent type is a type whose definition depends on a value.",
        "Two common examples of dependent types are dependent functions and dependent pairs.",
        "The return type of a dependent function may depend on the value (not just type) of one of its arguments.",
        "A dependent pair may have a second value of which the type depends on the first value.",
        "An intersection type is a type containing those values that are members of two specified types.",
        "For example, in Java the class Boolean implements both the Serializable and the Comparable interfaces.",
        "Therefore, an object of type Boolean is a member of the type Serializable & Comparable.",
        "Considering types as sets of values, the intersection type σ ∩ τ {\\displaystyle \\sigma \\cap au } is the set-theoretic intersection of σ {\\displaystyle \\sigma } and τ {\\displaystyle au } .",
        "It is also possible to define a dependent intersection type, denoted ( x : σ ) ∩ τ {\\displaystyle (x:\\sigma )\\cap au } , where the type τ {\\displaystyle au } may depend on the term variable x {\\displaystyle x} .",
        "Some programming languages represent the type information as data, enabling type introspection and reflective programming (reflection).",
        "In contrast, higher order type systems, while allowing types to be constructed from other types and passed to functions as values, typically avoid basing computational decisions on them.",
        "For convenience, high-level languages and databases may supply ready-made \"real world\" data types, for instance times, dates, and monetary values (currency).",
        "These may be built-in to the language or implemented as composite types in a library."
      ],
      "metadata": {
        "title": "Data type",
        "url": "https://en.wikipedia.org/wiki/Data_type",
        "word_count": 2911,
        "char_count": 17826,
        "sentence_count": 138,
        "scraped_at": "2025-08-09T14:47:14.303383",
        "language": "en",
        "processing_time": 0.003541707992553711,
        "source_hash": "be90884e1a7837344883b796b15b9d78"
      }
    },
    {
      "title": "Statistics",
      "url": "https://en.wikipedia.org/wiki/Statistics",
      "raw_text": "Statistics (from German: Statistik, orig. \"description of a state, a country\") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.\nWhen census data (comprising every member of the target population) cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences made using mathematical statistics employ the framework of probability theory, which deals with the analysis of random phenomena.\nA standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is rejected when it is in fact true, giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected when it is in fact false, giving a \"false negative\"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.\nStatistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\n\n== Introduction ==\n\n\"Statistics is both the science of uncertainty and the technology of extracting information from data.\" - featured in the International Encyclopedia of Statistical Science.Statistics is the discipline that deals with data, facts and figures with which meaningful information is inferred. Data may represent a numerical value, in form of quantitative data, or a label, as with qualitative data. Data may be collected, presented and summarised, in one of two methods called descriptive statistics. Two elementary summaries of data, singularly called a statistic, are the mean and dispersion. Whereas inferential statistics interprets data from a population sample to induce statements and predictions about a population.\nStatistics is regarded as a body of science or a branch of mathematics. It is based on probability, a branch of mathematics that studies random events. Statistics is considered the science of uncertainty. This arises from the ways to cope with measurement and sampling error as well as dealing with uncertanties in modelling. Although probability and statistics were once paired together as a single subject, they are conceptually distinct from one another. The former is based on deducing answers to specific situations from a general theory of probability, meanwhile statistics induces statements about a population based on a data set. Statistics serves to bridge the gap between probability and applied mathematical fields.\nSome consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is generally concerned with the use of data in the context of uncertainty and decision-making in the face of uncertainty. Statistics is indexed at 62, a subclass of probability theory and stochastic processes, in the Mathematics Subject Classification. Mathematical statistics is covered in the range 276-280 of subclass QA (science > mathematics) in the Library of Congress Classification.\nThe word statistics ultimately comes from the Latin word Status, meaning \"situation\" or \"condition\" in society, which in late Latin adopted the meaning \"state\". Derived from this, political scientist Gottfried Achenwall, coined the German word statistik (a summary of how things stand). In 1770, the term entered the English language through German and referred to the study of political arrangements. The term gained its modern meaning in the 1790s in John Sinclair's works. In modern German, the term statistik is synonymous with mathematical statistics. The term statistic, in singular form, is used to describe a function that returns its value of the same name.\n\n\n== Statistical data ==\n\n\n=== Data collection ===\n\n\n==== Sampling ====\nWhen full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models.\nTo use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.\nSampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.\n\n\n==== Experimental and observational studies ====\nA common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements with different levels using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data—like natural experiments and observational studies—for which a statistician would use a modified, more structured estimation method (e.g., difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.\n\n\n===== Experiments =====\nThe basic steps of a statistical experiment are:\n\nPlanning the research, including finding the number of replicates of the study, using the following information:  preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.\nDesign of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data.\nPerforming the experiment following the experimental protocol and analyzing the data following the experimental protocol.\nFurther examining the data set in secondary analyses, to suggest new hypotheses for future study.\nDocumenting and presenting the results of the study.\nExperiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.\n\n\n===== Observational study =====\nAn example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group. A case-control study is another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected.\n\n\n=== Types of data ===\n\nVarious attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one (injective) transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.\nBecause variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating-point arithmetic. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.\nOther categorizations have been proposed. For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. (See also: Chrisman (1998), van den Berg (1991).)\nThe issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer.\"\n\n\n== Methods ==\n\n\n=== Descriptive statistics ===\n\nA descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features of a collection of information, while descriptive statistics in the mass noun sense is the process of using and analyzing those statistics. Descriptive statistics is distinguished from inferential statistics (or inductive statistics), in that descriptive statistics aims to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent.\n\n\n=== Inferential statistics ===\n\nStatistical inference is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.  It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.\n\n\n==== Terminology and theory of inferential statistics ====\n\n\n===== Statistics, estimators and pivotal quantities =====\nConsider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables. The population being examined is described by a probability distribution that may have unknown parameters.\nA statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters. Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.\nA random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.\nBetween two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.\nOther desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.\nThis still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.\n\n\n===== Null hypothesis and alternative hypothesis =====\nInterpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time. The alternative hypothesis is the name of the hypothesis that contradicts the null hypothesis.\nThe best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (the status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.\n\n\n===== Error =====\nWorking from a null hypothesis, two broad categories of error are recognized:\n\nType I errors where the null hypothesis is falsely rejected, giving a \"false positive\".\nType II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed, giving a \"false negative\".\nStandard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.\nA statistical error is the amount by which an observation differs from its expected value. A residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).\nMean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.\n\nMany statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.\nMeasurement processes that generate statistical data are also subject to error.  Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\n\n===== Interval estimation =====\n\nMost studies only sample part of a population, so results do not fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable.  Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.\nIn principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.\n\n\n===== Significance =====\n\nStatistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).\n\nThe standard approach is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator does not belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.\nReferring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.\nAlthough in principle the acceptable level of statistical significance may be subject to debate, the significance level is the largest p-value that allows the test to reject the null hypothesis. This test is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the significance level, the lower the probability of committing type I error.\nSome problems are usually associated with this framework (See criticism of hypothesis testing):\n\nA difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this. One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to report confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.\nFallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is the probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.\nRejecting the null hypothesis does not automatically prove the alternative hypothesis.\nAs everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.\n\n\n===== Examples =====\nSome well-known statistical tests and procedures are:\n\n\n=== Bayesian Statistics ===\n\nAn alternative paradigm to the popular frequentist paradigm is to use Bayes' theorem to update the prior probability of the hypotheses in consideration based on the relative likelihood of the evidence gathered to obtain a posterior probability. Bayesian methods have been aided by the increase in available computing power to compute the posterior probability using numerical approximation techniques like Markov Chain Monte Carlo.\nFor statistically modelling purposes, Bayesian models tend to be hierarchical, for example, one could model each YouTube channel as having video views distributed as a normal distribution with channel dependent mean and variance \n  \n    \n      \n        \n          \n            N\n          \n        \n        (\n        \n          μ\n          \n            i\n          \n        \n        ,\n        \n          σ\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}}(\\mu _{i},\\sigma _{i})}\n  \n, while modeling the channel means as themselves coming from a normal distribution representing the distribution of average video view counts per channel, and the variances as coming from another distribution.\nThe concept of using likelihood ratio can also be prominently seen in medical diagnostic testing.\n\n\n=== Exploratory data analysis ===\n\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n\n\n=== Mathematical statistics ===\n\nMathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory. All statistical analyses make use of at least some mathematics, and mathematical statistics can therefore be regarded as a fundamental component of general statistics.\n\n\n== History ==\n\nFormal discussions on inference date back to the mathematicians and cryptographers of the Islamic Golden Age between the 8th and 13th centuries. Al-Khalil (717–786) wrote the Book of Cryptographic Messages, which contains one of the first uses of permutations and combinations, to list all possible Arabic words with and without vowels. Al-Kindi's Manuscript on Deciphering Cryptographic Messages gave a detailed description of how to use frequency analysis to decipher encrypted messages, providing an early example of statistical inference for decoding. Ibn Adlan (1187–1268) later made an important contribution on the use of sample size in frequency analysis.\nAlthough the term statistic was introduced by the Italian scholar Girolamo Ghilini in 1589 with reference to a collection of facts and information about a state, it was the German Gottfried Achenwall in 1749 who started using the term as a collection of quantitative information, in the modern use for this science. The earliest writing containing statistics in Europe dates back to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt. Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\n\nThe mathematical foundations of statistics developed from discussions concerning games of chance among mathematicians such as Gerolamo Cardano, Blaise Pascal, Pierre de Fermat, and Christiaan Huygens. Although the idea of probability was already examined in ancient and medieval law and philosophy (such as the work of Juan Caramuel), probability theory as a mathematical discipline only took shape at the very end of the 17th century, particularly in Jacob Bernoulli's posthumous work Ars Conjectandi. This was the first book where the realm of games of chance and the realm of the probable (which concerned opinion, evidence, and argument) were combined and submitted to mathematical analysis. The method of least squares was first described by Adrien-Marie Legendre in 1805, though Carl Friedrich Gauss presumably made use of it a decade earlier in 1795.\n\nThe modern field of statistics emerged in the late 19th and early 20th century in three stages. The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics—height, weight and eyelash length among others. Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment, the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things. Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.\nThe second wave of the 1910s and 20s was initiated by William Sealy Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance (which was the first to use the statistical term, variance), his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments, where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information. He also coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\". In his 1930 book The Genetical Theory of Natural Selection, he applied statistics to various biological concepts such as Fisher's principle (which A. W. F. Edwards called \"probably the most celebrated argument in evolutionary biology\") and Fisherian runaway, a concept in sexual selection about a positive feedback runaway effect found in evolution.\nThe final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.\nAmong the early attempts to measure national economic activity were those of William Petty in the 17th century. In the 20th century the uniform System of National Accounts was developed.\nToday, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze big data.\n\n\n== Applications ==\n\n\n=== Applied statistics, theoretical statistics and mathematical statistics ===\nApplied statistics, sometimes referred to as Statistical science, comprises descriptive statistics and the application of inferential statistics. Theoretical statistics concerns the logical arguments underlying justification of approaches to statistical inference, as well as encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.\nStatistical consultants can help organizations and companies that do not have in-house expertise relevant to their particular questions.\n\n\n=== Machine learning and data mining ===\nMachine learning models are statistical and probabilistic models that capture patterns in the data through use of computational algorithms.\n\n\n=== Statistics in academia ===\nStatistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Business statistics applies statistical methods in econometrics, auditing and production and operations, including services improvement and marketing research. A study of two journals in tropical biology found that the 12 most frequent statistical tests are: analysis of variance (ANOVA), chi-squared test, Student's t-test, linear regression, Pearson's correlation coefficient, Mann-Whitney U test, Kruskal-Wallis test, Shannon's diversity index, Tukey's range test, cluster analysis, Spearman's rank correlation coefficient and principal component analysis.\nA typical statistics course covers descriptive statistics, probability, binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation. Modern fundamental statistical courses for undergraduate students focus on correct test selection, results interpretation, and use of free statistics software.\n\n\n=== Statistical computing ===\n\nThe rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.\nIncreased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with a new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R.\n\n\n=== Business statistics ===\n\nIn business, \"statistics\" is a widely used management- and decision support tool. It is particularly applied in financial management, marketing management, and production, services and operations management. Statistics is also heavily used in management accounting and auditing. The discipline of Management Science formalizes the use of statistics, and other mathematics, in business. (Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships.)\nA typical \"Business Statistics\" course is intended for business majors, and covers descriptive statistics (collection, description, analysis, and summary of data), probability (typically the binomial and normal distributions), test of hypotheses and confidence intervals, linear regression, and correlation; (follow-on) courses may include forecasting, time series, decision trees, multiple linear regression, and other topics from business analytics more generally. Professional certification programs, such as the CFA, often include topics in statistics.\n\n\n== Specialized disciplines ==\n\nStatistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:\n\nIn addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:\n\nStatistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions.\n\n\n== Misuse ==\n\nMisuse of statistics can produce subtle but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.\nEven when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.\nThere is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter. A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics, by Darrell Huff, outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).\nWays to avoid misuse of statistics include using proper diagrams and avoiding bias. Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias. Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs. Most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented. To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole. According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"\nTo assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:\n\nWho says so? (Does he/she have an axe to grind?)\nHow does he/she know? (Does he/she have the resources to know the facts?)\nWhat's missing? (Does he/she give us a complete picture?)\nDid someone change the subject? (Does he/she offer us the right answer to the wrong problem?)\nDoes it make sense? (Is his/her conclusion logical and consistent with what we already know?)\n\n\n=== Misinterpretation: correlation ===\n\nThe concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death, might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables.\n\n\n== See also ==\n\nFoundations and major areas of statistics\n\n\n== References ==\n\n\n== Further reading ==\n\nLydia Denworth, \"A Significant Problem: Standard scientific methods are under fire. Will anything change?\", Scientific American, vol. 321, no. 4 (October 2019), pp. 62–67. \"The use of p values for nearly a century [since 1925] to determine statistical significance of experimental results has contributed to an illusion of certainty and [to] reproducibility crises in many scientific fields. There is growing determination to reform statistical analysis... Some [researchers] suggest changing statistical methods, whereas others would do away with a threshold for defining \"significant\" results\". (p. 63.)\nBarbara Illowsky; Susan Dean (2014). Introductory Statistics. OpenStax CNX. ISBN 978-1938168208.\nStockburger, David W. \"Introductory Statistics: Concepts, Models, and Applications\". Missouri State University (3rd Web ed.). Archived from the original on 28 May 2020.\nOpenIntro Statistics Archived 2019-06-16 at the Wayback Machine, 3rd edition by Diez, Barr, and Cetinkaya-Rundel\nStephen Jones, 2010. Statistics in Psychology: Explanations without Equations. Palgrave Macmillan. ISBN 978-1137282392.\nCohen, J (1990). \"Things I have learned (so far)\" (PDF). American Psychologist. 45 (12): 1304–1312. doi:10.1037/0003-066x.45.12.1304. S2CID 7180431. Archived from the original (PDF) on 2017-10-18.\nGigerenzer, G (2004). \"Mindless statistics\". Journal of Socio-Economics. 33 (5): 587–606. doi:10.1016/j.socec.2004.09.033.\nIoannidis, J.P.A. (2005). \"Why most published research findings are false\". PLOS Medicine. 2 (4): 696–701. doi:10.1371/journal.pmed.0040168. PMC 1855693. PMID 17456002.\n\n\n== External links ==\n\n(Electronic Version): TIBCO Software Inc. (2020). Data Science Textbook.\nOnline Statistics Education: An Interactive Multimedia Course of Study. Developed by Rice University (Lead Developer), University of Houston Clear Lake, Tufts University, and National Science Foundation.\nUCLA Statistical Computing Resources (archived 17 July 2006)\nPhilosophy of Statistics from the Stanford Encyclopedia of Philosophy",
      "cleaned_text": "Statistics (from German: Statistik, orig. \"description of a state, a country\") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments. When census data (comprising every member of the target population) cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences made using mathematical statistics employ the framework of probability theory, which deals with the analysis of random phenomena. A standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is rejected when it is in fact true, giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected when it is in fact false, giving a \"false negative\"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis. Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems. \"Statistics is both the science of uncertainty and the technology of extracting information from data.\" - featured in the International Encyclopedia of Statistical Science.Statistics is the discipline that deals with data, facts and figures with which meaningful information is inferred. Data may represent a numerical value, in form of quantitative data, or a label, as with qualitative data. Data may be collected, presented and summarised, in one of two methods called descriptive statistics. Two elementary summaries of data, singularly called a statistic, are the mean and dispersion. Whereas inferential statistics interprets data from a population sample to induce statements and predictions about a population. Statistics is regarded as a body of science or a branch of mathematics. It is based on probability, a branch of mathematics that studies random events. Statistics is considered the science of uncertainty. This arises from the ways to cope with measurement and sampling error as well as dealing with uncertanties in modelling. Although probability and statistics were once paired together as a single subject, they are conceptually distinct from one another. The former is based on deducing answers to specific situations from a general theory of probability, meanwhile statistics induces statements about a population based on a data set. Statistics serves to bridge the gap between probability and applied mathematical fields. Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is generally concerned with the use of data in the context of uncertainty and decision-making in the face of uncertainty. Statistics is indexed at 62, a subclass of probability theory and stochastic processes, in the Mathematics Subject Classification. Mathematical statistics is covered in the range 276-280 of subclass QA (science > mathematics) in the Library of Congress Classification. The word statistics ultimately comes from the Latin word Status, meaning \"situation\" or \"condition\" in society, which in late Latin adopted the meaning \"state\". Derived from this, political scientist Gottfried Achenwall, coined the German word statistik (a summary of how things stand). In 1770, the term entered the English language through German and referred to the study of political arrangements. The term gained its modern meaning in the 1790s in John Sinclair's works. In modern German, the term statistik is synonymous with mathematical statistics. The term statistic, in singular form, is used to describe a function that returns its value of the same name. When full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models. To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population. Sampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction-inductively inferring from samples to the parameters of a larger or total population. A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements with different levels using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data-like natural experiments and observational studies-for which a statistician would use a modified, more structured estimation method (e.g., difference in differences estimation and instrumental variables, among many others) that produce consistent estimators. The basic steps of a statistical experiment are: Planning the research, including finding the number of replicates of the study, using the following information: preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects. Design of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data. Performing the experiment following the experimental protocol and analyzing the data following the experimental protocol. Further examining the data set in secondary analyses, to suggest new hypotheses for future study. Documenting and presenting the results of the study. Experiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed. An example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group. A case-control study is another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected. Various attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one (injective) transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation. Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating-point arithmetic. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented. Other categorizations have been proposed. For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. (See also: Chrisman (1998), van den Berg (1991).) The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer.\" A descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features of a collection of information, while descriptive statistics in the mass noun sense is the process of using and analyzing those statistics. Descriptive statistics is distinguished from inferential statistics (or inductive statistics), in that descriptive statistics aims to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent. Statistical inference is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population. Consider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables. The population being examined is described by a probability distribution that may have unknown parameters. A statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters. Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance. A random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value. Between two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter. Other desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter. This still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations. Interpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time. The alternative hypothesis is the name of the hypothesis that contradicts the null hypothesis. The best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (the status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors. Working from a null hypothesis, two broad categories of error are recognized: Type I errors where the null hypothesis is falsely rejected, giving a \"false positive\". Type II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed, giving a \"false negative\". Standard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean. A statistical error is the amount by which an observation differs from its expected value. A residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction). Mean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error. Many statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve. Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems. Most studies only sample part of a population, so results do not fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability. In principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds. Statistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value). The standard approach is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator does not belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false. Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably. Although in principle the acceptable level of statistical significance may be subject to debate, the significance level is the largest p-value that allows the test to reject the null hypothesis. This test is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the significance level, the lower the probability of committing type I error. Some problems are usually associated with this framework (See criticism of hypothesis testing): A difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this. One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to report confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it. Fallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is the probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability. Rejecting the null hypothesis does not automatically prove the alternative hypothesis. As everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed. Some well-known statistical tests and procedures are: An alternative paradigm to the popular frequentist paradigm is to use Bayes' theorem to update the prior probability of the hypotheses in consideration based on the relative likelihood of the evidence gathered to obtain a posterior probability. Bayesian methods have been aided by the increase in available computing power to compute the posterior probability using numerical approximation techniques like Markov Chain Monte Carlo. For statistically modelling purposes, Bayesian models tend to be hierarchical, for example, one could model each YouTube channel as having video views distributed as a normal distribution with channel dependent mean and variance N ( μ i , σ i ) {\\displaystyle {\\mathcal {N}}(\\mu _{i},\\sigma _{i})} , while modeling the channel means as themselves coming from a normal distribution representing the distribution of average video view counts per channel, and the variances as coming from another distribution. The concept of using likelihood ratio can also be prominently seen in medical diagnostic testing. Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Mathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory. All statistical analyses make use of at least some mathematics, and mathematical statistics can therefore be regarded as a fundamental component of general statistics. Formal discussions on inference date back to the mathematicians and cryptographers of the Islamic Golden Age between the 8th and 13th centuries. Al-Khalil (717-786) wrote the Book of Cryptographic Messages, which contains one of the first uses of permutations and combinations, to list all possible Arabic words with and without vowels. Al-Kindi's Manuscript on Deciphering Cryptographic Messages gave a detailed description of how to use frequency analysis to decipher encrypted messages, providing an early example of statistical inference for decoding. Ibn Adlan (1187-1268) later made an important contribution on the use of sample size in frequency analysis. Although the term statistic was introduced by the Italian scholar Girolamo Ghilini in 1589 with reference to a collection of facts and information about a state, it was the German Gottfried Achenwall in 1749 who started using the term as a collection of quantitative information, in the modern use for this science. The earliest writing containing statistics in Europe dates back to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt. Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences. The mathematical foundations of statistics developed from discussions concerning games of chance among mathematicians such as Gerolamo Cardano, Blaise Pascal, Pierre de Fermat, and Christiaan Huygens. Although the idea of probability was already examined in ancient and medieval law and philosophy (such as the work of Juan Caramuel), probability theory as a mathematical discipline only took shape at the very end of the 17th century, particularly in Jacob Bernoulli's posthumous work Ars Conjectandi. This was the first book where the realm of games of chance and the realm of the probable (which concerned opinion, evidence, and argument) were combined and submitted to mathematical analysis. The method of least squares was first described by Adrien-Marie Legendre in 1805, though Carl Friedrich Gauss presumably made use of it a decade earlier in 1795. The modern field of statistics emerged in the late 19th and early 20th century in three stages. The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics-height, weight and eyelash length among others. Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment, the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things. Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London. The second wave of the 1910s and 20s was initiated by William Sealy Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance (which was the first to use the statistical term, variance), his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments, where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information. He also coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\". In his 1930 book The Genetical Theory of Natural Selection, he applied statistics to various biological concepts such as Fisher's principle (which A. W. F. Edwards called \"probably the most celebrated argument in evolutionary biology\") and Fisherian runaway, a concept in sexual selection about a positive feedback runaway effect found in evolution. The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling. Among the early attempts to measure national economic activity were those of William Petty in the 17th century. In the 20th century the uniform System of National Accounts was developed. Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze big data. Applied statistics, sometimes referred to as Statistical science, comprises descriptive statistics and the application of inferential statistics. Theoretical statistics concerns the logical arguments underlying justification of approaches to statistical inference, as well as encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments. Statistical consultants can help organizations and companies that do not have in-house expertise relevant to their particular questions. Machine learning models are statistical and probabilistic models that capture patterns in the data through use of computational algorithms. Statistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Business statistics applies statistical methods in econometrics, auditing and production and operations, including services improvement and marketing research. A study of two journals in tropical biology found that the 12 most frequent statistical tests are: analysis of variance (ANOVA), chi-squared test, Student's t-test, linear regression, Pearson's correlation coefficient, Mann-Whitney U test, Kruskal-Wallis test, Shannon's diversity index, Tukey's range test, cluster analysis, Spearman's rank correlation coefficient and principal component analysis. A typical statistics course covers descriptive statistics, probability, binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation. Modern fundamental statistical courses for undergraduate students focus on correct test selection, results interpretation, and use of free statistics software. The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models. Increased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with a new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R. In business, \"statistics\" is a widely used management- and decision support tool. It is particularly applied in financial management, marketing management, and production, services and operations management. Statistics is also heavily used in management accounting and auditing. The discipline of Management Science formalizes the use of statistics, and other mathematics, in business. (Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships.) A typical \"Business Statistics\" course is intended for business majors, and covers descriptive statistics (collection, description, analysis, and summary of data), probability (typically the binomial and normal distributions), test of hypotheses and confidence intervals, linear regression, and correlation; (follow-on) courses may include forecasting, time series, decision trees, multiple linear regression, and other topics from business analytics more generally. Professional certification programs, such as the CFA, often include topics in statistics. Statistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include: In addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology: Statistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. Misuse of statistics can produce subtle but serious errors in description and interpretation-subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics. Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data-which measures the extent to which a trend could be caused by random variation in the sample-may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy. There is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter. A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics, by Darrell Huff, outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)). Ways to avoid misuse of statistics include using proper diagrams and avoiding bias. Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias. Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs. Most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented. To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole. According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\" To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case: Who says so? (Does he/she have an axe to grind?) How does he/she know? (Does he/she have the resources to know the facts?) What's missing? (Does he/she give us a complete picture?) Did someone change the subject? (Does he/she offer us the right answer to the wrong problem?) Does it make sense? (Is his/her conclusion logical and consistent with what we already know?) The concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death, might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables.",
      "sentences": [
        "Statistics (from German: Statistik, orig.",
        "\"description of a state, a country\") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.",
        "In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied.",
        "Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\".",
        "Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.",
        "When census data (comprising every member of the target population) cannot be collected, statisticians collect data by developing specific experiment designs and survey samples.",
        "Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole.",
        "An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements.",
        "In contrast, an observational study does not involve experimental manipulation.",
        "Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).",
        "Inferences made using mathematical statistics employ the framework of probability theory, which deals with the analysis of random phenomena.",
        "A standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model.",
        "A hypothesis is proposed for the statistical relationship between the two data sets, an alternative to an idealized null hypothesis of no relationship between two data sets.",
        "Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test.",
        "Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is rejected when it is in fact true, giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected when it is in fact false, giving a \"false negative\").",
        "Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.",
        "Statistical measurement processes are also prone to error in regards to the data that they generate.",
        "The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.",
        "\"Statistics is both the science of uncertainty and the technology of extracting information from data.\"",
        "- featured in the International Encyclopedia of Statistical Science.Statistics is the discipline that deals with data, facts and figures with which meaningful information is inferred.",
        "Data may represent a numerical value, in form of quantitative data, or a label, as with qualitative data.",
        "Data may be collected, presented and summarised, in one of two methods called descriptive statistics.",
        "Two elementary summaries of data, singularly called a statistic, are the mean and dispersion.",
        "Whereas inferential statistics interprets data from a population sample to induce statements and predictions about a population.",
        "Statistics is regarded as a body of science or a branch of mathematics.",
        "It is based on probability, a branch of mathematics that studies random events.",
        "Statistics is considered the science of uncertainty.",
        "This arises from the ways to cope with measurement and sampling error as well as dealing with uncertanties in modelling.",
        "Although probability and statistics were once paired together as a single subject, they are conceptually distinct from one another.",
        "The former is based on deducing answers to specific situations from a general theory of probability, meanwhile statistics induces statements about a population based on a data set.",
        "Statistics serves to bridge the gap between probability and applied mathematical fields.",
        "Some consider statistics to be a distinct mathematical science rather than a branch of mathematics.",
        "While many scientific investigations make use of data, statistics is generally concerned with the use of data in the context of uncertainty and decision-making in the face of uncertainty.",
        "Statistics is indexed at 62, a subclass of probability theory and stochastic processes, in the Mathematics Subject Classification.",
        "Mathematical statistics is covered in the range 276-280 of subclass QA (science > mathematics) in the Library of Congress Classification.",
        "The word statistics ultimately comes from the Latin word Status, meaning \"situation\" or \"condition\" in society, which in late Latin adopted the meaning \"state\".",
        "Derived from this, political scientist Gottfried Achenwall, coined the German word statistik (a summary of how things stand).",
        "In 1770, the term entered the English language through German and referred to the study of political arrangements.",
        "The term gained its modern meaning in the 1790s in John Sinclair's works.",
        "In modern German, the term statistik is synonymous with mathematical statistics.",
        "The term statistic, in singular form, is used to describe a function that returns its value of the same name.",
        "When full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples.",
        "Statistics itself also provides tools for prediction and forecasting through statistical models.",
        "To use a sample as a guide to an entire population, it is important that it truly represents the overall population.",
        "Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole.",
        "A major problem lies in determining the extent that the sample chosen is actually representative.",
        "Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures.",
        "There are also methods of experimental design that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.",
        "Sampling theory is part of the mathematical discipline of probability theory.",
        "Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures.",
        "The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method.",
        "The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples.",
        "Statistical inference, however, moves in the opposite direction-inductively inferring from samples to the parameters of a larger or total population.",
        "A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables.",
        "There are two major types of causal statistical studies: experimental studies and observational studies.",
        "In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed.",
        "The difference between the two types lies in how the study is actually conducted.",
        "Each can be very effective.",
        "An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements with different levels using the same procedure to determine if the manipulation has modified the values of the measurements.",
        "In contrast, an observational study does not involve experimental manipulation.",
        "Instead, data are gathered and correlations between predictors and response are investigated.",
        "While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data-like natural experiments and observational studies-for which a statistician would use a modified, more structured estimation method (e.g., difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.",
        "The basic steps of a statistical experiment are: Planning the research, including finding the number of replicates of the study, using the following information: preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability.",
        "Consideration of the selection of experimental subjects and the ethics of research is necessary.",
        "Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.",
        "Design of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error.",
        "At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data.",
        "Performing the experiment following the experimental protocol and analyzing the data following the experimental protocol.",
        "Further examining the data set in secondary analyses, to suggest new hypotheses for future study.",
        "Documenting and presenting the results of the study.",
        "Experiments on human behavior have special concerns.",
        "The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company.",
        "The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers.",
        "The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity.",
        "It turned out that productivity indeed improved (under the experimental conditions).",
        "However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness.",
        "The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself.",
        "Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.",
        "An example of an observational study is one that explores the association between smoking and lung cancer.",
        "This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis.",
        "In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group.",
        "A case-control study is another type of observational study in which people with and without the outcome of interest (e.g.",
        "lung cancer) are invited to participate and their exposure histories are collected.",
        "Various attempts have been made to produce a taxonomy of levels of measurement.",
        "The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales.",
        "Nominal measurements do not have meaningful rank order among values, and permit any one-to-one (injective) transformation.",
        "Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation.",
        "Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation.",
        "Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.",
        "Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature.",
        "Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating-point arithmetic.",
        "But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.",
        "Other categorizations have been proposed.",
        "For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances.",
        "Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data.",
        "The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions.",
        "\"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations.",
        "Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer.\"",
        "A descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features of a collection of information, while descriptive statistics in the mass noun sense is the process of using and analyzing those statistics.",
        "Descriptive statistics is distinguished from inferential statistics (or inductive statistics), in that descriptive statistics aims to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent.",
        "Statistical inference is the process of using data analysis to deduce properties of an underlying probability distribution.",
        "Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.",
        "It is assumed that the observed data set is sampled from a larger population.",
        "Inferential statistics can be contrasted with descriptive statistics.",
        "Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.",
        "Consider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables.",
        "The population being examined is described by a probability distribution that may have unknown parameters.",
        "A statistic is a random variable that is a function of the random sample, but not a function of unknown parameters.",
        "The probability distribution of the statistic, though, may have unknown parameters.",
        "Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function.",
        "Commonly used estimators include sample mean, unbiased sample variance and sample covariance.",
        "A random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot.",
        "Widely used pivots include the z-score, the chi square statistic and Student's t-value.",
        "Between two estimators of a given parameter, the one with lower mean squared error is said to be more efficient.",
        "Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.",
        "Other desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.",
        "This still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.",
        "Interpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.",
        "The alternative hypothesis is the name of the hypothesis that contradicts the null hypothesis.",
        "The best illustration for a novice is the predicament encountered by a criminal trial.",
        "The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty.",
        "The indictment comes because of suspicion of the guilt.",
        "The H0 (the status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\".",
        "However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict.",
        "So the jury does not necessarily accept H0 but fails to reject H0.",
        "While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.",
        "Working from a null hypothesis, two broad categories of error are recognized: Type I errors where the null hypothesis is falsely rejected, giving a \"false positive\".",
        "Type II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed, giving a \"false negative\".",
        "Standard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.",
        "A statistical error is the amount by which an observation differs from its expected value.",
        "A residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).",
        "Mean squared error is used for obtaining efficient estimators, a widely used class of estimators.",
        "Root mean square error is simply the square root of mean squared error.",
        "Many statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations.",
        "The latter gives equal weight to small and big errors, while the former gives more weight to large errors.",
        "Residual sum of squares is also differentiable, which provides a handy property for doing regression.",
        "Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares.",
        "Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise.",
        "Measurement processes that generate statistical data are also subject to error.",
        "The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.",
        "Most studies only sample part of a population, so results do not fully represent the whole population.",
        "Any estimates obtained from the sample only approximate the population value.",
        "Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population.",
        "Often they are expressed as 95% confidence intervals.",
        "Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases.",
        "This does not imply that the probability that the true value is in the confidence interval is 95%.",
        "From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable.",
        "Either the true value is or is not within the given interval.",
        "However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables.",
        "One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.",
        "In principle confidence intervals can be symmetrical or asymmetrical.",
        "An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate.",
        "Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.",
        "Statistics rarely give a simple Yes/No type answer to the question under analysis.",
        "Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).",
        "The standard approach is to test a null hypothesis against an alternative hypothesis.",
        "A critical region is the set of values of the estimator that leads to refuting the null hypothesis.",
        "The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator does not belong to the critical region given that the alternative hypothesis is true.",
        "The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.",
        "Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms.",
        "For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.",
        "Although in principle the acceptable level of statistical significance may be subject to debate, the significance level is the largest p-value that allows the test to reject the null hypothesis.",
        "This test is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic.",
        "Therefore, the smaller the significance level, the lower the probability of committing type I error.",
        "Some problems are usually associated with this framework (See criticism of hypothesis testing): A difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this.",
        "One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted.",
        "The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies.",
        "A better and increasingly common approach is to report confidence intervals.",
        "Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.",
        "Fallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is the probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result.",
        "An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.",
        "Rejecting the null hypothesis does not automatically prove the alternative hypothesis.",
        "As everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.",
        "Some well-known statistical tests and procedures are: An alternative paradigm to the popular frequentist paradigm is to use Bayes' theorem to update the prior probability of the hypotheses in consideration based on the relative likelihood of the evidence gathered to obtain a posterior probability.",
        "Bayesian methods have been aided by the increase in available computing power to compute the posterior probability using numerical approximation techniques like Markov Chain Monte Carlo.",
        "The concept of using likelihood ratio can also be prominently seen in medical diagnostic testing.",
        "Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.",
        "A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.",
        "Mathematical statistics is the application of mathematics to statistics.",
        "Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.",
        "All statistical analyses make use of at least some mathematics, and mathematical statistics can therefore be regarded as a fundamental component of general statistics.",
        "Formal discussions on inference date back to the mathematicians and cryptographers of the Islamic Golden Age between the 8th and 13th centuries.",
        "Al-Khalil (717-786) wrote the Book of Cryptographic Messages, which contains one of the first uses of permutations and combinations, to list all possible Arabic words with and without vowels.",
        "Al-Kindi's Manuscript on Deciphering Cryptographic Messages gave a detailed description of how to use frequency analysis to decipher encrypted messages, providing an early example of statistical inference for decoding.",
        "Ibn Adlan (1187-1268) later made an important contribution on the use of sample size in frequency analysis.",
        "Although the term statistic was introduced by the Italian scholar Girolamo Ghilini in 1589 with reference to a collection of facts and information about a state, it was the German Gottfried Achenwall in 1749 who started using the term as a collection of quantitative information, in the modern use for this science.",
        "The earliest writing containing statistics in Europe dates back to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.",
        "Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology.",
        "The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general.",
        "Today, statistics is widely employed in government, business, and natural and social sciences.",
        "The mathematical foundations of statistics developed from discussions concerning games of chance among mathematicians such as Gerolamo Cardano, Blaise Pascal, Pierre de Fermat, and Christiaan Huygens.",
        "Although the idea of probability was already examined in ancient and medieval law and philosophy (such as the work of Juan Caramuel), probability theory as a mathematical discipline only took shape at the very end of the 17th century, particularly in Jacob Bernoulli's posthumous work Ars Conjectandi.",
        "This was the first book where the realm of games of chance and the realm of the probable (which concerned opinion, evidence, and argument) were combined and submitted to mathematical analysis.",
        "The method of least squares was first described by Adrien-Marie Legendre in 1805, though Carl Friedrich Gauss presumably made use of it a decade earlier in 1795.",
        "The modern field of statistics emerged in the late 19th and early 20th century in three stages.",
        "The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well.",
        "Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics-height, weight and eyelash length among others.",
        "Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment, the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.",
        "Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.",
        "The second wave of the 1910s and 20s was initiated by William Sealy Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world.",
        "Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance (which was the first to use the statistical term, variance), his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments, where he developed rigorous design of experiments models.",
        "He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.",
        "He also coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\".",
        "In his 1930 book The Genetical Theory of Natural Selection, he applied statistics to various biological concepts such as Fisher's principle (which A. W. F. Edwards called \"probably the most celebrated argument in evolutionary biology\") and Fisherian runaway, a concept in sexual selection about a positive feedback runaway effect found in evolution.",
        "The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s.",
        "They introduced the concepts of \"Type II\" error, power of a test and confidence intervals.",
        "Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.",
        "Among the early attempts to measure national economic activity were those of William Petty in the 17th century.",
        "In the 20th century the uniform System of National Accounts was developed.",
        "Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology.",
        "The use of modern computers has expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually.",
        "Statistics continues to be an area of active research, for example on the problem of how to analyze big data.",
        "Applied statistics, sometimes referred to as Statistical science, comprises descriptive statistics and the application of inferential statistics.",
        "Theoretical statistics concerns the logical arguments underlying justification of approaches to statistical inference, as well as encompassing mathematical statistics.",
        "Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.",
        "Statistical consultants can help organizations and companies that do not have in-house expertise relevant to their particular questions.",
        "Machine learning models are statistical and probabilistic models that capture patterns in the data through use of computational algorithms.",
        "Statistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business.",
        "Business statistics applies statistical methods in econometrics, auditing and production and operations, including services improvement and marketing research.",
        "A study of two journals in tropical biology found that the 12 most frequent statistical tests are: analysis of variance (ANOVA), chi-squared test, Student's t-test, linear regression, Pearson's correlation coefficient, Mann-Whitney U test, Kruskal-Wallis test, Shannon's diversity index, Tukey's range test, cluster analysis, Spearman's rank correlation coefficient and principal component analysis.",
        "A typical statistics course covers descriptive statistics, probability, binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation.",
        "Modern fundamental statistical courses for undergraduate students focus on correct test selection, results interpretation, and use of free statistics software.",
        "The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science.",
        "Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.",
        "Increased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible.",
        "The computer revolution has implications for the future of statistics with a new emphasis on \"experimental\" and \"empirical\" statistics.",
        "A large number of both general and special purpose statistical software are now available.",
        "Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R. In business, \"statistics\" is a widely used management- and decision support tool.",
        "It is particularly applied in financial management, marketing management, and production, services and operations management.",
        "Statistics is also heavily used in management accounting and auditing.",
        "The discipline of Management Science formalizes the use of statistics, and other mathematics, in business.",
        "(Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships.)",
        "Professional certification programs, such as the CFA, often include topics in statistics.",
        "Statistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research.",
        "Some fields of inquiry use applied statistics so extensively that they have specialized terminology.",
        "These disciplines include: In addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology: Statistics form a key basis tool in business and manufacturing as well.",
        "It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions.",
        "Misuse of statistics can produce subtle but serious errors in description and interpretation-subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors.",
        "For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.",
        "Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise.",
        "The statistical significance of a trend in the data-which measures the extent to which a trend could be caused by random variation in the sample-may or may not agree with an intuitive sense of its significance.",
        "The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.",
        "There is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter.",
        "A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\".",
        "Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics, by Darrell Huff, outlines a range of considerations.",
        "In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g.",
        "Warne, Lazo, Ramos, and Ritter (2012)).",
        "Ways to avoid misuse of statistics include using proper diagrams and avoiding bias.",
        "Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias.",
        "Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs.",
        "Most people do not look for bias or errors, so they are not noticed.",
        "Thus, people may often believe that something is true even if it is not well represented.",
        "To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.",
        "According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"",
        "To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case: Who says so?",
        "(Does he/she have an axe to grind?)",
        "How does he/she know?",
        "(Does he/she have the resources to know the facts?)",
        "What's missing?",
        "(Does he/she give us a complete picture?)",
        "Did someone change the subject?",
        "(Does he/she offer us the right answer to the wrong problem?)",
        "Does it make sense?",
        "(Is his/her conclusion logical and consistent with what we already know?)",
        "The concept of correlation is particularly noteworthy for the potential confusion it can cause.",
        "Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected.",
        "For example, a study of annual income that also looks at age of death, might find that poor people tend to have shorter lives than affluent people.",
        "The two variables are said to be correlated; however, they may or may not be the cause of one another.",
        "The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable.",
        "For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables."
      ],
      "metadata": {
        "title": "Statistics",
        "url": "https://en.wikipedia.org/wiki/Statistics",
        "word_count": 6357,
        "char_count": 42451,
        "sentence_count": 270,
        "scraped_at": "2025-08-09T14:47:20.031173",
        "language": "en",
        "processing_time": 0.015341043472290039,
        "source_hash": "178055afdc702bda97aa733d2d557b60"
      }
    },
    {
      "title": "Rape statistics",
      "url": "https://en.wikipedia.org/wiki/Rape_statistics",
      "raw_text": "Statistics on rape and other acts of sexual assault are commonly available in industrialized countries, and have become better documented throughout the world. Inconsistent definitions of rape, different rates of reporting, recording, prosecution and conviction for rape can create controversial statistical disparities, and lead to accusations that many rape statistics are unreliable or misleading.\nIn some jurisdictions, male on female rape is the only form of rape counted in the statistics. Some jurisdictions also don't count being forced to penetrate another as rape, creating further controversy around rape statistics. Countries may not define forced sex on a spouse as rape. Rape is an under-reported crime. Prevalence of reasons for not reporting rape differ across countries. They may include fear of retaliation, uncertainty about whether a crime was committed or if the offender intended harm, not wanting others to know about the rape, not wanting the offender to get in trouble, fear of prosecution (e.g. due to laws against premarital sex), and doubt in local law enforcement.\nA United Nations statistical report compiled from government sources showed that more than 250,000 cases of rape or attempted rape were recorded by police annually. The reported data covered 65 countries.\n\n\n== Research ==\n\nMost rape research and reporting to date has been limited to male-female forms of rape. Research on male-male and female-male is beginning to be done. However, almost no research has been done on female-female rape, though women can be charged with rape in a few jurisdictions. A few books, such as Violent Betrayal: Partner Abuse in Lesbian Relationships by Dr. Claire M. Renzetti, No More Secrets: Violence in Lesbian Relationships by Janice Ristock, and Woman-to-Woman Sexual Violence: Does She Call It Rape? by Lori B. Girshick also cover the topic of rape of women by other women.\n\n\n== By country ==\nThis table indicates the annual number of recorded rapes per capita by country for last available year. Each entry is based on that country's definition of rape, which varies widely throughout the world. It does not specify whether recorded means reported, brought to trial, or convicted. It does not include cases of rape which go unreported or unrecorded. Alternative estimates can show large differences, such as South Africa having around 500,000 rapes per year, or Egypt having more than 20,000 rapes a year.\n\n\n== Policy and statistics by country ==\n\n\n=== Afghanistan ===\n\nRape in Afghanistan is a crime which can be legally prosecuted, but in practice it is very rarely reported, because of the immense risks that women face if they report it. In 2011, Afghanistan made international news in regard to the story of a woman who was raped by a man, jailed for adultery, gave birth to a child in jail, and was then subsequently pardoned by president Hamid Karzai, and in the end married the man who raped her.\nIn 2012, Afghanistan recorded 240 cases of honour killings and 160 cases of rape, but did not distinguish between the two crimes. In 2013, in eastern Ghazni, a man attacked a woman and attempted to rape her, and as a result the relatives of the woman killed both the woman and the man in an honour killing. In Afghanistan, crimes such as adultery, rape and trafficking are often conflated with each other, and it is generally not acceptable for a woman and a man to be alone together (unless married or related). If this happens, the response can be very violent: an Afghan medical doctor and his female patient were attacked by an angry mob who threw stones at them after the two were discovered in his private examining room without a chaperon. In 2013, the security forces were also alleged to rape children in the country.\n\n\n=== Algeria ===\n\nArticle 336 of the Penal Code stipulates that rape is a punishable offence, but does not give a definition of rape, as this is left to the courts. The lack of a clear definition of rape in Algerian law makes it difficult for women to report the act and seek legal remedies.\nThe Hassi Messaoud was reported in 2001, 2005, and 2010 to sexually assault, traffic, and abuse women.\nThere have been continuous allegations that during the Algerian War French troops had engaged in acts of torture and rape against Algerians.\n\n\n=== Australia ===\nNon-consensual sexual penetration is termed \"rape\" in Victoria, Queensland, South Australia, and Tasmania; \"Sexual Assault\" in New South Wales; \"Sexual intercourse without consent\" in the ACT and the Northern Territory; \"Sexual penetration without consent\" in Western Australia. All these offences are gender neutral and applicable in marriage. The laws in Australia have evolved from the English common law offence of rape, but have gradually changed, especially in the late 20th century.\nIn Australia the reported rape rate per 100,000 people is relatively high, although it is in a decreasing trend, coming down from 91.6 in 2003 to 28.6 in 2010. This stands in contrast to reported rape rate of 1.2 per 100,000 in Japan, 1.8 per 100,000 in India, 4.6 rapes per 100,000 in Bahrain, 12.3 per 100,000 in Mexico, 24.1 per 100,000 in United Kingdom, 28.6 per 100,000 in United States, 66.5 per 100,000 in Sweden.\nDuring the 12 months prior to interview in 2011–12, an estimated 51,200 (0.3%) Australians aged 18 years and over were a victim of sexual assault. Almost a third (30%) of victims of sexual assault had the most recent incident they experienced reported to the police.\nThe Australian Women's Safety Survey conducted by the Bureau of Statistics in 1996 involved a random sample of 6,300 women aged 18 and over. It produced incidence finding of 1.9 per cent for sexual assault in the previous 12 months. Men who are known to the woman accounted for over two-thirds of assailants (68%). Only 15% of the assaulted women in the sample reported to the police.\n\n\n=== Bangladesh ===\nBangladesh has received criticism for its employment of the \"two-finger test\" in rape investigations. This test consists of a physical examination of women who report rape during which a doctor inserts two fingers in the woman's vagina to determine whether the woman is \"habituated to sex\". This examination has its origin in the country's British colonial-era laws dating back to 1872. This deters many women from reporting rape. More than 100 experts, including doctors, lawyers, police, and women's rights activists, signed a joint statement in 2013 asking for the test, which they called \"demeaning\", to be abolished, as it \"does not provide any evidence that is relevant to proving the offence.\" On 12 April 2018 Bangladesh High Court banned the \"two-finger test\" on the ground that the tests have no scientific merit or evidential value.\nBetween the years of 2010 and 2013, the United Nations Multi-country Study on Men and Violence asked men in rural and urban Bangladesh if they had forced a woman to have sex at any point in their lives. 14.1% of men in rural Bangladesh and 9.5% of men in urban Bangladesh said yes (10% averaged). 2.7% of men in rural Bangladesh and 0.5% (6/1252) in urban Bangladesh had raped in the past year. In rural Bangladesh 41.4% of rapists perpetrated more than once, 3.7% had four or more victims, and 40% first raped as a teenager. 82% of rural Bangladeshi and 79% of urban Bangladeshi men cited entitlement as their reason for rape. 61.2% of urban Bangladeshi men who had raped did not feel guilty or worried afterwards, and 95.1% experienced no legal consequences. 3.7% of men in rural Bangladesh had raped another man. 89.2% of urban Bangladeshi men agreed to the statement \"if a woman doesn't physically fight back, it's not rape.\"\n\n\n=== Belgium ===\n\nIn 2008, the incidence of rapes recorded by the police was 26.3 per 100,000 people, according to data by UNODC.\nRape in Belgium is defined by Article 375 of the Penal Code as \"any act of sexual penetration, of whatever sort and by whatever means, committed on a non-consenting person\". Marital rape is also illegal under this law.\nApart from criminal proceedings, committing marital rape has also consequences in a divorce case. The new amendments of the Civil Code regulating marriage and divorce, that came into effect in September 2007, state that any of the spouses, following a divorce, may receive alimony if they need the money; but a spouse who has committed rape or other violent crimes against the other spouse cannot receive alimony. Article 301 reads: \"The court may refuse to grant the application for a alimony if the defendant proves that the applicant has committed a serious offense that rendered it impossible to continue living together. Under no circumstances will alimony be given to a spouse who was found guilty of an act referred to in Articles 375, 398-400, 402, 403 or 405 of the Penal Code, committed against the person of the defendant, or an attempt to commit an act referred to in Articles 375, 393, 394 or 397 of the Code against the same person.\"\n\n\n=== Belize ===\nAccording to the Belize Police Department, in 2013 there were 26 cases of rape reported in the country. The estimated total population in 2013 was 334,297.\nIn 2006, the incidence of rapes recorded by the police was 15.3 per 100,000 people, according to data by UNODC.\nAccording to a 2009 report, bribery in rape investigations, including those involving child rape, is common. Suspects often offer money to the police or to the victims/their families.\nThe laws were amended in Belize in 1999 to criminalize marital rape; the law defines marital rape that happens at the time the spouses are cohabiting more narrowly than rape in other circumstances; it stipulates that the act is criminal if \"The act of sexual intercourse is preceded or accompanied by or associated with, assault and battery, harm or injury to the female spouse\". Rape between unmarried persons, or between separated spouses is defined by lack of consent.\n\n\n=== Bosnia and Herzegovina ===\n\nDuring the Bosnian war, rape was prevalent. In 1993, a European Community commission estimated that around 20,000 women were raped, while the Bosnian Government put the figure at 50,000.\n\n\n=== Botswana ===\nIn a 2009 study, 4.9% of 1244 women of 13–24 years reported having been raped in their lifetimes. 10.3% of 654 women reported that they had been raped in their lifetimes in a 2011 study. 4.6% had been raped in the past year. 3.9% of 613 men had been raped in their lifetimes and 4.2% had raped in the past year.\n\n\n=== Brazil ===\nIn Brazil, rape is \"alarmingly under-reported\" and there are no accurate data to compare rape rates among the country's twenty-six states and federal district. However, in 2012, there were 6,029 rapes in the state of Rio de Janeiro; 4,993 of the victims were women. On average, 416 women a month were raped that year and according to Rio's state Institute of Public Security (ISP) the rate of rape in the state is 37 per 100,000 population for victims of both sexes. Rio's civil police say that in the first quarter of 2013, 1,822 rapes were committed, while there were only 70 individuals arrested for the crimes. Typically, the victims were mainly black women, aged between 20 and 30 years, and coming from any social class.\n\n\n=== Burma ===\nSystematic rape committed by the military against civilians has been documented in Myanmar. A 2002 report by The Shan Human Rights Foundation and The Shan Women's Action Network, titled License to Rape, details incidents of sexual violence committed by Tatmadaw (Burmese Army) troops in Shan State, mostly between 1996 and 2001.\nThe military of Burma has also been accused of continuing to use rape as a weapon of war after the elections of 2010. In 2014, a women's group, The Women's League of Burma, said it had documented more than 100 cases of rape by the military since 2010.\nAccording to a 2012 report by Human Rights Watch, the Burmese security forces have committed killings, rape, and mass arrests against Rohingya Muslims.\n\n\n=== Burundi ===\nMarital rape was criminalized in 2009, albeit with a rather symbolic sentence of only 8 days imprisonment and a fine of 10.000 to 50.000 Fbu. The new 2009 Criminal Code also criminalized homosexuality which was legal before; but it also abolished the capital punishment in the country, therefore the new Code received mixed reactions from human rights organizations.\nA report by Amnesty International found that rape was very common in Burundi, rarely prosecuted, and that victims faced strong social stigma and a high risk of reprisal.\n\n\n=== Cambodia ===\nIn Cambodia, rape is estimated by local and international NGOs to be common, but only a very small minority of these assaults are ever reported to authorities, due to the social stigma associated to being the victim of a sexual crime, and, in particular, to losing virginity before marriage (regardless of how this happened). From November 2008 to November 2009, police had recorded 468 cases of rape, attempted rape and sexual harassment, a 2.4 percent increase over the previous year. Breaking the Silence – Sexual Violence in Cambodia is a report produced by Amnesty International, and released in 2010, which examined the situation of sexual violence in Cambodia. The report found that, in the small minority of rapes which are reported, a very common response is for law-enforcement officials, including\npolice and court staff, to arrange extralegal out-of-court 'agreements' between the victim and the perpetrator (or their families), in which the rapist pays a sum of money which is shared between the authorities and the victim (and her family), after which the victim has to withdraw any criminal complaint against the perpetrator, and public prosecutors close the case. When a rape is investigated, a complainant is generally expected to pay an extralegal sum of money to the authorities, to ensure that the court investigates the case, otherwise progress is slow, and it may take over two years for anything to happen. During the pre-trial period, there is always a risk that the perpetrator's family will pay a bribe to secure his acquittal or reduced charge.\nThe UN reported results in 2013 from a study that they did in six Asia-Pacific countries about violence against women. 20.4% of Cambodian men said that they had raped a woman in their lifetime and 11.3% had raped in the past year. 3.3% had raped another man at some point and 23% had participated in gang rape, the largest percentage out of the nine areas surveyed. Cambodia was the only area where gang rape was the most commonly reported form of non-partner rape. 45% answered that sexual entitlement was their motive for raping a woman and 42% said they raped to punish a woman. 11.7% of rapists had raped 4 or more women. 52% first perpetrated rape as teenagers and 15.8% first did so under the age of 15. 44.5% of rapists experienced no legal consequences.\n\n\n=== Canada ===\nIn Canadian colonies, rape was an offence at common law. The conceptualization of rape was based on English common law understanding of this offence. English legal precedent was very important. Canada got its first statutory definition of rape in 1892, under the 1892 Criminal Code, which read: \"Rape is the act of a man having carnal knowledge of a woman who is not his wife without her consent, or with consent which has been extorted by threats or fear of bodily harm, or obtained by personating the woman’s husband, or by false and fraudulent representations as to the nature and quality of the act.\" A boy under 14 could not be convicted of rape.\nThe rape law remained virtually unchanged until 1983, when the criminal offence of \"rape\" was abolished and replaced by three sexual assault offences. Unlike the previous rape offence, the sexual assault offences are applicable in marriage and are gender neutral. These three offences are:\n\nSexual assault\nSexual assault with a weapon, threats to a third party or causing bodily harm\nAggravated sexual assault.\nThe most frequently cited research on sexual violence was conducted by Statistics Canada in 1992, which involved a national random sample of 12,300 women (Johnson and Sacco, 1995). The research found that over one in three women had experienced a sexual assault and that only 6% of sexual assaults were reported to the police. According to Justice Institute of British Columbia, one out of every 17 women is raped, 62% of rape victims were physically injured, 9% were beaten or disfigured.\n\n\n=== China ===\n\nBetween the years of 2010 and 2013, the United Nations multi-country Study on Men and Violence in Asia and the Pacific asked men in urban and rural areas of China if they had ever forced a female to have sex. 22.2% said yes. 9.3% had done so in the past year. 55% of the men who had raped had done so more than once and 23.2% had raped more than one woman. 86% cited sexual entitlement as their motive, 57% said that they raped out of fun or boredom, and 46% out of anger or punishment. And despite 47% of them reporting consequences of punishment, threats or violence as a result, 72.4% had not experienced legal consequences. 1.7% had raped another man. 2.2% had participated in gang rape. 25% who had raped reported first doing so as a teenager, the lowest percentage in the study. And while only 11.8% of men and 10.2% of women surveyed approved of generally blaming the victim, 53.7% and 53.5% of each agreed with the statement \"if a woman doesn't physically fight back, it's not rape.\"\nAccording to the US Department of State, there were 31,833 cases of rape in China in 2007.\n\n\n=== Colombia ===\nThe armed conflict in Colombia has resulted in increased sexual violence against women; and Colombian authorities have been accused of failing to investigate rape complaints and failing to control sexual attacks in the country. Marital rape was criminalized in 1996. Rape is very common among internally displaced women: it is reported that one in five of these women were raped.\n\n\n=== Democratic Republic of the Congo ===\n\nIn eastern Congo, the prevalence and intensity of rape and other sexual violence is described as the worst in the world. It is estimated that there are as many as 200,000 surviving rape victims living in the Democratic Republic of the Congo today. A new study says more than 400,000 women are raped in the Democratic Republic of Congo annually. War rape in the Democratic Republic of Congo has frequently been described as a \"weapon of war\" by commentators. Louise Nzigire, a local social worker, states that \"this violence was designed to exterminate the population.\" Nzigire observes that rape has been a \"cheap, simple weapon for all parties in the war, more easily obtainable than bullets or bombs.\"\nIn an analysis of 2565 patients who received medical care in the Médecins Sans Frontières sexual violence clinic in the capital of Ituri, Bunia, between 2005 and 2006, 73% (95.2% of male victims) reported being raped by armed men. 74.5% experienced gang rape (89.3% of male and 73.9% of female victims), with attack by between two and four perpetrators being the most common scenario (58.9%) for both sexes. 48.6% of victims were attacked while doing daily domestic activities outside their homes.\n\n\n=== Denmark ===\nAlthough only approximately 500 rapes are reported to the Danish police annually, several studies estimate that only a small minority of all rapes are actually reported, and only one in five reported rapes result in a conviction in court. For example, according to a 2014 study published by the European Union Agency for Fundamental Rights, Denmark had the highest prevalence rate of physical and sexual violence against women in Europe.\nThe Danish government was harshly criticized for inadequate laws in regard to sexual violence in a 2008 report produced by Amnesty International. The Danish criminal provisions regarding sexual crimes had remained nearly unchanged for 30 years, which lead Amnesty International to declare that \"legislation on rape and sexual violence [conflicted] with human rights principles concerning the need to protect an individual's sexual and physical integrity and right to self-determination.\" The organization repeatedly urged Denmark to bring legislation on rape in line with international law over several years, which lead to an amendment to the sexual offences code in 2013, following a change in government after the 2011 elections.\nSexual offences (Danish: Seksualforbrydelser) are defined in the Danish Penal Code, Chapter 24, Section 216–236. References in legislation to marriage were removed following the 2013 amendment (previously providing for a reduced sentence or a pardon), and sexual acts performed on victims in a helpless state now also count as rape.\nIn Denmark it was only 1999 that the first rape crisis centre was established.\n\n\n=== East Timor ===\nMarital rape was made illegal in East Timor in 2010, under the Law on Domestic Violence, Law No. 7/2010 which states that \"Sexual violence is understood as any conduct that induces the person to witness, to maintain or participate in unwanted sexual relations, even within a marriage, through intimidation, threats, coercion or use of force, or which limits or nullifies the exercise of sexual and reproductive rights\".\nThe UN claimed that thousands of East Timorese women were raped during the Indonesian occupation of East Timor and that rape was used by the Indonesian military as a weapon of war. The UN commission stated that: \"Rape, sexual slavery and sexual violence were tools used as part of the campaign designed to inflict a deep experience of terror, powerlessness and hopelessness upon pro-independence supporters.\"\n\n\n=== Egypt ===\n\nMarital rape is not a criminal offence in Egypt. Unlike many other countries in the Middle East, Egypt has, in 1999, abolished the law which stipulated that a man could escape a rape conviction if he married his victim after the fact.\nWomen are generally fearful when it comes to reporting rape. Engy Ghozlan of Egyptian Centre for Women's Rights and others suggest that the number of rape cases is over 200,000 every year. Ghozlan further adds that rapes are not decreasing because young men lack adequate income and employment, so their marriages are delayed.\nDuring the 2013-13 Egyptian protests, rape has been carried out publicly. On 3 July 2013, it was reported that about 91 women were raped and sexually abused in Tahrir Square in four days. By some estimates, the figure was about 169.\n\n\n=== Ethiopia ===\nRape is a very serious problem in Ethiopia, and the country is infamous for the practice of marriage by abduction, with the prevalence of this practice in Ethiopia being one of the highest in the world. In many parts of Ethiopia, it is common for a man, working in co-ordination with his friends, to kidnap a girl or woman, sometimes using a horse to ease the escape. The abductor will then hide his intended bride and rape her until she becomes pregnant. As the father of the woman's child, the man can claim her as his wife. Subsequently, the kidnapper may try to negotiate a bride price with the village elders to legitimize the marriage. Girls as young as eleven years old are reported to have been kidnapped for the purpose of marriage.\nEthiopia is estimated to have one of the highest rates of violence against women in the world. A report by the UN found that women in Ethiopia are the most likely to suffer domestic violence at the hands of their partners, and that nearly 60% of Ethiopian women were subjected to sexual violence. The 2004 Criminal Code of Ethiopia creates the offence of rape, by Article 620, which states that: \"Whoever compels a woman to submit to sexual intercourse outside wedlock, whether by the use of violence or grave intimidation, or after having rendered her unconscious or incapable of resistance, is punishable with rigorous imprisonment from five years to fifteen years\". There are also certain aggravated circumstances which lead to an increased punishment for rape. Apart from the criminal offence of rape, there are also other sexual offences in the Criminal Code. The age of consent is 18. As can be seen above, a woman cannot charge her husband with rape. However, the 2004 Criminal Code brings major improvements for women's rights in the country, by criminalizing several forms of violence against women, such as female genital mutilation, violence against pregnant women, marriage by abduction, child marriage, trafficking and sexual harassment, though Chapter III – Crimes Committed against life, person and health through harmful traditional practices (Articles 561–570) and other provisions (Articles 587, 597, 625, 635, 637, 648). Article 564 – Violence Against a Marriage Partner or a Person Cohabiting in an Irregular Union is a major step forward.\nThe Ethiopian military has been accused of committing systematic rapes against civilians. Human Rights Watch has repeatedly claimed that the army has attacked, beaten, raped and killed civilians, something which the Ethiopian authorities have denied. However, US scientists said that satellite images confirmed reports that the Ethiopian military had burnt towns and villages in Ethiopia's Somali region.\nA study in Addis Ababa of high school boys found that 4.3% had been raped in their lives. According to the WHO Multi-country Study on Women's Health and Domestic Violence against Women, 59% of women reported sexual abuse by a partner; while one third of women reported being \"physically forced\" to have sex against their will with their partner within the past 12 months. This was the highest prevalence of all countries surveyed.\n\n\n=== Finland ===\n\nIn Finland, the legal regulations on sexual offences were revised with a law that came into effect on 1. January 1999. Under this revision, sexual offences were divided into three levels: rape, aggravated rape and forcing someone into a sexual act. The revision also affects the cause of action. The law on rape (Chapter 20 - Sex offences Section 1 - Rape) states that: \"(1) A person who forces another into sexual intercourse by the use or threat of violence shall be sentenced for rape to imprisonment for at least one year and at most six years.\n(2) Also a person who, by taking advantage of the fact that another person, due to unconsciousness, illness, disability, state of fear or other state of helplessness, is unable to defend himself or herself or to formulate or express his or her will, has sexual intercourse with him or her, shall be sentenced for rape.\" The Finnish government does not produce data on rape on a regular basis, beyond the raw numbers of reported rape to Finnish police. The laws and guidelines have been criticized for not making specific reference to \"consent\" and for offering the possibility of mediation between the victim and perpetrator. Specific information on women victims of rape can be found only from separate studies, the last one made in 2004, and that study was based on reported rape offences during the years 1998–1999. The study showed that of 468 rapes or attempted rapes reported to the police, only 47 rape charges were made, or that merely 10 per cent of the rapes reported to the police lead to a prosecution. In most cases the rape victim and the offender knew each other, only in every fourth case was the woman attacked by a stranger. Almost half the rape occurred among acquaintances (corresponding to a date rape), and intimate or family relations were involved in 13 per cent of the cases.\nFinland had 980 cases of reported rape in 2013. The number of reported rape had increased in 2006 by 91% when measured since 1977, and by 27% when measured from 1997. According to a 2014 study published by the European Union Agency for Fundamental Rights, approximately 47% of women surveyed in Finland were said to have suffered physical and/or sexual abuse; which was the second highest rate after Denmark. Finland was one of the last countries in the EU to criminalize marital rape, making it illegal in 1994, after years of debate.\nConvicted rapists receive very short penalties compared to other countries, although this may be due to the fact that Finland has one of the lowest incarceration rates in the world. During 2001–2003, the average sentence for rape was two years' imprisonment, and only 63% of offenders served their sentences in prison, as 37% of sentences were conditional. The average sentence for aggravated rape was four years' imprisonment. For the offence of coercion into sexual intercourse, sentences were most often one year conditional imprisonment, but only 4% of offenders went to prison.\nThe issue of violence against women in Finland has been of major international interest and the situation has been described as a paradox, because otherwise the country has offered women high professional and social opportunities. According to Turku University law professor Kevät Nousiainen, \"the way Finns conceive gender is different. It's assumed women are perfectly capable of taking care of themselves, and if they are not able to do so, that is unacceptable.\" After World War I, Finland fought a war of independence, a civil war, and two decades later the Winter War, the Continuation War, and the Lapland War, which made up Finland's part in World War II. In each case Finland fought as a poorly trained underdog in brutal conditions that Nousiainen says left men \"unbalanced\". \"Violence was taken somehow for granted, it was tolerated. And then you have to consider the transfer of violent behaviour from generation to generation,\" she said.\n\n\n=== France ===\n\nArticle 222-23 of the criminal code reads: \"Any act of sexual penetration, whatever its nature, committed against another person by violence, constraint, threat or surprise, is rape\".\n\n\n=== Germany ===\n\nUnder German law, a person commits rape if he or she employs any of these three types of coercion: 1. force; or 2. threat of imminent danger to life or limb; or 3. exploitation of a situation in which the victim is unprotected and at the mercy of the offender. Germany was one of the last Western countries to criminalize marital rape, it did so only in 1997, after a lengthy political battle which started in the 1970s. The criminalization of marital rape has been delayed by political disagreement: even when there was consensus that it should be criminalized, there was disagreement between those who wanted it punished and prosecuted in the same way as non-marital rape and those who opposed this. These disagreements have delayed the criminalization until 1997, when rape in marriage was made illegal being treated in the same way as non-marital rape. In Germany the age of consent is 14, although some limitations do exist up to the age of 18 (regarding the exploitation of the lack of capacity for sexual self-determination of 14–15 y/o; and engaging in sexual activity with a person under 18 \"by taking advantage of an exploitative situation\"; or paying for sex with a minor under 18 - Section 182 of the Criminal Code). Chapter 13 of the Criminal Code is called \"Offences against sexual self-determination\" and consists of Sections 174 to 184 which define sexual crimes.\n\n\n=== Ghana ===\nIn a survey of Ghanaians, 8% of women reported having been raped by a man in their lifetimes and 5% of men reported having raped a wife or girlfriend.\n\n\n=== Iceland ===\nRape in Iceland is defined by Article 194 of the Penal Code which states: \"Any person who has sexual intercourse or other sexual relations with a person by means of using violence, threats or other unlawful coercion shall be guilty of rape and shall be imprisoned for a minimum of 1 year and a maximum of 16 years. ‘Violence’ here refers to the deprivation of independence by means of confinement, drugs or other comparable means. Exploiting a person's psychiatric disorder or other mental handicap, or the fact that, for other reasons, he or she is not in a condition to be able to resist the action or to understand its significance, in order to have sexual intercourse or other sexual relations with him or her, shall also be considered as rape, and shall result in the same punishment as specified in the first paragraph of this article.\"\nAlthough a Nordic country, known for a high level of gender equality, Iceland has, until recently, maintained outdated provisions in its sexual offences laws. Before 2007, the law in regard to rape and certain other sexual offences stated that, if after the assault the victim and the perpetrator got married or entered into an informal cohabitation, then the punishment could be waived; if the assault took place between married or cohabiting partners, and following the act, the victim continued to live together with the perpetrator, then the punishment could also be waived. These provisions were repealed by Act No. 61/2007. Other legal changes which were made included the broadening of the definition of rape and other sexual offences, and the raising of the age of consent to 15, from 14.\nIn 2008, the incidence of rapes recorded by the police was 21.6 per 100,000 people, according to data by UNODC. A 2010 study found that 6% of Icelandic women had been sexually victimized in an intimate relationship during their lifetime.\n\n\n=== India ===\n\nAccording to latest available statistics from the National Crime Records Bureau (NCRB), the country had a reported rape rate of 2.8 per 100,000 people as of 2022. However, the incidence of rape and its rates of reporting vary widely from rural to urban areas, and across India's 28 states and 8 union territories.\nIn 2018, official data showed that 1 rape was reported every 15 minutes in India. Of the 34,000 cases reported, just over 85 per cent led to charges, and 27 per cent ultimately led to convictions. Of these, 31,320 were committed by perpetrators known to the victim (93.9% of the cases). As high as 27.8 per cent of victims were minors or below 18, the legal age of consent. This high percentage of perpetrators being a close family member or acquaintance has remained constant over the years.\nIn 2015, the Times of India reported 300 rapes and 500 molestation cases were reported in two months from January to February 2015. But the absolute number of rapes reported have remained broadly similar since 2015.\nAs of 2018, Madhya Pradesh had the highest raw number of rape reports among Indian states. Among metropolitan cities, the national capital of Delhi continued to have the highest incidence of rape per capita.\nIn 2016, Union Minister for Women and Child Development Maneka Gandhi reported to Lok Sabha that 13,766 cases of child rape were reported to the National Crime Records Bureau in 2014. India ranked 94th in ranking of reported rape cases per 100,000 population in 2010.\n\n\n=== Indonesia ===\nThe United Nations Multi-country Study on Men and Violence studied three different sites of Indonesia (Jakarta, rural Java, and Jayapura). In the rural area, the lifetime prevalence of perpetration of rape towards a female/females was 19.5% and gang rape 7%. When rapists were asked why they perpetrated their last non-partner rape, 76.5% of the men in the three areas averaged cited sexual entitlement, 55.2% entertainment-seeking, and 29.7% anger/punishment.\n\n\n=== Italy ===\nDuring the first half of the 20th century, in some areas of Italy, rape victims were often expected and forced to marry their rapist. In 1965, a 17-year-old girl from Sicily, created a sensation when – fully supported by her poor family and the local police – she refused to marry the man who kidnapped and raped her. In refusing this \"rehabilitating marriage\" to the perpetrator, she went against the traditional social norms of the time which dictated such a solution and the rapist was sentenced to ten years in prison followed by two years of internal exile in another region. His seven accomplices were sentenced to five years. The Criminal Code of Italy also supported this practice, by exonerating the rapist who married the victim. The article of law whereby a rapist could extinguish his crime by marrying his victim was abolished in 1981. The Franca Viola incident was made into a movie called La moglie più bella.\nIn 1999, in an infamous case that gained international attention, the Court of Cassation of Italy ordered a new Appeal trial for a man a lower Court had found guilty of the rape of a woman who was wearing tight jeans. The Court did not claim that it is impossible to forcibly remove tight jeans \"without the collaboration of the person wearing them\" if she resists, but it claimed that such impossibility was plausible only in the given case and that, combined with other more significant evidence come up during the trial, it had led \"in abundantiam\" to ruling in favour of a new trial. The court did not equated the removal of the jeans with consent to sexual penetration as stated by Italian and Anglo-Saxon press. Following this ruling, there was outrage, both in Italy and abroad. In Italy, female politicians wore jeans to parliament in protest. In 2008 the Court of Cassation did not overturn this infamous ruling: it confirmed the guilty verdict of a lower Court in a case where the victim had removed her own jeans under duress.\nIn another case that sparked outrage, in 2006, the Court of Cassation ruled that a 41-year-old man who raped his 14-year-old stepdaughter could seek to have his sentence reduced – but not overturned – in a new appellate trial, due to the fact that the girl had already been sexually active: \"since the age of 13 [she] had had many sexual relations with men of every age and it's right to assume that at the time of the encounter with the suspect, her personality, from a sexual point of view, was much more developed than what one might normally expect from a girl of her age\". Therefore, the Court of Cassation ruled that the first Appeal Court should not have apodictically based its rejection of the existence of mitigating circumstances only on the long-life consequences of the rape.\nUNICEF in Italy stated that the decision \"seriously violates human rights and the dignity of a minor.\"\n\n\n=== Japan ===\n\nIn Japan, rape is defined as a crime of forced sexual intercourse in Article 177 of the Penal Code of Japan. National Police Agency publishes rape statistics in Japan.\n\n\n=== Jordan ===\nUnder the law of Jordan, rape is defined by Article 292, which reads: \"Whoever has sexual intercourse with a woman, other than his wife, without her consent—whether through coercion, threat, deception, or fraud—is punished with hard labor for no less than 15 years\". According to UNODC statistics, in 2006, the incidence of rapes recorded by the police was 1.9 per 100,000 people.\n\n\n=== Latvia ===\nThe laws on sexual offences were modified in Latvia in 2014, broadening the scope of the legislation. In Latvia, a person who commits an act of sexual intercourse by means of violence, threats, taking advantage of the state of helplessness of the victim, or by abuse of authority, is guilty of rape. (Section 159 of the Criminal Code). Rape and other sexual crimes are defined under Chapter XVI called \"Criminal Offences against Morals and Sexual Inviolability\". In 2014, Section 48 called Aggravating Circumstances (which defines circumstances which constitute an aggravation to a crime) was modified, ensuring that marital rape is covered by legislation, by defining as an aggravating circumstance the fact that: \"(15) a criminal offence related to violence or threats of violence, or against morals and sexual inviolability, is committed against a person to whom the perpetrator is related in the first or the second degree of kinship, against the spouse or former spouse, or against a person with whom the perpetrator is or has been in unregistered marital relationship, or against a person with whom the perpetrator has a joint (single) household\". In 2008, according to data by UNODC, the incidence of rapes recorded by the police was 4.4 per 100,000 people. In J. L. v. Latvia (2012), the European Court of Human Rights found that Latvia had failed to comply with its obligation under Article 3 of the European Convention on Human Rights to carry out an effective investigation into allegations of ill-treatment, because it had failed to properly investigate a prisoner's allegations of rape and assault by fellow inmates, who sought revenge against the victim due to his co-operation with the police.\n\n\n=== Lebanon ===\n\nIn December 2016, the Campaign Against Lebanese Rape Law - Article 522 was launched in order to abolish the article that allowed a rapist to avoid prison by marrying the victim. Prior to its abolishment in February 2017, the article read: \"If a valid contract of marriage is made between the perpetrator of any of the offences mentioned in this section, and the victim, the prosecution is suspended. If judgment was already passed, the implementation of the punishment is suspended.\"\nSince February 2017, other articles of the penal code are being amended to reinforce penalties against rapists that commit sexual assault on girls under 15.\n\n\n=== Lesotho ===\nRape is one of Lesotho's main social issues. According to UNODC, the incidence of rapes recorded in 2008 by the police in Lesotho was the highest incidence of any country. In a study of 1,049 women, 33% said they had been raped by the age of 18. In 66% of cases the rapist was a boyfriend. In the 2009 DHS survey 15.7% of men said that a husband is justified in hitting or beating his wife if she refuses to have sex with him, while 16% said a husband is justified to use force to have sex. HIV/AIDS in Lesotho is a very serious problem, with 23.1% of adults aged 15 to 49 living with it. In a study, researchers have concluded that \"Given the high prevalence of HIV in Lesotho, programs should address women's right to control their sexuality.\"\n\n\n=== Libya ===\nVictims of rape in Libya are often deemed as having 'dishonoured' their families and communities, and may face serious violence, including honour killings. According to UNHCR, \"In Libya when rape occurs, it seems to be a whole village or town which is seen to be dishonoured\". Women who have been raped experience extreme shame; according to a charity worker, being raped is \"worse than death for them [the victims]\".\n\n\n=== Mexico ===\nMexico has a federal law, as well as state laws. Mexican laws have been modernized significantly from the 1990s onwards. Rape laws used to include stipulations that the penalty was to be reduced if the victim had \"provoked\" the attacker. In 2005, the Supreme Court of Mexico ruled that forced sex in marriage is rape. In doing so, it overturned its prior verdict from 1994 when it had ruled that the laws on rape were inapplicable in marriage. In Mexico, the rape laws did not include a statutory exemption for marriage, but were, as elsewhere, generally understood as inapplicable in this context. This has started to be challenged in the late 20th century. Following the Court's decision in 1994, women's organizations worked to pass state laws against marital rape in order to overturn this precedent. The new 2005 verdict has been interpreted as evidence of the improvement of the position of women in the country. Mexico has recently been plagued by scandals of child sexual abuse in Catholic institutions. A 2013 violent gang rape of six Spanish tourist women in Acapulco has raised questions about how safe Mexico is for tourists.\n\n\n=== Netherlands ===\nThe law on rape in the Netherlands states that: \"A person who by an act of violence or another act or by threat of violence or threat of another act compels a person to submit to acts comprising or including sexual penetration of the body is guilty of rape and liable to a term of imprisonment of not more than twelve years or a fine of the fifth category.\"\nApart from the offence of 'rape', there are also other sexual offences. Marital rape was made illegal in 1991; before that date, rape was defined as a man forcing, by violence or threat of thereof, a woman to engage in sexual intercourse outside of marriage.\nAccording to a 2014 study published by the European Union Agency for Fundamental Rights, the Netherlands had the fourth highest prevalence rate of physical and sexual violence against women in Europe, with 45% of women having experienced such violence, which is well above the European average of 33%.\nIn 2020 an investigation by the daily newspaper Algemeen Dagblad revealed that since 2017 a judge has never imposed the maximum prison sentence of 12 years in a Dutch rape case. The highest sentence was six years, the lowest 21 days. On average, a rapist was jailed for a year and five months.\n\n\n=== Nicaragua ===\nIn a 2010 report on sexual violence in Nicaragua, Amnesty International stated that \"Rape of girls is endemic\". In Nicaragua, between 1998 and 2008, police recorded 14,377 cases of rape, with more than two thirds of reports involving girls under the age of 17. Reporting of rape, however, is estimated to be low, because rape victims often face social hostility and indifference from authorities. Since 2008, abortion is illegal without any exception, and this ban has been criticized as oppressive to rape victims who become pregnant.\nIn 2012, Nicaragua enacted Law no 779 – Integral Law against Violence against Women (Ley Integral contra la Violencia hacia la Mujer). This law criminalizes a wide range of acts of violence against women, such as domestic violence, including marital rape.\n\n\n=== Nigeria ===\nAccording to Amnesty International, police forces in Nigeria are reported to have perpetrated acts of rape and other sexual abuse against women, in public locations, or while women were transferred to police stations, or while women visited male detainees in police custody; and sometimes police used sexual violence in order to extract confessions and other information.\nA study of students of the Polytechnic, Ibadan found that in their lifetimes 1.7% (2.5% of males and 1.1% of females) had raped and 2.7% (5.3% of males and 0.9% of females) had attempted rape.\nOut of a sample of 295 female students from Ebonyi State University Abakaliki in Southeast Nigeria, 36.7% had experienced sexual harassment/victimization at least once on campus. Of this, 32.4% had been raped (10.8% of the sample).\nA study comparing the sexual practises of 12- to 19-year-old students with and without mild/moderate intellectual disabilities from schools across Oyo State, Nigeria found that 68.3% of the sexually experienced intellectually disabled females reported a history of rape victimization compared to 2.9% of the sexually experienced non-disabled females.\nA study analysing the hospital records of 76 sexual assault victims in Ile-Ife from 2007 to 2011 found that the majority (76.1%) of the victims that sought help at a hospital did so within 24 hours of their sexual assault, but forensic evidence was not gathered because rape kits have yet to be introduced in the country.\nIn a 2013 poll of 585 randomly selected adults from six Nigerian geopolitical zones by NOI Polls, 34% answered \"What do you think is the most prevalent cause of rape in the society?\" with 'indecent dressing'. 29% said they personally knew a victim of rape.\n\n\n=== North Korea ===\nThe situation regarding sexual violence in North Korea is very difficult to assess because of the unwillingness of the North Korean authorities to allow foreign investigators access in the country. According to Amnesty International, the analysis of satellite images of political prison camps (kwanliso) suggest that these camps are in continuous use and expansion. Amnesty International stated that hundreds of thousands of people, including children, are detained in these institutions, where they are subjected to extreme forms of abuse and violence, including rape. The organization cited a former security official at a kwanliso who worked there in the 1980s until the mid-1990s, and who confirmed these accounts. A United Nations panel has stated that the inmate population at political camps has been subjected to systematic extermination, torture, rape, forced abortions and starvation. According to the UN report, women at these camps are systematically subjected to rape by guards or bought and sold by human traffickers in China.\n\n\n=== Norway ===\nRape is defined by Section 192 of the Criminal Code which states: \"Any person who a) engages in sexual activity by means of violence or threats, or b) engages in sexual activity with any person who is unconscious or incapable for any other reason of resisting the act, or c) by means of violence or threats compels any person to engage in sexual activity with another person, or to carry out similar acts with himself or herself, shall be guilty of rape [...].\" Sexual crimes in Norway are defined in Chapter 19 – Sexual Offenses, which contains Sections 192 to 208.\nThe incidence of reported rape in Norway for 2010 is given as about 35 out of 100,000; there is no in-depth national statistic. A report released in February 2014, found that 9.4 percent of the 2435 women surveyed and 1.1 percent of the 2091 interviewed men stated that they were raped. A new report released in February 2023, found that 22 percent of the women surveyed stated that they were raped.\n\n\n==== Legislative history ====\nNorway overhauled its sexual offences legislation in 2000. The definition of rape was broadened to include also acts committed with persons incapable of resisting, rather than only acts enforced through direct violence or threat. The law is gender-neutral. In 1974, the Supreme Court of Norway confirmed the applicability of the rape law to marital intercourse, convicting for the first time a man of raping his wife.\n\n\n==== Concerns ====\nThere are concerns in Norway about the low reporting and conviction rate for rape. According to Amnesty International, 84% of rape cases reported to the police do not reach court; of those that reach trial, 36% end in acquittal. In 2003, the CEDAW Committee expressed concern about the situation of sexual violence in Norway, stating, \"[The Committee] is also concerned that an extremely low percentage of reported rapes results in convictions and that the police and public prosecutors dismiss an increasing number of such cases.\"\n\n\n=== Pakistan ===\n\nRape in Pakistan has been notable, and in recent times have continued to spike. In one case, a teenage girl was burnt alive, as she resisted the rape.\nIn another notable case a woman was raped on the orders of a village council, which functions as a lower-level judiciary. In 2002, 30-year-old Mukhtaran Bibi was gang-raped on the orders of the village council as an \"honor rape\" after allegations that her 12-year-old brother had had sexual relations with a woman from a higher caste. Although custom would expect her to commit suicide after being raped, Mukhtaran spoke up, and pursued the case, which was picked up by both domestic and international media. On 1 September 2002, an anti-terrorism court sentenced six men (including the four rapists) to death for rape. In 2005, the Lahore High Court cited \"insufficient evidence\" and acquitted 5 of the 6 convicted, and commuted the punishment for the sixth man to a life sentence. Mukhtaran and the government appealed this decision, and the Supreme Court suspended the acquittal and held appeal hearings. In 2011, the Supreme Court too acquitted the accused.\nIn 2015, a massive child molesting crime in Pakistani history was discovered. About 280 were raped and filmed. Some of these rapists used these video clips to blackmail the parents of those children. Most of victims were below 14 years old. Malik Ahmed Saeed Khan, a member of Provincial Assembly (MPA) of Punjab, also a member of the ruling party, Pakistan Muslim League-Nawaz, was accused for his involvement in this series of crimes, including selling rape video clips (around 400 clips) to the international market.\n\n\n=== Papua New Guinea ===\n\nPapua New Guinea has a very high rate of sexual violence, which has been attributed to the interaction between a very male-dominated culture and a culture which is also very accepting of violence in day-to-day life. Marital rape was criminalized in 2003. According to a 1993 survey by the PNG Institute of Medical Research, an estimated 55% of Papua New Guinean women have experienced rape.\nThe United Nations Multi-country Study on Men and Violence found that 62% of men from Bougainville Island had raped a woman and 7.6% had raped a man. 14% had participated in gang rape. 7% said they had been raped by another man.\nNon-partner rape was more commonly perpetrated than partner rape. 69.3% of the men who reported rape had raped more than once. 15.5% had raped four or more women or girls. 71% reported their motivation behind rape being sexual entitlement, 63% said they raped for entertainment, and 50% said they raped out of anger or to punish a woman. 52.2% had been jailed for their crime. Out of the nine areas surveyed, this was the highest rate of imprisonment.\n\n\n=== Qatar ===\nIn Qatar, like in most countries in the Middle East, sex outside of marriage is illegal. Women who report rape or sexual violence risk being charged with \"illicit relations\". Amnesty International has reported that migrant domestic workers are at very high risk of sexual abuse. In 2012, the UN Committee against Torture made reference to \"numerous allegations by migrant workers of physical abuse, sexual violence, rape and attempted rape\".\n\n\n=== Rwanda ===\n\nThe UN estimates that in Rwanda between 100,000 and 250,000 women were raped during the genocide in 1994. Rape was used as a weapon of war, and there are numerous children who were born from these rapes. Many of the women who were raped were also infected with HIV/AIDS.\nIn 2009 Rwanda adopted a law (Law on prevention and punishment of Gender Based Violence) which recognizes, for the first time in the country's history, marital rape as a crime.\n\n\n=== Saudi Arabia ===\nSaudi Arabia has general crime rates 100 times lower than that of America. In 1981, the rates of forcible rape were 0.33 out of 100,000. Badr-el-din Ali suggests this may be due to Saudi Arabia having a synnomic state of culture, where everyone uncompromisingly shares the same values.\n\n\n=== Somalia ===\nIn 2012, the reported nationwide prevalence rate ranged from 2% to 13%. Most incidents of sexual assault occurred within the context of the insurgency in southern Somalia. Over the first quarter of 2013, Amnesty International reported that 56.7% of victims in Mogadishu were internally displaced persons. According to the UN, there were at least 2,924 rape cases in IDP settlements in 2012. A third of the victims were under the age of 18. 70% of the perpetrators were armed men wearing uniforms, although it was not always clear whether they were members of militias, security forces or other individuals or groups. To address the issue, the central authorities as of December 2013 were in the process of forming a special crime unit to investigate and counter gender-based violence, as well as constructing a clinic set aside for victims of sexual assault. The national judiciary, security and police forces were all concurrently receiving specialized gender training as part of the broader reform effort. In June 2014, the Somali government also launched a National Action Plan against sexual violence in conjunction with local civil society groups.\n\n\n=== South Africa ===\n\nThe Criminal Law (Sexual Offences and Related Matters) Amendment Act, 2007 is the relevant legislation in South Africa. Despite the fact that this act provides modern and progressive laws, that ban rape and other forms of sexual abuse, including sexual violence within marriage, South Africa remains a country where sexual attacks are common. The country has some of the highest incidences of child and baby rape in the world with more than 67,000 cases of rape and sexual assaults against children reported in 2000, with welfare groups believing that unreported incidents could be up to 10 times higher. In 2001, a nine-month-old was raped and likely lost consciousness as the pain was too much to bear. Another nine-month-old baby was raped by six men, aged between 24 and 66, after the infant had been left unattended by her teenage mother. A four-year-old girl died after being raped by her father. A 14-month-old girl was raped by her two uncles. In February 2002, an eight-month-old infant was reportedly gang raped by four men. One has been charged. The infant has required extensive reconstructive surgery. The eight-month-old infant's injuries were so extensive, increased attention on prosecution has occurred. A significant contributing factor for the escalation in child abuse is the widespread myth in HIV-ravaged South Africa that having sex with a virgin will cure a man of AIDS. According to official figures, circa 11% of South Africans are infected with the virus. Edith Kriel, a social worker who helps child victims in the Eastern Cape, said: \"Child abusers are often relatives of their victims – even their fathers and providers.\"\nOne in three of the 4,000 women questioned by the Community of Information, Empowerment and Transparency said they had been raped in the past year. More than 25% of South African men questioned in a survey admitted to raping someone; of those, nearly half said they had raped more than one person, according to a new study conducted by the Medical Research Council (MRC). A 2010 study led by the government-funded Medical Research Foundation says that in Gauteng province, more than 37 percent of men said they had raped a woman. Nearly 7 percent of the 487 men surveyed said they had participated in a gang rape. Among children, a survey found 11% of boys and 4% of girls admitted to forcing someone else to have sex with them while in another survey among 1,500 schoolchildren in the Soweto township, a quarter of all the boys interviewed said that 'jackrolling', a term for gang rape, was fun.\nIn 2013 a study of 1991 grade nine boys at 46 secondary schools in Cape Town and Port Elizabeth found that 17.2% had raped.\nSouth Africa has some of the highest incidences of child and baby rape in the world. More than 25% of a sample of 1,738 South African men from the KwaZulu-Natal and Eastern Cape Provinces admitted when anonymously questioned to raping someone; of those, nearly half said they had raped more than one person, according to a non-peer-reviewed policy brief issued by the Medical Research Council (MRC). 4.95% had raped or attempted rape in the past year at the time of the survey. Several news publications extrapolated these results to the rest of the South African population. The humanitarian news organization IRIN claims that an estimated 500,000 rapes are committed annually in South Africa.\n\nAccording to University of Durban-Westville anthropology lecturer and researcher Suzanne Leclerc-Madlala, the myth that sex with a virgin is a cure for AIDS is not confined to South Africa. \"Fellow AIDS researchers in Zambia, Zimbabwe and Nigeria have told me that the myth also exists in these countries and that it is being blamed for the high rate of sexual abuse against young children.\"\n\"In South Africa, rape is so common it barely makes the news. The rapes of elderly women and babies are outlined in four-line stories on the inside pages of local newspapers, but most sexual assaults get no public attention.\"\nIn 2016, the police recorded 39,828 rapes which means rape rate of 71.3.\n\n\n=== South Sudan ===\nIn South Sudan, marital rape is not criminalized; the law on rape excludes it from its definition by stating that \"Sexual intercourse by a married couple is not rape, within the meaning of this section\". (Art 247). Child marriage is common in the country, and this often leads to child sexual abuse; while the law on rape sets an age of consent of 18, this does not apply inside marriage. The Criminal Code criminalizes, among other behaviours, adultery and homosexuality.\nAmnesty International has stated that the security forces in South Sudan have shot and raped civilians while carrying out a civilian disarmament campaign in Jonglei State.\n\n\n=== South Korea ===\nIn recent years, changes have been made to update South Korea's sex crime laws at the behest of President Park Geun-Hye, resulting in an increase in reported incidents. In 2015, reports of sexual assaults against foreigners were up 40% over 2008 numbers. Reports, apprehensions, and prosecutions have all risen with recent changes. However, victims often experience stigma due to traditional views of women's place in society and, although if convicted of rape an offender may be sentenced to between seven years and life in prison, convictions seldom result in a prison sentence.\n\n\n=== Sri Lanka ===\nIn Sri Lanka there have been recent allegations that rape and torture by the Sri Lankan security forces have continued for years after the civil war ended. An average rape case in Sri Lanka takes 6 to 12 years to be resolved.\nThe UN Multi-country Study on Men and Violence found that 14.5% of the sample of Sri Lankan men had perpetrated rape at some point in their lives. 4.9% had raped in the past year. 2.7% had raped another man. 1.6% had taken part in a gang rape. 96.5% of the men who had raped experienced no legal consequences. 65.8% did not feel worried or guilty afterwards. 64.9% of rapists had raped more than once, and 11.1% had raped four or more girls or women.\n\n\n=== Sudan ===\nThe law on rape states that: \"There shall be deemed to commit the offence of rape, whoever makes sexual intercourse, by way of adultery, or sodomy, with any person without his consent\".\nRape and other forms of sexual violence have been reported as being used on a large scale as a weapon of war in Darfur.\n\n\n=== Sweden ===\n\nA frequently cited source when comparing Swedish rape statistics internationally is the regularly published report by the United Nations Office on Drugs and Crime (UNODC), based on official statistics provided by each member state. In 2012, Sweden had 66 cases of reported rapes per 100,000 population, according to the Swedish National Council for Crime Prevention (Brå). This was unequivocally the biggest number reported to the UNODC in 2012. However, widely differing legal systems, offence definitions, terminological variations, recording practices and statistical conventions makes any cross-national comparison on rape statistics difficult, which is why the UNODC itself cautions against using their figures. It should also be noted that many countries do not report any rape statistics at all to the UNODC, and some report very low numbers, despite studies that indicate otherwise.\n\nThe Swedish police record each instance of sexual violence in every case separately, leading to an inflated number of cases compared to other countries. Sweden also has a comparatively wide definition of rape. This means that more sexual crimes are registered as rape than in most other countries. For example, in 2005 Sweden reformed its sex crime legislation and made the legal definition of rape much wider, which led to a marked increase in reports. Additionally, the Swedish police have improved the handling of rape cases, in an effort to decrease the number of unreported cases. For this reason, large-scale victimization surveys have been presented by criminologists as a more reliable indicator of rape prevalence. An EU-wide survey on sexual violence against women, published by the European Union Agency for Fundamental Rights (FRA) in 2014, showed Sweden was only third highest, below Denmark and Finland and a previous assessment by Brå have placed Sweden at an average level among European nations.\nAccording to the FRA study there is a strong correlation between higher levels of gender equality and disclosure of sexual violence. This, and a greater willingness among Swedish women to report rape in relationships, may also explain the relatively high rates of reported rape in Sweden, which has a long-standing tradition of gender equality policy and legislation, as well as an established women's movement, and has been ranked as the number one country in sex equality.\n\n\n=== Syria ===\nThe Syrian Civil War has been associated with a high incidence of war rape, which has led to the stigmatization of victims by their relatives and communities, and in turn to honour killings, forced marriages, and child marriages. According to the Euro Mediterranean Human Rights Network (EMHRN), about 6,000 women have been raped since the start of the conflict.\n\n\n=== Tanzania ===\nIn a survey of 1004 women (defined as 12 or older), 20% reported being raped in their lifetimes. 10% reported the event to police. In 92.4% of the events the perpetrator was known to the victim. There was no statistically significant difference between the rate of rape for women living in urban or suburban areas. 7% of the sample reported a rape occurring in the past two years. The socially closer the perpetrator, the lesser was the frequency of disclosure to either legal organs or other people, and vice versa. The prevalence of forced sexual initiation among women varied between 14% (province) and 17% (city), according to the WHO Multi-country Study on Women's Health and Domestic Violence against Women.\n\n\n=== Turkey ===\nIn Turkey some commonly expressed views on rape were presented when individuals from various professions were asked to agree or disagree with the statement \"some women deserve rape\". Thirty-three to sixty-six percent of the police officers agreed with the statement as well as nearly 50% of other professional groups. The exception were the responses of psychologists about 18% and 27% of psychiatrists who agreed with the statement. Some of these suggested that \"the physical appearance and behaviors of women tempt men to rape.\"\nIn 2013, The Guardian reported on claims by activists for the Kurdish terrorist group the PKK of widespread sexual abuse of prisoners allegedly used by the Turkish government to suppress dissent.\n\n\n=== United Kingdom ===\n\nThe Sexual Offences Act 2003 (for England and Wales), the Sexual Offences (Scotland) Act 2009 and the Sexual Offences (Northern Ireland) Order 2008 are relevant legislative acts in the United Kingdom.\nUnlike other jurisdictions, such as Australia, much of the US, and many Western countries, 'rape' in the UK is not a gender-neutral offence: it is an offence that can only be committed by a male against a person (female or male). Also the UK has not to date followed the trend in many Western countries of classifying acts other than penetration with a penis (e.g. penetration with an object, finger) as rape. These must be prosecuted under the other, equally severe, statute of assault by penetration.\nThe British Crime Survey 2000 found that 61,000 women were raped in England and Wales in 1999. It was also reported that approximately 754,000 women, over the age of 16, have been raped at least once in their lifetime. In 2001, the BCS (British Crime Survey) found that in the previous year, 47,000 women over the age of 16 were reported to have been raped.\nThe 2006–07 Crime Survey for England and Wales (formerly the British Crime Survey) reports that 1 in every 200 women were raped in that period. It also showed that only 800 people were convicted of rape crimes that same year, meaning that less than 1 in every 100 occurrences of rape led to a conviction. According to the 2015 Crime Survey for England and Wales, from January 2015 to Dec 2015, there were 34,000 incidences of rape.\nAccording to the NCPCC, 1 in 20 children have been sexually abused in the UK. 12% of boys and 3% of girls reported committing sexual violence against their partners. In 2013, a Ministry of Justice report stated that only 15 per cent of victims of the most serious sexual offences reported the incident to the police.\nA 2013 Rape Crisis survey found that one third of the 1000 women surveyed thought that if a woman did not fight back, then she could not have experienced rape. Meanwhile, 60% thought that a woman could not have experienced rape if she did not say 'no'.\nAccording to the charity Rape Crisis 85,000 women and 12,000 men are raped each year in England and Wales, and only 15% of victims chose to report the crime to police.\nIn the year to the end of March 2020, 58,856 cases of rape were recorded by police forces in England and Wales. These led to just 2,102 prosecutions, compared with 3,043 in the previous 12 months.\n\nOver the years conviction rates have increased, however the UK remains as one of the countries with the lowest conviction rate for rape in Europe.\n\n\n=== United States ===\n\nIn 2011, the US Centers for Disease Control and Prevention (CDC) found that \"nearly 20% of all women\" in the United States suffered attempted rape or rape sometime in their lives. More than a third of the victims were raped before the age of 18.\nThe Centers for Disease Control and Prevention maintains recent statistics and standardized definitions upon which their statistics are based.  A 2011 report on prison rape stated that \"in 2008 there were at least 69,800 inmates who were raped under conditions involving force or threat of force, and more than 216,600 total victims of sexual abuse, in America’s prisons, jails, and juvenile detention centers.\"\nData on the prevalence of rape vary greatly depending on what definition of rape is used. The FBI recorded 85,593 rapes in 2010. The Centers for Disease Control and Prevention reported nearly 1.3 million incidents that year. It should however be noted that the CDC's definition of rape \"represents the public health perspective\" and takes into account the ability of the victim to consent to sex because he or she had been drinking or taking drugs while the FBI defines rape as \"Penetration, no matter how slight, of the vagina or anus with any body part or object, or oral penetration by a sex organ of another person, without the consent of the victim.\"\nA 2007 survey by the National Institute of Justice found that 19.0% of college women and 6.1% of college men experienced either sexual assault or attempted sexual assault since entering college. In the University of Pennsylvania Law Review in 2017, D. Tuerkheimer reviewed the literature on rape allegations, and reported on the problems surrounding the credibility of rape victims, and how that relates to false rape accusations. She pointed to national survey data from the Centers for Disease Control and Prevention that indicates 1 in every 5 women (and 1 in 71 men) will be raped during their lifetime at some point. Despite the prevalence of rape and the fact that false rape allegations are rare, Tuerkheimer reported that law enforcement officers often default to disbelief about an alleged rape. This documented prejudice leads to reduced investigation and criminal justice outcomes that are faulty compared to other crimes. Tuerkheimer says that women face \"credibility discounts\" at all stages of the justice system, including from police, jurors, judges, and prosecutors. These credibility discounts are especially pronounced when the victim is acquainted with the accuser, and the vast majority of rapes fall into this category. The U.S. Department of Justice estimated from 2005 to 2007 that about 2% of victims who were raped while incapacitated (from drugs, alcohol, or other reasons) reported the rape to the police, compared to 13% of victims who experienced physically forced sexual assault.\nThe 1998 National Violence Against Women Survey, based on a sample size of 8,000, estimated the incidence of rape to be 1 in 6 for women and 1 in 33 for men, based on reports of attempted or completed rapes over the course of her or his lifetime.\nA 1997 study on the non-institutionalized, non-military population by the U.S. Bureau of Justice Statistics, which defines rape as forced penetration by the offender, found that 91% of reported rape victims are female and 9% are male.\nThe majority of rapes in the United States go unreported. According to the American Medical Association (1995), sexual violence, and rape in particular, is considered the most under-reported violent crime. The US Bureau of Justice Criminal Victimization Statistics reports that up to 66.1% of rapes go unreported.  Some of the most common reasons given by victims for not reporting rapes are when the victim considers it a personal or private matter, and the fear of reprisal from the assailant. Under-reporting affects the accuracy of this data.\nA significant number of rapes reported to the police do not advance to prosecution. Twenty-five percent of reported rapes result in arrest. Only 16% of rapes and sexual assaults are reported to the police (Rape in America: A Report to the Nation. 1992 and United Nations Populations Fund, 2000a). Factoring in unreported rapes, about 5% of rapists will ever spend a day in jail.\nContrary to widespread belief, rape outdoors is rare. Over two thirds of all rapes occur in someone's home. 31% occur in the perpetrators' homes, 27% in the victims' homes and 10% in homes shared by the victim and perpetrator. 7% occur at parties, 7% in vehicles, 4% outdoors and 2% in bars. From 2000 to 2005, 59% of rapes were not reported to law enforcement. One factor relating to this is the misconception that most rapes are committed by strangers. In reality, studies indicate the following varying numbers:\n\nIn a 2012 news story, The New York Times reported, \"according to a survey by the Alaska Federation of Natives, the rate of sexual violence in rural villages like Emmonak is as much as 12 times the national rate. And interviews with Native American women here and across the nation’s tribal reservations suggest an even grimmer reality: They say few, if any, female relatives or close friends have escaped sexual violence.\" In a national survey conducted in the United States of America, 14.8% of women over 17 years of age reported having been raped in their lifetime (with an additional 2.8% having experienced attempted rape) and 0.3% of the sample reported having been raped in the previous year.\nDrug use, especially alcohol, is frequently involved in rape. A study (only of rape victims that were female and reachable by phone) reported detailed findings related to tactics. In 47% of such rapes, both the victim and the perpetrator had been drinking. In 17%, only the perpetrator had been. 7% of the time, only the victim had been drinking. Rapes where neither the victim nor the perpetrator had been drinking were 29% of all rapes. Not only has it been a factor in the rates of sexual assault on campus, but because of the prevalence, assaults are also being affected specifically by the inability to give consent when intoxicated and bystanders not knowing when to intervene due to their own intoxication or the intoxication of the victim.\nKoss, Gidycz and Wi published a study in 1987 where they interviewed approximately 6,000 college students on 32 college campuses nationwide. They asked several questions covering a wide range of behaviours. From this study, 15% of college women answered \"yes\" to questions about whether they experienced something that met the definition of rape. 12% of women answered \"yes\" to questions about whether they experienced something that met the definition of attempted rape. Moreover, depending on the region, 2-6% of the men interviewed admitted to rape. While the study focused on female victims and male perpetrators; it did not consider rape of men or rape in LGBT relationships.\nIn 1995, the CDC replicated part of this study with 8,810 students on 138 college campuses. They examined rape only, and did not look at attempted rape. They found that 20% of women and 4% of men had experienced rape in the course of her or his lifetime.\nIn 2000, the National Institute of Justice and the Bureau of Justice Statistics published a study called \"The Sexual Victimization of College Women\" based on a 1996–1997 survey. The study found that 3.1% of undergraduate women reported experiencing an act that met the researchers' definition of rape or attempted rape during a 6–7-month academic year. However, of those found to have experienced completed rape, only 46.5% of the victims answered that they considered the incident to be a rape, while 48.8% did not and 4.7% were unsure. The study also found that 10.1% of college women experienced rape and 10.9% experienced attempted rape prior to entering college. Victimization of men was not considered as part of this study.\nIn a different section of the report, the authors speculated about whether statistics during an academic year generalize to an entire college experience. For a full discussion, read more on page 10 of the report, stating that \"the percentage of completed or attempted rape victimization among women in higher educational institutions might climb to between one-fifth and one-quarter\" and further acknowledging in the corresponding footnote, #18, that \"These projections are suggestive. To assess accurately the victimization risk for women throughout a college career, longitudinal research following a cohort of female students across time is needed.\"\n80,000 American children are known to have been sexually abused each year. But unreported cases are higher, due to the fear among children. Over ninety percent of the time, the perpetrator is someone familiar or close with the child. Sexually violent crimes targeting children involve forced sexual activities such as intercourse, masturbation, and/or other explicit contact with a minor. According to Child Protective Services, eighty percent of the time, a parent ends up being the perpetrator. Children who become victims of this crime often end up developing phobias, depression, and post-traumatic stress disorder, as well as performing poorly in school. Sexually violent crimes of all ages occur often.\nAccording to United States Department of Justice document Criminal Victimization in the United States, there were overall 191,670 victims of rape or sexual assault reported in 2005.\nMyriam Denov (2004) states that societal responses to the issue of female perpetrators of sexual assault \"point to a widespread denial of women as potential sexual aggressors that could work to obscure the true dimensions of the problem.\" Particularly as an increasing population of un-convicted felons and rapists who continue to insist that accusation of sexual assault is a punishment in lieu of justice through law enforcement agencies. It is thought that to be accused of rape brings shame to their families and social communities.\nAccording to the National Crime Victimization Survey, the adjusted per-capita victimization rate of rape has declined from about 2.4 per 1000 people (age 12 and above) in 1980 to about 0.4 per 1000 people in 2006, a decline of about 85%. But other government surveys, such as the Sexual Victimization of College Women study, critique the NCVS on the basis it includes only those acts perceived as crimes by the victim, and report a higher victimization rate. Despite a decline of 60% since 1993, the US still has a relatively high rate of rape when compared to other developed countries.\nRAINN asserts that from 2000 to 2005, 59% of rapes were not reported to law enforcement. For college students, the figure was 95% in 2000. One factor relating to this is the misconception that most rapes are committed by strangers. According to the Bureau of Justice Statistics, 38% of victims were raped by a friend or acquaintance, 28% by \"an intimate\" and 7% by another relative, and 26% were committed by a stranger to the victim. About four out of ten sexual assaults take place at the victim's own home.\n\n\n=== Yemen ===\nYemen law does not recognize marital rape and does not provide a minimum age for marriage. The issues of child marriage and child rape inside marriage have made international news and have led to calls for legislative changes. There have been several reports of deaths of young girls due to violent rape by adult husbands, as well as young girls dying during childbirth. Human Rights Watch stated that \"Child marriages and forced marriages remain widespread, exposing young girls to domestic violence and maternal mortality and truncating their education.\"\n\n\n== See also ==\nCrime statistics\nEstimates of sexual violence\n\n\n== Note ==\n\n\n== References ==\n\n\n== Further reading ==\nMacdonalds, J. (2007). Rape. In The World Book Encyclopedia. United States of America: World Book Inc.\n\"What is rape and sexual assault?\". Metropolitan Police UK. Retrieved 10 December 2024. the legal definition of rape is when a person intentionally penetrates another's vagina, anus or mouth with a penis, without the other person's consent.\nRape (2007). In The New Encyclopædia Britannica (Vol. 9). Chicago, Il.: Britannica.\nHoward, Angela & Kavenik Francis. (2000). Handbook of American Women's History. CA: Sage Publications Inc.\nRanking of US States by Rate of Rape -- Per Capita -- Compiled From FBI UCR \n\n\n== External links ==\nFBI's Uniform Crime Reports\nFemale Victims of Sexual Violence, 1994-2010 Bureau of Justice Statistics\nStatistics from RAINN\nWere 75 percent of Liberian women and girls raped? No. So why is the U.N. repeating that misleading 'statistic'?",
      "cleaned_text": "Statistics on rape and other acts of sexual assault are commonly available in industrialized countries, and have become better documented throughout the world. Inconsistent definitions of rape, different rates of reporting, recording, prosecution and conviction for rape can create controversial statistical disparities, and lead to accusations that many rape statistics are unreliable or misleading. In some jurisdictions, male on female rape is the only form of rape counted in the statistics. Some jurisdictions also don't count being forced to penetrate another as rape, creating further controversy around rape statistics. Countries may not define forced sex on a spouse as rape. Rape is an under-reported crime. Prevalence of reasons for not reporting rape differ across countries. They may include fear of retaliation, uncertainty about whether a crime was committed or if the offender intended harm, not wanting others to know about the rape, not wanting the offender to get in trouble, fear of prosecution (e.g. due to laws against premarital sex), and doubt in local law enforcement. A United Nations statistical report compiled from government sources showed that more than 250,000 cases of rape or attempted rape were recorded by police annually. The reported data covered 65 countries. Most rape research and reporting to date has been limited to male-female forms of rape. Research on male-male and female-male is beginning to be done. However, almost no research has been done on female-female rape, though women can be charged with rape in a few jurisdictions. A few books, such as Violent Betrayal: Partner Abuse in Lesbian Relationships by Dr. Claire M. Renzetti, No More Secrets: Violence in Lesbian Relationships by Janice Ristock, and Woman-to-Woman Sexual Violence: Does She Call It Rape? by Lori B. Girshick also cover the topic of rape of women by other women. This table indicates the annual number of recorded rapes per capita by country for last available year. Each entry is based on that country's definition of rape, which varies widely throughout the world. It does not specify whether recorded means reported, brought to trial, or convicted. It does not include cases of rape which go unreported or unrecorded. Alternative estimates can show large differences, such as South Africa having around 500,000 rapes per year, or Egypt having more than 20,000 rapes a year. Rape in Afghanistan is a crime which can be legally prosecuted, but in practice it is very rarely reported, because of the immense risks that women face if they report it. In 2011, Afghanistan made international news in regard to the story of a woman who was raped by a man, jailed for adultery, gave birth to a child in jail, and was then subsequently pardoned by president Hamid Karzai, and in the end married the man who raped her. In 2012, Afghanistan recorded 240 cases of honour killings and 160 cases of rape, but did not distinguish between the two crimes. In 2013, in eastern Ghazni, a man attacked a woman and attempted to rape her, and as a result the relatives of the woman killed both the woman and the man in an honour killing. In Afghanistan, crimes such as adultery, rape and trafficking are often conflated with each other, and it is generally not acceptable for a woman and a man to be alone together (unless married or related). If this happens, the response can be very violent: an Afghan medical doctor and his female patient were attacked by an angry mob who threw stones at them after the two were discovered in his private examining room without a chaperon. In 2013, the security forces were also alleged to rape children in the country. Article 336 of the Penal Code stipulates that rape is a punishable offence, but does not give a definition of rape, as this is left to the courts. The lack of a clear definition of rape in Algerian law makes it difficult for women to report the act and seek legal remedies. The Hassi Messaoud was reported in 2001, 2005, and 2010 to sexually assault, traffic, and abuse women. There have been continuous allegations that during the Algerian War French troops had engaged in acts of torture and rape against Algerians. Non-consensual sexual penetration is termed \"rape\" in Victoria, Queensland, South Australia, and Tasmania; \"Sexual Assault\" in New South Wales; \"Sexual intercourse without consent\" in the ACT and the Northern Territory; \"Sexual penetration without consent\" in Western Australia. All these offences are gender neutral and applicable in marriage. The laws in Australia have evolved from the English common law offence of rape, but have gradually changed, especially in the late 20th century. In Australia the reported rape rate per 100,000 people is relatively high, although it is in a decreasing trend, coming down from 91.6 in 2003 to 28.6 in 2010. This stands in contrast to reported rape rate of 1.2 per 100,000 in Japan, 1.8 per 100,000 in India, 4.6 rapes per 100,000 in Bahrain, 12.3 per 100,000 in Mexico, 24.1 per 100,000 in United Kingdom, 28.6 per 100,000 in United States, 66.5 per 100,000 in Sweden. During the 12 months prior to interview in 2011-12, an estimated 51,200 (0.3%) Australians aged 18 years and over were a victim of sexual assault. Almost a third (30%) of victims of sexual assault had the most recent incident they experienced reported to the police. The Australian Women's Safety Survey conducted by the Bureau of Statistics in 1996 involved a random sample of 6,300 women aged 18 and over. It produced incidence finding of 1.9 per cent for sexual assault in the previous 12 months. Men who are known to the woman accounted for over two-thirds of assailants (68%). Only 15% of the assaulted women in the sample reported to the police. Bangladesh has received criticism for its employment of the \"two-finger test\" in rape investigations. This test consists of a physical examination of women who report rape during which a doctor inserts two fingers in the woman's vagina to determine whether the woman is \"habituated to sex\". This examination has its origin in the country's British colonial-era laws dating back to 1872. This deters many women from reporting rape. More than 100 experts, including doctors, lawyers, police, and women's rights activists, signed a joint statement in 2013 asking for the test, which they called \"demeaning\", to be abolished, as it \"does not provide any evidence that is relevant to proving the offence.\" On 12 April 2018 Bangladesh High Court banned the \"two-finger test\" on the ground that the tests have no scientific merit or evidential value. Between the years of 2010 and 2013, the United Nations Multi-country Study on Men and Violence asked men in rural and urban Bangladesh if they had forced a woman to have sex at any point in their lives. 14.1% of men in rural Bangladesh and 9.5% of men in urban Bangladesh said yes (10% averaged). 2.7% of men in rural Bangladesh and 0.5% (6/1252) in urban Bangladesh had raped in the past year. In rural Bangladesh 41.4% of rapists perpetrated more than once, 3.7% had four or more victims, and 40% first raped as a teenager. 82% of rural Bangladeshi and 79% of urban Bangladeshi men cited entitlement as their reason for rape. 61.2% of urban Bangladeshi men who had raped did not feel guilty or worried afterwards, and 95.1% experienced no legal consequences. 3.7% of men in rural Bangladesh had raped another man. 89.2% of urban Bangladeshi men agreed to the statement \"if a woman doesn't physically fight back, it's not rape.\" In 2008, the incidence of rapes recorded by the police was 26.3 per 100,000 people, according to data by UNODC. Rape in Belgium is defined by Article 375 of the Penal Code as \"any act of sexual penetration, of whatever sort and by whatever means, committed on a non-consenting person\". Marital rape is also illegal under this law. Apart from criminal proceedings, committing marital rape has also consequences in a divorce case. The new amendments of the Civil Code regulating marriage and divorce, that came into effect in September 2007, state that any of the spouses, following a divorce, may receive alimony if they need the money; but a spouse who has committed rape or other violent crimes against the other spouse cannot receive alimony. Article 301 reads: \"The court may refuse to grant the application for a alimony if the defendant proves that the applicant has committed a serious offense that rendered it impossible to continue living together. Under no circumstances will alimony be given to a spouse who was found guilty of an act referred to in Articles 375, 398-400, 402, 403 or 405 of the Penal Code, committed against the person of the defendant, or an attempt to commit an act referred to in Articles 375, 393, 394 or 397 of the Code against the same person.\" According to the Belize Police Department, in 2013 there were 26 cases of rape reported in the country. The estimated total population in 2013 was 334,297. In 2006, the incidence of rapes recorded by the police was 15.3 per 100,000 people, according to data by UNODC. According to a 2009 report, bribery in rape investigations, including those involving child rape, is common. Suspects often offer money to the police or to the victims/their families. The laws were amended in Belize in 1999 to criminalize marital rape; the law defines marital rape that happens at the time the spouses are cohabiting more narrowly than rape in other circumstances; it stipulates that the act is criminal if \"The act of sexual intercourse is preceded or accompanied by or associated with, assault and battery, harm or injury to the female spouse\". Rape between unmarried persons, or between separated spouses is defined by lack of consent. During the Bosnian war, rape was prevalent. In 1993, a European Community commission estimated that around 20,000 women were raped, while the Bosnian Government put the figure at 50,000. In a 2009 study, 4.9% of 1244 women of 13-24 years reported having been raped in their lifetimes. 10.3% of 654 women reported that they had been raped in their lifetimes in a 2011 study. 4.6% had been raped in the past year. 3.9% of 613 men had been raped in their lifetimes and 4.2% had raped in the past year. In Brazil, rape is \"alarmingly under-reported\" and there are no accurate data to compare rape rates among the country's twenty-six states and federal district. However, in 2012, there were 6,029 rapes in the state of Rio de Janeiro; 4,993 of the victims were women. On average, 416 women a month were raped that year and according to Rio's state Institute of Public Security (ISP) the rate of rape in the state is 37 per 100,000 population for victims of both sexes. Rio's civil police say that in the first quarter of 2013, 1,822 rapes were committed, while there were only 70 individuals arrested for the crimes. Typically, the victims were mainly black women, aged between 20 and 30 years, and coming from any social class. Systematic rape committed by the military against civilians has been documented in Myanmar. A 2002 report by The Shan Human Rights Foundation and The Shan Women's Action Network, titled License to Rape, details incidents of sexual violence committed by Tatmadaw (Burmese Army) troops in Shan State, mostly between 1996 and 2001. The military of Burma has also been accused of continuing to use rape as a weapon of war after the elections of 2010. In 2014, a women's group, The Women's League of Burma, said it had documented more than 100 cases of rape by the military since 2010. According to a 2012 report by Human Rights Watch, the Burmese security forces have committed killings, rape, and mass arrests against Rohingya Muslims. Marital rape was criminalized in 2009, albeit with a rather symbolic sentence of only 8 days imprisonment and a fine of 10.000 to 50.000 Fbu. The new 2009 Criminal Code also criminalized homosexuality which was legal before; but it also abolished the capital punishment in the country, therefore the new Code received mixed reactions from human rights organizations. A report by Amnesty International found that rape was very common in Burundi, rarely prosecuted, and that victims faced strong social stigma and a high risk of reprisal. In Cambodia, rape is estimated by local and international NGOs to be common, but only a very small minority of these assaults are ever reported to authorities, due to the social stigma associated to being the victim of a sexual crime, and, in particular, to losing virginity before marriage (regardless of how this happened). From November 2008 to November 2009, police had recorded 468 cases of rape, attempted rape and sexual harassment, a 2.4 percent increase over the previous year. Breaking the Silence - Sexual Violence in Cambodia is a report produced by Amnesty International, and released in 2010, which examined the situation of sexual violence in Cambodia. The report found that, in the small minority of rapes which are reported, a very common response is for law-enforcement officials, including police and court staff, to arrange extralegal out-of-court 'agreements' between the victim and the perpetrator (or their families), in which the rapist pays a sum of money which is shared between the authorities and the victim (and her family), after which the victim has to withdraw any criminal complaint against the perpetrator, and public prosecutors close the case. When a rape is investigated, a complainant is generally expected to pay an extralegal sum of money to the authorities, to ensure that the court investigates the case, otherwise progress is slow, and it may take over two years for anything to happen. During the pre-trial period, there is always a risk that the perpetrator's family will pay a bribe to secure his acquittal or reduced charge. The UN reported results in 2013 from a study that they did in six Asia-Pacific countries about violence against women. 20.4% of Cambodian men said that they had raped a woman in their lifetime and 11.3% had raped in the past year. 3.3% had raped another man at some point and 23% had participated in gang rape, the largest percentage out of the nine areas surveyed. Cambodia was the only area where gang rape was the most commonly reported form of non-partner rape. 45% answered that sexual entitlement was their motive for raping a woman and 42% said they raped to punish a woman. 11.7% of rapists had raped 4 or more women. 52% first perpetrated rape as teenagers and 15.8% first did so under the age of 15. 44.5% of rapists experienced no legal consequences. In Canadian colonies, rape was an offence at common law. The conceptualization of rape was based on English common law understanding of this offence. English legal precedent was very important. Canada got its first statutory definition of rape in 1892, under the 1892 Criminal Code, which read: \"Rape is the act of a man having carnal knowledge of a woman who is not his wife without her consent, or with consent which has been extorted by threats or fear of bodily harm, or obtained by personating the woman’s husband, or by false and fraudulent representations as to the nature and quality of the act.\" A boy under 14 could not be convicted of rape. The rape law remained virtually unchanged until 1983, when the criminal offence of \"rape\" was abolished and replaced by three sexual assault offences. Unlike the previous rape offence, the sexual assault offences are applicable in marriage and are gender neutral. These three offences are: Sexual assault Sexual assault with a weapon, threats to a third party or causing bodily harm Aggravated sexual assault. The most frequently cited research on sexual violence was conducted by Statistics Canada in 1992, which involved a national random sample of 12,300 women (Johnson and Sacco, 1995). The research found that over one in three women had experienced a sexual assault and that only 6% of sexual assaults were reported to the police. According to Justice Institute of British Columbia, one out of every 17 women is raped, 62% of rape victims were physically injured, 9% were beaten or disfigured. Between the years of 2010 and 2013, the United Nations multi-country Study on Men and Violence in Asia and the Pacific asked men in urban and rural areas of China if they had ever forced a female to have sex. 22.2% said yes. 9.3% had done so in the past year. 55% of the men who had raped had done so more than once and 23.2% had raped more than one woman. 86% cited sexual entitlement as their motive, 57% said that they raped out of fun or boredom, and 46% out of anger or punishment. And despite 47% of them reporting consequences of punishment, threats or violence as a result, 72.4% had not experienced legal consequences. 1.7% had raped another man. 2.2% had participated in gang rape. 25% who had raped reported first doing so as a teenager, the lowest percentage in the study. And while only 11.8% of men and 10.2% of women surveyed approved of generally blaming the victim, 53.7% and 53.5% of each agreed with the statement \"if a woman doesn't physically fight back, it's not rape.\" According to the US Department of State, there were 31,833 cases of rape in China in 2007. The armed conflict in Colombia has resulted in increased sexual violence against women; and Colombian authorities have been accused of failing to investigate rape complaints and failing to control sexual attacks in the country. Marital rape was criminalized in 1996. Rape is very common among internally displaced women: it is reported that one in five of these women were raped. In eastern Congo, the prevalence and intensity of rape and other sexual violence is described as the worst in the world. It is estimated that there are as many as 200,000 surviving rape victims living in the Democratic Republic of the Congo today. A new study says more than 400,000 women are raped in the Democratic Republic of Congo annually. War rape in the Democratic Republic of Congo has frequently been described as a \"weapon of war\" by commentators. Louise Nzigire, a local social worker, states that \"this violence was designed to exterminate the population.\" Nzigire observes that rape has been a \"cheap, simple weapon for all parties in the war, more easily obtainable than bullets or bombs.\" In an analysis of 2565 patients who received medical care in the Médecins Sans Frontières sexual violence clinic in the capital of Ituri, Bunia, between 2005 and 2006, 73% (95.2% of male victims) reported being raped by armed men. 74.5% experienced gang rape (89.3% of male and 73.9% of female victims), with attack by between two and four perpetrators being the most common scenario (58.9%) for both sexes. 48.6% of victims were attacked while doing daily domestic activities outside their homes. Although only approximately 500 rapes are reported to the Danish police annually, several studies estimate that only a small minority of all rapes are actually reported, and only one in five reported rapes result in a conviction in court. For example, according to a 2014 study published by the European Union Agency for Fundamental Rights, Denmark had the highest prevalence rate of physical and sexual violence against women in Europe. The Danish government was harshly criticized for inadequate laws in regard to sexual violence in a 2008 report produced by Amnesty International. The Danish criminal provisions regarding sexual crimes had remained nearly unchanged for 30 years, which lead Amnesty International to declare that \"legislation on rape and sexual violence [conflicted] with human rights principles concerning the need to protect an individual's sexual and physical integrity and right to self-determination.\" The organization repeatedly urged Denmark to bring legislation on rape in line with international law over several years, which lead to an amendment to the sexual offences code in 2013, following a change in government after the 2011 elections. Sexual offences (Danish: Seksualforbrydelser) are defined in the Danish Penal Code, Chapter 24, Section 216-236. References in legislation to marriage were removed following the 2013 amendment (previously providing for a reduced sentence or a pardon), and sexual acts performed on victims in a helpless state now also count as rape. In Denmark it was only 1999 that the first rape crisis centre was established. Marital rape was made illegal in East Timor in 2010, under the Law on Domestic Violence, Law No. 7/2010 which states that \"Sexual violence is understood as any conduct that induces the person to witness, to maintain or participate in unwanted sexual relations, even within a marriage, through intimidation, threats, coercion or use of force, or which limits or nullifies the exercise of sexual and reproductive rights\". The UN claimed that thousands of East Timorese women were raped during the Indonesian occupation of East Timor and that rape was used by the Indonesian military as a weapon of war. The UN commission stated that: \"Rape, sexual slavery and sexual violence were tools used as part of the campaign designed to inflict a deep experience of terror, powerlessness and hopelessness upon pro-independence supporters.\" Marital rape is not a criminal offence in Egypt. Unlike many other countries in the Middle East, Egypt has, in 1999, abolished the law which stipulated that a man could escape a rape conviction if he married his victim after the fact. Women are generally fearful when it comes to reporting rape. Engy Ghozlan of Egyptian Centre for Women's Rights and others suggest that the number of rape cases is over 200,000 every year. Ghozlan further adds that rapes are not decreasing because young men lack adequate income and employment, so their marriages are delayed. During the 2013-13 Egyptian protests, rape has been carried out publicly. On 3 July 2013, it was reported that about 91 women were raped and sexually abused in Tahrir Square in four days. By some estimates, the figure was about 169. Rape is a very serious problem in Ethiopia, and the country is infamous for the practice of marriage by abduction, with the prevalence of this practice in Ethiopia being one of the highest in the world. In many parts of Ethiopia, it is common for a man, working in co-ordination with his friends, to kidnap a girl or woman, sometimes using a horse to ease the escape. The abductor will then hide his intended bride and rape her until she becomes pregnant. As the father of the woman's child, the man can claim her as his wife. Subsequently, the kidnapper may try to negotiate a bride price with the village elders to legitimize the marriage. Girls as young as eleven years old are reported to have been kidnapped for the purpose of marriage. Ethiopia is estimated to have one of the highest rates of violence against women in the world. A report by the UN found that women in Ethiopia are the most likely to suffer domestic violence at the hands of their partners, and that nearly 60% of Ethiopian women were subjected to sexual violence. The 2004 Criminal Code of Ethiopia creates the offence of rape, by Article 620, which states that: \"Whoever compels a woman to submit to sexual intercourse outside wedlock, whether by the use of violence or grave intimidation, or after having rendered her unconscious or incapable of resistance, is punishable with rigorous imprisonment from five years to fifteen years\". There are also certain aggravated circumstances which lead to an increased punishment for rape. Apart from the criminal offence of rape, there are also other sexual offences in the Criminal Code. The age of consent is 18. As can be seen above, a woman cannot charge her husband with rape. However, the 2004 Criminal Code brings major improvements for women's rights in the country, by criminalizing several forms of violence against women, such as female genital mutilation, violence against pregnant women, marriage by abduction, child marriage, trafficking and sexual harassment, though Chapter III - Crimes Committed against life, person and health through harmful traditional practices (Articles 561-570) and other provisions (Articles 587, 597, 625, 635, 637, 648). Article 564 - Violence Against a Marriage Partner or a Person Cohabiting in an Irregular Union is a major step forward. The Ethiopian military has been accused of committing systematic rapes against civilians. Human Rights Watch has repeatedly claimed that the army has attacked, beaten, raped and killed civilians, something which the Ethiopian authorities have denied. However, US scientists said that satellite images confirmed reports that the Ethiopian military had burnt towns and villages in Ethiopia's Somali region. A study in Addis Ababa of high school boys found that 4.3% had been raped in their lives. According to the WHO Multi-country Study on Women's Health and Domestic Violence against Women, 59% of women reported sexual abuse by a partner; while one third of women reported being \"physically forced\" to have sex against their will with their partner within the past 12 months. This was the highest prevalence of all countries surveyed. In Finland, the legal regulations on sexual offences were revised with a law that came into effect on 1. January 1999. Under this revision, sexual offences were divided into three levels: rape, aggravated rape and forcing someone into a sexual act. The revision also affects the cause of action. The law on rape (Chapter 20 - Sex offences Section 1 - Rape) states that: \"(1) A person who forces another into sexual intercourse by the use or threat of violence shall be sentenced for rape to imprisonment for at least one year and at most six years. (2) Also a person who, by taking advantage of the fact that another person, due to unconsciousness, illness, disability, state of fear or other state of helplessness, is unable to defend himself or herself or to formulate or express his or her will, has sexual intercourse with him or her, shall be sentenced for rape.\" The Finnish government does not produce data on rape on a regular basis, beyond the raw numbers of reported rape to Finnish police. The laws and guidelines have been criticized for not making specific reference to \"consent\" and for offering the possibility of mediation between the victim and perpetrator. Specific information on women victims of rape can be found only from separate studies, the last one made in 2004, and that study was based on reported rape offences during the years 1998-1999. The study showed that of 468 rapes or attempted rapes reported to the police, only 47 rape charges were made, or that merely 10 per cent of the rapes reported to the police lead to a prosecution. In most cases the rape victim and the offender knew each other, only in every fourth case was the woman attacked by a stranger. Almost half the rape occurred among acquaintances (corresponding to a date rape), and intimate or family relations were involved in 13 per cent of the cases. Finland had 980 cases of reported rape in 2013. The number of reported rape had increased in 2006 by 91% when measured since 1977, and by 27% when measured from 1997. According to a 2014 study published by the European Union Agency for Fundamental Rights, approximately 47% of women surveyed in Finland were said to have suffered physical and/or sexual abuse; which was the second highest rate after Denmark. Finland was one of the last countries in the EU to criminalize marital rape, making it illegal in 1994, after years of debate. Convicted rapists receive very short penalties compared to other countries, although this may be due to the fact that Finland has one of the lowest incarceration rates in the world. During 2001-2003, the average sentence for rape was two years' imprisonment, and only 63% of offenders served their sentences in prison, as 37% of sentences were conditional. The average sentence for aggravated rape was four years' imprisonment. For the offence of coercion into sexual intercourse, sentences were most often one year conditional imprisonment, but only 4% of offenders went to prison. The issue of violence against women in Finland has been of major international interest and the situation has been described as a paradox, because otherwise the country has offered women high professional and social opportunities. According to Turku University law professor Kevät Nousiainen, \"the way Finns conceive gender is different. It's assumed women are perfectly capable of taking care of themselves, and if they are not able to do so, that is unacceptable.\" After World War I, Finland fought a war of independence, a civil war, and two decades later the Winter War, the Continuation War, and the Lapland War, which made up Finland's part in World War II. In each case Finland fought as a poorly trained underdog in brutal conditions that Nousiainen says left men \"unbalanced\". \"Violence was taken somehow for granted, it was tolerated. And then you have to consider the transfer of violent behaviour from generation to generation,\" she said. Article 222-23 of the criminal code reads: \"Any act of sexual penetration, whatever its nature, committed against another person by violence, constraint, threat or surprise, is rape\". Under German law, a person commits rape if he or she employs any of these three types of coercion: 1. force; or 2. threat of imminent danger to life or limb; or 3. exploitation of a situation in which the victim is unprotected and at the mercy of the offender. Germany was one of the last Western countries to criminalize marital rape, it did so only in 1997, after a lengthy political battle which started in the 1970s. The criminalization of marital rape has been delayed by political disagreement: even when there was consensus that it should be criminalized, there was disagreement between those who wanted it punished and prosecuted in the same way as non-marital rape and those who opposed this. These disagreements have delayed the criminalization until 1997, when rape in marriage was made illegal being treated in the same way as non-marital rape. In Germany the age of consent is 14, although some limitations do exist up to the age of 18 (regarding the exploitation of the lack of capacity for sexual self-determination of 14-15 y/o; and engaging in sexual activity with a person under 18 \"by taking advantage of an exploitative situation\"; or paying for sex with a minor under 18 - Section 182 of the Criminal Code). Chapter 13 of the Criminal Code is called \"Offences against sexual self-determination\" and consists of Sections 174 to 184 which define sexual crimes. In a survey of Ghanaians, 8% of women reported having been raped by a man in their lifetimes and 5% of men reported having raped a wife or girlfriend. Rape in Iceland is defined by Article 194 of the Penal Code which states: \"Any person who has sexual intercourse or other sexual relations with a person by means of using violence, threats or other unlawful coercion shall be guilty of rape and shall be imprisoned for a minimum of 1 year and a maximum of 16 years. ‘Violence’ here refers to the deprivation of independence by means of confinement, drugs or other comparable means. Exploiting a person's psychiatric disorder or other mental handicap, or the fact that, for other reasons, he or she is not in a condition to be able to resist the action or to understand its significance, in order to have sexual intercourse or other sexual relations with him or her, shall also be considered as rape, and shall result in the same punishment as specified in the first paragraph of this article.\" Although a Nordic country, known for a high level of gender equality, Iceland has, until recently, maintained outdated provisions in its sexual offences laws. Before 2007, the law in regard to rape and certain other sexual offences stated that, if after the assault the victim and the perpetrator got married or entered into an informal cohabitation, then the punishment could be waived; if the assault took place between married or cohabiting partners, and following the act, the victim continued to live together with the perpetrator, then the punishment could also be waived. These provisions were repealed by Act No. 61/2007. Other legal changes which were made included the broadening of the definition of rape and other sexual offences, and the raising of the age of consent to 15, from 14. In 2008, the incidence of rapes recorded by the police was 21.6 per 100,000 people, according to data by UNODC. A 2010 study found that 6% of Icelandic women had been sexually victimized in an intimate relationship during their lifetime. According to latest available statistics from the National Crime Records Bureau (NCRB), the country had a reported rape rate of 2.8 per 100,000 people as of 2022. However, the incidence of rape and its rates of reporting vary widely from rural to urban areas, and across India's 28 states and 8 union territories. In 2018, official data showed that 1 rape was reported every 15 minutes in India. Of the 34,000 cases reported, just over 85 per cent led to charges, and 27 per cent ultimately led to convictions. Of these, 31,320 were committed by perpetrators known to the victim (93.9% of the cases). As high as 27.8 per cent of victims were minors or below 18, the legal age of consent. This high percentage of perpetrators being a close family member or acquaintance has remained constant over the years. In 2015, the Times of India reported 300 rapes and 500 molestation cases were reported in two months from January to February 2015. But the absolute number of rapes reported have remained broadly similar since 2015. As of 2018, Madhya Pradesh had the highest raw number of rape reports among Indian states. Among metropolitan cities, the national capital of Delhi continued to have the highest incidence of rape per capita. In 2016, Union Minister for Women and Child Development Maneka Gandhi reported to Lok Sabha that 13,766 cases of child rape were reported to the National Crime Records Bureau in 2014. India ranked 94th in ranking of reported rape cases per 100,000 population in 2010. The United Nations Multi-country Study on Men and Violence studied three different sites of Indonesia (Jakarta, rural Java, and Jayapura). In the rural area, the lifetime prevalence of perpetration of rape towards a female/females was 19.5% and gang rape 7%. When rapists were asked why they perpetrated their last non-partner rape, 76.5% of the men in the three areas averaged cited sexual entitlement, 55.2% entertainment-seeking, and 29.7% anger/punishment. During the first half of the 20th century, in some areas of Italy, rape victims were often expected and forced to marry their rapist. In 1965, a 17-year-old girl from Sicily, created a sensation when - fully supported by her poor family and the local police - she refused to marry the man who kidnapped and raped her. In refusing this \"rehabilitating marriage\" to the perpetrator, she went against the traditional social norms of the time which dictated such a solution and the rapist was sentenced to ten years in prison followed by two years of internal exile in another region. His seven accomplices were sentenced to five years. The Criminal Code of Italy also supported this practice, by exonerating the rapist who married the victim. The article of law whereby a rapist could extinguish his crime by marrying his victim was abolished in 1981. The Franca Viola incident was made into a movie called La moglie più bella. In 1999, in an infamous case that gained international attention, the Court of Cassation of Italy ordered a new Appeal trial for a man a lower Court had found guilty of the rape of a woman who was wearing tight jeans. The Court did not claim that it is impossible to forcibly remove tight jeans \"without the collaboration of the person wearing them\" if she resists, but it claimed that such impossibility was plausible only in the given case and that, combined with other more significant evidence come up during the trial, it had led \"in abundantiam\" to ruling in favour of a new trial. The court did not equated the removal of the jeans with consent to sexual penetration as stated by Italian and Anglo-Saxon press. Following this ruling, there was outrage, both in Italy and abroad. In Italy, female politicians wore jeans to parliament in protest. In 2008 the Court of Cassation did not overturn this infamous ruling: it confirmed the guilty verdict of a lower Court in a case where the victim had removed her own jeans under duress. In another case that sparked outrage, in 2006, the Court of Cassation ruled that a 41-year-old man who raped his 14-year-old stepdaughter could seek to have his sentence reduced - but not overturned - in a new appellate trial, due to the fact that the girl had already been sexually active: \"since the age of 13 [she] had had many sexual relations with men of every age and it's right to assume that at the time of the encounter with the suspect, her personality, from a sexual point of view, was much more developed than what one might normally expect from a girl of her age\". Therefore, the Court of Cassation ruled that the first Appeal Court should not have apodictically based its rejection of the existence of mitigating circumstances only on the long-life consequences of the rape. UNICEF in Italy stated that the decision \"seriously violates human rights and the dignity of a minor.\" In Japan, rape is defined as a crime of forced sexual intercourse in Article 177 of the Penal Code of Japan. National Police Agency publishes rape statistics in Japan. Under the law of Jordan, rape is defined by Article 292, which reads: \"Whoever has sexual intercourse with a woman, other than his wife, without her consent-whether through coercion, threat, deception, or fraud-is punished with hard labor for no less than 15 years\". According to UNODC statistics, in 2006, the incidence of rapes recorded by the police was 1.9 per 100,000 people. The laws on sexual offences were modified in Latvia in 2014, broadening the scope of the legislation. In Latvia, a person who commits an act of sexual intercourse by means of violence, threats, taking advantage of the state of helplessness of the victim, or by abuse of authority, is guilty of rape. (Section 159 of the Criminal Code). Rape and other sexual crimes are defined under Chapter XVI called \"Criminal Offences against Morals and Sexual Inviolability\". In 2014, Section 48 called Aggravating Circumstances (which defines circumstances which constitute an aggravation to a crime) was modified, ensuring that marital rape is covered by legislation, by defining as an aggravating circumstance the fact that: \"(15) a criminal offence related to violence or threats of violence, or against morals and sexual inviolability, is committed against a person to whom the perpetrator is related in the first or the second degree of kinship, against the spouse or former spouse, or against a person with whom the perpetrator is or has been in unregistered marital relationship, or against a person with whom the perpetrator has a joint (single) household\". In 2008, according to data by UNODC, the incidence of rapes recorded by the police was 4.4 per 100,000 people. In J. L. v. Latvia (2012), the European Court of Human Rights found that Latvia had failed to comply with its obligation under Article 3 of the European Convention on Human Rights to carry out an effective investigation into allegations of ill-treatment, because it had failed to properly investigate a prisoner's allegations of rape and assault by fellow inmates, who sought revenge against the victim due to his co-operation with the police. In December 2016, the Campaign Against Lebanese Rape Law - Article 522 was launched in order to abolish the article that allowed a rapist to avoid prison by marrying the victim. Prior to its abolishment in February 2017, the article read: \"If a valid contract of marriage is made between the perpetrator of any of the offences mentioned in this section, and the victim, the prosecution is suspended. If judgment was already passed, the implementation of the punishment is suspended.\" Since February 2017, other articles of the penal code are being amended to reinforce penalties against rapists that commit sexual assault on girls under 15. Rape is one of Lesotho's main social issues. According to UNODC, the incidence of rapes recorded in 2008 by the police in Lesotho was the highest incidence of any country. In a study of 1,049 women, 33% said they had been raped by the age of 18. In 66% of cases the rapist was a boyfriend. In the 2009 DHS survey 15.7% of men said that a husband is justified in hitting or beating his wife if she refuses to have sex with him, while 16% said a husband is justified to use force to have sex. HIV/AIDS in Lesotho is a very serious problem, with 23.1% of adults aged 15 to 49 living with it. In a study, researchers have concluded that \"Given the high prevalence of HIV in Lesotho, programs should address women's right to control their sexuality.\" Victims of rape in Libya are often deemed as having 'dishonoured' their families and communities, and may face serious violence, including honour killings. According to UNHCR, \"In Libya when rape occurs, it seems to be a whole village or town which is seen to be dishonoured\". Women who have been raped experience extreme shame; according to a charity worker, being raped is \"worse than death for them [the victims]\". Mexico has a federal law, as well as state laws. Mexican laws have been modernized significantly from the 1990s onwards. Rape laws used to include stipulations that the penalty was to be reduced if the victim had \"provoked\" the attacker. In 2005, the Supreme Court of Mexico ruled that forced sex in marriage is rape. In doing so, it overturned its prior verdict from 1994 when it had ruled that the laws on rape were inapplicable in marriage. In Mexico, the rape laws did not include a statutory exemption for marriage, but were, as elsewhere, generally understood as inapplicable in this context. This has started to be challenged in the late 20th century. Following the Court's decision in 1994, women's organizations worked to pass state laws against marital rape in order to overturn this precedent. The new 2005 verdict has been interpreted as evidence of the improvement of the position of women in the country. Mexico has recently been plagued by scandals of child sexual abuse in Catholic institutions. A 2013 violent gang rape of six Spanish tourist women in Acapulco has raised questions about how safe Mexico is for tourists. The law on rape in the Netherlands states that: \"A person who by an act of violence or another act or by threat of violence or threat of another act compels a person to submit to acts comprising or including sexual penetration of the body is guilty of rape and liable to a term of imprisonment of not more than twelve years or a fine of the fifth category.\" Apart from the offence of 'rape', there are also other sexual offences. Marital rape was made illegal in 1991; before that date, rape was defined as a man forcing, by violence or threat of thereof, a woman to engage in sexual intercourse outside of marriage. According to a 2014 study published by the European Union Agency for Fundamental Rights, the Netherlands had the fourth highest prevalence rate of physical and sexual violence against women in Europe, with 45% of women having experienced such violence, which is well above the European average of 33%. In 2020 an investigation by the daily newspaper Algemeen Dagblad revealed that since 2017 a judge has never imposed the maximum prison sentence of 12 years in a Dutch rape case. The highest sentence was six years, the lowest 21 days. On average, a rapist was jailed for a year and five months. In a 2010 report on sexual violence in Nicaragua, Amnesty International stated that \"Rape of girls is endemic\". In Nicaragua, between 1998 and 2008, police recorded 14,377 cases of rape, with more than two thirds of reports involving girls under the age of 17. Reporting of rape, however, is estimated to be low, because rape victims often face social hostility and indifference from authorities. Since 2008, abortion is illegal without any exception, and this ban has been criticized as oppressive to rape victims who become pregnant. In 2012, Nicaragua enacted Law no 779 - Integral Law against Violence against Women (Ley Integral contra la Violencia hacia la Mujer). This law criminalizes a wide range of acts of violence against women, such as domestic violence, including marital rape. According to Amnesty International, police forces in Nigeria are reported to have perpetrated acts of rape and other sexual abuse against women, in public locations, or while women were transferred to police stations, or while women visited male detainees in police custody; and sometimes police used sexual violence in order to extract confessions and other information. A study of students of the Polytechnic, Ibadan found that in their lifetimes 1.7% (2.5% of males and 1.1% of females) had raped and 2.7% (5.3% of males and 0.9% of females) had attempted rape. Out of a sample of 295 female students from Ebonyi State University Abakaliki in Southeast Nigeria, 36.7% had experienced sexual harassment/victimization at least once on campus. Of this, 32.4% had been raped (10.8% of the sample). A study comparing the sexual practises of 12- to 19-year-old students with and without mild/moderate intellectual disabilities from schools across Oyo State, Nigeria found that 68.3% of the sexually experienced intellectually disabled females reported a history of rape victimization compared to 2.9% of the sexually experienced non-disabled females. A study analysing the hospital records of 76 sexual assault victims in Ile-Ife from 2007 to 2011 found that the majority (76.1%) of the victims that sought help at a hospital did so within 24 hours of their sexual assault, but forensic evidence was not gathered because rape kits have yet to be introduced in the country. In a 2013 poll of 585 randomly selected adults from six Nigerian geopolitical zones by NOI Polls, 34% answered \"What do you think is the most prevalent cause of rape in the society?\" with 'indecent dressing'. 29% said they personally knew a victim of rape. The situation regarding sexual violence in North Korea is very difficult to assess because of the unwillingness of the North Korean authorities to allow foreign investigators access in the country. According to Amnesty International, the analysis of satellite images of political prison camps (kwanliso) suggest that these camps are in continuous use and expansion. Amnesty International stated that hundreds of thousands of people, including children, are detained in these institutions, where they are subjected to extreme forms of abuse and violence, including rape. The organization cited a former security official at a kwanliso who worked there in the 1980s until the mid-1990s, and who confirmed these accounts. A United Nations panel has stated that the inmate population at political camps has been subjected to systematic extermination, torture, rape, forced abortions and starvation. According to the UN report, women at these camps are systematically subjected to rape by guards or bought and sold by human traffickers in China. Rape is defined by Section 192 of the Criminal Code which states: \"Any person who a) engages in sexual activity by means of violence or threats, or b) engages in sexual activity with any person who is unconscious or incapable for any other reason of resisting the act, or c) by means of violence or threats compels any person to engage in sexual activity with another person, or to carry out similar acts with himself or herself, shall be guilty of rape [...].\" Sexual crimes in Norway are defined in Chapter 19 - Sexual Offenses, which contains Sections 192 to 208. The incidence of reported rape in Norway for 2010 is given as about 35 out of 100,000; there is no in-depth national statistic. A report released in February 2014, found that 9.4 percent of the 2435 women surveyed and 1.1 percent of the 2091 interviewed men stated that they were raped. A new report released in February 2023, found that 22 percent of the women surveyed stated that they were raped. Norway overhauled its sexual offences legislation in 2000. The definition of rape was broadened to include also acts committed with persons incapable of resisting, rather than only acts enforced through direct violence or threat. The law is gender-neutral. In 1974, the Supreme Court of Norway confirmed the applicability of the rape law to marital intercourse, convicting for the first time a man of raping his wife. There are concerns in Norway about the low reporting and conviction rate for rape. According to Amnesty International, 84% of rape cases reported to the police do not reach court; of those that reach trial, 36% end in acquittal. In 2003, the CEDAW Committee expressed concern about the situation of sexual violence in Norway, stating, \"[The Committee] is also concerned that an extremely low percentage of reported rapes results in convictions and that the police and public prosecutors dismiss an increasing number of such cases.\" Rape in Pakistan has been notable, and in recent times have continued to spike. In one case, a teenage girl was burnt alive, as she resisted the rape. In another notable case a woman was raped on the orders of a village council, which functions as a lower-level judiciary. In 2002, 30-year-old Mukhtaran Bibi was gang-raped on the orders of the village council as an \"honor rape\" after allegations that her 12-year-old brother had had sexual relations with a woman from a higher caste. Although custom would expect her to commit suicide after being raped, Mukhtaran spoke up, and pursued the case, which was picked up by both domestic and international media. On 1 September 2002, an anti-terrorism court sentenced six men (including the four rapists) to death for rape. In 2005, the Lahore High Court cited \"insufficient evidence\" and acquitted 5 of the 6 convicted, and commuted the punishment for the sixth man to a life sentence. Mukhtaran and the government appealed this decision, and the Supreme Court suspended the acquittal and held appeal hearings. In 2011, the Supreme Court too acquitted the accused. In 2015, a massive child molesting crime in Pakistani history was discovered. About 280 were raped and filmed. Some of these rapists used these video clips to blackmail the parents of those children. Most of victims were below 14 years old. Malik Ahmed Saeed Khan, a member of Provincial Assembly (MPA) of Punjab, also a member of the ruling party, Pakistan Muslim League-Nawaz, was accused for his involvement in this series of crimes, including selling rape video clips (around 400 clips) to the international market. Papua New Guinea has a very high rate of sexual violence, which has been attributed to the interaction between a very male-dominated culture and a culture which is also very accepting of violence in day-to-day life. Marital rape was criminalized in 2003. According to a 1993 survey by the PNG Institute of Medical Research, an estimated 55% of Papua New Guinean women have experienced rape. The United Nations Multi-country Study on Men and Violence found that 62% of men from Bougainville Island had raped a woman and 7.6% had raped a man. 14% had participated in gang rape. 7% said they had been raped by another man. Non-partner rape was more commonly perpetrated than partner rape. 69.3% of the men who reported rape had raped more than once. 15.5% had raped four or more women or girls. 71% reported their motivation behind rape being sexual entitlement, 63% said they raped for entertainment, and 50% said they raped out of anger or to punish a woman. 52.2% had been jailed for their crime. Out of the nine areas surveyed, this was the highest rate of imprisonment. In Qatar, like in most countries in the Middle East, sex outside of marriage is illegal. Women who report rape or sexual violence risk being charged with \"illicit relations\". Amnesty International has reported that migrant domestic workers are at very high risk of sexual abuse. In 2012, the UN Committee against Torture made reference to \"numerous allegations by migrant workers of physical abuse, sexual violence, rape and attempted rape\". The UN estimates that in Rwanda between 100,000 and 250,000 women were raped during the genocide in 1994. Rape was used as a weapon of war, and there are numerous children who were born from these rapes. Many of the women who were raped were also infected with HIV/AIDS. In 2009 Rwanda adopted a law (Law on prevention and punishment of Gender Based Violence) which recognizes, for the first time in the country's history, marital rape as a crime. Saudi Arabia has general crime rates 100 times lower than that of America. In 1981, the rates of forcible rape were 0.33 out of 100,000. Badr-el-din Ali suggests this may be due to Saudi Arabia having a synnomic state of culture, where everyone uncompromisingly shares the same values. In 2012, the reported nationwide prevalence rate ranged from 2% to 13%. Most incidents of sexual assault occurred within the context of the insurgency in southern Somalia. Over the first quarter of 2013, Amnesty International reported that 56.7% of victims in Mogadishu were internally displaced persons. According to the UN, there were at least 2,924 rape cases in IDP settlements in 2012. A third of the victims were under the age of 18. 70% of the perpetrators were armed men wearing uniforms, although it was not always clear whether they were members of militias, security forces or other individuals or groups. To address the issue, the central authorities as of December 2013 were in the process of forming a special crime unit to investigate and counter gender-based violence, as well as constructing a clinic set aside for victims of sexual assault. The national judiciary, security and police forces were all concurrently receiving specialized gender training as part of the broader reform effort. In June 2014, the Somali government also launched a National Action Plan against sexual violence in conjunction with local civil society groups. The Criminal Law (Sexual Offences and Related Matters) Amendment Act, 2007 is the relevant legislation in South Africa. Despite the fact that this act provides modern and progressive laws, that ban rape and other forms of sexual abuse, including sexual violence within marriage, South Africa remains a country where sexual attacks are common. The country has some of the highest incidences of child and baby rape in the world with more than 67,000 cases of rape and sexual assaults against children reported in 2000, with welfare groups believing that unreported incidents could be up to 10 times higher. In 2001, a nine-month-old was raped and likely lost consciousness as the pain was too much to bear. Another nine-month-old baby was raped by six men, aged between 24 and 66, after the infant had been left unattended by her teenage mother. A four-year-old girl died after being raped by her father. A 14-month-old girl was raped by her two uncles. In February 2002, an eight-month-old infant was reportedly gang raped by four men. One has been charged. The infant has required extensive reconstructive surgery. The eight-month-old infant's injuries were so extensive, increased attention on prosecution has occurred. A significant contributing factor for the escalation in child abuse is the widespread myth in HIV-ravaged South Africa that having sex with a virgin will cure a man of AIDS. According to official figures, circa 11% of South Africans are infected with the virus. Edith Kriel, a social worker who helps child victims in the Eastern Cape, said: \"Child abusers are often relatives of their victims - even their fathers and providers.\" One in three of the 4,000 women questioned by the Community of Information, Empowerment and Transparency said they had been raped in the past year. More than 25% of South African men questioned in a survey admitted to raping someone; of those, nearly half said they had raped more than one person, according to a new study conducted by the Medical Research Council (MRC). A 2010 study led by the government-funded Medical Research Foundation says that in Gauteng province, more than 37 percent of men said they had raped a woman. Nearly 7 percent of the 487 men surveyed said they had participated in a gang rape. Among children, a survey found 11% of boys and 4% of girls admitted to forcing someone else to have sex with them while in another survey among 1,500 schoolchildren in the Soweto township, a quarter of all the boys interviewed said that 'jackrolling', a term for gang rape, was fun. In 2013 a study of 1991 grade nine boys at 46 secondary schools in Cape Town and Port Elizabeth found that 17.2% had raped. South Africa has some of the highest incidences of child and baby rape in the world. More than 25% of a sample of 1,738 South African men from the KwaZulu-Natal and Eastern Cape Provinces admitted when anonymously questioned to raping someone; of those, nearly half said they had raped more than one person, according to a non-peer-reviewed policy brief issued by the Medical Research Council (MRC). 4.95% had raped or attempted rape in the past year at the time of the survey. Several news publications extrapolated these results to the rest of the South African population. The humanitarian news organization IRIN claims that an estimated 500,000 rapes are committed annually in South Africa. According to University of Durban-Westville anthropology lecturer and researcher Suzanne Leclerc-Madlala, the myth that sex with a virgin is a cure for AIDS is not confined to South Africa. \"Fellow AIDS researchers in Zambia, Zimbabwe and Nigeria have told me that the myth also exists in these countries and that it is being blamed for the high rate of sexual abuse against young children.\" \"In South Africa, rape is so common it barely makes the news. The rapes of elderly women and babies are outlined in four-line stories on the inside pages of local newspapers, but most sexual assaults get no public attention.\" In 2016, the police recorded 39,828 rapes which means rape rate of 71.3. In South Sudan, marital rape is not criminalized; the law on rape excludes it from its definition by stating that \"Sexual intercourse by a married couple is not rape, within the meaning of this section\". (Art 247). Child marriage is common in the country, and this often leads to child sexual abuse; while the law on rape sets an age of consent of 18, this does not apply inside marriage. The Criminal Code criminalizes, among other behaviours, adultery and homosexuality. Amnesty International has stated that the security forces in South Sudan have shot and raped civilians while carrying out a civilian disarmament campaign in Jonglei State. In recent years, changes have been made to update South Korea's sex crime laws at the behest of President Park Geun-Hye, resulting in an increase in reported incidents. In 2015, reports of sexual assaults against foreigners were up 40% over 2008 numbers. Reports, apprehensions, and prosecutions have all risen with recent changes. However, victims often experience stigma due to traditional views of women's place in society and, although if convicted of rape an offender may be sentenced to between seven years and life in prison, convictions seldom result in a prison sentence. In Sri Lanka there have been recent allegations that rape and torture by the Sri Lankan security forces have continued for years after the civil war ended. An average rape case in Sri Lanka takes 6 to 12 years to be resolved. The UN Multi-country Study on Men and Violence found that 14.5% of the sample of Sri Lankan men had perpetrated rape at some point in their lives. 4.9% had raped in the past year. 2.7% had raped another man. 1.6% had taken part in a gang rape. 96.5% of the men who had raped experienced no legal consequences. 65.8% did not feel worried or guilty afterwards. 64.9% of rapists had raped more than once, and 11.1% had raped four or more girls or women. The law on rape states that: \"There shall be deemed to commit the offence of rape, whoever makes sexual intercourse, by way of adultery, or sodomy, with any person without his consent\". Rape and other forms of sexual violence have been reported as being used on a large scale as a weapon of war in Darfur. A frequently cited source when comparing Swedish rape statistics internationally is the regularly published report by the United Nations Office on Drugs and Crime (UNODC), based on official statistics provided by each member state. In 2012, Sweden had 66 cases of reported rapes per 100,000 population, according to the Swedish National Council for Crime Prevention (Brå). This was unequivocally the biggest number reported to the UNODC in 2012. However, widely differing legal systems, offence definitions, terminological variations, recording practices and statistical conventions makes any cross-national comparison on rape statistics difficult, which is why the UNODC itself cautions against using their figures. It should also be noted that many countries do not report any rape statistics at all to the UNODC, and some report very low numbers, despite studies that indicate otherwise. The Swedish police record each instance of sexual violence in every case separately, leading to an inflated number of cases compared to other countries. Sweden also has a comparatively wide definition of rape. This means that more sexual crimes are registered as rape than in most other countries. For example, in 2005 Sweden reformed its sex crime legislation and made the legal definition of rape much wider, which led to a marked increase in reports. Additionally, the Swedish police have improved the handling of rape cases, in an effort to decrease the number of unreported cases. For this reason, large-scale victimization surveys have been presented by criminologists as a more reliable indicator of rape prevalence. An EU-wide survey on sexual violence against women, published by the European Union Agency for Fundamental Rights (FRA) in 2014, showed Sweden was only third highest, below Denmark and Finland and a previous assessment by Brå have placed Sweden at an average level among European nations. According to the FRA study there is a strong correlation between higher levels of gender equality and disclosure of sexual violence. This, and a greater willingness among Swedish women to report rape in relationships, may also explain the relatively high rates of reported rape in Sweden, which has a long-standing tradition of gender equality policy and legislation, as well as an established women's movement, and has been ranked as the number one country in sex equality. The Syrian Civil War has been associated with a high incidence of war rape, which has led to the stigmatization of victims by their relatives and communities, and in turn to honour killings, forced marriages, and child marriages. According to the Euro Mediterranean Human Rights Network (EMHRN), about 6,000 women have been raped since the start of the conflict. In a survey of 1004 women (defined as 12 or older), 20% reported being raped in their lifetimes. 10% reported the event to police. In 92.4% of the events the perpetrator was known to the victim. There was no statistically significant difference between the rate of rape for women living in urban or suburban areas. 7% of the sample reported a rape occurring in the past two years. The socially closer the perpetrator, the lesser was the frequency of disclosure to either legal organs or other people, and vice versa. The prevalence of forced sexual initiation among women varied between 14% (province) and 17% (city), according to the WHO Multi-country Study on Women's Health and Domestic Violence against Women. In Turkey some commonly expressed views on rape were presented when individuals from various professions were asked to agree or disagree with the statement \"some women deserve rape\". Thirty-three to sixty-six percent of the police officers agreed with the statement as well as nearly 50% of other professional groups. The exception were the responses of psychologists about 18% and 27% of psychiatrists who agreed with the statement. Some of these suggested that \"the physical appearance and behaviors of women tempt men to rape.\" In 2013, The Guardian reported on claims by activists for the Kurdish terrorist group the PKK of widespread sexual abuse of prisoners allegedly used by the Turkish government to suppress dissent. The Sexual Offences Act 2003 (for England and Wales), the Sexual Offences (Scotland) Act 2009 and the Sexual Offences (Northern Ireland) Order 2008 are relevant legislative acts in the United Kingdom. Unlike other jurisdictions, such as Australia, much of the US, and many Western countries, 'rape' in the UK is not a gender-neutral offence: it is an offence that can only be committed by a male against a person (female or male). Also the UK has not to date followed the trend in many Western countries of classifying acts other than penetration with a penis (e.g. penetration with an object, finger) as rape. These must be prosecuted under the other, equally severe, statute of assault by penetration. The British Crime Survey 2000 found that 61,000 women were raped in England and Wales in 1999. It was also reported that approximately 754,000 women, over the age of 16, have been raped at least once in their lifetime. In 2001, the BCS (British Crime Survey) found that in the previous year, 47,000 women over the age of 16 were reported to have been raped. The 2006-07 Crime Survey for England and Wales (formerly the British Crime Survey) reports that 1 in every 200 women were raped in that period. It also showed that only 800 people were convicted of rape crimes that same year, meaning that less than 1 in every 100 occurrences of rape led to a conviction. According to the 2015 Crime Survey for England and Wales, from January 2015 to Dec 2015, there were 34,000 incidences of rape. According to the NCPCC, 1 in 20 children have been sexually abused in the UK. 12% of boys and 3% of girls reported committing sexual violence against their partners. In 2013, a Ministry of Justice report stated that only 15 per cent of victims of the most serious sexual offences reported the incident to the police. A 2013 Rape Crisis survey found that one third of the 1000 women surveyed thought that if a woman did not fight back, then she could not have experienced rape. Meanwhile, 60% thought that a woman could not have experienced rape if she did not say 'no'. According to the charity Rape Crisis 85,000 women and 12,000 men are raped each year in England and Wales, and only 15% of victims chose to report the crime to police. In the year to the end of March 2020, 58,856 cases of rape were recorded by police forces in England and Wales. These led to just 2,102 prosecutions, compared with 3,043 in the previous 12 months. Over the years conviction rates have increased, however the UK remains as one of the countries with the lowest conviction rate for rape in Europe. In 2011, the US Centers for Disease Control and Prevention (CDC) found that \"nearly 20% of all women\" in the United States suffered attempted rape or rape sometime in their lives. More than a third of the victims were raped before the age of 18. The Centers for Disease Control and Prevention maintains recent statistics and standardized definitions upon which their statistics are based. A 2011 report on prison rape stated that \"in 2008 there were at least 69,800 inmates who were raped under conditions involving force or threat of force, and more than 216,600 total victims of sexual abuse, in America’s prisons, jails, and juvenile detention centers.\" Data on the prevalence of rape vary greatly depending on what definition of rape is used. The FBI recorded 85,593 rapes in 2010. The Centers for Disease Control and Prevention reported nearly 1.3 million incidents that year. It should however be noted that the CDC's definition of rape \"represents the public health perspective\" and takes into account the ability of the victim to consent to sex because he or she had been drinking or taking drugs while the FBI defines rape as \"Penetration, no matter how slight, of the vagina or anus with any body part or object, or oral penetration by a sex organ of another person, without the consent of the victim.\" A 2007 survey by the National Institute of Justice found that 19.0% of college women and 6.1% of college men experienced either sexual assault or attempted sexual assault since entering college. In the University of Pennsylvania Law Review in 2017, D. Tuerkheimer reviewed the literature on rape allegations, and reported on the problems surrounding the credibility of rape victims, and how that relates to false rape accusations. She pointed to national survey data from the Centers for Disease Control and Prevention that indicates 1 in every 5 women (and 1 in 71 men) will be raped during their lifetime at some point. Despite the prevalence of rape and the fact that false rape allegations are rare, Tuerkheimer reported that law enforcement officers often default to disbelief about an alleged rape. This documented prejudice leads to reduced investigation and criminal justice outcomes that are faulty compared to other crimes. Tuerkheimer says that women face \"credibility discounts\" at all stages of the justice system, including from police, jurors, judges, and prosecutors. These credibility discounts are especially pronounced when the victim is acquainted with the accuser, and the vast majority of rapes fall into this category. The U.S. Department of Justice estimated from 2005 to 2007 that about 2% of victims who were raped while incapacitated (from drugs, alcohol, or other reasons) reported the rape to the police, compared to 13% of victims who experienced physically forced sexual assault. The 1998 National Violence Against Women Survey, based on a sample size of 8,000, estimated the incidence of rape to be 1 in 6 for women and 1 in 33 for men, based on reports of attempted or completed rapes over the course of her or his lifetime. A 1997 study on the non-institutionalized, non-military population by the U.S. Bureau of Justice Statistics, which defines rape as forced penetration by the offender, found that 91% of reported rape victims are female and 9% are male. The majority of rapes in the United States go unreported. According to the American Medical Association (1995), sexual violence, and rape in particular, is considered the most under-reported violent crime. The US Bureau of Justice Criminal Victimization Statistics reports that up to 66.1% of rapes go unreported. Some of the most common reasons given by victims for not reporting rapes are when the victim considers it a personal or private matter, and the fear of reprisal from the assailant. Under-reporting affects the accuracy of this data. A significant number of rapes reported to the police do not advance to prosecution. Twenty-five percent of reported rapes result in arrest. Only 16% of rapes and sexual assaults are reported to the police (Rape in America: A Report to the Nation. 1992 and United Nations Populations Fund, 2000a). Factoring in unreported rapes, about 5% of rapists will ever spend a day in jail. Contrary to widespread belief, rape outdoors is rare. Over two thirds of all rapes occur in someone's home. 31% occur in the perpetrators' homes, 27% in the victims' homes and 10% in homes shared by the victim and perpetrator. 7% occur at parties, 7% in vehicles, 4% outdoors and 2% in bars. From 2000 to 2005, 59% of rapes were not reported to law enforcement. One factor relating to this is the misconception that most rapes are committed by strangers. In reality, studies indicate the following varying numbers: In a 2012 news story, The New York Times reported, \"according to a survey by the Alaska Federation of Natives, the rate of sexual violence in rural villages like Emmonak is as much as 12 times the national rate. And interviews with Native American women here and across the nation’s tribal reservations suggest an even grimmer reality: They say few, if any, female relatives or close friends have escaped sexual violence.\" In a national survey conducted in the United States of America, 14.8% of women over 17 years of age reported having been raped in their lifetime (with an additional 2.8% having experienced attempted rape) and 0.3% of the sample reported having been raped in the previous year. Drug use, especially alcohol, is frequently involved in rape. A study (only of rape victims that were female and reachable by phone) reported detailed findings related to tactics. In 47% of such rapes, both the victim and the perpetrator had been drinking. In 17%, only the perpetrator had been. 7% of the time, only the victim had been drinking. Rapes where neither the victim nor the perpetrator had been drinking were 29% of all rapes. Not only has it been a factor in the rates of sexual assault on campus, but because of the prevalence, assaults are also being affected specifically by the inability to give consent when intoxicated and bystanders not knowing when to intervene due to their own intoxication or the intoxication of the victim. Koss, Gidycz and Wi published a study in 1987 where they interviewed approximately 6,000 college students on 32 college campuses nationwide. They asked several questions covering a wide range of behaviours. From this study, 15% of college women answered \"yes\" to questions about whether they experienced something that met the definition of rape. 12% of women answered \"yes\" to questions about whether they experienced something that met the definition of attempted rape. Moreover, depending on the region, 2-6% of the men interviewed admitted to rape. While the study focused on female victims and male perpetrators; it did not consider rape of men or rape in LGBT relationships. In 1995, the CDC replicated part of this study with 8,810 students on 138 college campuses. They examined rape only, and did not look at attempted rape. They found that 20% of women and 4% of men had experienced rape in the course of her or his lifetime. In 2000, the National Institute of Justice and the Bureau of Justice Statistics published a study called \"The Sexual Victimization of College Women\" based on a 1996-1997 survey. The study found that 3.1% of undergraduate women reported experiencing an act that met the researchers' definition of rape or attempted rape during a 6-7-month academic year. However, of those found to have experienced completed rape, only 46.5% of the victims answered that they considered the incident to be a rape, while 48.8% did not and 4.7% were unsure. The study also found that 10.1% of college women experienced rape and 10.9% experienced attempted rape prior to entering college. Victimization of men was not considered as part of this study. In a different section of the report, the authors speculated about whether statistics during an academic year generalize to an entire college experience. For a full discussion, read more on page 10 of the report, stating that \"the percentage of completed or attempted rape victimization among women in higher educational institutions might climb to between one-fifth and one-quarter\" and further acknowledging in the corresponding footnote, #18, that \"These projections are suggestive. To assess accurately the victimization risk for women throughout a college career, longitudinal research following a cohort of female students across time is needed.\" 80,000 American children are known to have been sexually abused each year. But unreported cases are higher, due to the fear among children. Over ninety percent of the time, the perpetrator is someone familiar or close with the child. Sexually violent crimes targeting children involve forced sexual activities such as intercourse, masturbation, and/or other explicit contact with a minor. According to Child Protective Services, eighty percent of the time, a parent ends up being the perpetrator. Children who become victims of this crime often end up developing phobias, depression, and post-traumatic stress disorder, as well as performing poorly in school. Sexually violent crimes of all ages occur often. According to United States Department of Justice document Criminal Victimization in the United States, there were overall 191,670 victims of rape or sexual assault reported in 2005. Myriam Denov (2004) states that societal responses to the issue of female perpetrators of sexual assault \"point to a widespread denial of women as potential sexual aggressors that could work to obscure the true dimensions of the problem.\" Particularly as an increasing population of un-convicted felons and rapists who continue to insist that accusation of sexual assault is a punishment in lieu of justice through law enforcement agencies. It is thought that to be accused of rape brings shame to their families and social communities. According to the National Crime Victimization Survey, the adjusted per-capita victimization rate of rape has declined from about 2.4 per 1000 people (age 12 and above) in 1980 to about 0.4 per 1000 people in 2006, a decline of about 85%. But other government surveys, such as the Sexual Victimization of College Women study, critique the NCVS on the basis it includes only those acts perceived as crimes by the victim, and report a higher victimization rate. Despite a decline of 60% since 1993, the US still has a relatively high rate of rape when compared to other developed countries. RAINN asserts that from 2000 to 2005, 59% of rapes were not reported to law enforcement. For college students, the figure was 95% in 2000. One factor relating to this is the misconception that most rapes are committed by strangers. According to the Bureau of Justice Statistics, 38% of victims were raped by a friend or acquaintance, 28% by \"an intimate\" and 7% by another relative, and 26% were committed by a stranger to the victim. About four out of ten sexual assaults take place at the victim's own home. Yemen law does not recognize marital rape and does not provide a minimum age for marriage. The issues of child marriage and child rape inside marriage have made international news and have led to calls for legislative changes. There have been several reports of deaths of young girls due to violent rape by adult husbands, as well as young girls dying during childbirth. Human Rights Watch stated that \"Child marriages and forced marriages remain widespread, exposing young girls to domestic violence and maternal mortality and truncating their education.\"",
      "sentences": [
        "Statistics on rape and other acts of sexual assault are commonly available in industrialized countries, and have become better documented throughout the world.",
        "Inconsistent definitions of rape, different rates of reporting, recording, prosecution and conviction for rape can create controversial statistical disparities, and lead to accusations that many rape statistics are unreliable or misleading.",
        "In some jurisdictions, male on female rape is the only form of rape counted in the statistics.",
        "Some jurisdictions also don't count being forced to penetrate another as rape, creating further controversy around rape statistics.",
        "Countries may not define forced sex on a spouse as rape.",
        "Rape is an under-reported crime.",
        "Prevalence of reasons for not reporting rape differ across countries.",
        "They may include fear of retaliation, uncertainty about whether a crime was committed or if the offender intended harm, not wanting others to know about the rape, not wanting the offender to get in trouble, fear of prosecution (e.g.",
        "due to laws against premarital sex), and doubt in local law enforcement.",
        "A United Nations statistical report compiled from government sources showed that more than 250,000 cases of rape or attempted rape were recorded by police annually.",
        "The reported data covered 65 countries.",
        "Most rape research and reporting to date has been limited to male-female forms of rape.",
        "Research on male-male and female-male is beginning to be done.",
        "However, almost no research has been done on female-female rape, though women can be charged with rape in a few jurisdictions.",
        "A few books, such as Violent Betrayal: Partner Abuse in Lesbian Relationships by Dr. Claire M. Renzetti, No More Secrets: Violence in Lesbian Relationships by Janice Ristock, and Woman-to-Woman Sexual Violence: Does She Call It Rape?",
        "by Lori B. Girshick also cover the topic of rape of women by other women.",
        "This table indicates the annual number of recorded rapes per capita by country for last available year.",
        "Each entry is based on that country's definition of rape, which varies widely throughout the world.",
        "It does not specify whether recorded means reported, brought to trial, or convicted.",
        "It does not include cases of rape which go unreported or unrecorded.",
        "Alternative estimates can show large differences, such as South Africa having around 500,000 rapes per year, or Egypt having more than 20,000 rapes a year.",
        "Rape in Afghanistan is a crime which can be legally prosecuted, but in practice it is very rarely reported, because of the immense risks that women face if they report it.",
        "In 2011, Afghanistan made international news in regard to the story of a woman who was raped by a man, jailed for adultery, gave birth to a child in jail, and was then subsequently pardoned by president Hamid Karzai, and in the end married the man who raped her.",
        "In 2012, Afghanistan recorded 240 cases of honour killings and 160 cases of rape, but did not distinguish between the two crimes.",
        "In 2013, in eastern Ghazni, a man attacked a woman and attempted to rape her, and as a result the relatives of the woman killed both the woman and the man in an honour killing.",
        "In Afghanistan, crimes such as adultery, rape and trafficking are often conflated with each other, and it is generally not acceptable for a woman and a man to be alone together (unless married or related).",
        "If this happens, the response can be very violent: an Afghan medical doctor and his female patient were attacked by an angry mob who threw stones at them after the two were discovered in his private examining room without a chaperon.",
        "In 2013, the security forces were also alleged to rape children in the country.",
        "Article 336 of the Penal Code stipulates that rape is a punishable offence, but does not give a definition of rape, as this is left to the courts.",
        "The lack of a clear definition of rape in Algerian law makes it difficult for women to report the act and seek legal remedies.",
        "The Hassi Messaoud was reported in 2001, 2005, and 2010 to sexually assault, traffic, and abuse women.",
        "There have been continuous allegations that during the Algerian War French troops had engaged in acts of torture and rape against Algerians.",
        "Non-consensual sexual penetration is termed \"rape\" in Victoria, Queensland, South Australia, and Tasmania; \"Sexual Assault\" in New South Wales; \"Sexual intercourse without consent\" in the ACT and the Northern Territory; \"Sexual penetration without consent\" in Western Australia.",
        "All these offences are gender neutral and applicable in marriage.",
        "The laws in Australia have evolved from the English common law offence of rape, but have gradually changed, especially in the late 20th century.",
        "In Australia the reported rape rate per 100,000 people is relatively high, although it is in a decreasing trend, coming down from 91.6 in 2003 to 28.6 in 2010.",
        "This stands in contrast to reported rape rate of 1.2 per 100,000 in Japan, 1.8 per 100,000 in India, 4.6 rapes per 100,000 in Bahrain, 12.3 per 100,000 in Mexico, 24.1 per 100,000 in United Kingdom, 28.6 per 100,000 in United States, 66.5 per 100,000 in Sweden.",
        "During the 12 months prior to interview in 2011-12, an estimated 51,200 (0.3%) Australians aged 18 years and over were a victim of sexual assault.",
        "Almost a third (30%) of victims of sexual assault had the most recent incident they experienced reported to the police.",
        "The Australian Women's Safety Survey conducted by the Bureau of Statistics in 1996 involved a random sample of 6,300 women aged 18 and over.",
        "It produced incidence finding of 1.9 per cent for sexual assault in the previous 12 months.",
        "Men who are known to the woman accounted for over two-thirds of assailants (68%).",
        "Only 15% of the assaulted women in the sample reported to the police.",
        "Bangladesh has received criticism for its employment of the \"two-finger test\" in rape investigations.",
        "This test consists of a physical examination of women who report rape during which a doctor inserts two fingers in the woman's vagina to determine whether the woman is \"habituated to sex\".",
        "This examination has its origin in the country's British colonial-era laws dating back to 1872.",
        "This deters many women from reporting rape.",
        "More than 100 experts, including doctors, lawyers, police, and women's rights activists, signed a joint statement in 2013 asking for the test, which they called \"demeaning\", to be abolished, as it \"does not provide any evidence that is relevant to proving the offence.\"",
        "On 12 April 2018 Bangladesh High Court banned the \"two-finger test\" on the ground that the tests have no scientific merit or evidential value.",
        "Between the years of 2010 and 2013, the United Nations Multi-country Study on Men and Violence asked men in rural and urban Bangladesh if they had forced a woman to have sex at any point in their lives.",
        "14.1% of men in rural Bangladesh and 9.5% of men in urban Bangladesh said yes (10% averaged).",
        "2.7% of men in rural Bangladesh and 0.5% (6/1252) in urban Bangladesh had raped in the past year.",
        "In rural Bangladesh 41.4% of rapists perpetrated more than once, 3.7% had four or more victims, and 40% first raped as a teenager.",
        "82% of rural Bangladeshi and 79% of urban Bangladeshi men cited entitlement as their reason for rape.",
        "61.2% of urban Bangladeshi men who had raped did not feel guilty or worried afterwards, and 95.1% experienced no legal consequences.",
        "3.7% of men in rural Bangladesh had raped another man.",
        "89.2% of urban Bangladeshi men agreed to the statement \"if a woman doesn't physically fight back, it's not rape.\"",
        "In 2008, the incidence of rapes recorded by the police was 26.3 per 100,000 people, according to data by UNODC.",
        "Rape in Belgium is defined by Article 375 of the Penal Code as \"any act of sexual penetration, of whatever sort and by whatever means, committed on a non-consenting person\".",
        "Marital rape is also illegal under this law.",
        "Apart from criminal proceedings, committing marital rape has also consequences in a divorce case.",
        "The new amendments of the Civil Code regulating marriage and divorce, that came into effect in September 2007, state that any of the spouses, following a divorce, may receive alimony if they need the money; but a spouse who has committed rape or other violent crimes against the other spouse cannot receive alimony.",
        "Article 301 reads: \"The court may refuse to grant the application for a alimony if the defendant proves that the applicant has committed a serious offense that rendered it impossible to continue living together.",
        "Under no circumstances will alimony be given to a spouse who was found guilty of an act referred to in Articles 375, 398-400, 402, 403 or 405 of the Penal Code, committed against the person of the defendant, or an attempt to commit an act referred to in Articles 375, 393, 394 or 397 of the Code against the same person.\"",
        "According to the Belize Police Department, in 2013 there were 26 cases of rape reported in the country.",
        "The estimated total population in 2013 was 334,297.",
        "In 2006, the incidence of rapes recorded by the police was 15.3 per 100,000 people, according to data by UNODC.",
        "According to a 2009 report, bribery in rape investigations, including those involving child rape, is common.",
        "Suspects often offer money to the police or to the victims/their families.",
        "The laws were amended in Belize in 1999 to criminalize marital rape; the law defines marital rape that happens at the time the spouses are cohabiting more narrowly than rape in other circumstances; it stipulates that the act is criminal if \"The act of sexual intercourse is preceded or accompanied by or associated with, assault and battery, harm or injury to the female spouse\".",
        "Rape between unmarried persons, or between separated spouses is defined by lack of consent.",
        "During the Bosnian war, rape was prevalent.",
        "In 1993, a European Community commission estimated that around 20,000 women were raped, while the Bosnian Government put the figure at 50,000.",
        "In a 2009 study, 4.9% of 1244 women of 13-24 years reported having been raped in their lifetimes.",
        "10.3% of 654 women reported that they had been raped in their lifetimes in a 2011 study.",
        "4.6% had been raped in the past year.",
        "3.9% of 613 men had been raped in their lifetimes and 4.2% had raped in the past year.",
        "In Brazil, rape is \"alarmingly under-reported\" and there are no accurate data to compare rape rates among the country's twenty-six states and federal district.",
        "However, in 2012, there were 6,029 rapes in the state of Rio de Janeiro; 4,993 of the victims were women.",
        "On average, 416 women a month were raped that year and according to Rio's state Institute of Public Security (ISP) the rate of rape in the state is 37 per 100,000 population for victims of both sexes.",
        "Rio's civil police say that in the first quarter of 2013, 1,822 rapes were committed, while there were only 70 individuals arrested for the crimes.",
        "Typically, the victims were mainly black women, aged between 20 and 30 years, and coming from any social class.",
        "Systematic rape committed by the military against civilians has been documented in Myanmar.",
        "A 2002 report by The Shan Human Rights Foundation and The Shan Women's Action Network, titled License to Rape, details incidents of sexual violence committed by Tatmadaw (Burmese Army) troops in Shan State, mostly between 1996 and 2001.",
        "The military of Burma has also been accused of continuing to use rape as a weapon of war after the elections of 2010.",
        "In 2014, a women's group, The Women's League of Burma, said it had documented more than 100 cases of rape by the military since 2010.",
        "According to a 2012 report by Human Rights Watch, the Burmese security forces have committed killings, rape, and mass arrests against Rohingya Muslims.",
        "Marital rape was criminalized in 2009, albeit with a rather symbolic sentence of only 8 days imprisonment and a fine of 10.000 to 50.000 Fbu.",
        "The new 2009 Criminal Code also criminalized homosexuality which was legal before; but it also abolished the capital punishment in the country, therefore the new Code received mixed reactions from human rights organizations.",
        "A report by Amnesty International found that rape was very common in Burundi, rarely prosecuted, and that victims faced strong social stigma and a high risk of reprisal.",
        "In Cambodia, rape is estimated by local and international NGOs to be common, but only a very small minority of these assaults are ever reported to authorities, due to the social stigma associated to being the victim of a sexual crime, and, in particular, to losing virginity before marriage (regardless of how this happened).",
        "From November 2008 to November 2009, police had recorded 468 cases of rape, attempted rape and sexual harassment, a 2.4 percent increase over the previous year.",
        "Breaking the Silence - Sexual Violence in Cambodia is a report produced by Amnesty International, and released in 2010, which examined the situation of sexual violence in Cambodia.",
        "When a rape is investigated, a complainant is generally expected to pay an extralegal sum of money to the authorities, to ensure that the court investigates the case, otherwise progress is slow, and it may take over two years for anything to happen.",
        "During the pre-trial period, there is always a risk that the perpetrator's family will pay a bribe to secure his acquittal or reduced charge.",
        "The UN reported results in 2013 from a study that they did in six Asia-Pacific countries about violence against women.",
        "20.4% of Cambodian men said that they had raped a woman in their lifetime and 11.3% had raped in the past year.",
        "3.3% had raped another man at some point and 23% had participated in gang rape, the largest percentage out of the nine areas surveyed.",
        "Cambodia was the only area where gang rape was the most commonly reported form of non-partner rape.",
        "45% answered that sexual entitlement was their motive for raping a woman and 42% said they raped to punish a woman.",
        "11.7% of rapists had raped 4 or more women.",
        "52% first perpetrated rape as teenagers and 15.8% first did so under the age of 15.",
        "44.5% of rapists experienced no legal consequences.",
        "In Canadian colonies, rape was an offence at common law.",
        "The conceptualization of rape was based on English common law understanding of this offence.",
        "English legal precedent was very important.",
        "Canada got its first statutory definition of rape in 1892, under the 1892 Criminal Code, which read: \"Rape is the act of a man having carnal knowledge of a woman who is not his wife without her consent, or with consent which has been extorted by threats or fear of bodily harm, or obtained by personating the woman’s husband, or by false and fraudulent representations as to the nature and quality of the act.\"",
        "A boy under 14 could not be convicted of rape.",
        "The rape law remained virtually unchanged until 1983, when the criminal offence of \"rape\" was abolished and replaced by three sexual assault offences.",
        "Unlike the previous rape offence, the sexual assault offences are applicable in marriage and are gender neutral.",
        "These three offences are: Sexual assault Sexual assault with a weapon, threats to a third party or causing bodily harm Aggravated sexual assault.",
        "The most frequently cited research on sexual violence was conducted by Statistics Canada in 1992, which involved a national random sample of 12,300 women (Johnson and Sacco, 1995).",
        "The research found that over one in three women had experienced a sexual assault and that only 6% of sexual assaults were reported to the police.",
        "According to Justice Institute of British Columbia, one out of every 17 women is raped, 62% of rape victims were physically injured, 9% were beaten or disfigured.",
        "Between the years of 2010 and 2013, the United Nations multi-country Study on Men and Violence in Asia and the Pacific asked men in urban and rural areas of China if they had ever forced a female to have sex.",
        "22.2% said yes.",
        "9.3% had done so in the past year.",
        "55% of the men who had raped had done so more than once and 23.2% had raped more than one woman.",
        "86% cited sexual entitlement as their motive, 57% said that they raped out of fun or boredom, and 46% out of anger or punishment.",
        "And despite 47% of them reporting consequences of punishment, threats or violence as a result, 72.4% had not experienced legal consequences.",
        "1.7% had raped another man.",
        "2.2% had participated in gang rape.",
        "25% who had raped reported first doing so as a teenager, the lowest percentage in the study.",
        "And while only 11.8% of men and 10.2% of women surveyed approved of generally blaming the victim, 53.7% and 53.5% of each agreed with the statement \"if a woman doesn't physically fight back, it's not rape.\"",
        "According to the US Department of State, there were 31,833 cases of rape in China in 2007.",
        "The armed conflict in Colombia has resulted in increased sexual violence against women; and Colombian authorities have been accused of failing to investigate rape complaints and failing to control sexual attacks in the country.",
        "Marital rape was criminalized in 1996.",
        "Rape is very common among internally displaced women: it is reported that one in five of these women were raped.",
        "In eastern Congo, the prevalence and intensity of rape and other sexual violence is described as the worst in the world.",
        "It is estimated that there are as many as 200,000 surviving rape victims living in the Democratic Republic of the Congo today.",
        "A new study says more than 400,000 women are raped in the Democratic Republic of Congo annually.",
        "War rape in the Democratic Republic of Congo has frequently been described as a \"weapon of war\" by commentators.",
        "Louise Nzigire, a local social worker, states that \"this violence was designed to exterminate the population.\"",
        "Nzigire observes that rape has been a \"cheap, simple weapon for all parties in the war, more easily obtainable than bullets or bombs.\"",
        "In an analysis of 2565 patients who received medical care in the Médecins Sans Frontières sexual violence clinic in the capital of Ituri, Bunia, between 2005 and 2006, 73% (95.2% of male victims) reported being raped by armed men.",
        "74.5% experienced gang rape (89.3% of male and 73.9% of female victims), with attack by between two and four perpetrators being the most common scenario (58.9%) for both sexes.",
        "48.6% of victims were attacked while doing daily domestic activities outside their homes.",
        "Although only approximately 500 rapes are reported to the Danish police annually, several studies estimate that only a small minority of all rapes are actually reported, and only one in five reported rapes result in a conviction in court.",
        "For example, according to a 2014 study published by the European Union Agency for Fundamental Rights, Denmark had the highest prevalence rate of physical and sexual violence against women in Europe.",
        "The Danish government was harshly criticized for inadequate laws in regard to sexual violence in a 2008 report produced by Amnesty International.",
        "The Danish criminal provisions regarding sexual crimes had remained nearly unchanged for 30 years, which lead Amnesty International to declare that \"legislation on rape and sexual violence [conflicted] with human rights principles concerning the need to protect an individual's sexual and physical integrity and right to self-determination.\"",
        "The organization repeatedly urged Denmark to bring legislation on rape in line with international law over several years, which lead to an amendment to the sexual offences code in 2013, following a change in government after the 2011 elections.",
        "Sexual offences (Danish: Seksualforbrydelser) are defined in the Danish Penal Code, Chapter 24, Section 216-236.",
        "References in legislation to marriage were removed following the 2013 amendment (previously providing for a reduced sentence or a pardon), and sexual acts performed on victims in a helpless state now also count as rape.",
        "In Denmark it was only 1999 that the first rape crisis centre was established.",
        "Marital rape was made illegal in East Timor in 2010, under the Law on Domestic Violence, Law No.",
        "7/2010 which states that \"Sexual violence is understood as any conduct that induces the person to witness, to maintain or participate in unwanted sexual relations, even within a marriage, through intimidation, threats, coercion or use of force, or which limits or nullifies the exercise of sexual and reproductive rights\".",
        "The UN claimed that thousands of East Timorese women were raped during the Indonesian occupation of East Timor and that rape was used by the Indonesian military as a weapon of war.",
        "The UN commission stated that: \"Rape, sexual slavery and sexual violence were tools used as part of the campaign designed to inflict a deep experience of terror, powerlessness and hopelessness upon pro-independence supporters.\"",
        "Marital rape is not a criminal offence in Egypt.",
        "Unlike many other countries in the Middle East, Egypt has, in 1999, abolished the law which stipulated that a man could escape a rape conviction if he married his victim after the fact.",
        "Women are generally fearful when it comes to reporting rape.",
        "Engy Ghozlan of Egyptian Centre for Women's Rights and others suggest that the number of rape cases is over 200,000 every year.",
        "Ghozlan further adds that rapes are not decreasing because young men lack adequate income and employment, so their marriages are delayed.",
        "During the 2013-13 Egyptian protests, rape has been carried out publicly.",
        "On 3 July 2013, it was reported that about 91 women were raped and sexually abused in Tahrir Square in four days.",
        "By some estimates, the figure was about 169.",
        "Rape is a very serious problem in Ethiopia, and the country is infamous for the practice of marriage by abduction, with the prevalence of this practice in Ethiopia being one of the highest in the world.",
        "In many parts of Ethiopia, it is common for a man, working in co-ordination with his friends, to kidnap a girl or woman, sometimes using a horse to ease the escape.",
        "The abductor will then hide his intended bride and rape her until she becomes pregnant.",
        "As the father of the woman's child, the man can claim her as his wife.",
        "Subsequently, the kidnapper may try to negotiate a bride price with the village elders to legitimize the marriage.",
        "Girls as young as eleven years old are reported to have been kidnapped for the purpose of marriage.",
        "Ethiopia is estimated to have one of the highest rates of violence against women in the world.",
        "A report by the UN found that women in Ethiopia are the most likely to suffer domestic violence at the hands of their partners, and that nearly 60% of Ethiopian women were subjected to sexual violence.",
        "The 2004 Criminal Code of Ethiopia creates the offence of rape, by Article 620, which states that: \"Whoever compels a woman to submit to sexual intercourse outside wedlock, whether by the use of violence or grave intimidation, or after having rendered her unconscious or incapable of resistance, is punishable with rigorous imprisonment from five years to fifteen years\".",
        "There are also certain aggravated circumstances which lead to an increased punishment for rape.",
        "Apart from the criminal offence of rape, there are also other sexual offences in the Criminal Code.",
        "The age of consent is 18.",
        "As can be seen above, a woman cannot charge her husband with rape.",
        "However, the 2004 Criminal Code brings major improvements for women's rights in the country, by criminalizing several forms of violence against women, such as female genital mutilation, violence against pregnant women, marriage by abduction, child marriage, trafficking and sexual harassment, though Chapter III - Crimes Committed against life, person and health through harmful traditional practices (Articles 561-570) and other provisions (Articles 587, 597, 625, 635, 637, 648).",
        "Article 564 - Violence Against a Marriage Partner or a Person Cohabiting in an Irregular Union is a major step forward.",
        "The Ethiopian military has been accused of committing systematic rapes against civilians.",
        "Human Rights Watch has repeatedly claimed that the army has attacked, beaten, raped and killed civilians, something which the Ethiopian authorities have denied.",
        "However, US scientists said that satellite images confirmed reports that the Ethiopian military had burnt towns and villages in Ethiopia's Somali region.",
        "A study in Addis Ababa of high school boys found that 4.3% had been raped in their lives.",
        "According to the WHO Multi-country Study on Women's Health and Domestic Violence against Women, 59% of women reported sexual abuse by a partner; while one third of women reported being \"physically forced\" to have sex against their will with their partner within the past 12 months.",
        "This was the highest prevalence of all countries surveyed.",
        "In Finland, the legal regulations on sexual offences were revised with a law that came into effect on 1.",
        "January 1999.",
        "Under this revision, sexual offences were divided into three levels: rape, aggravated rape and forcing someone into a sexual act.",
        "The revision also affects the cause of action.",
        "The law on rape (Chapter 20 - Sex offences Section 1 - Rape) states that: \"(1) A person who forces another into sexual intercourse by the use or threat of violence shall be sentenced for rape to imprisonment for at least one year and at most six years.",
        "(2) Also a person who, by taking advantage of the fact that another person, due to unconsciousness, illness, disability, state of fear or other state of helplessness, is unable to defend himself or herself or to formulate or express his or her will, has sexual intercourse with him or her, shall be sentenced for rape.\"",
        "The Finnish government does not produce data on rape on a regular basis, beyond the raw numbers of reported rape to Finnish police.",
        "The laws and guidelines have been criticized for not making specific reference to \"consent\" and for offering the possibility of mediation between the victim and perpetrator.",
        "Specific information on women victims of rape can be found only from separate studies, the last one made in 2004, and that study was based on reported rape offences during the years 1998-1999.",
        "The study showed that of 468 rapes or attempted rapes reported to the police, only 47 rape charges were made, or that merely 10 per cent of the rapes reported to the police lead to a prosecution.",
        "In most cases the rape victim and the offender knew each other, only in every fourth case was the woman attacked by a stranger.",
        "Almost half the rape occurred among acquaintances (corresponding to a date rape), and intimate or family relations were involved in 13 per cent of the cases.",
        "Finland had 980 cases of reported rape in 2013.",
        "The number of reported rape had increased in 2006 by 91% when measured since 1977, and by 27% when measured from 1997.",
        "According to a 2014 study published by the European Union Agency for Fundamental Rights, approximately 47% of women surveyed in Finland were said to have suffered physical and/or sexual abuse; which was the second highest rate after Denmark.",
        "Finland was one of the last countries in the EU to criminalize marital rape, making it illegal in 1994, after years of debate.",
        "Convicted rapists receive very short penalties compared to other countries, although this may be due to the fact that Finland has one of the lowest incarceration rates in the world.",
        "During 2001-2003, the average sentence for rape was two years' imprisonment, and only 63% of offenders served their sentences in prison, as 37% of sentences were conditional.",
        "The average sentence for aggravated rape was four years' imprisonment.",
        "For the offence of coercion into sexual intercourse, sentences were most often one year conditional imprisonment, but only 4% of offenders went to prison.",
        "The issue of violence against women in Finland has been of major international interest and the situation has been described as a paradox, because otherwise the country has offered women high professional and social opportunities.",
        "According to Turku University law professor Kevät Nousiainen, \"the way Finns conceive gender is different.",
        "It's assumed women are perfectly capable of taking care of themselves, and if they are not able to do so, that is unacceptable.\"",
        "After World War I, Finland fought a war of independence, a civil war, and two decades later the Winter War, the Continuation War, and the Lapland War, which made up Finland's part in World War II.",
        "In each case Finland fought as a poorly trained underdog in brutal conditions that Nousiainen says left men \"unbalanced\".",
        "\"Violence was taken somehow for granted, it was tolerated.",
        "And then you have to consider the transfer of violent behaviour from generation to generation,\" she said.",
        "Article 222-23 of the criminal code reads: \"Any act of sexual penetration, whatever its nature, committed against another person by violence, constraint, threat or surprise, is rape\".",
        "Under German law, a person commits rape if he or she employs any of these three types of coercion: 1. force; or 2. threat of imminent danger to life or limb; or 3. exploitation of a situation in which the victim is unprotected and at the mercy of the offender.",
        "Germany was one of the last Western countries to criminalize marital rape, it did so only in 1997, after a lengthy political battle which started in the 1970s.",
        "The criminalization of marital rape has been delayed by political disagreement: even when there was consensus that it should be criminalized, there was disagreement between those who wanted it punished and prosecuted in the same way as non-marital rape and those who opposed this.",
        "These disagreements have delayed the criminalization until 1997, when rape in marriage was made illegal being treated in the same way as non-marital rape.",
        "In Germany the age of consent is 14, although some limitations do exist up to the age of 18 (regarding the exploitation of the lack of capacity for sexual self-determination of 14-15 y/o; and engaging in sexual activity with a person under 18 \"by taking advantage of an exploitative situation\"; or paying for sex with a minor under 18 - Section 182 of the Criminal Code).",
        "Chapter 13 of the Criminal Code is called \"Offences against sexual self-determination\" and consists of Sections 174 to 184 which define sexual crimes.",
        "In a survey of Ghanaians, 8% of women reported having been raped by a man in their lifetimes and 5% of men reported having raped a wife or girlfriend.",
        "Rape in Iceland is defined by Article 194 of the Penal Code which states: \"Any person who has sexual intercourse or other sexual relations with a person by means of using violence, threats or other unlawful coercion shall be guilty of rape and shall be imprisoned for a minimum of 1 year and a maximum of 16 years.",
        "‘Violence’ here refers to the deprivation of independence by means of confinement, drugs or other comparable means.",
        "Exploiting a person's psychiatric disorder or other mental handicap, or the fact that, for other reasons, he or she is not in a condition to be able to resist the action or to understand its significance, in order to have sexual intercourse or other sexual relations with him or her, shall also be considered as rape, and shall result in the same punishment as specified in the first paragraph of this article.\"",
        "Although a Nordic country, known for a high level of gender equality, Iceland has, until recently, maintained outdated provisions in its sexual offences laws.",
        "Before 2007, the law in regard to rape and certain other sexual offences stated that, if after the assault the victim and the perpetrator got married or entered into an informal cohabitation, then the punishment could be waived; if the assault took place between married or cohabiting partners, and following the act, the victim continued to live together with the perpetrator, then the punishment could also be waived.",
        "These provisions were repealed by Act No.",
        "Other legal changes which were made included the broadening of the definition of rape and other sexual offences, and the raising of the age of consent to 15, from 14.",
        "In 2008, the incidence of rapes recorded by the police was 21.6 per 100,000 people, according to data by UNODC.",
        "A 2010 study found that 6% of Icelandic women had been sexually victimized in an intimate relationship during their lifetime.",
        "According to latest available statistics from the National Crime Records Bureau (NCRB), the country had a reported rape rate of 2.8 per 100,000 people as of 2022.",
        "However, the incidence of rape and its rates of reporting vary widely from rural to urban areas, and across India's 28 states and 8 union territories.",
        "In 2018, official data showed that 1 rape was reported every 15 minutes in India.",
        "Of the 34,000 cases reported, just over 85 per cent led to charges, and 27 per cent ultimately led to convictions.",
        "Of these, 31,320 were committed by perpetrators known to the victim (93.9% of the cases).",
        "As high as 27.8 per cent of victims were minors or below 18, the legal age of consent.",
        "This high percentage of perpetrators being a close family member or acquaintance has remained constant over the years.",
        "In 2015, the Times of India reported 300 rapes and 500 molestation cases were reported in two months from January to February 2015.",
        "But the absolute number of rapes reported have remained broadly similar since 2015.",
        "As of 2018, Madhya Pradesh had the highest raw number of rape reports among Indian states.",
        "Among metropolitan cities, the national capital of Delhi continued to have the highest incidence of rape per capita.",
        "In 2016, Union Minister for Women and Child Development Maneka Gandhi reported to Lok Sabha that 13,766 cases of child rape were reported to the National Crime Records Bureau in 2014.",
        "India ranked 94th in ranking of reported rape cases per 100,000 population in 2010.",
        "The United Nations Multi-country Study on Men and Violence studied three different sites of Indonesia (Jakarta, rural Java, and Jayapura).",
        "In the rural area, the lifetime prevalence of perpetration of rape towards a female/females was 19.5% and gang rape 7%.",
        "When rapists were asked why they perpetrated their last non-partner rape, 76.5% of the men in the three areas averaged cited sexual entitlement, 55.2% entertainment-seeking, and 29.7% anger/punishment.",
        "During the first half of the 20th century, in some areas of Italy, rape victims were often expected and forced to marry their rapist.",
        "In 1965, a 17-year-old girl from Sicily, created a sensation when - fully supported by her poor family and the local police - she refused to marry the man who kidnapped and raped her.",
        "In refusing this \"rehabilitating marriage\" to the perpetrator, she went against the traditional social norms of the time which dictated such a solution and the rapist was sentenced to ten years in prison followed by two years of internal exile in another region.",
        "His seven accomplices were sentenced to five years.",
        "The Criminal Code of Italy also supported this practice, by exonerating the rapist who married the victim.",
        "The article of law whereby a rapist could extinguish his crime by marrying his victim was abolished in 1981.",
        "The Franca Viola incident was made into a movie called La moglie più bella.",
        "In 1999, in an infamous case that gained international attention, the Court of Cassation of Italy ordered a new Appeal trial for a man a lower Court had found guilty of the rape of a woman who was wearing tight jeans.",
        "The Court did not claim that it is impossible to forcibly remove tight jeans \"without the collaboration of the person wearing them\" if she resists, but it claimed that such impossibility was plausible only in the given case and that, combined with other more significant evidence come up during the trial, it had led \"in abundantiam\" to ruling in favour of a new trial.",
        "The court did not equated the removal of the jeans with consent to sexual penetration as stated by Italian and Anglo-Saxon press.",
        "Following this ruling, there was outrage, both in Italy and abroad.",
        "In Italy, female politicians wore jeans to parliament in protest.",
        "In 2008 the Court of Cassation did not overturn this infamous ruling: it confirmed the guilty verdict of a lower Court in a case where the victim had removed her own jeans under duress.",
        "Therefore, the Court of Cassation ruled that the first Appeal Court should not have apodictically based its rejection of the existence of mitigating circumstances only on the long-life consequences of the rape.",
        "UNICEF in Italy stated that the decision \"seriously violates human rights and the dignity of a minor.\"",
        "In Japan, rape is defined as a crime of forced sexual intercourse in Article 177 of the Penal Code of Japan.",
        "National Police Agency publishes rape statistics in Japan.",
        "Under the law of Jordan, rape is defined by Article 292, which reads: \"Whoever has sexual intercourse with a woman, other than his wife, without her consent-whether through coercion, threat, deception, or fraud-is punished with hard labor for no less than 15 years\".",
        "According to UNODC statistics, in 2006, the incidence of rapes recorded by the police was 1.9 per 100,000 people.",
        "The laws on sexual offences were modified in Latvia in 2014, broadening the scope of the legislation.",
        "In Latvia, a person who commits an act of sexual intercourse by means of violence, threats, taking advantage of the state of helplessness of the victim, or by abuse of authority, is guilty of rape.",
        "(Section 159 of the Criminal Code).",
        "Rape and other sexual crimes are defined under Chapter XVI called \"Criminal Offences against Morals and Sexual Inviolability\".",
        "In 2008, according to data by UNODC, the incidence of rapes recorded by the police was 4.4 per 100,000 people.",
        "In J. L. v. Latvia (2012), the European Court of Human Rights found that Latvia had failed to comply with its obligation under Article 3 of the European Convention on Human Rights to carry out an effective investigation into allegations of ill-treatment, because it had failed to properly investigate a prisoner's allegations of rape and assault by fellow inmates, who sought revenge against the victim due to his co-operation with the police.",
        "In December 2016, the Campaign Against Lebanese Rape Law - Article 522 was launched in order to abolish the article that allowed a rapist to avoid prison by marrying the victim.",
        "Prior to its abolishment in February 2017, the article read: \"If a valid contract of marriage is made between the perpetrator of any of the offences mentioned in this section, and the victim, the prosecution is suspended.",
        "If judgment was already passed, the implementation of the punishment is suspended.\"",
        "Since February 2017, other articles of the penal code are being amended to reinforce penalties against rapists that commit sexual assault on girls under 15.",
        "Rape is one of Lesotho's main social issues.",
        "According to UNODC, the incidence of rapes recorded in 2008 by the police in Lesotho was the highest incidence of any country.",
        "In a study of 1,049 women, 33% said they had been raped by the age of 18.",
        "In 66% of cases the rapist was a boyfriend.",
        "In the 2009 DHS survey 15.7% of men said that a husband is justified in hitting or beating his wife if she refuses to have sex with him, while 16% said a husband is justified to use force to have sex.",
        "HIV/AIDS in Lesotho is a very serious problem, with 23.1% of adults aged 15 to 49 living with it.",
        "In a study, researchers have concluded that \"Given the high prevalence of HIV in Lesotho, programs should address women's right to control their sexuality.\"",
        "Victims of rape in Libya are often deemed as having 'dishonoured' their families and communities, and may face serious violence, including honour killings.",
        "According to UNHCR, \"In Libya when rape occurs, it seems to be a whole village or town which is seen to be dishonoured\".",
        "Women who have been raped experience extreme shame; according to a charity worker, being raped is \"worse than death for them [the victims]\".",
        "Mexico has a federal law, as well as state laws.",
        "Mexican laws have been modernized significantly from the 1990s onwards.",
        "Rape laws used to include stipulations that the penalty was to be reduced if the victim had \"provoked\" the attacker.",
        "In 2005, the Supreme Court of Mexico ruled that forced sex in marriage is rape.",
        "In doing so, it overturned its prior verdict from 1994 when it had ruled that the laws on rape were inapplicable in marriage.",
        "In Mexico, the rape laws did not include a statutory exemption for marriage, but were, as elsewhere, generally understood as inapplicable in this context.",
        "This has started to be challenged in the late 20th century.",
        "Following the Court's decision in 1994, women's organizations worked to pass state laws against marital rape in order to overturn this precedent.",
        "The new 2005 verdict has been interpreted as evidence of the improvement of the position of women in the country.",
        "Mexico has recently been plagued by scandals of child sexual abuse in Catholic institutions.",
        "A 2013 violent gang rape of six Spanish tourist women in Acapulco has raised questions about how safe Mexico is for tourists.",
        "The law on rape in the Netherlands states that: \"A person who by an act of violence or another act or by threat of violence or threat of another act compels a person to submit to acts comprising or including sexual penetration of the body is guilty of rape and liable to a term of imprisonment of not more than twelve years or a fine of the fifth category.\"",
        "Apart from the offence of 'rape', there are also other sexual offences.",
        "Marital rape was made illegal in 1991; before that date, rape was defined as a man forcing, by violence or threat of thereof, a woman to engage in sexual intercourse outside of marriage.",
        "According to a 2014 study published by the European Union Agency for Fundamental Rights, the Netherlands had the fourth highest prevalence rate of physical and sexual violence against women in Europe, with 45% of women having experienced such violence, which is well above the European average of 33%.",
        "In 2020 an investigation by the daily newspaper Algemeen Dagblad revealed that since 2017 a judge has never imposed the maximum prison sentence of 12 years in a Dutch rape case.",
        "The highest sentence was six years, the lowest 21 days.",
        "On average, a rapist was jailed for a year and five months.",
        "In a 2010 report on sexual violence in Nicaragua, Amnesty International stated that \"Rape of girls is endemic\".",
        "In Nicaragua, between 1998 and 2008, police recorded 14,377 cases of rape, with more than two thirds of reports involving girls under the age of 17.",
        "Reporting of rape, however, is estimated to be low, because rape victims often face social hostility and indifference from authorities.",
        "Since 2008, abortion is illegal without any exception, and this ban has been criticized as oppressive to rape victims who become pregnant.",
        "In 2012, Nicaragua enacted Law no 779 - Integral Law against Violence against Women (Ley Integral contra la Violencia hacia la Mujer).",
        "This law criminalizes a wide range of acts of violence against women, such as domestic violence, including marital rape.",
        "According to Amnesty International, police forces in Nigeria are reported to have perpetrated acts of rape and other sexual abuse against women, in public locations, or while women were transferred to police stations, or while women visited male detainees in police custody; and sometimes police used sexual violence in order to extract confessions and other information.",
        "A study of students of the Polytechnic, Ibadan found that in their lifetimes 1.7% (2.5% of males and 1.1% of females) had raped and 2.7% (5.3% of males and 0.9% of females) had attempted rape.",
        "Out of a sample of 295 female students from Ebonyi State University Abakaliki in Southeast Nigeria, 36.7% had experienced sexual harassment/victimization at least once on campus.",
        "Of this, 32.4% had been raped (10.8% of the sample).",
        "A study comparing the sexual practises of 12- to 19-year-old students with and without mild/moderate intellectual disabilities from schools across Oyo State, Nigeria found that 68.3% of the sexually experienced intellectually disabled females reported a history of rape victimization compared to 2.9% of the sexually experienced non-disabled females.",
        "A study analysing the hospital records of 76 sexual assault victims in Ile-Ife from 2007 to 2011 found that the majority (76.1%) of the victims that sought help at a hospital did so within 24 hours of their sexual assault, but forensic evidence was not gathered because rape kits have yet to be introduced in the country.",
        "In a 2013 poll of 585 randomly selected adults from six Nigerian geopolitical zones by NOI Polls, 34% answered \"What do you think is the most prevalent cause of rape in the society?\"",
        "with 'indecent dressing'.",
        "29% said they personally knew a victim of rape.",
        "The situation regarding sexual violence in North Korea is very difficult to assess because of the unwillingness of the North Korean authorities to allow foreign investigators access in the country.",
        "According to Amnesty International, the analysis of satellite images of political prison camps (kwanliso) suggest that these camps are in continuous use and expansion.",
        "Amnesty International stated that hundreds of thousands of people, including children, are detained in these institutions, where they are subjected to extreme forms of abuse and violence, including rape.",
        "The organization cited a former security official at a kwanliso who worked there in the 1980s until the mid-1990s, and who confirmed these accounts.",
        "A United Nations panel has stated that the inmate population at political camps has been subjected to systematic extermination, torture, rape, forced abortions and starvation.",
        "According to the UN report, women at these camps are systematically subjected to rape by guards or bought and sold by human traffickers in China.",
        "Rape is defined by Section 192 of the Criminal Code which states: \"Any person who a) engages in sexual activity by means of violence or threats, or b) engages in sexual activity with any person who is unconscious or incapable for any other reason of resisting the act, or c) by means of violence or threats compels any person to engage in sexual activity with another person, or to carry out similar acts with himself or herself, shall be guilty of rape [...].\"",
        "Sexual crimes in Norway are defined in Chapter 19 - Sexual Offenses, which contains Sections 192 to 208.",
        "The incidence of reported rape in Norway for 2010 is given as about 35 out of 100,000; there is no in-depth national statistic.",
        "A report released in February 2014, found that 9.4 percent of the 2435 women surveyed and 1.1 percent of the 2091 interviewed men stated that they were raped.",
        "A new report released in February 2023, found that 22 percent of the women surveyed stated that they were raped.",
        "Norway overhauled its sexual offences legislation in 2000.",
        "The definition of rape was broadened to include also acts committed with persons incapable of resisting, rather than only acts enforced through direct violence or threat.",
        "The law is gender-neutral.",
        "In 1974, the Supreme Court of Norway confirmed the applicability of the rape law to marital intercourse, convicting for the first time a man of raping his wife.",
        "There are concerns in Norway about the low reporting and conviction rate for rape.",
        "According to Amnesty International, 84% of rape cases reported to the police do not reach court; of those that reach trial, 36% end in acquittal.",
        "In 2003, the CEDAW Committee expressed concern about the situation of sexual violence in Norway, stating, \"[The Committee] is also concerned that an extremely low percentage of reported rapes results in convictions and that the police and public prosecutors dismiss an increasing number of such cases.\"",
        "Rape in Pakistan has been notable, and in recent times have continued to spike.",
        "In one case, a teenage girl was burnt alive, as she resisted the rape.",
        "In another notable case a woman was raped on the orders of a village council, which functions as a lower-level judiciary.",
        "In 2002, 30-year-old Mukhtaran Bibi was gang-raped on the orders of the village council as an \"honor rape\" after allegations that her 12-year-old brother had had sexual relations with a woman from a higher caste.",
        "Although custom would expect her to commit suicide after being raped, Mukhtaran spoke up, and pursued the case, which was picked up by both domestic and international media.",
        "On 1 September 2002, an anti-terrorism court sentenced six men (including the four rapists) to death for rape.",
        "In 2005, the Lahore High Court cited \"insufficient evidence\" and acquitted 5 of the 6 convicted, and commuted the punishment for the sixth man to a life sentence.",
        "Mukhtaran and the government appealed this decision, and the Supreme Court suspended the acquittal and held appeal hearings.",
        "In 2011, the Supreme Court too acquitted the accused.",
        "In 2015, a massive child molesting crime in Pakistani history was discovered.",
        "About 280 were raped and filmed.",
        "Some of these rapists used these video clips to blackmail the parents of those children.",
        "Most of victims were below 14 years old.",
        "Malik Ahmed Saeed Khan, a member of Provincial Assembly (MPA) of Punjab, also a member of the ruling party, Pakistan Muslim League-Nawaz, was accused for his involvement in this series of crimes, including selling rape video clips (around 400 clips) to the international market.",
        "Papua New Guinea has a very high rate of sexual violence, which has been attributed to the interaction between a very male-dominated culture and a culture which is also very accepting of violence in day-to-day life.",
        "Marital rape was criminalized in 2003.",
        "According to a 1993 survey by the PNG Institute of Medical Research, an estimated 55% of Papua New Guinean women have experienced rape.",
        "The United Nations Multi-country Study on Men and Violence found that 62% of men from Bougainville Island had raped a woman and 7.6% had raped a man.",
        "14% had participated in gang rape.",
        "7% said they had been raped by another man.",
        "Non-partner rape was more commonly perpetrated than partner rape.",
        "69.3% of the men who reported rape had raped more than once.",
        "15.5% had raped four or more women or girls.",
        "71% reported their motivation behind rape being sexual entitlement, 63% said they raped for entertainment, and 50% said they raped out of anger or to punish a woman.",
        "52.2% had been jailed for their crime.",
        "Out of the nine areas surveyed, this was the highest rate of imprisonment.",
        "In Qatar, like in most countries in the Middle East, sex outside of marriage is illegal.",
        "Women who report rape or sexual violence risk being charged with \"illicit relations\".",
        "Amnesty International has reported that migrant domestic workers are at very high risk of sexual abuse.",
        "In 2012, the UN Committee against Torture made reference to \"numerous allegations by migrant workers of physical abuse, sexual violence, rape and attempted rape\".",
        "The UN estimates that in Rwanda between 100,000 and 250,000 women were raped during the genocide in 1994.",
        "Rape was used as a weapon of war, and there are numerous children who were born from these rapes.",
        "Many of the women who were raped were also infected with HIV/AIDS.",
        "In 2009 Rwanda adopted a law (Law on prevention and punishment of Gender Based Violence) which recognizes, for the first time in the country's history, marital rape as a crime.",
        "Saudi Arabia has general crime rates 100 times lower than that of America.",
        "In 1981, the rates of forcible rape were 0.33 out of 100,000.",
        "Badr-el-din Ali suggests this may be due to Saudi Arabia having a synnomic state of culture, where everyone uncompromisingly shares the same values.",
        "In 2012, the reported nationwide prevalence rate ranged from 2% to 13%.",
        "Most incidents of sexual assault occurred within the context of the insurgency in southern Somalia.",
        "Over the first quarter of 2013, Amnesty International reported that 56.7% of victims in Mogadishu were internally displaced persons.",
        "According to the UN, there were at least 2,924 rape cases in IDP settlements in 2012.",
        "A third of the victims were under the age of 18.",
        "70% of the perpetrators were armed men wearing uniforms, although it was not always clear whether they were members of militias, security forces or other individuals or groups.",
        "To address the issue, the central authorities as of December 2013 were in the process of forming a special crime unit to investigate and counter gender-based violence, as well as constructing a clinic set aside for victims of sexual assault.",
        "The national judiciary, security and police forces were all concurrently receiving specialized gender training as part of the broader reform effort.",
        "In June 2014, the Somali government also launched a National Action Plan against sexual violence in conjunction with local civil society groups.",
        "The Criminal Law (Sexual Offences and Related Matters) Amendment Act, 2007 is the relevant legislation in South Africa.",
        "Despite the fact that this act provides modern and progressive laws, that ban rape and other forms of sexual abuse, including sexual violence within marriage, South Africa remains a country where sexual attacks are common.",
        "The country has some of the highest incidences of child and baby rape in the world with more than 67,000 cases of rape and sexual assaults against children reported in 2000, with welfare groups believing that unreported incidents could be up to 10 times higher.",
        "In 2001, a nine-month-old was raped and likely lost consciousness as the pain was too much to bear.",
        "Another nine-month-old baby was raped by six men, aged between 24 and 66, after the infant had been left unattended by her teenage mother.",
        "A four-year-old girl died after being raped by her father.",
        "A 14-month-old girl was raped by her two uncles.",
        "In February 2002, an eight-month-old infant was reportedly gang raped by four men.",
        "One has been charged.",
        "The infant has required extensive reconstructive surgery.",
        "The eight-month-old infant's injuries were so extensive, increased attention on prosecution has occurred.",
        "A significant contributing factor for the escalation in child abuse is the widespread myth in HIV-ravaged South Africa that having sex with a virgin will cure a man of AIDS.",
        "According to official figures, circa 11% of South Africans are infected with the virus.",
        "Edith Kriel, a social worker who helps child victims in the Eastern Cape, said: \"Child abusers are often relatives of their victims - even their fathers and providers.\"",
        "One in three of the 4,000 women questioned by the Community of Information, Empowerment and Transparency said they had been raped in the past year.",
        "More than 25% of South African men questioned in a survey admitted to raping someone; of those, nearly half said they had raped more than one person, according to a new study conducted by the Medical Research Council (MRC).",
        "A 2010 study led by the government-funded Medical Research Foundation says that in Gauteng province, more than 37 percent of men said they had raped a woman.",
        "Nearly 7 percent of the 487 men surveyed said they had participated in a gang rape.",
        "Among children, a survey found 11% of boys and 4% of girls admitted to forcing someone else to have sex with them while in another survey among 1,500 schoolchildren in the Soweto township, a quarter of all the boys interviewed said that 'jackrolling', a term for gang rape, was fun.",
        "In 2013 a study of 1991 grade nine boys at 46 secondary schools in Cape Town and Port Elizabeth found that 17.2% had raped.",
        "South Africa has some of the highest incidences of child and baby rape in the world.",
        "More than 25% of a sample of 1,738 South African men from the KwaZulu-Natal and Eastern Cape Provinces admitted when anonymously questioned to raping someone; of those, nearly half said they had raped more than one person, according to a non-peer-reviewed policy brief issued by the Medical Research Council (MRC).",
        "4.95% had raped or attempted rape in the past year at the time of the survey.",
        "Several news publications extrapolated these results to the rest of the South African population.",
        "The humanitarian news organization IRIN claims that an estimated 500,000 rapes are committed annually in South Africa.",
        "According to University of Durban-Westville anthropology lecturer and researcher Suzanne Leclerc-Madlala, the myth that sex with a virgin is a cure for AIDS is not confined to South Africa.",
        "\"Fellow AIDS researchers in Zambia, Zimbabwe and Nigeria have told me that the myth also exists in these countries and that it is being blamed for the high rate of sexual abuse against young children.\"",
        "\"In South Africa, rape is so common it barely makes the news.",
        "The rapes of elderly women and babies are outlined in four-line stories on the inside pages of local newspapers, but most sexual assaults get no public attention.\"",
        "In 2016, the police recorded 39,828 rapes which means rape rate of 71.3.",
        "In South Sudan, marital rape is not criminalized; the law on rape excludes it from its definition by stating that \"Sexual intercourse by a married couple is not rape, within the meaning of this section\".",
        "(Art 247).",
        "Child marriage is common in the country, and this often leads to child sexual abuse; while the law on rape sets an age of consent of 18, this does not apply inside marriage.",
        "The Criminal Code criminalizes, among other behaviours, adultery and homosexuality.",
        "Amnesty International has stated that the security forces in South Sudan have shot and raped civilians while carrying out a civilian disarmament campaign in Jonglei State.",
        "In recent years, changes have been made to update South Korea's sex crime laws at the behest of President Park Geun-Hye, resulting in an increase in reported incidents.",
        "In 2015, reports of sexual assaults against foreigners were up 40% over 2008 numbers.",
        "Reports, apprehensions, and prosecutions have all risen with recent changes.",
        "However, victims often experience stigma due to traditional views of women's place in society and, although if convicted of rape an offender may be sentenced to between seven years and life in prison, convictions seldom result in a prison sentence.",
        "In Sri Lanka there have been recent allegations that rape and torture by the Sri Lankan security forces have continued for years after the civil war ended.",
        "An average rape case in Sri Lanka takes 6 to 12 years to be resolved.",
        "The UN Multi-country Study on Men and Violence found that 14.5% of the sample of Sri Lankan men had perpetrated rape at some point in their lives.",
        "4.9% had raped in the past year.",
        "2.7% had raped another man.",
        "1.6% had taken part in a gang rape.",
        "96.5% of the men who had raped experienced no legal consequences.",
        "65.8% did not feel worried or guilty afterwards.",
        "64.9% of rapists had raped more than once, and 11.1% had raped four or more girls or women.",
        "The law on rape states that: \"There shall be deemed to commit the offence of rape, whoever makes sexual intercourse, by way of adultery, or sodomy, with any person without his consent\".",
        "Rape and other forms of sexual violence have been reported as being used on a large scale as a weapon of war in Darfur.",
        "A frequently cited source when comparing Swedish rape statistics internationally is the regularly published report by the United Nations Office on Drugs and Crime (UNODC), based on official statistics provided by each member state.",
        "In 2012, Sweden had 66 cases of reported rapes per 100,000 population, according to the Swedish National Council for Crime Prevention (Brå).",
        "This was unequivocally the biggest number reported to the UNODC in 2012.",
        "However, widely differing legal systems, offence definitions, terminological variations, recording practices and statistical conventions makes any cross-national comparison on rape statistics difficult, which is why the UNODC itself cautions against using their figures.",
        "It should also be noted that many countries do not report any rape statistics at all to the UNODC, and some report very low numbers, despite studies that indicate otherwise.",
        "The Swedish police record each instance of sexual violence in every case separately, leading to an inflated number of cases compared to other countries.",
        "Sweden also has a comparatively wide definition of rape.",
        "This means that more sexual crimes are registered as rape than in most other countries.",
        "For example, in 2005 Sweden reformed its sex crime legislation and made the legal definition of rape much wider, which led to a marked increase in reports.",
        "Additionally, the Swedish police have improved the handling of rape cases, in an effort to decrease the number of unreported cases.",
        "For this reason, large-scale victimization surveys have been presented by criminologists as a more reliable indicator of rape prevalence.",
        "An EU-wide survey on sexual violence against women, published by the European Union Agency for Fundamental Rights (FRA) in 2014, showed Sweden was only third highest, below Denmark and Finland and a previous assessment by Brå have placed Sweden at an average level among European nations.",
        "According to the FRA study there is a strong correlation between higher levels of gender equality and disclosure of sexual violence.",
        "This, and a greater willingness among Swedish women to report rape in relationships, may also explain the relatively high rates of reported rape in Sweden, which has a long-standing tradition of gender equality policy and legislation, as well as an established women's movement, and has been ranked as the number one country in sex equality.",
        "The Syrian Civil War has been associated with a high incidence of war rape, which has led to the stigmatization of victims by their relatives and communities, and in turn to honour killings, forced marriages, and child marriages.",
        "According to the Euro Mediterranean Human Rights Network (EMHRN), about 6,000 women have been raped since the start of the conflict.",
        "In a survey of 1004 women (defined as 12 or older), 20% reported being raped in their lifetimes.",
        "10% reported the event to police.",
        "In 92.4% of the events the perpetrator was known to the victim.",
        "There was no statistically significant difference between the rate of rape for women living in urban or suburban areas.",
        "7% of the sample reported a rape occurring in the past two years.",
        "The socially closer the perpetrator, the lesser was the frequency of disclosure to either legal organs or other people, and vice versa.",
        "The prevalence of forced sexual initiation among women varied between 14% (province) and 17% (city), according to the WHO Multi-country Study on Women's Health and Domestic Violence against Women.",
        "In Turkey some commonly expressed views on rape were presented when individuals from various professions were asked to agree or disagree with the statement \"some women deserve rape\".",
        "Thirty-three to sixty-six percent of the police officers agreed with the statement as well as nearly 50% of other professional groups.",
        "The exception were the responses of psychologists about 18% and 27% of psychiatrists who agreed with the statement.",
        "Some of these suggested that \"the physical appearance and behaviors of women tempt men to rape.\"",
        "In 2013, The Guardian reported on claims by activists for the Kurdish terrorist group the PKK of widespread sexual abuse of prisoners allegedly used by the Turkish government to suppress dissent.",
        "Unlike other jurisdictions, such as Australia, much of the US, and many Western countries, 'rape' in the UK is not a gender-neutral offence: it is an offence that can only be committed by a male against a person (female or male).",
        "Also the UK has not to date followed the trend in many Western countries of classifying acts other than penetration with a penis (e.g.",
        "penetration with an object, finger) as rape.",
        "These must be prosecuted under the other, equally severe, statute of assault by penetration.",
        "The British Crime Survey 2000 found that 61,000 women were raped in England and Wales in 1999.",
        "It was also reported that approximately 754,000 women, over the age of 16, have been raped at least once in their lifetime.",
        "In 2001, the BCS (British Crime Survey) found that in the previous year, 47,000 women over the age of 16 were reported to have been raped.",
        "The 2006-07 Crime Survey for England and Wales (formerly the British Crime Survey) reports that 1 in every 200 women were raped in that period.",
        "It also showed that only 800 people were convicted of rape crimes that same year, meaning that less than 1 in every 100 occurrences of rape led to a conviction.",
        "According to the 2015 Crime Survey for England and Wales, from January 2015 to Dec 2015, there were 34,000 incidences of rape.",
        "According to the NCPCC, 1 in 20 children have been sexually abused in the UK.",
        "12% of boys and 3% of girls reported committing sexual violence against their partners.",
        "In 2013, a Ministry of Justice report stated that only 15 per cent of victims of the most serious sexual offences reported the incident to the police.",
        "A 2013 Rape Crisis survey found that one third of the 1000 women surveyed thought that if a woman did not fight back, then she could not have experienced rape.",
        "Meanwhile, 60% thought that a woman could not have experienced rape if she did not say 'no'.",
        "According to the charity Rape Crisis 85,000 women and 12,000 men are raped each year in England and Wales, and only 15% of victims chose to report the crime to police.",
        "In the year to the end of March 2020, 58,856 cases of rape were recorded by police forces in England and Wales.",
        "These led to just 2,102 prosecutions, compared with 3,043 in the previous 12 months.",
        "Over the years conviction rates have increased, however the UK remains as one of the countries with the lowest conviction rate for rape in Europe.",
        "In 2011, the US Centers for Disease Control and Prevention (CDC) found that \"nearly 20% of all women\" in the United States suffered attempted rape or rape sometime in their lives.",
        "More than a third of the victims were raped before the age of 18.",
        "The Centers for Disease Control and Prevention maintains recent statistics and standardized definitions upon which their statistics are based.",
        "A 2011 report on prison rape stated that \"in 2008 there were at least 69,800 inmates who were raped under conditions involving force or threat of force, and more than 216,600 total victims of sexual abuse, in America’s prisons, jails, and juvenile detention centers.\"",
        "Data on the prevalence of rape vary greatly depending on what definition of rape is used.",
        "The FBI recorded 85,593 rapes in 2010.",
        "The Centers for Disease Control and Prevention reported nearly 1.3 million incidents that year.",
        "It should however be noted that the CDC's definition of rape \"represents the public health perspective\" and takes into account the ability of the victim to consent to sex because he or she had been drinking or taking drugs while the FBI defines rape as \"Penetration, no matter how slight, of the vagina or anus with any body part or object, or oral penetration by a sex organ of another person, without the consent of the victim.\"",
        "A 2007 survey by the National Institute of Justice found that 19.0% of college women and 6.1% of college men experienced either sexual assault or attempted sexual assault since entering college.",
        "In the University of Pennsylvania Law Review in 2017, D. Tuerkheimer reviewed the literature on rape allegations, and reported on the problems surrounding the credibility of rape victims, and how that relates to false rape accusations.",
        "She pointed to national survey data from the Centers for Disease Control and Prevention that indicates 1 in every 5 women (and 1 in 71 men) will be raped during their lifetime at some point.",
        "Despite the prevalence of rape and the fact that false rape allegations are rare, Tuerkheimer reported that law enforcement officers often default to disbelief about an alleged rape.",
        "This documented prejudice leads to reduced investigation and criminal justice outcomes that are faulty compared to other crimes.",
        "Tuerkheimer says that women face \"credibility discounts\" at all stages of the justice system, including from police, jurors, judges, and prosecutors.",
        "These credibility discounts are especially pronounced when the victim is acquainted with the accuser, and the vast majority of rapes fall into this category.",
        "The U.S. Department of Justice estimated from 2005 to 2007 that about 2% of victims who were raped while incapacitated (from drugs, alcohol, or other reasons) reported the rape to the police, compared to 13% of victims who experienced physically forced sexual assault.",
        "The 1998 National Violence Against Women Survey, based on a sample size of 8,000, estimated the incidence of rape to be 1 in 6 for women and 1 in 33 for men, based on reports of attempted or completed rapes over the course of her or his lifetime.",
        "A 1997 study on the non-institutionalized, non-military population by the U.S. Bureau of Justice Statistics, which defines rape as forced penetration by the offender, found that 91% of reported rape victims are female and 9% are male.",
        "The majority of rapes in the United States go unreported.",
        "According to the American Medical Association (1995), sexual violence, and rape in particular, is considered the most under-reported violent crime.",
        "The US Bureau of Justice Criminal Victimization Statistics reports that up to 66.1% of rapes go unreported.",
        "Some of the most common reasons given by victims for not reporting rapes are when the victim considers it a personal or private matter, and the fear of reprisal from the assailant.",
        "Under-reporting affects the accuracy of this data.",
        "A significant number of rapes reported to the police do not advance to prosecution.",
        "Twenty-five percent of reported rapes result in arrest.",
        "Only 16% of rapes and sexual assaults are reported to the police (Rape in America: A Report to the Nation.",
        "1992 and United Nations Populations Fund, 2000a).",
        "Factoring in unreported rapes, about 5% of rapists will ever spend a day in jail.",
        "Contrary to widespread belief, rape outdoors is rare.",
        "Over two thirds of all rapes occur in someone's home.",
        "31% occur in the perpetrators' homes, 27% in the victims' homes and 10% in homes shared by the victim and perpetrator.",
        "7% occur at parties, 7% in vehicles, 4% outdoors and 2% in bars.",
        "From 2000 to 2005, 59% of rapes were not reported to law enforcement.",
        "One factor relating to this is the misconception that most rapes are committed by strangers.",
        "In reality, studies indicate the following varying numbers: In a 2012 news story, The New York Times reported, \"according to a survey by the Alaska Federation of Natives, the rate of sexual violence in rural villages like Emmonak is as much as 12 times the national rate.",
        "And interviews with Native American women here and across the nation’s tribal reservations suggest an even grimmer reality: They say few, if any, female relatives or close friends have escaped sexual violence.\"",
        "In a national survey conducted in the United States of America, 14.8% of women over 17 years of age reported having been raped in their lifetime (with an additional 2.8% having experienced attempted rape) and 0.3% of the sample reported having been raped in the previous year.",
        "Drug use, especially alcohol, is frequently involved in rape.",
        "A study (only of rape victims that were female and reachable by phone) reported detailed findings related to tactics.",
        "In 47% of such rapes, both the victim and the perpetrator had been drinking.",
        "In 17%, only the perpetrator had been.",
        "7% of the time, only the victim had been drinking.",
        "Rapes where neither the victim nor the perpetrator had been drinking were 29% of all rapes.",
        "Not only has it been a factor in the rates of sexual assault on campus, but because of the prevalence, assaults are also being affected specifically by the inability to give consent when intoxicated and bystanders not knowing when to intervene due to their own intoxication or the intoxication of the victim.",
        "Koss, Gidycz and Wi published a study in 1987 where they interviewed approximately 6,000 college students on 32 college campuses nationwide.",
        "They asked several questions covering a wide range of behaviours.",
        "From this study, 15% of college women answered \"yes\" to questions about whether they experienced something that met the definition of rape.",
        "12% of women answered \"yes\" to questions about whether they experienced something that met the definition of attempted rape.",
        "Moreover, depending on the region, 2-6% of the men interviewed admitted to rape.",
        "While the study focused on female victims and male perpetrators; it did not consider rape of men or rape in LGBT relationships.",
        "In 1995, the CDC replicated part of this study with 8,810 students on 138 college campuses.",
        "They examined rape only, and did not look at attempted rape.",
        "They found that 20% of women and 4% of men had experienced rape in the course of her or his lifetime.",
        "In 2000, the National Institute of Justice and the Bureau of Justice Statistics published a study called \"The Sexual Victimization of College Women\" based on a 1996-1997 survey.",
        "The study found that 3.1% of undergraduate women reported experiencing an act that met the researchers' definition of rape or attempted rape during a 6-7-month academic year.",
        "However, of those found to have experienced completed rape, only 46.5% of the victims answered that they considered the incident to be a rape, while 48.8% did not and 4.7% were unsure.",
        "The study also found that 10.1% of college women experienced rape and 10.9% experienced attempted rape prior to entering college.",
        "Victimization of men was not considered as part of this study.",
        "In a different section of the report, the authors speculated about whether statistics during an academic year generalize to an entire college experience.",
        "For a full discussion, read more on page 10 of the report, stating that \"the percentage of completed or attempted rape victimization among women in higher educational institutions might climb to between one-fifth and one-quarter\" and further acknowledging in the corresponding footnote, #18, that \"These projections are suggestive.",
        "To assess accurately the victimization risk for women throughout a college career, longitudinal research following a cohort of female students across time is needed.\"",
        "80,000 American children are known to have been sexually abused each year.",
        "But unreported cases are higher, due to the fear among children.",
        "Over ninety percent of the time, the perpetrator is someone familiar or close with the child.",
        "Sexually violent crimes targeting children involve forced sexual activities such as intercourse, masturbation, and/or other explicit contact with a minor.",
        "According to Child Protective Services, eighty percent of the time, a parent ends up being the perpetrator.",
        "Children who become victims of this crime often end up developing phobias, depression, and post-traumatic stress disorder, as well as performing poorly in school.",
        "Sexually violent crimes of all ages occur often.",
        "According to United States Department of Justice document Criminal Victimization in the United States, there were overall 191,670 victims of rape or sexual assault reported in 2005.",
        "Myriam Denov (2004) states that societal responses to the issue of female perpetrators of sexual assault \"point to a widespread denial of women as potential sexual aggressors that could work to obscure the true dimensions of the problem.\"",
        "Particularly as an increasing population of un-convicted felons and rapists who continue to insist that accusation of sexual assault is a punishment in lieu of justice through law enforcement agencies.",
        "It is thought that to be accused of rape brings shame to their families and social communities.",
        "According to the National Crime Victimization Survey, the adjusted per-capita victimization rate of rape has declined from about 2.4 per 1000 people (age 12 and above) in 1980 to about 0.4 per 1000 people in 2006, a decline of about 85%.",
        "But other government surveys, such as the Sexual Victimization of College Women study, critique the NCVS on the basis it includes only those acts perceived as crimes by the victim, and report a higher victimization rate.",
        "Despite a decline of 60% since 1993, the US still has a relatively high rate of rape when compared to other developed countries.",
        "RAINN asserts that from 2000 to 2005, 59% of rapes were not reported to law enforcement.",
        "For college students, the figure was 95% in 2000.",
        "One factor relating to this is the misconception that most rapes are committed by strangers.",
        "According to the Bureau of Justice Statistics, 38% of victims were raped by a friend or acquaintance, 28% by \"an intimate\" and 7% by another relative, and 26% were committed by a stranger to the victim.",
        "About four out of ten sexual assaults take place at the victim's own home.",
        "Yemen law does not recognize marital rape and does not provide a minimum age for marriage.",
        "The issues of child marriage and child rape inside marriage have made international news and have led to calls for legislative changes.",
        "There have been several reports of deaths of young girls due to violent rape by adult husbands, as well as young girls dying during childbirth.",
        "Human Rights Watch stated that \"Child marriages and forced marriages remain widespread, exposing young girls to domestic violence and maternal mortality and truncating their education.\""
      ],
      "metadata": {
        "title": "Rape statistics",
        "url": "https://en.wikipedia.org/wiki/Rape_statistics",
        "word_count": 13130,
        "char_count": 78988,
        "sentence_count": 555,
        "scraped_at": "2025-08-09T14:47:20.056010",
        "language": "en",
        "processing_time": 0.02368903160095215,
        "source_hash": "9da977cda047261b10fd0cf159a35dc9"
      }
    },
    {
      "title": "Bureau of Labor Statistics",
      "url": "https://en.wikipedia.org/wiki/Bureau_of_Labor_Statistics",
      "raw_text": "The Bureau of Labor Statistics (BLS) is a unit of the United States Department of Labor. It is the principal fact-finding agency for the U.S. government in the broad field of labor economics and statistics and serves as a principal agency of the U.S. Federal Statistical System. The BLS collects, processes, analyzes, and disseminates essential statistical data to the American public, the U.S. Congress, other Federal agencies, State and local governments, business, and labor representatives. The BLS also serves as a statistical resource to the United States Department of Labor, and conducts research measuring the income levels families need to maintain a satisfactory quality of life.\nBLS data must satisfy a number of criteria, including relevance to current social and economic issues, timeliness in reflecting today's rapidly changing economic conditions, accuracy and consistently high statistical quality, impartiality in both subject matter and presentation, and accessibility to all. To avoid the appearance of partiality, the dates of major data releases are scheduled more than a year in advance, in coordination with the Office of Management and Budget.\n\n\n== History ==\n\nThe Bureau of Labor was established within the Department of the Interior on June 27, 1884, to collect information about employment and labor. Its creation under the Bureau of Labor Act (23 Stat. 60) stemmed from the findings of U.S. Senator Henry W. Blair's \"Labor and Capital Hearings\", which examined labor issues and working conditions in the U.S. Statistician Carroll D. Wright became the first U.S. Commissioner of Labor in 1885, a position he held until 1905. The Bureau's placement within the federal government structure changed three times in the first 29 years following its formation. It was made an independent (sub-Cabinet) department by the Department of Labor Act (25 Stat. 182) on June 13, 1888. The Bureau was then incorporated into the Department of Commerce and Labor by the Department of Commerce Act (32 Stat. 827) on February 14, 1903. Finally, it was transferred under the Department of Labor in 1913, where it resides today. Starting in 1992, BLS was headquartered in the Postal Square Building near Washington Union Station. During 2024, BLS headquarters were moved to the Suitland Federal Center in Suitland, Maryland, into the same facility that houses the Bureau of the Census headquarters.\nSince 1915, the BLS has published the Monthly Labor Review, a journal focused on the data and methodologies of labor statistics.\nThe BLS is headed by a commissioner who serves a four-year term from the date he or she takes office. The most recent Commissioner of Labor Statistics is Erika McEntarfer, who was confirmed by the U.S. Senate for the office on January 11, 2024.  \nErica Groshen was confirmed by the U.S. Senate on January 2, 2013, and sworn in as the 14th Commissioner of Labor Statistics on January 29, 2013, for a term that ended on January 27, 2017. William Wiatrowski, Deputy Commissioner of the BLS, served as Acting Commissioner until the next commissioner, William Beach was sworn in. Beach served until January 2024, at which time he was succeeded by Erika McEntarfer.\n\n\n=== Firing of Commissioner McEntarfer ===\n\nOn August 1, 2025, President Donald Trump announced he would fire Commissioner McEntarfer, hours after a downward revision in job creation was published in the Bureau's July jobs report. According to the BBC, \"[t]he decision shocked Wall Street and raised alarm about White House interference in economic data.\" Commentators pointed out that due to BLS security precautions, the commissioner did not have access to the systems that collect the data for this report, and could not change the results without a large number of people knowing and at least some of them complaining publicly.\n\n\n== US Jobs Report ==\nFollowing Trump's firing of Commissioner McEntarfer, the media explained how The Employment Situation reports are made, stating that revisions are not unusual, nor are large changes in them abnormal. The BLS considers its initial job numbers as preliminary when they are first published, because some businesses don't report their payroll data by the deadline (only about 60% do), making the report harder to estimate. The BLS continues collecting the payroll data (three months after the deadline, more than a 90% of workplaces have responded), and revising it according to seasonal adjustments; if more complete data is much above or below the preliminary data, \"revisions can be exacerbated by the BLS’ seasonal adjustments, which sometimes need to be recalculated.\" The data is revised in each of the two months following the initial report, also in a preliminary annual revision (August), and in a final annual revision (February), adding unemployment insurance data; there is a 10-year revision with census data. The BLS doesn't give lengthy analysis of the revisions; according to William Beach , \"it's normal for BLS not to explain those differences, because then they're doing a job outside of the job they're supposed to do, which is to take the data, and statistically make modifications to the estimates based on the data.\"\nAccording to the WSJ, the BLS has faced issues with data collection in recent years. Budget cuts—including a governmental hiring freeze earlier this year—and response rates have made providing real-time, accurate data more difficult. For example, the BLS surveys about 120,000 employers by phone or online to track the number of jobs in the economy and about 30% to 40% don’t reply on time—up from under 20% a decade ago.\n\n\n== Commissioners ==\nCommissioners of Labor Statistics (1885 to present):\n\nTable notes:\n\n\n== Statistical reporting ==\nStatistics published by the BLS fall into four main categories:\n\n\n=== Prices ===\nU.S. Consumer Price Index\nProducer Price Index\nU.S. Import and Export Price Indexes\nConsumer Expenditure Survey\n\n\n=== Employment and unemployment ===\n\nCurrent Population Survey (The \"Household Survey\")\nThe American Time Use Survey\nCurrent Employment Statistics (The \"Establishment Survey\")\nPayroll Employment\nJOLTS report - Job Openings and Labor Turnover Survey\nEconomic geography\nSalary Data\nLocal Area Unemployment Statistics (LAUS)\nList of U.S. states by unemployment rate\nCurrent Employment Statistics State and Area program\nThe Job Openings and Labor Turnover Survey (JOLTS)\nThe Quarterly Census of Employment and Wages (QCEW)\nThe Business Employment Dynamics (BED) program\nTen year occupational employment projections\nOccupational Employment and Wage Statistics, called OES until recently\nMass Layoff Statistics--discontinued in 2013\n\n\n=== Compensation and working conditions ===\nNational Compensation Survey\nEmployment Cost Index\nWorkplace Injury and Fatality Statistics\n\n\n=== Productivity ===\nLabor productivity, aggregate and by industry\nMultifactor productivity\nState labor productivity\nWork Stoppage data\n\n\n== Statistical regions ==\nData produced by the BLS is often categorized into groups of states known as Census Regions.  There are four Census Regions, which are further categorized by Census Division as follows:\nNortheast Region\n\nNew England Division: Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont.\nMiddle Atlantic Division: New Jersey, New York, and Pennsylvania.\nSouth Region\n\nSouth Atlantic Division: Delaware, District of Columbia, Florida, Georgia, Maryland, North Carolina, South Carolina, Virginia, and West Virginia.\nEast South Central Division: Alabama, Kentucky, Mississippi, and Tennessee.\nWest South Central Division: Arkansas, Louisiana, Oklahoma, and Texas.\nMidwest Region\n\nEast North Central Division: Illinois, Indiana, Michigan, Ohio, and Wisconsin.\nWest North Central Division: Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, and South Dakota.\nWest Region\n\nMountain Division: Arizona, Colorado, Idaho, Montana, Nevada, New Mexico, Utah, and Wyoming.\nPacific Division: Alaska, California, Hawaii, Oregon, and Washington.\n\n\n== See also ==\nAlternative employment arrangements\nBureau of Economic Analysis\nCareer Guide to Industries\nData.gov\nEconomic reports\nIndex of Leading Indicators\nJob Creation Index\nMonthly Labor Review\nNational Income and Product Accounts\nOccupational Outlook Handbook\nU.S. Census Bureau\nUSAFacts\n\n\n== Footnotes ==\n\n\n== Further reading ==\nJoseph P. Goldberg and William T. Moye, The First 100 Years of the Bureau of Labor Statistics. Bulletin No. 2235. Washington, D.C.: U.S. Government Printing Office, 1985.\nWilliam J. Wiatrowski, \"BLS at 125: Using historic principles to track the 21st-century economy\". Monthly Labor Review, June 2009, pp. 3–25.\n\n\n== External links ==\n\nOfficial website\nRecords of the Bureau of Labor Statistics in the National Archives (Record Group 257)\nBureau of Labor Statistics in the Federal Register\nPublications of the BLS available on FRASER\nBulletins of the United States Bureau of Labor Statistics, dating back to 1895\nLocal Area Unemployment Reports",
      "cleaned_text": "The Bureau of Labor Statistics (BLS) is a unit of the United States Department of Labor. It is the principal fact-finding agency for the U.S. government in the broad field of labor economics and statistics and serves as a principal agency of the U.S. Federal Statistical System. The BLS collects, processes, analyzes, and disseminates essential statistical data to the American public, the U.S. Congress, other Federal agencies, State and local governments, business, and labor representatives. The BLS also serves as a statistical resource to the United States Department of Labor, and conducts research measuring the income levels families need to maintain a satisfactory quality of life. BLS data must satisfy a number of criteria, including relevance to current social and economic issues, timeliness in reflecting today's rapidly changing economic conditions, accuracy and consistently high statistical quality, impartiality in both subject matter and presentation, and accessibility to all. To avoid the appearance of partiality, the dates of major data releases are scheduled more than a year in advance, in coordination with the Office of Management and Budget. The Bureau of Labor was established within the Department of the Interior on June 27, 1884, to collect information about employment and labor. Its creation under the Bureau of Labor Act (23 Stat. 60) stemmed from the findings of U.S. Senator Henry W. Blair's \"Labor and Capital Hearings\", which examined labor issues and working conditions in the U.S. Statistician Carroll D. Wright became the first U.S. Commissioner of Labor in 1885, a position he held until 1905. The Bureau's placement within the federal government structure changed three times in the first 29 years following its formation. It was made an independent (sub-Cabinet) department by the Department of Labor Act (25 Stat. 182) on June 13, 1888. The Bureau was then incorporated into the Department of Commerce and Labor by the Department of Commerce Act (32 Stat. 827) on February 14, 1903. Finally, it was transferred under the Department of Labor in 1913, where it resides today. Starting in 1992, BLS was headquartered in the Postal Square Building near Washington Union Station. During 2024, BLS headquarters were moved to the Suitland Federal Center in Suitland, Maryland, into the same facility that houses the Bureau of the Census headquarters. Since 1915, the BLS has published the Monthly Labor Review, a journal focused on the data and methodologies of labor statistics. The BLS is headed by a commissioner who serves a four-year term from the date he or she takes office. The most recent Commissioner of Labor Statistics is Erika McEntarfer, who was confirmed by the U.S. Senate for the office on January 11, 2024. Erica Groshen was confirmed by the U.S. Senate on January 2, 2013, and sworn in as the 14th Commissioner of Labor Statistics on January 29, 2013, for a term that ended on January 27, 2017. William Wiatrowski, Deputy Commissioner of the BLS, served as Acting Commissioner until the next commissioner, William Beach was sworn in. Beach served until January 2024, at which time he was succeeded by Erika McEntarfer. On August 1, 2025, President Donald Trump announced he would fire Commissioner McEntarfer, hours after a downward revision in job creation was published in the Bureau's July jobs report. According to the BBC, \"[t]he decision shocked Wall Street and raised alarm about White House interference in economic data.\" Commentators pointed out that due to BLS security precautions, the commissioner did not have access to the systems that collect the data for this report, and could not change the results without a large number of people knowing and at least some of them complaining publicly. Following Trump's firing of Commissioner McEntarfer, the media explained how The Employment Situation reports are made, stating that revisions are not unusual, nor are large changes in them abnormal. The BLS considers its initial job numbers as preliminary when they are first published, because some businesses don't report their payroll data by the deadline (only about 60% do), making the report harder to estimate. The BLS continues collecting the payroll data (three months after the deadline, more than a 90% of workplaces have responded), and revising it according to seasonal adjustments; if more complete data is much above or below the preliminary data, \"revisions can be exacerbated by the BLS’ seasonal adjustments, which sometimes need to be recalculated.\" The data is revised in each of the two months following the initial report, also in a preliminary annual revision (August), and in a final annual revision (February), adding unemployment insurance data; there is a 10-year revision with census data. The BLS doesn't give lengthy analysis of the revisions; according to William Beach , \"it's normal for BLS not to explain those differences, because then they're doing a job outside of the job they're supposed to do, which is to take the data, and statistically make modifications to the estimates based on the data.\" According to the WSJ, the BLS has faced issues with data collection in recent years. Budget cuts-including a governmental hiring freeze earlier this year-and response rates have made providing real-time, accurate data more difficult. For example, the BLS surveys about 120,000 employers by phone or online to track the number of jobs in the economy and about 30% to 40% don’t reply on time-up from under 20% a decade ago. Commissioners of Labor Statistics (1885 to present): Table notes: Statistics published by the BLS fall into four main categories: U.S. Consumer Price Index Producer Price Index U.S. Import and Export Price Indexes Consumer Expenditure Survey Current Population Survey (The \"Household Survey\") The American Time Use Survey Current Employment Statistics (The \"Establishment Survey\") Payroll Employment JOLTS report - Job Openings and Labor Turnover Survey Economic geography Salary Data Local Area Unemployment Statistics (LAUS) List of U.S. states by unemployment rate Current Employment Statistics State and Area program The Job Openings and Labor Turnover Survey (JOLTS) The Quarterly Census of Employment and Wages (QCEW) The Business Employment Dynamics (BED) program Ten year occupational employment projections Occupational Employment and Wage Statistics, called OES until recently Mass Layoff Statistics--discontinued in 2013 National Compensation Survey Employment Cost Index Workplace Injury and Fatality Statistics Labor productivity, aggregate and by industry Multifactor productivity State labor productivity Work Stoppage data Data produced by the BLS is often categorized into groups of states known as Census Regions. There are four Census Regions, which are further categorized by Census Division as follows: Northeast Region New England Division: Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont. Middle Atlantic Division: New Jersey, New York, and Pennsylvania. South Region South Atlantic Division: Delaware, District of Columbia, Florida, Georgia, Maryland, North Carolina, South Carolina, Virginia, and West Virginia. East South Central Division: Alabama, Kentucky, Mississippi, and Tennessee. West South Central Division: Arkansas, Louisiana, Oklahoma, and Texas. Midwest Region East North Central Division: Illinois, Indiana, Michigan, Ohio, and Wisconsin. West North Central Division: Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, and South Dakota. West Region Mountain Division: Arizona, Colorado, Idaho, Montana, Nevada, New Mexico, Utah, and Wyoming. Pacific Division: Alaska, California, Hawaii, Oregon, and Washington.",
      "sentences": [
        "The Bureau of Labor Statistics (BLS) is a unit of the United States Department of Labor.",
        "It is the principal fact-finding agency for the U.S. government in the broad field of labor economics and statistics and serves as a principal agency of the U.S. Federal Statistical System.",
        "The BLS collects, processes, analyzes, and disseminates essential statistical data to the American public, the U.S. Congress, other Federal agencies, State and local governments, business, and labor representatives.",
        "The BLS also serves as a statistical resource to the United States Department of Labor, and conducts research measuring the income levels families need to maintain a satisfactory quality of life.",
        "BLS data must satisfy a number of criteria, including relevance to current social and economic issues, timeliness in reflecting today's rapidly changing economic conditions, accuracy and consistently high statistical quality, impartiality in both subject matter and presentation, and accessibility to all.",
        "To avoid the appearance of partiality, the dates of major data releases are scheduled more than a year in advance, in coordination with the Office of Management and Budget.",
        "The Bureau of Labor was established within the Department of the Interior on June 27, 1884, to collect information about employment and labor.",
        "Its creation under the Bureau of Labor Act (23 Stat.",
        "60) stemmed from the findings of U.S.",
        "Senator Henry W. Blair's \"Labor and Capital Hearings\", which examined labor issues and working conditions in the U.S. Statistician Carroll D. Wright became the first U.S. Commissioner of Labor in 1885, a position he held until 1905.",
        "The Bureau's placement within the federal government structure changed three times in the first 29 years following its formation.",
        "It was made an independent (sub-Cabinet) department by the Department of Labor Act (25 Stat.",
        "182) on June 13, 1888.",
        "The Bureau was then incorporated into the Department of Commerce and Labor by the Department of Commerce Act (32 Stat.",
        "827) on February 14, 1903.",
        "Finally, it was transferred under the Department of Labor in 1913, where it resides today.",
        "Starting in 1992, BLS was headquartered in the Postal Square Building near Washington Union Station.",
        "During 2024, BLS headquarters were moved to the Suitland Federal Center in Suitland, Maryland, into the same facility that houses the Bureau of the Census headquarters.",
        "Since 1915, the BLS has published the Monthly Labor Review, a journal focused on the data and methodologies of labor statistics.",
        "The BLS is headed by a commissioner who serves a four-year term from the date he or she takes office.",
        "The most recent Commissioner of Labor Statistics is Erika McEntarfer, who was confirmed by the U.S. Senate for the office on January 11, 2024.",
        "Erica Groshen was confirmed by the U.S. Senate on January 2, 2013, and sworn in as the 14th Commissioner of Labor Statistics on January 29, 2013, for a term that ended on January 27, 2017.",
        "William Wiatrowski, Deputy Commissioner of the BLS, served as Acting Commissioner until the next commissioner, William Beach was sworn in.",
        "Beach served until January 2024, at which time he was succeeded by Erika McEntarfer.",
        "On August 1, 2025, President Donald Trump announced he would fire Commissioner McEntarfer, hours after a downward revision in job creation was published in the Bureau's July jobs report.",
        "According to the BBC, \"[t]he decision shocked Wall Street and raised alarm about White House interference in economic data.\"",
        "Commentators pointed out that due to BLS security precautions, the commissioner did not have access to the systems that collect the data for this report, and could not change the results without a large number of people knowing and at least some of them complaining publicly.",
        "Following Trump's firing of Commissioner McEntarfer, the media explained how The Employment Situation reports are made, stating that revisions are not unusual, nor are large changes in them abnormal.",
        "The BLS considers its initial job numbers as preliminary when they are first published, because some businesses don't report their payroll data by the deadline (only about 60% do), making the report harder to estimate.",
        "The BLS continues collecting the payroll data (three months after the deadline, more than a 90% of workplaces have responded), and revising it according to seasonal adjustments; if more complete data is much above or below the preliminary data, \"revisions can be exacerbated by the BLS’ seasonal adjustments, which sometimes need to be recalculated.\"",
        "The data is revised in each of the two months following the initial report, also in a preliminary annual revision (August), and in a final annual revision (February), adding unemployment insurance data; there is a 10-year revision with census data.",
        "The BLS doesn't give lengthy analysis of the revisions; according to William Beach , \"it's normal for BLS not to explain those differences, because then they're doing a job outside of the job they're supposed to do, which is to take the data, and statistically make modifications to the estimates based on the data.\"",
        "According to the WSJ, the BLS has faced issues with data collection in recent years.",
        "Budget cuts-including a governmental hiring freeze earlier this year-and response rates have made providing real-time, accurate data more difficult.",
        "For example, the BLS surveys about 120,000 employers by phone or online to track the number of jobs in the economy and about 30% to 40% don’t reply on time-up from under 20% a decade ago.",
        "Commissioners of Labor Statistics (1885 to present): Table notes: Statistics published by the BLS fall into four main categories: U.S. Consumer Price Index Producer Price Index U.S.",
        "There are four Census Regions, which are further categorized by Census Division as follows: Northeast Region New England Division: Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont.",
        "Middle Atlantic Division: New Jersey, New York, and Pennsylvania.",
        "South Region South Atlantic Division: Delaware, District of Columbia, Florida, Georgia, Maryland, North Carolina, South Carolina, Virginia, and West Virginia.",
        "East South Central Division: Alabama, Kentucky, Mississippi, and Tennessee.",
        "West South Central Division: Arkansas, Louisiana, Oklahoma, and Texas.",
        "Midwest Region East North Central Division: Illinois, Indiana, Michigan, Ohio, and Wisconsin.",
        "West North Central Division: Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, and South Dakota.",
        "West Region Mountain Division: Arizona, Colorado, Idaho, Montana, Nevada, New Mexico, Utah, and Wyoming.",
        "Pacific Division: Alaska, California, Hawaii, Oregon, and Washington."
      ],
      "metadata": {
        "title": "Bureau of Labor Statistics",
        "url": "https://en.wikipedia.org/wiki/Bureau_of_Labor_Statistics",
        "word_count": 1174,
        "char_count": 7709,
        "sentence_count": 45,
        "scraped_at": "2025-08-09T14:47:20.058284",
        "language": "en",
        "processing_time": 0.002039194107055664,
        "source_hash": "d2a2828b93dea4c100dce609b3c85660"
      }
    },
    {
      "title": "Index (statistics)",
      "url": "https://en.wikipedia.org/wiki/Index_(statistics)",
      "raw_text": "In statistics and research design, an index is a composite statistic – a measure of changes in a representative group of individual data points, or in other words, a compound measure that aggregates multiple indicators. Indices – also known as indexes and composite indicators – summarize and rank specific observations.\nMuch data in the field of social sciences and sustainability are represented in various indices such as Gender Gap Index, Human Development Index or the Dow Jones Industrial Average. The ‘Report by the Commission on the Measurement of Economic Performance and Social Progress’, written by Joseph Stiglitz, Amartya Sen, and Jean-Paul Fitoussi in 2009  suggests that these measures have experienced a dramatic growth in recent years due to three concurring factors: \n\nimprovements in the level of literacy (including statistical)\nincreased complexity of modern societies and economies, and\nwidespread availability of information technology.\nAccording to Earl Babbie, items in indices are usually weighted equally, unless there are some reasons against it (for example, if two items reflect essentially the same aspect of a variable, they could have a weight of 0.5 each).\nAccording to the same author, constructing the items involves four steps. First, items should be selected based on their content validity, unidimensionality, the degree of specificity in which a dimension is to be measured, and their amount of variance. Items should be empirically related to one another, which leads to the second step of examining their multivariate relationships. Third, index scores are designed, which involves determining score ranges and weights for the items. Finally, indices should be validated, which involves testing whether they can predict indicators related to the measured variable not used in their construction.\nA handbook for the construction of composite indicators (CIs) was published jointly by the OECD and by the European Commission's Joint Research Centre in 2008. The handbook – officially endorsed by the OECD high level statistical committee, describe ten recursive steps for developing an index:\n\nStep 1: Theoretical framework\nStep 2: Data selection\nStep 3: Imputation of missing data\nStep 4: Multivariate analysis\nStep 5: Normalisation\nStep 6: Weighting\nStep 7: Aggregating indicators\nStep 8: Sensitivity analysis\nStep 9: Link to other measures\nStep 10: Visualisation\nAs suggested by the list, many modelling choices are needed to construct a composite indicator, which makes their use controversial. The delicate issue of assigning and validating weights is discussed e.g. in. A sociological reading of the nature of composite indicators is offered by Paul-Marie Boulanger, who sees these measures at the intersection of three movements: \n\nthe democratisation of expertise, the concept that more knowledge is needed to tackle societal and environmental issues that can be provided by the sole experts – this line of thought connects to the concept of extended peer community developed by post-normal science\nthe impulse to the creation of a new public through a process of social discovery, which can be reconnected to the work of pragmatists such as John Dewey\nthe semiotic of Charles Sanders Peirce; Thus a CI is not just a sign or a number, but suggests an action or a behaviour.\nA subsequent work by Boulanger  analyses composite indicators in light of the social system theories of Niklas Luhmann to investigate how different measurements of progress are or are not taken up.\n\n\n== See also ==\nIndex (economics)\nScale (social sciences)\n\n\n== References ==",
      "cleaned_text": "In statistics and research design, an index is a composite statistic - a measure of changes in a representative group of individual data points, or in other words, a compound measure that aggregates multiple indicators. Indices - also known as indexes and composite indicators - summarize and rank specific observations. Much data in the field of social sciences and sustainability are represented in various indices such as Gender Gap Index, Human Development Index or the Dow Jones Industrial Average. The ‘Report by the Commission on the Measurement of Economic Performance and Social Progress’, written by Joseph Stiglitz, Amartya Sen, and Jean-Paul Fitoussi in 2009 suggests that these measures have experienced a dramatic growth in recent years due to three concurring factors: improvements in the level of literacy (including statistical) increased complexity of modern societies and economies, and widespread availability of information technology. According to Earl Babbie, items in indices are usually weighted equally, unless there are some reasons against it (for example, if two items reflect essentially the same aspect of a variable, they could have a weight of 0.5 each). According to the same author, constructing the items involves four steps. First, items should be selected based on their content validity, unidimensionality, the degree of specificity in which a dimension is to be measured, and their amount of variance. Items should be empirically related to one another, which leads to the second step of examining their multivariate relationships. Third, index scores are designed, which involves determining score ranges and weights for the items. Finally, indices should be validated, which involves testing whether they can predict indicators related to the measured variable not used in their construction. A handbook for the construction of composite indicators (CIs) was published jointly by the OECD and by the European Commission's Joint Research Centre in 2008. The handbook - officially endorsed by the OECD high level statistical committee, describe ten recursive steps for developing an index: Step 1: Theoretical framework Step 2: Data selection Step 3: Imputation of missing data Step 4: Multivariate analysis Step 5: Normalisation Step 6: Weighting Step 7: Aggregating indicators Step 8: Sensitivity analysis Step 9: Link to other measures Step 10: Visualisation As suggested by the list, many modelling choices are needed to construct a composite indicator, which makes their use controversial. The delicate issue of assigning and validating weights is discussed e.g. in. A sociological reading of the nature of composite indicators is offered by Paul-Marie Boulanger, who sees these measures at the intersection of three movements: the democratisation of expertise, the concept that more knowledge is needed to tackle societal and environmental issues that can be provided by the sole experts - this line of thought connects to the concept of extended peer community developed by post-normal science the impulse to the creation of a new public through a process of social discovery, which can be reconnected to the work of pragmatists such as John Dewey the semiotic of Charles Sanders Peirce; Thus a CI is not just a sign or a number, but suggests an action or a behaviour. A subsequent work by Boulanger analyses composite indicators in light of the social system theories of Niklas Luhmann to investigate how different measurements of progress are or are not taken up.",
      "sentences": [
        "In statistics and research design, an index is a composite statistic - a measure of changes in a representative group of individual data points, or in other words, a compound measure that aggregates multiple indicators.",
        "Indices - also known as indexes and composite indicators - summarize and rank specific observations.",
        "Much data in the field of social sciences and sustainability are represented in various indices such as Gender Gap Index, Human Development Index or the Dow Jones Industrial Average.",
        "The ‘Report by the Commission on the Measurement of Economic Performance and Social Progress’, written by Joseph Stiglitz, Amartya Sen, and Jean-Paul Fitoussi in 2009 suggests that these measures have experienced a dramatic growth in recent years due to three concurring factors: improvements in the level of literacy (including statistical) increased complexity of modern societies and economies, and widespread availability of information technology.",
        "According to Earl Babbie, items in indices are usually weighted equally, unless there are some reasons against it (for example, if two items reflect essentially the same aspect of a variable, they could have a weight of 0.5 each).",
        "According to the same author, constructing the items involves four steps.",
        "First, items should be selected based on their content validity, unidimensionality, the degree of specificity in which a dimension is to be measured, and their amount of variance.",
        "Items should be empirically related to one another, which leads to the second step of examining their multivariate relationships.",
        "Third, index scores are designed, which involves determining score ranges and weights for the items.",
        "Finally, indices should be validated, which involves testing whether they can predict indicators related to the measured variable not used in their construction.",
        "A handbook for the construction of composite indicators (CIs) was published jointly by the OECD and by the European Commission's Joint Research Centre in 2008.",
        "The delicate issue of assigning and validating weights is discussed e.g.",
        "A subsequent work by Boulanger analyses composite indicators in light of the social system theories of Niklas Luhmann to investigate how different measurements of progress are or are not taken up."
      ],
      "metadata": {
        "title": "Index (statistics)",
        "url": "https://en.wikipedia.org/wiki/Index_(statistics)",
        "word_count": 544,
        "char_count": 3512,
        "sentence_count": 13,
        "scraped_at": "2025-08-09T14:47:20.059097",
        "language": "en",
        "processing_time": 0.0007278919219970703,
        "source_hash": "e913e0c57d8c476cdf064ed72dfb2b5e"
      }
    }
  ],
  "failed_articles": []
}