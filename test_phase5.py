#!/usr/bin/env python3
"""
Test Phase 5: Knowledge Graph Construction
==========================================

Test script to verify Phase 5 (Knowledge Graph Construction) functionality.
Tests RAGAS-compatible knowledge graph building and enhanced retrieval.

Run from project root:
    python test_phase5.py
"""

import sys
import os
from pathlib import Path

# Add utils to path
sys.path.append(str(Path(__file__).parent / "utils"))

from pipeline import SemanticRAGPipeline


def test_phase5():
    """Test Phase 5: Knowledge Graph Construction."""
    print("ğŸ§ª Testing Phase 5: Knowledge Graph Construction")
    print("(SPARSE VERSION - Dramatically reduced from 800MB to ~20MB)")
    print("=" * 65)
    
    try:
        # Initialize pipeline
        config_path = "config.yaml"
        if not Path(config_path).exists():
            print(f"âŒ Config file not found: {config_path}")
            print("ğŸ’¡ Make sure you're running from the project root directory")
            return False
        
        print(f"ğŸ“‹ Using config: {config_path}")
        pipeline = SemanticRAGPipeline(config_path)
        
        # Load config and setup
        pipeline._load_config()
        
        # Override config for testing
        print("ğŸ”§ Configuring for Phase 5 testing...")
        
        # Set mode to full pipeline to include Phase 5
        pipeline.config['execution']['mode'] = 'full_pipeline'
        pipeline.config['execution']['skip_phases'] = []
        
        # Use smaller models for faster testing
        pipeline.config['models']['embedding_models'] = [
            "sentence-transformers/all-MiniLM-L6-v2"  # Faster model for testing
        ]
        
        # Configure settings for testing
        pipeline.config['retrieval']['semantic_traversal']['num_anchors'] = 2
        pipeline.config['retrieval']['semantic_traversal']['max_hops'] = 2
        pipeline.config['retrieval']['semantic_traversal']['max_results'] = 8
        pipeline.config['retrieval']['baseline_vector']['top_k'] = 5
        
        # Configure knowledge graph settings for testing
        pipeline.config['knowledge_graph'] = {
            'use_cached': True,
            'extractors': {
                'ner': {'enabled': True, 'entity_types': ['PERSON', 'ORG', 'GPE', 'MISC']},
                'keyphrases': {'enabled': True, 'max_features': 15},
                'themes': {'enabled': True, 'max_themes': 5},
                'summary': {'enabled': True, 'max_sentences': 2}
            },
            'relationships': {
                'entity_similarity': {'enabled': True, 'min_similarity': 0.1},
                'thematic_similarity': {'enabled': True, 'min_similarity': 0.2},
                'embedding_similarity': {'enabled': True, 'min_similarity': 0.3},
                'hierarchical': {'enabled': True}
            }
        }
        
        # Enable force recompute to see fresh sparse generation
        pipeline.config['execution']['force_recompute'] = ['knowledge_graph']
        
        # Remove old dense knowledge graph to see fresh sparse version
        old_kg_path = Path(pipeline.config['directories']['data']) / "knowledge_graph.json"
        if old_kg_path.exists():
            old_size_mb = old_kg_path.stat().st_size / (1024 * 1024)
            old_kg_path.unlink()
            print(f"   Removed old dense knowledge graph ({old_size_mb:.1f} MB)")
        
        print(f"   Models: {pipeline.config['models']['embedding_models']}")
        print(f"   Algorithm: {pipeline.config['retrieval']['algorithm']}")
        print(f"   Knowledge graph: enabled")
        print(f"   Extractors: NER, keyphrases, themes, summary")
        print(f"   Relationships: entity, thematic, embedding, hierarchical")
        
        # Run pipeline phases 1-5
        print("\nğŸš€ Running pipeline phases 1-5...")
        
        # Phase 1: Setup
        pipeline._phase_1_setup_and_initialization()
        print(f"âœ… Phase 1: Setup completed (device: {pipeline.device})")
        
        # Phase 2: Data Acquisition (if needed)
        wiki_path = Path(pipeline.config['directories']['data']) / "wiki.json"
        if not wiki_path.exists():
            print("ğŸ“¥ No cached articles found, running Phase 2...")
            pipeline._phase_2_data_acquisition()
        else:
            print("ğŸ“‚ Loading cached articles...")
            from wiki import WikiEngine
            wiki_engine = WikiEngine(pipeline.config, pipeline.logger)
            pipeline.articles = wiki_engine._load_cached_articles(wiki_path)
            pipeline.corpus_stats = wiki_engine.get_corpus_statistics()
        
        print(f"âœ… Phase 2: Loaded {len(pipeline.articles)} articles")
        
        # Phase 3: Embedding Generation (if needed)
        if not pipeline.embeddings:
            print("ğŸ§  Running Phase 3: Embedding Generation...")
            pipeline._phase_3_embedding_generation()
        else:
            print("âœ… Phase 3: Embeddings already available")
        
        total_embeddings = sum(len(embeddings) for embeddings in pipeline.embeddings.values())
        print(f"âœ… Phase 3: {total_embeddings:,} chunks embedded")
        
        # Phase 4: Similarity Matrix Construction (if needed)
        if not pipeline.similarities:
            print("ğŸ•° Running Phase 4: Similarity Matrix Construction...")
            pipeline._phase_4_similarity_matrices()
        else:
            print("âœ… Phase 4: Similarity matrices already available")
        
        print(f"âœ… Phase 4: Similarity matrices for {len(pipeline.similarities)} models")
        
        # Phase 5: Knowledge Graph Construction
        print("ğŸ—ï¸  Running Phase 5: Knowledge Graph Construction...")
        pipeline._phase_5_knowledge_graph_construction()
        
        # Verify results
        print("\nğŸ” Verifying Phase 5 results...")
        
        # Check knowledge graph
        if not pipeline.knowledge_graph:
            print("âŒ No knowledge graph was created")
            return False
        
        print(f"âœ… Knowledge graph created successfully")
        
        # Check knowledge graph statistics
        if pipeline.kg_stats:
            stats = pipeline.kg_stats
            print(f"   ğŸ“Š Knowledge Graph Statistics:")
            print(f"      Total nodes: {stats['total_nodes']:,}")
            print(f"      Total relationships: {stats['total_relationships']:,}")
            print(f"      Node types: {stats['node_types']}")
            print(f"      Relationship types: {stats['relationship_types']}")
            print(f"      Build time: {stats.get('build_time', 0):.2f}s")
        
        # Test knowledge graph functionality
        print("\nğŸ§ª Testing knowledge graph functionality...")
        
        # Test node access
        sample_nodes = pipeline.knowledge_graph.nodes[:3]
        print(f"   Sample nodes ({len(sample_nodes)} shown):")
        for i, node in enumerate(sample_nodes):
            print(f"      {i+1}. {node.node_type} - {node.node_id}")
            if 'title' in node.properties:
                print(f"         Title: {node.properties['title']}")
            if 'entities' in node.properties:
                entities = node.properties['entities']
                total_entities = sum(len(v) for v in entities.values())
                print(f"         Entities: {total_entities} total")
            if 'themes' in node.properties:
                themes = node.properties['themes']
                print(f"         Themes: {themes[:3]}...")
        
        # Test relationship access
        sample_relationships = pipeline.knowledge_graph.relationships[:5]
        print(f"\n   Sample relationships ({len(sample_relationships)} shown):")
        for i, rel in enumerate(sample_relationships):
            print(f"      {i+1}. {rel.relationship_type}: {rel.source_id} -> {rel.target_id} (weight: {rel.weight:.3f})")
        
        # Test neighbor finding
        if sample_nodes:
            test_node = sample_nodes[0]
            neighbors = pipeline.knowledge_graph.get_neighbors(test_node.node_id)
            print(f"\n   Neighbors of {test_node.node_id}: {len(neighbors)} found")
            for neighbor in neighbors[:3]:
                print(f"      -> {neighbor.node_type}: {neighbor.node_id}")
        
        # Check retrieval engine
        if not pipeline.retrieval_engine:
            print("âŒ No retrieval engine was created")
            return False
        
        print(f"\nâœ… Enhanced retrieval engine created successfully")
        
        # Check retrieval statistics
        if pipeline.retrieval_stats:
            stats = pipeline.retrieval_stats
            print(f"   ğŸ“Š Enhanced Retrieval Statistics:")
            print(f"      Algorithm: {stats['algorithm']}")
            print(f"      Models: {stats['models_available']}")
            print(f"      Knowledge graph enabled: {pipeline.knowledge_graph is not None}")
            for model, count in stats['total_chunks_per_model'].items():
                print(f"      {model}: {count:,} chunks")
        
        # Test actual retrieval with sample queries
        print("\nğŸ§ª Testing enhanced retrieval with sample queries...")
        
        test_queries = [
            "What is machine learning?",
            "How do neural networks work?",
            "What are the applications of artificial intelligence?",
            "Explain deep learning algorithms"
        ]
        
        model_name = list(pipeline.embeddings.keys())[0]  # Use first available model
        
        for i, query in enumerate(test_queries):
            print(f"\n   Query {i+1}: '{query}'")
            
            try:
                # Test semantic traversal
                semantic_result = pipeline.retrieval_engine.retrieve(
                    query, model_name, algorithm="semantic_traversal"
                )
                
                print(f"      ğŸ”„ Semantic Traversal:")
                print(f"         Results: {len(semantic_result.chunks)}")
                print(f"         Method: {semantic_result.retrieval_method}")
                
                if semantic_result.metadata:
                    metadata = semantic_result.metadata
                    print(f"         Anchors: {metadata.get('num_anchors', 'N/A')}")
                    print(f"         Paths: {metadata.get('total_paths', 'N/A')}")
                    print(f"         Unique chunks: {metadata.get('unique_chunks', 'N/A')}")
                    print(f"         Time: {metadata.get('retrieval_time', 0):.3f}s")
                
                # Show sample results
                if semantic_result.chunks:
                    print(f"         Sample result: '{semantic_result.chunks[0].chunk_text[:80]}...'")
                    print(f"         Score: {semantic_result.scores[0]:.3f}")
                
                # Test baseline for comparison
                baseline_result = pipeline.retrieval_engine.retrieve(
                    query, model_name, algorithm="baseline_vector"
                )
                
                print(f"      ğŸ“Š Baseline Vector:")
                print(f"         Results: {len(baseline_result.chunks)}")
                print(f"         Method: {baseline_result.retrieval_method}")
                
                if baseline_result.chunks:
                    print(f"         Sample result: '{baseline_result.chunks[0].chunk_text[:80]}...'")
                
                # Compare results
                semantic_ids = {chunk.chunk_id for chunk in semantic_result.chunks}
                baseline_ids = {chunk.chunk_id for chunk in baseline_result.chunks}
                overlap = len(semantic_ids & baseline_ids)
                
                print(f"      ğŸ” Comparison:")
                print(f"         Overlap: {overlap}/{len(semantic_result.chunks)} chunks")
                print(f"         Semantic unique: {len(semantic_ids - baseline_ids)}")
                print(f"         Baseline unique: {len(baseline_ids - semantic_ids)}")
                
            except Exception as e:
                print(f"      âŒ Query failed: {e}")
                continue
        
        # Check if knowledge graph was saved
        kg_path = Path(pipeline.config['directories']['data']) / "knowledge_graph.json"
        if kg_path.exists():
            print(f"\nâœ… Knowledge graph saved successfully to {kg_path}")
            
            # Show file size
            file_size = kg_path.stat().st_size / 1024  # KB
            print(f"   File size: {file_size:.1f} KB")
        else:
            print(f"\nâš ï¸  Knowledge graph file not found at {kg_path}")
        
        print("\nğŸ‰ Phase 5 test completed successfully!")
        print(f"ğŸ“‹ Experiment ID: {pipeline.experiment_id}")
        print(f"ğŸ“ Logs saved to: logs/{pipeline.experiment_id}.log")
        print(f"ğŸ“Š Knowledge graph saved to: {kg_path}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Phase 5 test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Main test function."""
    print("ğŸ§ª Semantic RAG Pipeline - Phase 5 Test")
    print("Testing SPARSE knowledge graph construction (solving the 800MB problem!)")
    print("=" * 70)
    
    # Check if we're in the right directory
    if not Path("config.yaml").exists():
        print("âŒ config.yaml not found!")
        print("ğŸ’¡ Make sure you're running this from the project root directory")
        print("ğŸ’¡ Expected directory structure:")
        print("   config.yaml")
        print("   utils/")
        print("   requirements.txt")
        return
    
    success = test_phase5()
    
    if success:
        print("\nâœ… All tests passed!")
        print("ğŸš€ Phase 5 is ready for production use")
        print("\nğŸ¯ Key features verified:")
        print("   â€¢ RAGAS-compatible knowledge graph construction")
        print("   â€¢ Entity extraction (NER, keyphrases, themes)")
        print("   â€¢ Multi-type relationship building")
        print("   â€¢ Hierarchical document structure")
        print("   â€¢ Enhanced retrieval with knowledge graph")
        print("   â€¢ JSON serialization and caching")
        print("   â€¢ Intelligent graph traversal capabilities")
    else:
        print("\nâŒ Tests failed!")
        print("ğŸ”§ Please check the error messages above")


if __name__ == "__main__":
    main()
