#!/usr/bin/env python3
"""
Test Phase 4: Multi-Granularity Similarity Matrix Construction
============================================================

Test script to verify Phase 4 (Multi-Granularity Similarity Matrix Construction) functionality.
Tests chunk-to-chunk, doc-to-doc, sentence-to-sentence, and cross-granularity similarities.

Run from project root:
    python test_phase4.py
"""

import sys
import os
from pathlib import Path

# Add utils to path
sys.path.append(str(Path(__file__).parent / "utils"))

from pipeline import SemanticRAGPipeline


def test_phase4():
    """Test Phase 4: Multi-Granularity Similarity Matrix Construction."""
    print("ğŸ§ª Testing Phase 4: Multi-Granularity Similarity Matrix Construction")
    print("=" * 70)

    try:
        # Initialize pipeline
        config_path = "config.yaml"
        if not Path(config_path).exists():
            print(f"âŒ Config file not found: {config_path}")
            print("ğŸ’¡ Make sure you're running from the project root directory")
            return False

        print(f"ğŸ“‹ Using config: {config_path}")
        pipeline = SemanticRAGPipeline(config_path)

        # Load config and setup
        pipeline._load_config()

        # Override config for testing
        print("ğŸ”§ Configuring for Phase 4 testing...")

        # Set mode to full pipeline to include Phase 4
        pipeline.config['execution']['mode'] = 'full_pipeline'
        pipeline.config['execution']['skip_phases'] = []

        # Use smaller models and settings for faster testing
        pipeline.config['models']['embedding_models'] = [
            "sentence-transformers/all-MiniLM-L6-v2"  # Faster model for testing
        ]
        pipeline.config['models']['embedding_batch_size'] = 16

        # Configure similarity settings for testing (CORRECT PATH!)
        pipeline.config['similarities']['granularity_types']['chunk_to_chunk']['intra_document']['top_k'] = 5
        pipeline.config['similarities']['granularity_types']['chunk_to_chunk']['inter_document']['top_x'] = 3
        pipeline.config['similarities']['batch_size'] = 500  # Smaller batches

        # Enable force recompute to avoid potential cache issues during testing
        pipeline.config['execution']['force_recompute'] = ['similarities']

        print(f"   Models: {pipeline.config['models']['embedding_models']}")
        print(
            f"   Intra-document top_k: {pipeline.config['similarities']['granularity_types']['chunk_to_chunk']['intra_document']['top_k']}")
        print(
            f"   Inter-document top_x: {pipeline.config['similarities']['granularity_types']['chunk_to_chunk']['inter_document']['top_x']}")
        print(f"   Similarity metric: {pipeline.config['similarities']['similarity_metric']}")

        # Run pipeline phases 1-4
        print("\nğŸš€ Running pipeline phases 1-4...")

        # Phase 1: Setup
        pipeline._phase_1_setup_and_initialization()
        print(f"âœ… Phase 1: Setup completed (device: {pipeline.device})")

        # Phase 2: Data Acquisition (if needed)
        wiki_path = Path(pipeline.config['directories']['data']) / "wiki.json"
        if not wiki_path.exists():
            print("ğŸ“¥ No cached articles found, running Phase 2...")
            pipeline._phase_2_data_acquisition()
        else:
            print("ğŸ“‚ Loading cached articles...")
            from wiki import WikiEngine
            wiki_engine = WikiEngine(pipeline.config, pipeline.logger)
            pipeline.articles = wiki_engine._load_cached_articles(wiki_path)
            pipeline.corpus_stats = wiki_engine.get_corpus_statistics()

        print(f"âœ… Phase 2: Loaded {len(pipeline.articles)} articles")

        # Phase 3: Multi-Granularity Embedding Generation (if needed)
        if not pipeline.embeddings:
            print("ğŸ§  Running Phase 3: Multi-Granularity Embedding Generation...")
            pipeline._phase_3_embedding_generation()
        else:
            print("âœ… Phase 3: Multi-granularity embeddings already available")

        # Check embeddings structure
        if pipeline.embeddings:
            model_name = list(pipeline.embeddings.keys())[0]
            granularity_embeddings = pipeline.embeddings[model_name]

            total_embeddings = sum(len(embeddings) for embeddings in granularity_embeddings.values())
            print(f"âœ… Phase 3: Generated {total_embeddings:,} total embeddings")

            for granularity_type, embeddings in granularity_embeddings.items():
                print(f"   {granularity_type}: {len(embeddings):,} embeddings")

        # Phase 4: Multi-Granularity Similarity Matrix Construction
        print("ğŸ•° Running Phase 4: Multi-Granularity Similarity Matrix Construction...")
        pipeline._phase_4_similarity_matrices()

        # Verify results
        print("\nğŸ” Verifying Phase 4 results...")

        # Check similarities
        if not pipeline.similarities:
            print("âŒ No similarity matrices were created")
            return False

        print(f"âœ… Created multi-granularity similarity matrices for {len(pipeline.similarities)} models")

        # Check similarity statistics for each model
        for model_name, similarity_data in pipeline.similarities.items():
            print(f"\nğŸ“Š Results for {model_name}:")

            metadata = similarity_data['metadata']
            matrices = similarity_data['matrices']

            print(f"   ğŸ“ˆ Multi-Granularity Metadata:")
            print(f"      Granularity counts: {metadata.granularity_counts}")
            print(f"      Total connections: {metadata.total_connections:,}")
            print(f"      Memory usage: {metadata.memory_usage_mb:.1f} MB")
            print(f"      Computation time: {metadata.computation_time:.2f}s")

            print(f"   ğŸ—ï¸  Matrix Types:")
            for matrix_name, matrix in matrices.items():
                print(f"      {matrix_name}: {matrix.shape} ({matrix.nnz:,} non-zero)")

            # Verify sparsity statistics
            sparsity_stats = metadata.sparsity_statistics
            print(f"   ğŸ“Š Sparsity Statistics:")
            for matrix_name, stats in sparsity_stats.items():
                print(
                    f"      {matrix_name}: {stats['stored_connections']:,} connections, sparsity={stats['sparsity_ratio']:.6f}")

        # Test matrix types are as expected
        expected_matrix_types = [
            'chunk_to_chunk_intra', 'chunk_to_chunk_inter', 'chunk_to_chunk_combined',
            'doc_to_doc',
            'sentence_to_sentence_semantic', 'sentence_to_sentence_sequential', 'sentence_to_sentence_combined',
            'sentence_to_chunk', 'chunk_to_doc'
        ]

        model_name = list(pipeline.similarities.keys())[0]
        actual_matrices = list(pipeline.similarities[model_name]['matrices'].keys())

        print(f"\nğŸ¯ Matrix Type Verification:")
        for expected_type in expected_matrix_types:
            if expected_type in actual_matrices:
                print(f"   âœ… {expected_type}")
            else:
                print(f"   âš ï¸  {expected_type} (may be disabled or empty)")

        # Test cache functionality
        print("\nğŸ’¾ Testing cache functionality...")
        print("   Running Phase 4 again to test caching...")

        try:
            # Run Phase 4 again (should use cache this time)
            # First clear force_recompute to allow caching
            pipeline.config['execution']['force_recompute'] = []

            original_time = pipeline.similarities[list(pipeline.similarities.keys())[0]]['metadata'].computation_time

            # Time the actual cache load operation
            import time
            cache_start = time.time()
            pipeline._phase_4_similarity_matrices()
            cache_end = time.time()
            actual_cache_time = cache_end - cache_start

            print(f"   Original computation: {original_time:.2f}s")
            print(f"   Cache load time: {actual_cache_time:.3f}s")
            print(f"   Speedup: {original_time / actual_cache_time:.1f}x faster")

            if actual_cache_time < 1.0:  # Should be very fast
                print("   âœ… Caching working effectively")
            else:
                print("   âš ï¸  Caching may not be working as expected")
        except Exception as e:
            print(f"   âš ï¸  Cache test failed: {e}")
            print("   (This is not critical for core functionality)")

        print("\nğŸ‰ Phase 4 multi-granularity test completed successfully!")
        print(f"ğŸ“‹ Experiment ID: {pipeline.experiment_id}")
        print(f"ğŸ“ Logs saved to: logs/{pipeline.experiment_id}.log")
        print(f"ğŸ’¾ Multi-granularity similarity matrices cached in: embeddings/similarities/")

        return True

    except Exception as e:
        print(f"\nâŒ Phase 4 test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Main test function."""
    print("ğŸ§ª Semantic RAG Pipeline - Phase 4 Test")
    print("Testing multi-granularity similarity matrix construction")
    print("=" * 80)

    # Check if we're in the right directory
    if not Path("config.yaml").exists():
        print("âŒ config.yaml not found!")
        print("ğŸ’¡ Make sure you're running this from the project root directory")
        print("ğŸ’¡ Expected directory structure:")
        print("   config.yaml")
        print("   utils/")
        print("   requirements.txt")
        return

    success = test_phase4()

    if success:
        print("\nâœ… All tests passed!")
        print("ğŸš€ Phase 4 Multi-Granularity is ready for production use")
        print("\nğŸ”— Key features verified:")
        print("   â€¢ Multi-granularity similarity matrix construction")
        print("   â€¢ Chunk-to-chunk (intra & inter-document) similarities")
        print("   â€¢ Document-to-document similarities")
        print("   â€¢ Sentence-to-sentence (semantic & sequential) similarities")
        print("   â€¢ Cross-granularity (sentenceâ†”chunk, chunkâ†”doc) similarities")
        print("   â€¢ Memory-efficient sparse matrix storage")
        print("   â€¢ Intelligent caching system")
    else:
        print("\nâŒ Tests failed!")
        print("ğŸ”§ Please check the error messages above")


if __name__ == "__main__":
    main()