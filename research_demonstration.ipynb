{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a8abd21-2273-4a3e-91e0-db2a2e6ebe19",
   "metadata": {},
   "source": [
    "# Semantic RAG Traversal Algorithms Research\n",
    "============================================================\n",
    "\n",
    "## Table of Contents:\n",
    "0. Introduction\n",
    "1. Foundation - Understanding the Problem\n",
    "2. Building Block 1 - Similarity Matrices\n",
    "3. Building Block 2 - Distance Calculations\n",
    "4. Building Block 3 - Dynamic Programming Framework\n",
    "5. Building Block 4 - Advanced Optimization Tools\n",
    "6. The Five Methods - Combining Building Blocks\n",
    "7. Comparative Analysis and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df2b6a-40fe-4a21-b3cc-b91733d1e1af",
   "metadata": {},
   "source": [
    "# Introduction/Overview\n",
    "\n",
    "Semantic retrieval-augmented generation (RAG) systems for large language models (LLM's) invoke a pipeline of retrieving semantically relevant and accurate information to a user's query and feeding it into an LLM so that it provides a relevant and accurate answer. Research has been underway for several years regarding ways to enhance the performance, and accuracy of these systems, to maximize LLM effectiveness. This notebook details the research and discovery process of novel semantic retrieval augmented generation traversal algorithms, from beginning to end. \n",
    "\n",
    "My initial introduction to semantic RAG was by focusing on solving the chunking problem via the since-depreciated Bittensor \"Chunking Subnet.\" This subnet focused on novel methods of semantic chunking, with a very specific reward heuristic, and it was by optimizing for this reward heuristic I discovered a way of visualizing and structuring documents based on semantic similarity by creating a similarity matrix of all pairwise sentence comparisons in a `numpy` array and architecting chunking algorithms from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c34610-6901-4d41-bf48-bb2252ead6b0",
   "metadata": {},
   "source": [
    "As I learned more about the full semantic RAG pipeline I realized that *chunking might not even be an issue.* By sparsely connecting documents together based on semantic similarity, you can create a *knowledge graph* in which each chunk or sentence is connected to others that are similar. This is not necessarily new, as Microsoft discovered GraphRAG in its own research, but I experiemented with different knowledge graph architectures to see where opportunity might be. Specifically with respect to maximizing retrieval quality within semantic RAG systems as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf16a2-3304-4ca0-ae30-9db1d55d784e",
   "metadata": {},
   "source": [
    "\n",
    "During this process, I wondered if *knowledge graphs can be traversed.* As it turns out, *yes it's possible,* but precision and accuracy were yet to be determined. How does one algorithmically determine where to traverse to? Where to start and when to stop? And is it any better than basic semantic RAG with traditional blob vector storage? With this goal in mind, I discovered a custom knowledge graph architecture that is specifically designed to be traversed very rapidly, and designed *three basic traversal algorithms* that can be used to traverse a knowledge graph to retrieve contexts. These algorithms are the core of the completed research here and their strengths and weaknesses will be demonstrated in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a60f4-2401-4635-b4f4-eaa58e1b8e2a",
   "metadata": {},
   "source": [
    "# 1.0: Identifying the Problem\n",
    "\n",
    "When we use natural language processing to talk to LLM's, we need to ensure we give them semantically relevant, accurate information. We also want to make sure we don't give them *too* much information as well, otherwise they are likely to provide inaccurate information, or hallucinate. This is why we build RAG systems that allow us to retrieve relevant information so that a model can give an accurate output based on the user's input prompt.\n",
    "\n",
    "Let's say a user talks to an LLM about a question inside a document. A typical LLM pipeline might look like this:\n",
    "\n",
    "0. Prior to any user interaction, the document would be chunked according to semantic similarity, and these chunks of text would be stored in a \"vector store\" for retrieval upon prompting (explained later).\n",
    "1. User inputs a prompt into an LLM.\n",
    "2. User's prompt is first directly fed to an embedding model.\n",
    "3. Embedding is sent to the vector store service that hosts the document's text and embeddings, almost like a \"library lookup\" for relevant text.\n",
    "4. Most semantically relevant text from the document is returned back to the model as additional context, along with the original text prompt.\n",
    "5. LLM is able to answer the question accurately and reliably without becoming overloaded.\n",
    "\n",
    "Semantic RAG pipelines are typical engineered based on use-case. Retrieving medical papers or court documents is much different than retrieving Harry Potter novels. This being said, each step in the process has its own fine-tuning to be done, and like I said before, my original introduction into semantic RAG was through the chunking problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee2b1d-26c0-435d-8413-8b634bdc29bf",
   "metadata": {},
   "source": [
    "# 2.0: Solving Semantic RAG Chunking Through Bittensor\n",
    "\n",
    "The beginning of this research started via *semantic RAG chunking.*\n",
    "\n",
    "How would you separate a Harry Potter article into paragraphs, without having the indentations? What about a phone book, or an excel spreadsheet full of medical records? What about a long series of emails across multiple topics? This is the core problem of chunking; which is separating out chunks of documents based on semantic similarity. It's an arguably infinitely complex problem. Not because humans can't do it, but because there's technically no right or wrong answer. Chunking heavily depends on the use-case of the documents being chunked. And how does one measure \"accurate\" chunking to begin with anyway?\n",
    "\n",
    "Enter Bittensor; a network of engineers being incentivized to solve impossible problems. From 2024-2025, the subnet known as Chunking, set out to solve this exact problem, with its own reward structure and heuristic for doing so. Miners were tasked with chunking documents based semantic similarity, using their own custom algorithms, and competed to return the \"best\" possible chunked documents.\n",
    "\n",
    "The next section comes directly from the evaluation documentation from Chunking and it outlines the reward heuristic in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72c048-3371-453d-b7f3-d70281542ae2",
   "metadata": {},
   "source": [
    "## Reward Function\n",
    "\n",
    "This subnet incentivizes responses that maximize intrachunk similarity and interchunk dissimilarity, without exceeding variable constraints, such as time or maximum chunk length. This subnet measures similarity by the dot product.\n",
    "\n",
    "Since the similarity score can be influenced significantly by the content in the provided document, evaluation is always done relatively, within groups of miners that all faced the same query. See [Incentive Mechanism](./incentive_mechanism.md) to learn more.\n",
    "\n",
    "In the default validator, [reward.py](../chunking/validator/reward.py) scores the responses of each miner.\n",
    "\n",
    "## Failure States\n",
    "\n",
    "### 1. No new words\n",
    "\n",
    "First, the validator confirms that each word in the chunked response also exists, in the same order, in the original document.\n",
    "\n",
    "```python\n",
    "# check that every word in chunk exists and is in the same order as the source document\n",
    "chunk_words = ' '.join(word_tokenize(chunks[i]))\n",
    "combined_chunk_words += ' ' + chunk_words\n",
    "if chunk_words not in ' '.join(document_words):\n",
    "    return 0\n",
    "```\n",
    "\n",
    "### 2. All words present\n",
    "\n",
    "Then, the validator confirms that every set of 3 adjacent words in the original document is also present within the chunked response.\n",
    "\n",
    "```python\n",
    "# check that every set of 3 adjacent words from the document appears in the chunks\n",
    "for i in range(0, len(document_words), 3):\n",
    "    if (len(' '.join(document_words[i:i+3])) < chunk_size\n",
    "        and ' '.join(document_words[i:i+3]) not in combined_chunk_words):\n",
    "        return 0\n",
    "```\n",
    "\n",
    "If either don't hold true, the score is 0.\n",
    "\n",
    "## Evaluating\n",
    "\n",
    "After passing the fail states, the validator parses through each chunk, creating 'small chunks' of 3 sentences or fewer.\n",
    "\n",
    "```python\n",
    "# create test segments\n",
    "sentences = sent_tokenize(chunks[i])\n",
    "for j in range(0, len(sentences), 3):\n",
    "    text = \" \".join(sentences[j:j+3])\n",
    "    smallChunks.append(smallChunk(i, text))\n",
    "```\n",
    "\n",
    "A random sample, of `num_embeddings` size, is taken and then embedded. The default value is 150.\n",
    "\n",
    "```python\n",
    "# pick out segments to use for evaluation\n",
    "if num_embeddings < len(smallChunks):\n",
    "    testChunks = sample(smallChunks, num_embeddings)\n",
    "else:\n",
    "    testChunks = smallChunks\n",
    "```\n",
    "\n",
    "Then, to calculate the similarity score, the dot product of every possible pair of embeddings is calculated. The average of each pair originating from the same chunk is added to the score (intrachunk similarity), while the average of each pair originating from different chunks is subtracted from the score (interchunk dissimilarity).\n",
    "\n",
    "```python\n",
    "for i in range(len(testChunks) - 1):\n",
    "    j = i + 1\n",
    "    while j < len(testChunks):\n",
    "        if testChunks[i].sourceChunk == testChunks[j].sourceChunk:\n",
    "            reward += np.dot(np.asarray(embeddings[i]), np.asarray(embeddings[j]))\n",
    "        else:\n",
    "            reward -= np.dot(np.asarray(embeddings[i]), np.asarray(embeddings[j]))\n",
    "        j += 1\n",
    "\n",
    "reward = (\n",
    "    (np.mean(intrachunk_similarities) if len(intrachunk_similarities) > 0 else 0)\n",
    "    - (np.mean(interchunk_similarities) if len(interchunk_similarities) > 0 else 0)\n",
    ")\n",
    "```\n",
    "\n",
    "Here is a visualization of how the validator calculates a miner‚Äôs score:\n",
    "\n",
    "![evaluations](docs/evaluations.png)\n",
    "\n",
    "## Penalties\n",
    "\n",
    "Finally, penalities are deducted from the score.\n",
    "\n",
    "Responses are penalized exponentially for each character over the maximum chunk length: `chunk_size`\n",
    "\n",
    "```python\n",
    "# add up size penalty to be applied later\n",
    "chunk_length = len(chunks[i])\n",
    "if chunk_length > chunk_size:\n",
    "    size_penalty += ((chunk_length / chunk_size) - 1) * 10\n",
    "    _verbose(f\"Chunk {i} is too long: {chunk_length} characters, new size penalty: {size_penalty}\")\n",
    "\n",
    "```\n",
    "\n",
    "And for each chunk over the maximum chunk quantity: `chunk_qty`\n",
    "\n",
    "```python\n",
    "# penalize an excessive number of chunks\n",
    "    num_chunks = len(chunks)\n",
    "    if num_chunks > chunk_qty:\n",
    "        qty_penalty += 10 * ((num_chunks / chunk_qty) - 1) * 10\n",
    "        _verbose(f\"Too many chunks: {num_chunks} chunks, new quantity penalty: {qty_penalty}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "reward *= (2/3) ** (size_penalty + qty_penalty)\n",
    "```\n",
    "\n",
    "Finally, note that there is a soft-time limit (default: 3.75 seconds). Validators exponentially penalize responses for each second they are late.\n",
    "\n",
    "```python\n",
    "if response.dendrite.process_time > response.time_soft_max:\n",
    "    over_time = response.dendrite.process_time - response.time_soft_max\n",
    "    _verbose(f\"Applying time penalty: {over_time} seconds over time\")\n",
    "    reward *= (2/3) ** over_time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d4e6c-92cd-42e0-af4a-755d19611dba",
   "metadata": {},
   "source": [
    "This method of scoring chunks was applied to fully chunked documents, typically resulted in a score near `1.0`, using the `text-embedding-ada-002` model from OpenAI. Crucially, the subnet required all chunks in the document to be within a max chunk size and chunk quantity. By maximizing the difference between intrachunk and interchunk similarity across many random samples, while staying within a max chunk size and max chunk quantity, algorithms highly optimized for this heuristic gradually produced better scores over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dfb00e-f930-4ef7-9c61-cf851553dff3",
   "metadata": {},
   "source": [
    "# 3.0: The Similarity Matrix\n",
    "\n",
    "The most important takeaway was that this strategy of scoring required outside-the-box thinking in terms of *global optimization*. After many failed attempts with different algorithmic approaches to chunking, I found that the best way to tackle this problem was by *comparing every single sentence to every other sentence* as a starting point. It looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b12d0-e774-46a0-9a79-d8f38c644b7d",
   "metadata": {},
   "source": [
    "![SIMILARITYMATRIX](docs/sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa3631c-e3b5-4918-b7b3-8e01bc42acbe",
   "metadata": {},
   "source": [
    "This is the *Neuroscience* article on Wikipedia. The document reads from top left to bottom right. You can see areas with high dot product/cosine similarity are in red, and low similarity are in blue. When chunking, this allowed me to \"visualize\" the chunks. You can see how certain sections, like sentences ~3 to ~15 share very little similarity with the rest of the document, as evidenced by the low similarity when compared to the rest of the document.\n",
    "\n",
    "It should also be mentioned that while the subnet used dot-product, we will use cosine similarity to keep things simple.\n",
    "\n",
    "Let's build a full similarity matrix right now using assets from the repository:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1bcb0-be72-4d50-9120-406e504fbe03",
   "metadata": {},
   "source": [
    "### First let's choose some topics for the remainder of this notebook. You may change these if you wish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2fb75-6671-4a57-a04a-5d13a8731093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 3 related topics for cross-document visualization\n",
    "topics = [\"Machine Learning\", \"Psychology\", \"Deep Learning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7bd805-7873-441a-a9f7-a3d99efe92df",
   "metadata": {},
   "source": [
    "### Next let's import our modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405e850-feb2-4324-bccf-723803c4ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd() \n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from utils.wiki import WikiEngine\n",
    "from utils.chunking import ChunkEngine\n",
    "from utils.models import MultiGranularityEmbeddingEngine\n",
    "from utils.similarity import SimilarityEngine\n",
    "from utils.knowledge_graph import KnowledgeGraphBuilder\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create temp directory in the project in a folder called \"temp_data\"\n",
    "Path('temp_data').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f765f-d095-4ac5-a17c-45a6b816d0d2",
   "metadata": {},
   "source": [
    "### Now we'll set configuration settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09fd642-4fcb-4533-bf89-53f759504677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# API KEYS - Set your API keys here\n",
    "# ============================================================================\n",
    "# For Google Colab or local use, set your OpenAI API key here\n",
    "# You can also use environment variables by setting OPENAI_API_KEY in your .env file\n",
    "\n",
    "OPENAI_API_KEY = \"optional-api-key\"\n",
    "\n",
    "# Set the environment variable so the code can access it\n",
    "if OPENAI_API_KEY != \"your-openai-api-key-here\":\n",
    "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "    print(\"OpenAI API key set\")\n",
    "else:\n",
    "    # Try to load from environment\n",
    "    if 'OPENAI_API_KEY' in os.environ:\n",
    "        print(\"Using OpenAI API key from environment\")\n",
    "    else:\n",
    "        print(\"WARNING: No OpenAI API key found.\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Auto-detect best device for your system\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = 'mps'  # Apple Silicon GPU\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Loading {len(topics)} related topics: {topics}\")\n",
    "\n",
    "config = {\n",
    "    'directories': {'data': 'temp_data', 'embeddings': 'temp_data/embeddings'},\n",
    "    'system': {'device': device},  # Auto-detected device\n",
    "    'wikipedia': {\n",
    "        'topics': topics,  # 3 related documents\n",
    "        'articles_per_topic': 1,\n",
    "        'max_article_length': 500000,\n",
    "        'min_article_length': 1000,\n",
    "        'language': 'en',\n",
    "        'rate_limit_delay': 1.0,\n",
    "        'retry_attempts': 3,\n",
    "        'timeout_seconds': 30,\n",
    "        'use_cached_articles': False\n",
    "    },\n",
    "    'text_processing': {\n",
    "        'clean_html': True, 'remove_references': True, 'remove_navigation': True,\n",
    "        'remove_tables': True, 'fix_encoding': True, 'normalize_whitespace': True,\n",
    "        'min_sentence_length': 10, 'max_sentence_length': 500\n",
    "    },\n",
    "    'chunking': {\n",
    "        'strategy': 'sliding_window', 'window_size': 3, 'overlap': 2\n",
    "    },\n",
    "    'models': {\n",
    "        'embedding_models': ['mixedbread-ai/mxbai-embed-large-v1'],\n",
    "        'embedding_batch_size': 32,\n",
    "        'granularity_types': {\n",
    "            'chunks': {'enabled': True},\n",
    "            'sentences': {'enabled': True},\n",
    "            'doc_summaries': {'enabled': True}  \n",
    "        }\n",
    "    },\n",
    "    'similarities': {\n",
    "        'similarity_metric': 'cosine', 'batch_size': 500,\n",
    "        'granularity_types': {\n",
    "            'chunk_to_chunk': {\n",
    "                'enabled': True,\n",
    "                'intra_document': {'enabled': True, 'top_k': 10, 'min_threshold': 0.3},\n",
    "                'inter_document': {'enabled': True, 'top_x': 5, 'min_threshold': 0.3}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'theme_extraction': {\n",
    "        'num_themes': 5,  # Number of themes to extract per document\n",
    "        'use_openai': False,  # Use OpenAI instead of Ollama\n",
    "        'openai_model': 'gpt-4o-mini',  # Cheaper model for theme extraction\n",
    "        'temperature': 0.1\n",
    "    },\n",
    "    'knowledge_graph': {\n",
    "        'quality_scoring': {\n",
    "            'enabled': False  # Keep this disabled - only need themes, not quality scoring\n",
    "        }\n",
    "    },\n",
    "    'retrieval': {\n",
    "        'semantic_traversal': {\n",
    "            'max_hops': 10,\n",
    "            'max_sentences': 20,\n",
    "            'similarity_threshold': 0.3,\n",
    "            'early_stopping': {\n",
    "                'enabled': True,\n",
    "                'min_sentences': 5,\n",
    "                'similarity_drop_threshold': 0.15,\n",
    "                'consecutive_drops': 3\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'reranking': {\n",
    "        'enabled': True,\n",
    "        'top_k': 20\n",
    "    },\n",
    "    'deepeval': {\n",
    "        'models': {\n",
    "            'question_generation': {\n",
    "                'provider': 'openai',\n",
    "                'model_name': 'gpt-4o',\n",
    "                'temperature': 0.7\n",
    "            },\n",
    "            'answer_generation': {\n",
    "                'provider': 'openai',\n",
    "                'model_name': 'gpt-4o',\n",
    "                'temperature': 0.7\n",
    "            },\n",
    "            'evaluation_judge': {\n",
    "                'provider': 'openrouter',\n",
    "                'model_name': 'meta-llama/llama-3.3-70b-instruct',\n",
    "                'temperature': 0.0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Configuration set for {len(topics)} topics\")\n",
    "print(f\"üé® Theme extraction: {'OpenAI' if config['theme_extraction']['use_openai'] else 'Ollama'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c37d791-69b9-479e-a4df-6704086f962c",
   "metadata": {},
   "source": [
    "### Let's fetch 3 Wikipedia documents on our topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3eaff-b169-404c-9467-a0c605e10886",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üîç Fetching Wikipedia articles for {len(topics)} topics: {topics}\")\n",
    "wiki_engine = WikiEngine(config, logger)\n",
    "articles = wiki_engine.acquire_articles(\"temp_data/wiki_multi.json\")\n",
    "\n",
    "if not articles:\n",
    "    raise ValueError(f\"Could not fetch articles for topics: {topics}\")\n",
    "\n",
    "print(f\"‚úÖ Fetched {len(articles)} articles:\")\n",
    "total_sentences = 0\n",
    "for i, article in enumerate(articles):\n",
    "    print(f\"   {i+1}. {article.title} ({len(article.sentences)} sentences)\")\n",
    "    total_sentences += len(article.sentences)\n",
    "    print(f\"      Preview: {article.cleaned_text[:150]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"üìÑ Total corpus: {total_sentences} sentences across {len(articles)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39e702-76a9-4ae7-8dd9-bfc75db66ffa",
   "metadata": {},
   "source": [
    "### Now let's chunk our documents:\n",
    "\n",
    "We're going to use *three sentence sliding windows* with a 2 sentence overlap. This would look like:\n",
    "\n",
    "Chunk 1. [S1, S2, S3]\n",
    "\n",
    "Chunk 2. [S2, S3, S4]\n",
    "\n",
    "Chunk 3. [S3, S4, S5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90312f-7e03-43c2-ad5b-d91993606ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_engine = ChunkEngine(config, logger)\n",
    "chunks = chunk_engine.create_chunks(articles)\n",
    "\n",
    "# Show document breakdown - FIX: use source_article instead of source_document\n",
    "chunks_per_doc = {}\n",
    "for chunk in chunks:\n",
    "    doc_name = chunk.get('source_article', 'Unknown')  # Fixed: changed from source_document\n",
    "    chunks_per_doc[doc_name] = chunks_per_doc.get(doc_name, 0) + 1\n",
    "\n",
    "print(\"\\nüìä Chunks per document:\")\n",
    "for doc_name, count in chunks_per_doc.items():\n",
    "    print(f\"   {doc_name}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50c222-8884-4eda-a035-e7ccee74a10a",
   "metadata": {},
   "source": [
    "### Now let's embed each 3 sentence sliding window chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4ecc2-b732-4f05-a316-4882a0afec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_engine = MultiGranularityEmbeddingEngine(config, logger)\n",
    "embeddings = embedding_engine.generate_embeddings(chunks, articles)\n",
    "print(f\"üß† Generated embeddings\")\n",
    "\n",
    "# Show embedding info\n",
    "model_name = list(embeddings.keys())[0]\n",
    "chunk_embeddings = embeddings[model_name]['chunks']\n",
    "print(f\"üìä Embedding info: {len(chunk_embeddings)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a6ee3-214c-414b-954f-938a6bf5d08a",
   "metadata": {},
   "source": [
    "### Now let's build similarity matrices from our documents directly from embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722b9c2-34f6-4eb9-ba5f-88d9312cd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group embeddings by document for clarity\n",
    "doc_embeddings = {}\n",
    "doc_names = []\n",
    "\n",
    "for chunk_emb in chunk_embeddings:\n",
    "    doc_name = chunk_emb.source_article if hasattr(chunk_emb, 'source_article') else 'Unknown'\n",
    "    \n",
    "    if doc_name not in doc_embeddings:\n",
    "        doc_embeddings[doc_name] = []\n",
    "        doc_names.append(doc_name)\n",
    "    \n",
    "    doc_embeddings[doc_name].append(chunk_emb.embedding)\n",
    "\n",
    "print(f\"\\nüìö Available documents:\")\n",
    "for i, (doc_name, embeddings_list) in enumerate(doc_embeddings.items()):\n",
    "    print(f\"   {i}: '{doc_name}' ({len(embeddings_list)} chunks)\")\n",
    "\n",
    "# Store for next step\n",
    "globals()['doc_embeddings'] = doc_embeddings\n",
    "globals()['doc_names'] = doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fet426oxupn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUMENT SELECTION - Change this to try different documents\n",
    "\n",
    "# CHOOSE WHICH DOCUMENT TO VISUALIZE:\n",
    "\n",
    "chosen_doc = \"Machine learning\"\n",
    "# chosen_doc = \"Neural network (machine learning)\" \n",
    "# chosen_doc = \"Deep learning\"\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to compute similarity matrix for: '{chosen_doc}'\")\n",
    "\n",
    "# Store for next step\n",
    "globals()['chosen_doc'] = chosen_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tci40kna37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cosine_similarity for computing the full matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarity matrix for the chosen document\n",
    "print(f\"üîÑ Computing similarity matrix for: '{chosen_doc}'\")\n",
    "\n",
    "# Get embeddings for chosen document\n",
    "chosen_embeddings = doc_embeddings[chosen_doc]\n",
    "\n",
    "# Convert to numpy matrix\n",
    "embedding_matrix = np.array(chosen_embeddings)\n",
    "print(f\"\\nüßÆ Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "print(f\"   ‚Üí {embedding_matrix.shape[0]} chunks √ó {embedding_matrix.shape[1]} dimensions\")\n",
    "\n",
    "# Compute the FULL similarity matrix (no filtering!)\n",
    "print(\"\\nüîÑ Computing cosine similarity matrix:\")\n",
    "print(f\"   Total comparisons: {embedding_matrix.shape[0] * (embedding_matrix.shape[0] - 1) // 2}\")\n",
    "\n",
    "similarity_matrix = cosine_similarity(embedding_matrix)\n",
    "\n",
    "print(f\"\\n‚úÖ Similarity matrix created!\")\n",
    "print(f\"   üìä Shape: {similarity_matrix.shape}\")\n",
    "print(f\"   üìä Value range: {similarity_matrix.min():.6f} to {similarity_matrix.max():.6f}\")\n",
    "print(f\"   üìä Diagonal values: {np.diag(similarity_matrix)[:5]} (should be 1.0)\")\n",
    "print(f\"   üìä Total elements: {similarity_matrix.size:,}\")\n",
    "print(f\"   üìä Zeros: {np.sum(similarity_matrix == 0.0)} (should be 0 for unfiltered matrix)\")\n",
    "\n",
    "# Store for visualization step\n",
    "globals()['similarity_matrix'] = similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6cc45e-7aae-4f1c-a3d4-6dbad71b5de6",
   "metadata": {},
   "source": [
    "### Visualize the Similarity Matrix\n",
    "\n",
    "Now let's see what this similarity matrix actually looks like! This visualization shows the semantic landscape of our chosen document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3j2ilik4n7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the visualization using seaborn heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8), dpi=150)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    cmap='RdYlBu_r',  # Red (high similarity) to Blue (low similarity)\n",
    "    vmin=0.0,\n",
    "    vmax=1.0,\n",
    "    square=True,\n",
    "    cbar_kws={'label': 'Cosine Similarity'},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(f\"{chosen_doc} - Semantic Similarity Matrix ({similarity_matrix.shape[0]} chunks)\", \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Chunk Index', fontsize=12)\n",
    "ax.set_ylabel('Chunk Index', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üé® Visualizing similarity matrix for: '{chosen_doc}'\")\n",
    "print(f\"üìä This {similarity_matrix.shape[0]}√ó{similarity_matrix.shape[0]} matrix contains {similarity_matrix.size:,} similarity scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e5f605-35b1-4e59-a7c6-4b1280c65355",
   "metadata": {},
   "source": [
    "# 4.0: The Knowledge Graph\n",
    "\n",
    "Using document similarity matrices, then building off Microsoft's GraphRAG, it is possible to create a *knowledge graph* that is, in essence, *many similarity matrices from different documents connected together.* In order to keep the file size down, we can sparse out our similarity matrices significantly by only storing the *most similar sentences* to each sentence, rather than every single pairwise comparison. This also includes *between-document connections*. \n",
    "\n",
    "We use `top_k` as our variable for top intra-document similarities, and `top_x` as our variable for top inter-document similarities. What then happens is each chunk gets connected to `top_k` number of chunks within the same document, and `top_x` number of chunks in all other documents we choose to store in our knowledge graph. This drastically minimizes the number of connections in our graph significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff1d6fc-fd96-4d4e-bbfb-36beeebe1273",
   "metadata": {},
   "source": [
    "Now, we've only been working with 3 sentence sliding window chunks, so why not use *individual sentences as well*? We can do this by including, in our knowledge graph, *individual sentences* and then *only storing them as children to the parent windows they came from.* This allows us to reference sentences within our knowledge graph for sentence-level accuracy, while minimizing unnecessary noise, as sentence nodes in the graph will only connect to their respective parent chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b962dcf-f12b-418b-b248-e400dd8d7fe8",
   "metadata": {},
   "source": [
    "Finally, we want to add *hierarchical properties* to each node. RAGAS and other libraries like it use tools like NER (Named Entity Recognition) to extract named entities and themes. To keep things simple, we can simply take the first 500 words of a Wikipedia document, give the whole thing to an LLM of choice, then have it generate *5 unique themes* from the summary. These 5 themes then get passed down to *all chunk and sentence nodes* inside that particular document. We can also take those five themes as a list, embed them as one chunk, then compare all thematic chunk similarities between each other to then, create another bridge between the most similar documents by theme, denoted by a `top_t`.\n",
    "\n",
    "The final result is a *lightweight knowledge graph* that is designed to be traversed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j21133oxlqe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract themes from document summaries using OpenAI\n",
    "from utils.extraction import ThemeExtractionEngine\n",
    "\n",
    "print(\"üé® Extracting themes from document summaries...\")\n",
    "\n",
    "theme_engine = ThemeExtractionEngine(config, logger)\n",
    "theme_data = theme_engine.extract_themes(\n",
    "    multi_granularity_embeddings=embeddings,\n",
    "    articles=articles,\n",
    "    force_recompute=True  # FORCE FRESH EXTRACTION with OpenAI\n",
    ")\n",
    "\n",
    "# Display extracted themes\n",
    "print(f\"\\nüìä Extracted Themes:\")\n",
    "for theme_result in theme_data['extraction_results']['document_themes']:\n",
    "    print(f\"\\n   {theme_result.doc_title}:\")\n",
    "    for i, theme in enumerate(theme_result.themes, 1):\n",
    "        print(f\"      {i}. {theme}\")\n",
    "    print(f\"   Method: {theme_result.extraction_method} | Model: {theme_result.model_used}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Theme extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frfee5nrr4q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build similarities and knowledge graph\n",
    "print(\"üîó Computing similarity matrices...\")\n",
    "\n",
    "similarity_engine = SimilarityEngine(config, logger)\n",
    "similarities = similarity_engine.compute_similarity_matrices(embeddings)\n",
    "\n",
    "print(f\"\\nüèóÔ∏è Building knowledge graph...\")\n",
    "\n",
    "kg_builder = KnowledgeGraphBuilder(config, logger)\n",
    "knowledge_graph = kg_builder.build_knowledge_graph(\n",
    "    chunks=chunks,\n",
    "    multi_granularity_embeddings=embeddings,\n",
    "    similarity_data=similarities,\n",
    "    theme_data=theme_data\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Knowledge graph built!\")\n",
    "print(f\"   üìä Total chunks: {len(knowledge_graph.chunks)}\")\n",
    "print(f\"   üìä Total documents: {len(knowledge_graph.documents)}\")\n",
    "print(f\"   üìä Total sentences: {len(knowledge_graph.sentences)}\")\n",
    "\n",
    "# Initialize retrieval orchestrator\n",
    "print(f\"üîç Initializing retrieval orchestrator...\")\n",
    "from utils.retrieval import RetrievalOrchestrator\n",
    "\n",
    "retrieval_orchestrator = RetrievalOrchestrator(\n",
    "    knowledge_graph=knowledge_graph,\n",
    "    config=config,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Retrieval orchestrator ready with {len(retrieval_orchestrator.algorithms)} algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yvxjpp775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity as compute_cosine_similarity\n",
    "\n",
    "print(f\"\\nüé® Creating Stage 3: Adding document themes at top...\")\n",
    "\n",
    "# Reuse the same document grouping\n",
    "doc_chunks_viz3 = defaultdict(list)\n",
    "chunk_to_doc_idx_viz3 = {}\n",
    "\n",
    "for chunk_id, chunk in knowledge_graph.chunks.items():\n",
    "    doc_name = chunk.source_document\n",
    "    doc_chunks_viz3[doc_name].append(chunk)\n",
    "\n",
    "doc_names_sorted_viz3 = sorted(doc_chunks_viz3.keys())[:2]\n",
    "\n",
    "for doc_name in doc_names_sorted_viz3:\n",
    "    doc_chunks_viz3[doc_name].sort(key=lambda c: c.chunk_id)\n",
    "    for idx, chunk in enumerate(doc_chunks_viz3[doc_name]):\n",
    "        chunk_to_doc_idx_viz3[chunk.chunk_id] = (doc_name, idx)\n",
    "\n",
    "# Get theme data and embeddings\n",
    "doc_themes = {}  # doc_name -> list of themes\n",
    "doc_theme_text = {}  # doc_name -> full theme text for embedding\n",
    "doc_theme_embeddings = {}  # doc_name -> embedding vector\n",
    "\n",
    "for theme_result in theme_data['extraction_results']['document_themes']:\n",
    "    doc_title = theme_result.doc_title\n",
    "    themes = theme_result.themes\n",
    "    \n",
    "    # The text that was embedded is the list of themes joined\n",
    "    theme_text = \", \".join(themes)\n",
    "    \n",
    "    doc_themes[doc_title] = themes\n",
    "    doc_theme_text[doc_title] = theme_text\n",
    "    \n",
    "print(f\"\\n   üìã Document themes:\")\n",
    "for doc_name in doc_names_sorted_viz3:\n",
    "    if doc_name in doc_themes:\n",
    "        print(f\"      {doc_name}: {doc_themes[doc_name]}\")\n",
    "\n",
    "# Get theme similarity from knowledge graph (uses fresh OpenAI theme embeddings)\n",
    "# Map document names to document IDs from knowledge graph\n",
    "doc_name_to_id = {}\n",
    "for doc_id, doc in knowledge_graph.documents.items():\n",
    "    doc_name_to_id[doc.title] = doc_id\n",
    "\n",
    "# Get theme similarity between the two visualized documents\n",
    "if len(doc_names_sorted_viz3) == 2:\n",
    "    doc_id_1 = doc_name_to_id.get(doc_names_sorted_viz3[0])\n",
    "    doc_id_2 = doc_name_to_id.get(doc_names_sorted_viz3[1])\n",
    "    \n",
    "    print(f\"\\n   üîç Debug: doc_names_sorted_viz3 = {doc_names_sorted_viz3}\")\n",
    "    print(f\"   üîç Debug: doc_id_1 = {doc_id_1}, doc_id_2 = {doc_id_2}\")\n",
    "    \n",
    "    if doc_id_1 and doc_id_2:\n",
    "        # Get similarity from knowledge graph (check both directions since top_r=1)\n",
    "        theme_similarity = knowledge_graph.get_theme_similarity_score(doc_id_1, doc_id_2)\n",
    "        if theme_similarity == 0.0:\n",
    "            theme_similarity = knowledge_graph.get_theme_similarity_score(doc_id_2, doc_id_1)\n",
    "        print(f\"   üîó Theme similarity from knowledge graph: {theme_similarity:.4f}\")\n",
    "    else:\n",
    "        theme_similarity = 0.0\n",
    "        print(f\"   ‚ö†Ô∏è  Could not find document IDs in knowledge graph\")\n",
    "else:\n",
    "    theme_similarity = 0.0\n",
    "    print(f\"   ‚ö†Ô∏è  Need exactly 2 documents for theme similarity\")\n",
    "\n",
    "# Create figure\n",
    "fig3 = go.Figure()\n",
    "\n",
    "# Layout parameters - wider spacing, reasonable column width\n",
    "column_spacing = 10.0  # INCREASED: More space between columns to avoid overlap\n",
    "column_width = 6.5     # Reasonable width for each column\n",
    "top_section_height = 0.15\n",
    "middle_section_height = 0.55\n",
    "bottom_section_height = 0.18\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "doc_colors = {doc: colors[i] for i, doc in enumerate(doc_names_sorted_viz3)}\n",
    "\n",
    "max_chunks = max(len(doc_chunks_viz3[doc]) for doc in doc_names_sorted_viz3)\n",
    "\n",
    "# Calculate section boundaries\n",
    "middle_y_start = top_section_height * max_chunks\n",
    "middle_y_end = (top_section_height + middle_section_height) * max_chunks\n",
    "sentence_y_start = middle_y_end\n",
    "sentence_y_end = (top_section_height + middle_section_height + bottom_section_height) * max_chunks\n",
    "\n",
    "# Reserve space for \"Knowledge Graph\" title at top of middle section\n",
    "kg_label_space = 3  # Space for the label\n",
    "kg_graph_y_start = middle_y_start + kg_label_space\n",
    "kg_graph_y_end = middle_y_end\n",
    "\n",
    "chunk_positions_viz3 = {}\n",
    "\n",
    "# === STAGE 1 CONTENT: Middle section with 2D graph ===\n",
    "for doc_idx, doc_name in enumerate(doc_names_sorted_viz3):\n",
    "    chunks_in_doc = doc_chunks_viz3[doc_name]\n",
    "    n_chunks = len(chunks_in_doc)\n",
    "    x_center = doc_idx * column_spacing\n",
    "    \n",
    "    # Build distance matrix and layout\n",
    "    distance_matrix = np.ones((n_chunks, n_chunks)) * 2.0\n",
    "    for i, chunk in enumerate(chunks_in_doc):\n",
    "        for target_id in chunk.intra_doc_connections:\n",
    "            if target_id in chunk_to_doc_idx_viz3:\n",
    "                tgt_doc, tgt_idx = chunk_to_doc_idx_viz3[target_id]\n",
    "                if tgt_doc == doc_name:\n",
    "                    similarity = chunk.connection_scores.get(target_id, 0.0)\n",
    "                    distance = 1.0 - similarity\n",
    "                    distance_matrix[i, tgt_idx] = distance\n",
    "                    distance_matrix[tgt_idx, i] = distance\n",
    "    \n",
    "    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "    positions_2d = mds.fit_transform(distance_matrix)\n",
    "    \n",
    "    x_min, x_max = positions_2d[:, 0].min(), positions_2d[:, 0].max()\n",
    "    y_min, y_max = positions_2d[:, 1].min(), positions_2d[:, 1].max()\n",
    "    x_range = x_max - x_min if x_max != x_min else 1.0\n",
    "    y_range = y_max - y_min if y_max != y_min else 1.0\n",
    "    \n",
    "    # Center horizontally: normalize to 0-1, then scale around x_center\n",
    "    x_normalized = (positions_2d[:, 0] - x_min) / x_range\n",
    "    # Scale to fit within column width, centered at x_center\n",
    "    x_scaled = x_center + (x_normalized - 0.5) * column_width * 0.75  # 75% of width for padding\n",
    "    \n",
    "    # Center vertically: normalize to 0-1, then scale around vertical center\n",
    "    y_normalized = (positions_2d[:, 1] - y_min) / y_range\n",
    "    graph_height = kg_graph_y_end - kg_graph_y_start\n",
    "    graph_center_y = kg_graph_y_start + graph_height / 2\n",
    "    # Scale to fit within available height, centered vertically\n",
    "    y_scaled = graph_center_y + (y_normalized - 0.5) * graph_height * 0.75  # 75% of height for padding\n",
    "    \n",
    "    for i, chunk in enumerate(chunks_in_doc):\n",
    "        chunk_positions_viz3[chunk.chunk_id] = (x_scaled[i], y_scaled[i])\n",
    "    \n",
    "    # Draw edges (intra-doc)\n",
    "    edge_x, edge_y = [], []\n",
    "    for chunk in chunks_in_doc:\n",
    "        src_pos = chunk_positions_viz3[chunk.chunk_id]\n",
    "        for target_id in chunk.intra_doc_connections:\n",
    "            if target_id in chunk_positions_viz3 and target_id in chunk_to_doc_idx_viz3:\n",
    "                tgt_doc, _ = chunk_to_doc_idx_viz3[target_id]\n",
    "                if tgt_doc == doc_name:\n",
    "                    tgt_pos = chunk_positions_viz3[target_id]\n",
    "                    edge_x.extend([src_pos[0], tgt_pos[0], None])\n",
    "                    edge_y.extend([src_pos[1], tgt_pos[1], None])\n",
    "    \n",
    "    if edge_x:\n",
    "        fig3.add_trace(go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            mode='lines',\n",
    "            line=dict(color='#CCCCCC', width=0.5),\n",
    "            hoverinfo='skip',\n",
    "            showlegend=False\n",
    "        ))\n",
    "    \n",
    "    # Draw chunk nodes\n",
    "    node_x = [chunk_positions_viz3[c.chunk_id][0] for c in chunks_in_doc]\n",
    "    node_y = [chunk_positions_viz3[c.chunk_id][1] for c in chunks_in_doc]\n",
    "    node_text = [f\"Chunk {i}<br>{c.chunk_text[:80]}...\" for i, c in enumerate(chunks_in_doc)]\n",
    "    \n",
    "    fig3.add_trace(go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color=doc_colors[doc_name], line=dict(width=1, color='black')),\n",
    "        text=node_text,\n",
    "        hovertemplate='%{text}<extra></extra>',\n",
    "        name=doc_name,\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "# Inter-document connections\n",
    "inter_edge_x, inter_edge_y = [], []\n",
    "for chunk_id, chunk in knowledge_graph.chunks.items():\n",
    "    if chunk_id not in chunk_positions_viz3:\n",
    "        continue\n",
    "    src_pos = chunk_positions_viz3[chunk_id]\n",
    "    for target_id in chunk.inter_doc_connections:\n",
    "        if target_id in chunk_positions_viz3:\n",
    "            tgt_pos = chunk_positions_viz3[target_id]\n",
    "            inter_edge_x.extend([src_pos[0], tgt_pos[0], None])\n",
    "            inter_edge_y.extend([src_pos[1], tgt_pos[1], None])\n",
    "\n",
    "if inter_edge_x:\n",
    "    fig3.add_trace(go.Scatter(\n",
    "        x=inter_edge_x, y=inter_edge_y,\n",
    "        mode='lines',\n",
    "        line=dict(color='red', width=0.5, dash='dot'),\n",
    "        hoverinfo='skip',\n",
    "        showlegend=False,\n",
    "        opacity=0.5\n",
    "    ))\n",
    "\n",
    "# === STAGE 2 CONTENT: Bottom section with chunks 5, 6, 7 and sentences ===\n",
    "print(f\"   üìä Adding sentence layer for chunks 5, 6, 7...\")\n",
    "\n",
    "sentence_count = 0\n",
    "for doc_idx, doc_name in enumerate(doc_names_sorted_viz3):\n",
    "    chunks_in_doc = doc_chunks_viz3[doc_name]\n",
    "    x_center = doc_idx * column_spacing\n",
    "    \n",
    "    if len(chunks_in_doc) > 7:\n",
    "        chunks_to_show = chunks_in_doc[5:8]\n",
    "    else:\n",
    "        chunks_to_show = chunks_in_doc[-3:] if len(chunks_in_doc) >= 3 else chunks_in_doc\n",
    "    \n",
    "    chunk_rect_width = column_width * 0.25\n",
    "    chunk_rect_height = (sentence_y_end - sentence_y_start) * 0.15\n",
    "    chunk_spacing = column_width * 0.03\n",
    "    \n",
    "    total_chunks_width = 3 * chunk_rect_width + 2 * chunk_spacing\n",
    "    start_x = x_center - total_chunks_width / 2\n",
    "    \n",
    "    all_chunk_sentences = []\n",
    "    for chunk in chunks_to_show:\n",
    "        sentences = knowledge_graph.get_chunk_sentences(chunk.chunk_id) or []\n",
    "        all_chunk_sentences.append(sentences)\n",
    "    \n",
    "    sentence_to_chunks = {}\n",
    "    all_unique_sentence_ids = []\n",
    "    sentence_obj_map = {}\n",
    "    \n",
    "    for chunk_idx, sentences in enumerate(all_chunk_sentences):\n",
    "        for s in sentences[:3]:\n",
    "            if s.sentence_id not in all_unique_sentence_ids:\n",
    "                all_unique_sentence_ids.append(s.sentence_id)\n",
    "                sentence_obj_map[s.sentence_id] = s\n",
    "            if s.sentence_id not in sentence_to_chunks:\n",
    "                sentence_to_chunks[s.sentence_id] = []\n",
    "            sentence_to_chunks[s.sentence_id].append(chunk_idx)\n",
    "    \n",
    "    chunk_centers = []\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks_to_show):\n",
    "        chunk_global_idx = chunks_in_doc.index(chunk)\n",
    "        chunk_x = start_x + chunk_idx * (chunk_rect_width + chunk_spacing)\n",
    "        chunk_y = sentence_y_start + (sentence_y_end - sentence_y_start) * 0.15\n",
    "        \n",
    "        chunk_centers.append((chunk_x + chunk_rect_width/2, chunk_y + chunk_rect_height))\n",
    "        \n",
    "        fig3.add_shape(\n",
    "            type=\"rect\",\n",
    "            x0=chunk_x, x1=chunk_x + chunk_rect_width,\n",
    "            y0=chunk_y, y1=chunk_y + chunk_rect_height,\n",
    "            line=dict(color=doc_colors[doc_name], width=2),\n",
    "            fillcolor=f'rgba{tuple(list(int(doc_colors[doc_name].lstrip(\"#\")[i:i+2], 16) for i in (0, 2, 4)) + [0.2])}',\n",
    "            layer='below'\n",
    "        )\n",
    "        \n",
    "        fig3.add_annotation(\n",
    "            x=chunk_x + chunk_rect_width/2,\n",
    "            y=chunk_y + chunk_rect_height/2,\n",
    "            text=f\"<b>C{chunk_global_idx}</b>\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=9, color='black')\n",
    "        )\n",
    "        \n",
    "        fig3.add_trace(go.Scatter(\n",
    "            x=[chunk_x + chunk_rect_width/2],\n",
    "            y=[chunk_y + chunk_rect_height/2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=20, color='rgba(0,0,0,0)'),\n",
    "            text=[f\"<b>Chunk {chunk_global_idx}</b><br>{chunk.chunk_text[:100]}...\"],\n",
    "            hovertemplate='%{text}<extra></extra>',\n",
    "            showlegend=False\n",
    "        ))\n",
    "    \n",
    "    if all_unique_sentence_ids:\n",
    "        sentence_y_pos = chunk_y + chunk_rect_height + (sentence_y_end - sentence_y_start) * 0.25\n",
    "        total_span_width = total_chunks_width\n",
    "        sentence_spacing_val = total_span_width / (len(all_unique_sentence_ids) + 1)\n",
    "        \n",
    "        sent_x_positions = []\n",
    "        sent_y_positions = []\n",
    "        sent_texts = []\n",
    "        \n",
    "        for sent_idx, sent_id in enumerate(all_unique_sentence_ids):\n",
    "            sent_x = start_x + (sent_idx + 1) * sentence_spacing_val\n",
    "            sentence_obj = sentence_obj_map[sent_id]\n",
    "            \n",
    "            sent_x_positions.append(sent_x)\n",
    "            sent_y_positions.append(sentence_y_pos)\n",
    "            \n",
    "            chunk_indices = sentence_to_chunks[sent_id]\n",
    "            chunk_labels = [str(chunks_in_doc.index(chunks_to_show[i])) for i in chunk_indices if i < len(chunks_to_show)]\n",
    "            sent_texts.append(f\"<b>S (chunks: {','.join(chunk_labels)})</b><br>{sentence_obj.sentence_text[:100]}...\")\n",
    "            \n",
    "            for chunk_idx in chunk_indices:\n",
    "                if chunk_idx < len(chunk_centers):\n",
    "                    chunk_cx, chunk_cy = chunk_centers[chunk_idx]\n",
    "                    fig3.add_trace(go.Scatter(\n",
    "                        x=[chunk_cx, sent_x],\n",
    "                        y=[chunk_cy, sentence_y_pos],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='#666666', width=1),\n",
    "                        hoverinfo='skip',\n",
    "                        showlegend=False\n",
    "                    ))\n",
    "        \n",
    "        fig3.add_trace(go.Scatter(\n",
    "            x=sent_x_positions,\n",
    "            y=sent_y_positions,\n",
    "            mode='markers',\n",
    "            marker=dict(size=8, color='lightgray', line=dict(color='black', width=1.5)),\n",
    "            text=sent_texts,\n",
    "            hovertemplate='%{text}<extra></extra>',\n",
    "            showlegend=False,\n",
    "            name=f'Sentences'\n",
    "        ))\n",
    "        \n",
    "        sentence_count += len(all_unique_sentence_ids)\n",
    "\n",
    "# === STAGE 3 CONTENT: Top section with document themes ===\n",
    "print(f\"   üé® Adding document themes at top...\")\n",
    "\n",
    "theme_node_positions = {}  # Store theme rectangle edges for connection line\n",
    "theme_rect_bounds = {}  # Store x0, x1 for each doc\n",
    "\n",
    "for doc_idx, doc_name in enumerate(doc_names_sorted_viz3):\n",
    "    x_center = doc_idx * column_spacing\n",
    "    \n",
    "    if doc_name in doc_themes:\n",
    "        # Theme rectangle\n",
    "        theme_rect_width = column_width * 0.8\n",
    "        theme_rect_height = middle_y_start * 0.6\n",
    "        theme_x = x_center - theme_rect_width / 2\n",
    "        theme_y = middle_y_start * 0.2\n",
    "        \n",
    "        # Store edge positions for connection line\n",
    "        theme_node_positions[doc_name] = (x_center, theme_y + theme_rect_height / 2)\n",
    "        theme_rect_bounds[doc_name] = {\n",
    "            'x0': theme_x,\n",
    "            'x1': theme_x + theme_rect_width,\n",
    "            'y': theme_y + theme_rect_height / 2\n",
    "        }\n",
    "        \n",
    "        # Draw theme rectangle\n",
    "        fig3.add_shape(\n",
    "            type=\"rect\",\n",
    "            x0=theme_x, x1=theme_x + theme_rect_width,\n",
    "            y0=theme_y, y1=theme_y + theme_rect_height,\n",
    "            line=dict(color=doc_colors[doc_name], width=3),\n",
    "            fillcolor=f'rgba{tuple(list(int(doc_colors[doc_name].lstrip(\"#\")[i:i+2], 16) for i in (0, 2, 4)) + [0.3])}',\n",
    "            layer='below'\n",
    "        )\n",
    "        \n",
    "        # Wrap theme text for better display - insert line breaks intelligently\n",
    "        theme_text_raw = doc_theme_text[doc_name]\n",
    "        # Split by commas and rejoin with line breaks every 2-3 items\n",
    "        theme_parts = [t.strip() for t in theme_text_raw.split(',')]\n",
    "        wrapped_lines = []\n",
    "        current_line = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for part in theme_parts:\n",
    "            # If adding this part would make line too long (>40 chars), start new line\n",
    "            if current_length + len(part) > 40 and current_line:\n",
    "                wrapped_lines.append(', '.join(current_line))\n",
    "                current_line = [part]\n",
    "                current_length = len(part)\n",
    "            else:\n",
    "                current_line.append(part)\n",
    "                current_length += len(part) + 2  # +2 for \", \"\n",
    "        \n",
    "        if current_line:\n",
    "            wrapped_lines.append(', '.join(current_line))\n",
    "        \n",
    "        theme_text_wrapped = '<br>'.join(wrapped_lines)\n",
    "        \n",
    "        # Add theme text with wrapping\n",
    "        fig3.add_annotation(\n",
    "            x=theme_x + theme_rect_width/2,\n",
    "            y=theme_y + theme_rect_height/2,\n",
    "            text=f\"<b>THEMES:</b><br>{theme_text_wrapped}\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=9, color='black'),\n",
    "            align='center'\n",
    "        )\n",
    "        \n",
    "        # Add hover area to show full theme list\n",
    "        fig3.add_trace(go.Scatter(\n",
    "            x=[x_center],\n",
    "            y=[theme_y + theme_rect_height/2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=30, color='rgba(0,0,0,0)'),\n",
    "            text=[f\"<b>Document Themes (Embedded Text):</b><br>{theme_text_raw}<br><br>Similarity: {theme_similarity:.4f}\"],\n",
    "            hovertemplate='%{text}<extra></extra>',\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "# Draw theme similarity connection line at the edges\n",
    "if len(theme_rect_bounds) == 2:\n",
    "    doc_list = list(theme_rect_bounds.keys())\n",
    "    bounds1 = theme_rect_bounds[doc_list[0]]\n",
    "    bounds2 = theme_rect_bounds[doc_list[1]]\n",
    "    \n",
    "    # Connect from right edge of left rect to left edge of right rect\n",
    "    x1 = bounds1['x1']  # Right edge of first theme\n",
    "    y1 = bounds1['y']\n",
    "    x2 = bounds2['x0']  # Left edge of second theme\n",
    "    y2 = bounds2['y']\n",
    "    \n",
    "    fig3.add_trace(go.Scatter(\n",
    "        x=[x1, x2],\n",
    "        y=[y1, y2],\n",
    "        mode='lines',\n",
    "        line=dict(color='red', width=0.5),\n",
    "        hoverinfo='skip',\n",
    "        showlegend=False,\n",
    "        name='Theme Similarity'\n",
    "    ))\n",
    "\n",
    "print(f\"   ‚úÖ Document themes added with similarity: {theme_similarity:.4f}\")\n",
    "\n",
    "# Draw column boundaries\n",
    "for doc_idx, doc_name in enumerate(doc_names_sorted_viz3):\n",
    "    x_center = doc_idx * column_spacing\n",
    "    \n",
    "    fig3.add_shape(\n",
    "        type=\"rect\",\n",
    "        x0=x_center - column_width/2, x1=x_center + column_width/2,\n",
    "        y0=0, y1=middle_y_start,\n",
    "        line=dict(color=doc_colors[doc_name], width=2, dash='dot'),\n",
    "        fillcolor='rgba(200,200,200,0.05)',\n",
    "        layer='below'\n",
    "    )\n",
    "    \n",
    "    fig3.add_shape(\n",
    "        type=\"rect\",\n",
    "        x0=x_center - column_width/2, x1=x_center + column_width/2,\n",
    "        y0=middle_y_start, y1=middle_y_end,\n",
    "        line=dict(color=doc_colors[doc_name], width=3),\n",
    "        fillcolor='rgba(255,255,255,0)',\n",
    "        layer='below'\n",
    "    )\n",
    "    \n",
    "    fig3.add_shape(\n",
    "        type=\"rect\",\n",
    "        x0=x_center - column_width/2, x1=x_center + column_width/2,\n",
    "        y0=sentence_y_start, y1=sentence_y_end,\n",
    "        line=dict(color=doc_colors[doc_name], width=3),\n",
    "        fillcolor='rgba(200,255,200,0.1)',\n",
    "        layer='below'\n",
    "    )\n",
    "    \n",
    "    fig3.add_annotation(\n",
    "        x=x_center, y=-max_chunks * 0.05,\n",
    "        text=f\"<b>{doc_name}</b>\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=12, color='black'),\n",
    "        bgcolor='rgba(255,255,255,0.9)',\n",
    "        borderpad=4\n",
    "    )\n",
    "    \n",
    "    # Add section labels INSIDE each column\n",
    "    # Knowledge Graph label - positioned inside at the top\n",
    "    fig3.add_annotation(\n",
    "        x=x_center,\n",
    "        y=middle_y_start + 1.5,  # Position inside, just below the top border\n",
    "        text=\"<b>Knowledge Graph</b>\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=11, color='black'),\n",
    "        xanchor='center',\n",
    "        yanchor='top'\n",
    "    )\n",
    "    \n",
    "    # Sentences label\n",
    "    fig3.add_annotation(\n",
    "        x=x_center,\n",
    "        y=sentence_y_start + (sentence_y_end - sentence_y_start) * 0.05,\n",
    "        text=\"<b>Sentences</b>\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=11, color='black'),\n",
    "        xanchor='center',\n",
    "        yanchor='top'\n",
    "    )\n",
    "\n",
    "# Layout\n",
    "fig3.update_layout(\n",
    "    title=f\"Knowledge Graph - Stage 3: Complete Hierarchy<br>\" +\n",
    "          f\"<i>Document Themes (top) ‚Üí Chunk Graph (middle) ‚Üí Sentence Children (bottom) | Theme Similarity: {theme_similarity:.3f}</i>\",\n",
    "    xaxis=dict(\n",
    "        showgrid=False, zeroline=False, showticklabels=False,\n",
    "        range=[-column_width*0.6, column_spacing + column_width*0.6]\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=False, zeroline=False, showticklabels=False,\n",
    "        autorange=\"reversed\",\n",
    "        range=[-max_chunks * 0.08, sentence_y_end * 1.02]\n",
    "    ),\n",
    "    plot_bgcolor='rgba(250,250,250,1)',\n",
    "    height=1100,\n",
    "    showlegend=True,\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    ")\n",
    "\n",
    "fig3.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Stage 3 visualization complete!\")\n",
    "print(f\"   üí° TOP: Document themes with embedded text\")\n",
    "print(f\"   üí° MIDDLE: 2D graph layout with chunk nodes\")\n",
    "print(f\"   üí° BOTTOM: Chunks 5, 6, 7 with sentence children\")\n",
    "print(f\"   üí° RED LINE: Theme similarity connection ({theme_similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bw7bzqxqypo",
   "metadata": {},
   "source": [
    "# 4.1: Knowledge In 3D\n",
    "Now let's visualize the knowledge graph structure in 3D space! We'll use PCA to reduce the high-dimensional embeddings down to 3D, then plot all chunks as nodes and their connections as edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a6bc6-6463-49f7-87c0-fe35165d4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Get all chunk embeddings\n",
    "print(f\"üé® Creating 3D visualization of knowledge graph\")\n",
    "print(f\"   üìä Total chunks: {len(chunk_embeddings)}\")\n",
    "\n",
    "# Extract embeddings and metadata\n",
    "all_embeddings = []\n",
    "chunk_ids = []\n",
    "chunk_docs = []\n",
    "chunk_texts = []\n",
    "\n",
    "for chunk_emb in chunk_embeddings:\n",
    "    all_embeddings.append(chunk_emb.embedding)\n",
    "    chunk_ids.append(chunk_emb.chunk_id)\n",
    "    chunk_docs.append(chunk_emb.source_article)\n",
    "    # Get text preview (first 100 chars)\n",
    "    text = chunk_emb.chunk_text if hasattr(chunk_emb, 'chunk_text') else ''\n",
    "    chunk_texts.append(text[:100] + '...' if len(text) > 100 else text)\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings_array = np.array(all_embeddings)\n",
    "print(f\"   üßÆ Embedding matrix: {embeddings_array.shape}\")\n",
    "\n",
    "# Step 2: Apply PCA to reduce to 3D\n",
    "print(f\"   üî¨ Applying PCA dimensionality reduction...\")\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "coords_3d = pca.fit_transform(embeddings_array)\n",
    "\n",
    "# Scale coordinates to spread out the visualization\n",
    "scale_factor = 6.0  # Increase this to spread points further apart\n",
    "coords_3d = coords_3d * scale_factor\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "print(f\"   ‚úÖ PCA complete - explained variance: {explained_var.sum():.1%}\")\n",
    "print(f\"   üìè Applied {scale_factor}x scaling for better spacing\")\n",
    "\n",
    "# Step 3: Create color mapping for documents\n",
    "unique_docs = list(set(chunk_docs))\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan']\n",
    "doc_color_map = {doc: colors[i % len(colors)] for i, doc in enumerate(unique_docs)}\n",
    "\n",
    "print(f\"   üìö Documents: {unique_docs}\")\n",
    "\n",
    "# Step 4: Compute FULL similarity matrix and extract strong connections\n",
    "print(f\"   üîó Computing full similarity matrix for visualization...\")\n",
    "sim_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "# Extract connections above threshold (for visualization only - not sparse!)\n",
    "threshold = 0.75  # Only visualize strong connections to avoid clutter\n",
    "edge_pairs = []\n",
    "for i in range(len(chunk_embeddings)):\n",
    "    for j in range(i+1, len(chunk_embeddings)):  # Only upper triangle\n",
    "        if sim_matrix[i, j] > threshold:\n",
    "            edge_pairs.append((i, j))\n",
    "\n",
    "print(f\"   ‚úÖ Found {len(edge_pairs)} connections above {threshold} threshold\")\n",
    "print(f\"   üí° (Using threshold for visualization clarity - full matrix computed)\")\n",
    "\n",
    "# Step 5: Create Plotly 3D scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add edges (connections between chunks)\n",
    "edge_x, edge_y, edge_z = [], [], []\n",
    "for src, dst in edge_pairs:\n",
    "    edge_x.extend([coords_3d[src, 0], coords_3d[dst, 0], None])\n",
    "    edge_y.extend([coords_3d[src, 1], coords_3d[dst, 1], None])\n",
    "    edge_z.extend([coords_3d[src, 2], coords_3d[dst, 2], None])\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=edge_x, y=edge_y, z=edge_z,\n",
    "    mode='lines',\n",
    "    line=dict(color='black', width=1),\n",
    "    hoverinfo='none',\n",
    "    showlegend=False,\n",
    "    name='Connections'\n",
    "))\n",
    "\n",
    "# Add nodes (chunks) - one trace per document for colored legend\n",
    "for doc in unique_docs:\n",
    "    doc_mask = [d == doc for d in chunk_docs]\n",
    "    doc_coords = coords_3d[doc_mask]\n",
    "    doc_texts = [chunk_texts[i] for i, mask in enumerate(doc_mask) if mask]\n",
    "    \n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=doc_coords[:, 0],\n",
    "        y=doc_coords[:, 1],\n",
    "        z=doc_coords[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=doc_color_map[doc],\n",
    "            line=dict(color='black', width=0.5)\n",
    "        ),\n",
    "        text=doc_texts,\n",
    "        hovertemplate='<b>%{text}</b><extra></extra>',\n",
    "        name=doc\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=f\"Knowledge Graph 3D Visualization<br>\" +\n",
    "          f\"<i>{len(chunk_embeddings)} chunks across {len(unique_docs)} documents | \" +\n",
    "          f\"{len(edge_pairs)} connections (>{threshold} similarity) | PCA variance: {explained_var.sum():.1%}</i>\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"PC1\",\n",
    "        yaxis_title=\"PC2\",\n",
    "        zaxis_title=\"PC3\",\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.5)),\n",
    "        bgcolor='rgba(240,240,240,0.9)'\n",
    "    ),\n",
    "    height=800,\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ 3D visualization created!\")\n",
    "print(f\"   üí° Hover over nodes to see chunk text\")\n",
    "print(f\"   üí° Drag to rotate, scroll to zoom\")\n",
    "print(f\"   üí° Adjust 'scale_factor' and 'threshold' variables to customize\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827c2e2-be45-40ab-94cf-d31a568bc16e",
   "metadata": {},
   "source": [
    "# 4.0 Traversing The Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f75061-f66a-4429-a67c-c175ba478ec1",
   "metadata": {},
   "source": [
    "Have you ever gone on a \"Wikipedia journey\" before? Where you first start reading about one topic out of curiosity, then click through similar topics to learn more? This is effectively the same rationale used for traversing knowledge graphs. Rather than attempting to immediately retrieve all relevant information from the \"top down\", we try to traverse *through* the graph with the goal of extracting the most similar information to what we've already found to answer our questions.\n",
    "\n",
    "If we were going to do this *algorithmically*, it would look something like this:\n",
    "\n",
    "1. A user asks an LLM a question.\n",
    "2. We find some kind of *starting point* within our knowledge graph that is the most relevant to the user's question.\n",
    "3. From that starting point, we traverse the knowledge graph, extracting semantically relevant information to the user's question.\n",
    "4. We have some kind of *early stopping* method to prevent extracting too much information.\n",
    "5. We return the text to the LLM to answer the user's question.\n",
    "\n",
    "This is *precisely* the objective of the algorithms in this notebook. Additionally, here are some ground rules for our algorithms:\n",
    "\n",
    "1. They need to be *fast and efficient.* Taking up too much memory or time can bloat a RAG system unnecessarily.\n",
    "2. They need to be *as accurate and precise as possible* for the use case.\n",
    "3. They should not provide *too little or too much context* as stated before.\n",
    "4. Ideally, they should be *AS effective, if not MORE effective, than basic RAG.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf9e84-b652-4f7d-8462-c60cd8854d0b",
   "metadata": {},
   "source": [
    "# 5.0: Traversal Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752ff81-a821-4932-b371-ef7c4a9c76a7",
   "metadata": {},
   "source": [
    "There are **seven** total algorithms in this repository that can be used for retrieval. Each will be explained in its own section:\n",
    "\n",
    "1. `basic_retrieval`\n",
    "2. `query_traversal`\n",
    "3. `kg_traversal`\n",
    "4. `triangulation_average`\n",
    "5. `triangulation_geometric_3d`\n",
    "6. `triangulation_fulldim`\n",
    "7. `llm-guided-traversal`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e0229-389f-40a4-8e46-72967e2380e4",
   "metadata": {},
   "source": [
    "## 5.1: `basic_retrieval`\n",
    "\n",
    "*Basic semantic RAG algorithm. Contains no traversal. Used as a control.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b205721a-9c0e-417b-a33c-80a0c0b3b3b2",
   "metadata": {},
   "source": [
    "![BASIC RETRIEVAL](docs/BASIC_RETRIEVAL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a425aa-9770-411c-955e-420a62727016",
   "metadata": {},
   "source": [
    "First, embed the user's query.\n",
    "\n",
    "---\n",
    "$$\\vec{q} = \\text{embed}(\\text{query})$$\n",
    "---\n",
    "\n",
    "Lookup the most semantically similar chunks in the knowledge graph directly based on cosine similarity.\n",
    "\n",
    "---\n",
    "$$\\text{sim}(\\vec{q}, \\vec{c}_i) = \\frac{\\vec{q} \\cdot \\vec{c}_i}{\\|\\vec{q}\\| \\|\\vec{c}_i\\|}$$\n",
    "---\n",
    "\n",
    "Continue selecting the cached top chunks until `max_sentences`, stopping early once sentence quality beats the next chunk option (after at least five sentences are gathered).\n",
    "\n",
    "---\n",
    "$$\\text{stop when } |S| = \\text{max\\_sentences}\\,\\, \\text{or}\\,\\, \\Big(|S| \\geq 5 \\land \\max_{s \\in S_{\\text{extracted}}} \\text{sim}(\\vec{q}, \\vec{s}) > \\text{sim}(\\vec{q}, \\vec{c}_{\\text{next}})\\Big)$$\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10442a5c",
   "metadata": {},
   "source": [
    "## 5.2: `query_traversal`\n",
    "\n",
    "*Query-guided graph traversal that always prioritizes similarity to the original query.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604d48f-6448-484b-a927-fe00fa55367d",
   "metadata": {},
   "source": [
    "![QUERY TRAVERSAL](docs/QUERY_TRAVERSAL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e086a",
   "metadata": {},
   "source": [
    "Embed the user's query and find the ***most similar chunk node in the graph to the query.*** This is called the *anchor chunk.* \n",
    "\n",
    "---\n",
    "$$\\vec{q} = \\text{embed}(\\text{query}), \\quad c_0 = \\arg\\max_{c_i} \\text{sim}(\\vec{q}, \\vec{c}_i)$$\n",
    "---\n",
    "\n",
    "Starting from the anchor chunk's top inter-document and top intra-document connections, at each hop, find the next node (chunk or sentence) most similar to the query.\n",
    "\n",
    "---\n",
    "$$n_{t+1} = \\arg\\max_{n \\in \\text{neighbors}(c_t)} \\text{sim}(\\vec{q}, \\vec{n})$$\n",
    "---\n",
    "\n",
    "Extract all sentences from each visited chunk (including the anchor chunk). Continue traversing to chunks with highest query similarity.\n",
    "\n",
    "---\n",
    "$$c_{t+1} = \\arg\\max_{c \\in C_{\\text{unvisited}}} \\text{sim}(\\vec{q}, \\vec{c})$$\n",
    "---\n",
    "\n",
    "Chunks are not revisited; newly extracted sentences are deduplicated against what has already been gathered.\n",
    "\n",
    "Stop when `max_sentences` reached or, once at least eight sentences have been collected, when the best extracted sentence exceeds the best available chunk (early stopping).\n",
    "\n",
    "---\n",
    "$$\\max_{s \\in S_{\\text{extracted}}} \\text{sim}(\\vec{q}, \\vec{s}) > \\max_{c \\in C_{\\text{available}}} \\text{sim}(\\vec{q}, \\vec{c}) \\implies \\text{stop}$$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65f2c1",
   "metadata": {},
   "source": [
    "## 5.3: `kg_traversal`\n",
    "\n",
    "*Chunk-centric graph traversal that follows local similarity paths (not query similarity), with a focus on greater graph exploration.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a34926-b64f-48b1-bcd0-8959242dbdf2",
   "metadata": {},
   "source": [
    "![KG TRAVERSAL](docs/KG_TRAVERSAL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96d9cd",
   "metadata": {},
   "source": [
    "Similar to the `query_traversal` algorithm, start at the anchor chunk.\n",
    "\n",
    "---\n",
    "$$c_0 = \\arg\\max_{c_i} \\text{sim}(\\vec{q}, \\vec{c}_i), \\quad S_0 = \\text{sentences}(c_0)$$\n",
    "---\n",
    "\n",
    "Unlike the `query_traveral` algorithm, starting from the anchor chunk's `top_x` and `top_k` connections, at each hop, find the next chunk ***most similar to the node that we are currently at in the graph.***\n",
    "\n",
    "---\n",
    "$$c_{t+1} = \\arg\\max_{c \\in \\text{neighbors}(c_t)} \\text{sim}(\\vec{c}_t, \\vec{c})$$\n",
    "---\n",
    "\n",
    "To encourage exploration and prevent too much read-through, prevent sentence overlap by skipping chunks containing already-extracted sentences, and traversal only considers chunk nodes (sentence nodes are ignored).\n",
    "\n",
    "---\n",
    "$$c \\notin C_{\\text{candidates}} \\text{ if } \\text{sentences}(c) \\cap S_{\\text{extracted}} \\neq \\emptyset$$\n",
    "---\n",
    "\n",
    "Stop when next chunk similarity ‚â§ previous hop similarity (exploration-potential early stopping), or at `max_sentences`.\n",
    "\n",
    "---\n",
    "$$\\text{sim}(\\vec{c}_t, \\vec{c}_{t+1}) \\leq \\text{sim}(\\vec{c}_{t-1}, \\vec{c}_t) \\implies \\text{stop}$$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12934539",
   "metadata": {},
   "source": [
    "## 5.4: `triangulation_average`\n",
    "\n",
    "*Averages the graph edge lengths between the query, current chunk, and prospective chunk/sentence nodes at each step. Creates a balanced averaged traversal that considers both the query and prospective chunks.* When a sentence belongs to a different chunk, sentence-to-chunk similarity is approximated from cached scores so the averaged triangle score stays comparable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8bb5c-3410-4ad3-a24e-5d089676c422",
   "metadata": {},
   "source": [
    "![TRIANGULATION AVERAGE](docs/TRIANGULATION_AVERAGE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe31b9",
   "metadata": {},
   "source": [
    "Embed query and start at anchor chunk.\n",
    "\n",
    "---\n",
    "$$\\vec{q} = \\text{embed}(\\text{query}), \\quad c_0 = \\arg\\max_{c_i} \\text{sim}(\\vec{q}, \\vec{c}_i)$$\n",
    "---\n",
    "\n",
    "At each traversal step, identify all graph edges between the current chunk, query, and prospective chunks. Consider these edges as triangles.\n",
    "\n",
    "---\n",
    "$$\\text{avg}(\\vec{q}, \\vec{c}_t, \\vec{c}_{\\text{candidate}}) = \\frac{\\text{sim}(\\vec{q}, \\vec{c}_t) + \\text{sim}(\\vec{q}, \\vec{c}_{\\text{candidate}}) + \\text{sim}(\\vec{c}_t, \\vec{c}_{\\text{candidate}})}{3}$$\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "$$n_{t+1} = \\arg\\max_{n \\in \\text{neighbors}(c_t)} \\text{avg}(\\vec{q}, \\vec{c}_t, \\vec{n})$$\n",
    "---\n",
    "\n",
    "Stop when best extracted sentence average exceeds best available chunk average (evaluated once at least eight sentences anchor the check), or when we hit `max_sentences`.\n",
    "\n",
    "---\n",
    "$$\\max_{s \\in S} \\text{avg}(\\vec{q}, \\vec{c}_t, \\vec{s}) > \\max_{c \\in C} \\text{avg}(\\vec{q}, \\vec{c}_t, \\vec{c})$$\n",
    "---\n",
    "When a sentence belongs to a different chunk, sentence-to-chunk similarity is approximated from cached scores so the averaged triangle score stays comparable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d280d22",
   "metadata": {},
   "source": [
    "## 5.5: `triangulation_geometric_3d`\n",
    "\n",
    "*Geometric triangulation of prospective chunk centroids using PCA-reduced 3D embeddings.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88014d0-ff0e-4f9f-9b17-0b1ce86d5203",
   "metadata": {},
   "source": [
    "![TRIANGULATION GEOMETRIC](docs/TRIANGULATION_GEOMETRIC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ab39d",
   "metadata": {},
   "source": [
    "Reduce all embeddings to 3D using PCA.\n",
    "\n",
    "---\n",
    "$$\\vec{q}_{3D}, \\vec{c}_{i,3D} = \\text{PCA}_{1024 \\to 3}(\\vec{q}, \\{\\vec{c}_i\\})$$\n",
    "---\n",
    "\n",
    "Similarly to `trangulation_average`, at each traversal step, identify all graph edges between the current chunk, query, and prospective chunks. This time, find the `triangle centroid` of each created triangle.\n",
    "\n",
    "---\n",
    "$$\\vec{\\text{centroid}} = \\frac{\\vec{q}_{3D} + \\vec{c}_{t,3D} + \\vec{n}_{3D}}{3}$$\n",
    "---\n",
    "\n",
    "Traverse to the node with its centroid closest to query (minimal Euclidean distance).\n",
    "\n",
    "---\n",
    "$$n_{t+1} = \\arg\\min_{n \\in \\text{neighbors}(c_t)} \\|\\vec{\\text{centroid}}(\\vec{q}, \\vec{c}_t, \\vec{n}) - \\vec{q}_{3D}\\|_2$$\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca36f966",
   "metadata": {},
   "source": [
    "## 5.6: `triangulation_fulldim`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba978cd",
   "metadata": {},
   "source": [
    "Geometric triangulation in *full* embedding space.\n",
    "\n",
    "Work directly with full embeddings instead of PCA 3D reduction.\n",
    "\n",
    "---\n",
    "$$\\vec{q}, \\vec{c}_i \\in \\mathbb{R}^{d}\\quad(\\text{dimension auto-detected per knowledge graph, default }d=1024)$$\n",
    "---\n",
    "\n",
    "Similarly to the other triangulation algorithms, identify triangles. But this time, in full embedding dimensional space.\n",
    "\n",
    "---\n",
    "$$\\vec{\\text{centroid}}_{1024D} = \\frac{\\vec{q} + \\vec{c}_t + \\vec{n}}{3}$$\n",
    "---\n",
    "\n",
    "Select node with centroid closest to query in full-dimensional Euclidean space.\n",
    "\n",
    "---\n",
    "$$n_{t+1} = \\arg\\min_{n \\in \\text{neighbors}(c_t)} \\|\\vec{\\text{centroid}}(\\vec{q}, \\vec{c}_t, \\vec{n}) - \\vec{q}\\|_2$$\n",
    "---\n",
    "\n",
    "Most mathematically rigorous approach, preserves all embedding information.\n",
    "\n",
    "---\n",
    "$$\\text{Edge lengths: } d(\\vec{q}, \\vec{c}) = \\|\\vec{q} - \\vec{c}\\|_2$$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee1d39-81e4-4e95-8d5c-a6576924b64c",
   "metadata": {},
   "source": [
    "## 5.7: `llm-guided-traversal`\n",
    "\n",
    "Modified version of `query_traversal` but uses a lightweight LLM instead. Trades speed and cost for accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983ff7f-b232-49be-aba0-036a95b3e0f2",
   "metadata": {},
   "source": [
    "![LLM GUIDED TRAVERSAL](docs/LLM_GUIDED_TRAVERSAL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4953665-af9a-421d-94e9-31621bcf2d86",
   "metadata": {},
   "source": [
    "Embed query and find anchor chunk (same as other algorithms).\n",
    "\n",
    "---\n",
    "$$\\vec{q} = \\text{embed}(\\text{query}), \\quad c_0 = \\arg\\max_{c_i} \\text{sim}(\\vec{q}, \\vec{c}_i)$$\n",
    "---\n",
    "\n",
    "At each hop, get `top_k` and `top_x` potential chunks based on query similarity.\n",
    "\n",
    "---\n",
    "$$C_{\\text{candidates}} = \\text{top-k}\\{c \\in \\text{neighbors}(c_t) \\mid \\text{sim}(\\vec{q}, \\vec{c})\\}$$\n",
    "---\n",
    "\n",
    "---\n",
    "At each hop, the LLM receives this prompt:\n",
    "\n",
    "```\n",
    "You are a knowledge graph traversal agent. Your goal: find relevant content to answer the query.\n",
    "\n",
    "QUERY: {user's question}\n",
    "\n",
    "ALREADY EXTRACTED ({n} sentences):\n",
    "{summary of first 5 sentences extracted so far...}\n",
    "\n",
    "CANDIDATE CHUNKS (pick ONE or STOP):\n",
    "1. [chunk_id_1] (similarity: 0.85)\n",
    "   Preview: {first 200 chars of chunk 1}...\n",
    "\n",
    "2. [chunk_id_2] (similarity: 0.78)\n",
    "   Preview: {first 200 chars of chunk 2}...\n",
    "\n",
    "3. [chunk_id_3] (similarity: 0.72)\n",
    "   Preview: {first 200 chars of chunk 3}...\n",
    "\n",
    "4. [chunk_id_4] (similarity: 0.69)\n",
    "   Preview: {first 200 chars of chunk 4}...\n",
    "\n",
    "5. [chunk_id_5] (similarity: 0.65)\n",
    "   Preview: {first 200 chars of chunk 5}...\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Choose the chunk number (1-5) that seems most relevant to answering the query\n",
    "- If you believe we have enough information to answer the query, respond with \"stop\"\n",
    "- Consider both what we've already extracted and what new information each candidate provides\n",
    "- Respond ONLY with a JSON object in this exact format:\n",
    "\n",
    "{\"choice\": <number 1-5 OR \"stop\">, \"reasoning\": \"brief explanation\"}\n",
    "\n",
    "Your response:\n",
    "```\n",
    "---\n",
    "\n",
    "Send the user's query, the currently extracted context, and previews of potential chunks to LLM. LLM chooses next chunk or stops.\n",
    "\n",
    "If the LLM response is invalid, fall back to the highest query similarity candidate to keep traversal moving.\n",
    "\n",
    "---\n",
    "$$c_{t+1} = \\text{LLM}(\\text{query}, S_{\\text{extracted}}, C_{\\text{candidates}})$$\n",
    "---\n",
    "\n",
    "LLM decides when to stop based on semantic reasoning (not just similarity).\n",
    "\n",
    "---\n",
    "$$\\text{LLM decides: continue or stop based on context sufficiency}$$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2b385-921a-4bff-82de-fc22c74fd099",
   "metadata": {},
   "source": [
    "# 6. Retrieval Testing\n",
    "\n",
    "This section lets you test any of the seven algorithms on the cached knowledge graph. Simply choose your algorithm and query, then run the cells to see traversal logs and visualizations.\n",
    "\n",
    "**Available Algorithms:**\n",
    "1. `basic_retrieval` - Basic semantic RAG (no traversal, control baseline)\n",
    "2. `query_traversal` - Query-guided graph traversal\n",
    "3. `kg_traversal` - Chunk-centric local similarity traversal\n",
    "4. `triangulation_average` - Averaged triangle score traversal\n",
    "5. `triangulation_geometric_3d` - Geometric triangulation (3D PCA)\n",
    "6. `triangulation_fulldim` - Full-dimensional geometric triangulation\n",
    "7. `llm_guided_traversal` - LLM-guided query traversal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buiksa214xi",
   "metadata": {},
   "source": [
    "## 6.1: Choose Your Algorithm and Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mxbs0pwfh5s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHOOSE YOUR ALGORITHM AND QUERY\n",
    "# ============================================================================\n",
    "\n",
    "# Select one of the seven algorithms\n",
    "algorithm = \"llm_guided_traversal\"  # Change this to any algorithm from the list above\n",
    "\n",
    "# Define your query\n",
    "query = \"What is the psychology behind learning? Do computers learn like humans do?\"  # Change this to your question\n",
    "\n",
    "# ============================================================================\n",
    "# RETRIEVAL PARAMETERS (Optional - modify to extend or limit retrieval)\n",
    "# ============================================================================\n",
    "\n",
    "# Maximum sentences to retrieve (default: 20)\n",
    "# Increase this to 50-100+ to really fly through the graph and explore more\n",
    "max_sentences = 50  \n",
    "\n",
    "# Maximum hops/jumps through the graph (safety limit, default: 10)\n",
    "max_hops = 20\n",
    "\n",
    "# Enable early stopping (stops when best sentence beats best available chunk)\n",
    "enable_early_stopping = False  # Set to False to always retrieve max_sentences\n",
    "\n",
    "# ============================================================================\n",
    "# Apply custom parameters by temporarily modifying config\n",
    "# ============================================================================\n",
    "\n",
    "# Store original values to restore later\n",
    "original_max_sentences = config['retrieval']['semantic_traversal'].get('min_sentence_threshold', 20)\n",
    "original_max_hops = config['retrieval']['semantic_traversal'].get('max_safety_hops', 50)\n",
    "original_early_stopping = config['retrieval']['semantic_traversal'].get('enable_early_stopping', True)\n",
    "original_max_results = config['retrieval']['semantic_traversal'].get('max_results', 50)\n",
    "\n",
    "# Apply custom parameters\n",
    "config['retrieval']['semantic_traversal']['min_sentence_threshold'] = max_sentences\n",
    "config['retrieval']['semantic_traversal']['max_safety_hops'] = max_hops\n",
    "config['retrieval']['semantic_traversal']['enable_early_stopping'] = enable_early_stopping\n",
    "config['retrieval']['semantic_traversal']['max_results'] = max_sentences  \n",
    "\n",
    "print(f\"üéØ Algorithm: {algorithm}\")\n",
    "print(f\"üîç Query: '{query}'\")\n",
    "print(f\"‚öôÔ∏è  Max Sentences: {max_sentences}\")\n",
    "print(f\"‚öôÔ∏è  Max Results: {max_sentences}\")\n",
    "print(f\"‚öôÔ∏è  Max Hops: {max_hops}\")\n",
    "print(f\"‚öôÔ∏è  Early Stopping: {'Enabled' if enable_early_stopping else 'Disabled'}\")\n",
    "print(f\"‚úÖ Ready to retrieve!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n700b63c0pc",
   "metadata": {},
   "source": [
    "## 6.2: Run Retrieval\n",
    "\n",
    "This will execute the selected algorithm and display traversal logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apvk1j8buxs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run retrieval using the cached knowledge graph and retrieval orchestrator\n",
    "print(f\"üöÄ Running {algorithm} algorithm...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "result = retrieval_orchestrator.retrieve(query, algorithm)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úÖ Retrieval complete!\")\n",
    "print(f\"   üìä Algorithm: {result.algorithm_name}\")\n",
    "print(f\"   üìù Retrieved {len(result.retrieved_content)} sentences\")\n",
    "print(f\"   ‚≠ê Final score: {result.final_score:.3f}\")\n",
    "print(f\"   ‚è±Ô∏è  Processing time: {result.processing_time:.3f}s\")\n",
    "\n",
    "# Show preview of retrieved content\n",
    "print(f\"\\nüìÑ Preview of first 3 retrieved sentences:\")\n",
    "for i, sentence in enumerate(result.retrieved_content[:3], 1):\n",
    "    preview = sentence[:150] + \"...\" if len(sentence) > 150 else sentence\n",
    "    print(f\"   {i}. {preview}\")\n",
    "\n",
    "# Restore original config values\n",
    "config['retrieval']['semantic_traversal']['min_sentence_threshold'] = original_max_sentences\n",
    "config['retrieval']['semantic_traversal']['max_safety_hops'] = original_max_hops\n",
    "config['retrieval']['semantic_traversal']['enable_early_stopping'] = original_early_stopping\n",
    "config['retrieval']['semantic_traversal']['max_results'] = original_max_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gaf7cffr2l",
   "metadata": {},
   "source": [
    "## 6.2.1: Generate Answer from Retrieved Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rbs89sd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Print ALL retrieved sentences to verify content\n",
    "print(\"üîç DEBUG - Full retrieved content:\")\n",
    "print(\"=\" * 80)\n",
    "for i, sentence in enumerate(result.retrieved_content, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "    print()\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mdymwbc3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANSWER GENERATION CONFIGURATION (Choose your provider and model)\n",
    "# ============================================================================\n",
    "\n",
    "# Choose provider: \"openai\", \"openrouter\", or \"ollama\"\n",
    "answer_provider = \"ollama\"\n",
    "\n",
    "# Choose model name based on provider:\n",
    "# - OpenAI: \"gpt-4o\", \"gpt-4o-mini\", \"gpt-3.5-turbo\"\n",
    "# - OpenRouter: \"meta-llama/llama-3.3-70b-instruct\", \"anthropic/claude-3.5-sonnet\", etc.\n",
    "# - Ollama: \"llama3\", \"mistral\", \"phi\", etc.\n",
    "answer_model_name = \"llama3.2\"\n",
    "\n",
    "# Temperature (0.0 = deterministic, 1.0 = creative)\n",
    "answer_temperature = 0.1\n",
    "\n",
    "# Max tokens for answer\n",
    "answer_max_tokens = 1000\n",
    "\n",
    "# ============================================================================\n",
    "# Generate answer using the retrieved content\n",
    "# ============================================================================\n",
    "from evaluation.models import ModelManager\n",
    "\n",
    "print(\"ü§ñ Generating answer from retrieved content...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Store original settings (use .get() to handle missing keys)\n",
    "    answer_config = config.get('deepeval', {}).get('models', {}).get('answer_generation', {})\n",
    "    original_provider = answer_config.get('provider', 'openai')\n",
    "    original_model = answer_config.get('model_name', 'gpt-4o')\n",
    "    original_temp = answer_config.get('temperature', 0.1)\n",
    "    original_tokens = answer_config.get('max_tokens', 2000)\n",
    "    \n",
    "    # Apply custom settings\n",
    "    if 'deepeval' not in config:\n",
    "        config['deepeval'] = {}\n",
    "    if 'models' not in config['deepeval']:\n",
    "        config['deepeval']['models'] = {}\n",
    "    if 'answer_generation' not in config['deepeval']['models']:\n",
    "        config['deepeval']['models']['answer_generation'] = {}\n",
    "    \n",
    "    config['deepeval']['models']['answer_generation']['provider'] = answer_provider\n",
    "    config['deepeval']['models']['answer_generation']['model_name'] = answer_model_name\n",
    "    config['deepeval']['models']['answer_generation']['temperature'] = answer_temperature\n",
    "    config['deepeval']['models']['answer_generation']['max_tokens'] = answer_max_tokens\n",
    "    \n",
    "    # Initialize model manager with custom settings\n",
    "    model_manager = ModelManager(config, logger)\n",
    "    answer_model = model_manager.get_answer_generation_model()\n",
    "    \n",
    "    # Show which provider is being used\n",
    "    print(f\"üì° Using: {answer_provider} / {answer_model_name}\")\n",
    "    \n",
    "    # Format retrieved sentences as context\n",
    "    context = \"\\n\".join(result.retrieved_content)\n",
    "    \n",
    "    print(f\"üìä Context: {len(result.retrieved_content)} sentences, {len(context)} characters\")\n",
    "    \n",
    "    # Create prompt for answer generation\n",
    "    prompt = f\"\"\"Based on the provided context, answer the following question. Use only the information from the context and be concise and accurate.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Generate the answer\n",
    "    print(f\"‚è≥ Generating response...\")\n",
    "    response = answer_model.generate(prompt)\n",
    "    \n",
    "    # Extract answer text - handle multiple response formats\n",
    "    if isinstance(response, tuple):\n",
    "        # Ollama sometimes returns (text, score) tuples\n",
    "        answer = response[0] if len(response) > 0 else str(response)\n",
    "    elif hasattr(response, 'response'):\n",
    "        answer = response.response\n",
    "    elif isinstance(response, str):\n",
    "        answer = response\n",
    "    else:\n",
    "        answer = str(response)\n",
    "    \n",
    "    print(f\"\\nüí¨ Generated Answer:\\n\")\n",
    "    print(f\"{answer}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Restore original config\n",
    "    config['deepeval']['models']['answer_generation']['provider'] = original_provider\n",
    "    config['deepeval']['models']['answer_generation']['model_name'] = original_model\n",
    "    config['deepeval']['models']['answer_generation']['temperature'] = original_temp\n",
    "    config['deepeval']['models']['answer_generation']['max_tokens'] = original_tokens\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"\\n‚ö†Ô∏è  Answer generation failed: {e}\")\n",
    "    print(f\"\\nüîç Full error traceback:\")\n",
    "    traceback.print_exc()\n",
    "    print(f\"\\nüí° Tip: Check your provider settings and API keys:\")\n",
    "    print(f\"   - OpenAI: Set OPENAI_API_KEY environment variable\")\n",
    "    print(f\"   - OpenRouter: Set OPENROUTER_API_KEY environment variable\")\n",
    "    print(f\"   - Ollama: Ensure Ollama server is running (ollama serve)\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # Restore original config even on error\n",
    "    try:\n",
    "        config['deepeval']['models']['answer_generation']['provider'] = original_provider\n",
    "        config['deepeval']['models']['answer_generation']['model_name'] = original_model\n",
    "        config['deepeval']['models']['answer_generation']['temperature'] = original_temp\n",
    "        config['deepeval']['models']['answer_generation']['max_tokens'] = original_tokens\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uc4vm6tlsz8",
   "metadata": {},
   "source": [
    "## 6.3: Visualize Traversal Path\n",
    "\n",
    "Generate 2D and 3D visualizations of the retrieval path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995d502-d2ba-49c4-a601-d0f8e034d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization functions\n",
    "from utils.matplotlib_visualizer import create_heatmap_visualization\n",
    "from utils.plotly_visualizer import create_algorithm_visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Visualization functions imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bohfx1lgyu5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Global View - Shows complete document landscape with full traversal path\n",
    "print(\"üé® Creating 2D GLOBAL visualization (strategic overview)...\")\n",
    "\n",
    "fig_matplotlib_global = create_heatmap_visualization(\n",
    "    result=result,\n",
    "    query=query,\n",
    "    knowledge_graph=knowledge_graph,\n",
    "    figure_size=(20, 8),\n",
    "    max_documents=6,\n",
    "    visualization_type=\"global\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Global visualization displayed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fw601et6w",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Sequential View - Shows hop-by-hop traversal analysis\n",
    "print(\"üé® Creating 2D SEQUENTIAL visualization (hop-by-hop analysis)...\")\n",
    "\n",
    "fig_matplotlib_sequential = create_heatmap_visualization(\n",
    "    result=result,\n",
    "    query=query,\n",
    "    knowledge_graph=knowledge_graph,\n",
    "    figure_size=(20, 8),\n",
    "    visualization_type=\"sequential\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sequential visualization displayed!\")\n",
    "print(\"üí° Look for the step numbers on each marker to see the traversal order!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0up9bpzohf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Interactive View - Plotly visualization with rotation and zoom\n",
    "print(\"üé® Creating 3D INTERACTIVE visualization...\")\n",
    "\n",
    "fig_plotly = create_algorithm_visualization(\n",
    "    result=result,\n",
    "    query=query,\n",
    "    knowledge_graph=knowledge_graph,\n",
    "    method=\"pca\",\n",
    "    max_nodes=200,  # Increased from 50 to show all traversal nodes\n",
    "    show_all_visited=True,\n",
    "    edge_threshold=0.70  # 0.6=very dense, 0.75=moderate, 0.8=sparse, 0.85=very sparse\n",
    ")\n",
    "\n",
    "fig_plotly.show()\n",
    "\n",
    "# Export to temp_data directory\n",
    "import os\n",
    "from datetime import datetime\n",
    "os.makedirs(\"temp_data\", exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "export_path = f\"temp_data/knowledge_graph_visualization_{timestamp}.html\"\n",
    "fig_plotly.write_html(export_path)\n",
    "print(f\"üíæ Visualization exported to: {export_path}\")\n",
    "\n",
    "print(\"‚úÖ Plotly 3D visualization displayed!\")\n",
    "print(\"üí° Tip: Drag to rotate, scroll to zoom, and hover over nodes for details!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e8d9ec-ac1d-4cb9-bf3b-505019404b39",
   "metadata": {},
   "source": [
    "# 7. Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7236d-8bd8-445d-9036-79726296f006",
   "metadata": {},
   "source": [
    "Benchmarking RAG systems can be done via a variety of different tools and libraries but it is primarily done via a series of universal metrics:\n",
    "\n",
    "- Precision: How relevant the retrieved chunks are for answering the question.\n",
    "- Recall: Whether all of the necessary info was retrieved to answer the question.\n",
    "- Faithfulness: How grounded in fact the generated answer and retrieved contexts are.\n",
    "- Answer Relevance: How relevant the geenerated answer was at actually answering the question.\n",
    "\n",
    "These metrics are tracked using libraries like `ragas` and `deepeval`. For this project, I used `deepeval` due to the modular dataset generation and context grouping support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257649f1-05af-4e28-b1ac-5ce78f866ba6",
   "metadata": {},
   "source": [
    "## 7.1 DeepEval\n",
    "\n",
    "The full benchmarking pipeline with `deepeval` can be viewed in **2 stages.**\n",
    "\n",
    "1. Synthetic Dataset Generation:\n",
    "   - Using our knowledge graph, create **context groups** of similar contexts that we send to `deepeval`'s question generation engine.\n",
    "   - Deepeval then uses a chosen LLM to generate a question from these contexts, as well as an expected answer for groundedness.\n",
    "   - We can then use `deepeval`'s *evolution* feature to evolve the question to be slightly more complex (reasoning, multi-context, etc).\n",
    "   - Finally, we can export the dataset as a `.csv` to be pushed to the dashboard, or cached locally.\n",
    "2. Full Evaluation:\n",
    "   - For each algorithm selected, we run through the full dataset and embed each generated query and attempt to retrieve the same contexts used for that question. We also attempt to generate an accurate answer. Each step in this process is done using any LLM of our choosing.\n",
    "   - `deepeval` then takes our responses and sends them to a critic LLM for an analysis on the metrics listed previously.\n",
    "   - We can then see the results in the `deepeval` dashboard, with detailed JSON responses from the critic model.\n",
    "\n",
    "In the next sections, we'll walk through this process in detail to demonstrate how each dataset was generated, and how you can modify the `config.yaml` file to attempt retrieval on your own if you so choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b06d31-512c-4017-9a1e-f4f94ae1c596",
   "metadata": {},
   "source": [
    "## 7.2 Context Grouping Algorithms\n",
    "\n",
    "There are a variety of different available options online for how to do context grouping. For this project, I wanted to see if I could, effectively, create context grouping algorithms that functioned extremely similarly to our retrieval algorithms by traversing the knowledge graph. There are currently *four* methods of context grouping available in this repository. Only *one* was used to generate the final dataset, but the other four are still available for use. The context grouping parameters can be customized using the `config.yaml`.\n",
    "\n",
    "---\n",
    "### `intra_document`\n",
    "\n",
    "Starting at a random chunk, traverse to the most similar chunk to the current chunk in the *current document* that ***does not contain a sentence we've already seen.*** This allows for a little bit more exploration. \n",
    "\n",
    "```\n",
    "context_strategies:\n",
    "  intra_document:\n",
    "    enabled: true\n",
    "    weight: 0.33\n",
    "    max_sentences: 10\n",
    "    description: \"Within-document exploratory context grouping.\"\n",
    "```\n",
    "\n",
    "---\n",
    "### `theme_based` (used for the 20 question dataset)\n",
    "\n",
    "Starting at a random chunk, rank other documents by theme similarity. Then take a look at the list of connected chunks from different documents that contain the highest theme overlap by theme similarity. Then at each step, traverse **between documents** to the most similar chunk to the current chunk. This creates an \"oscilation\" between multiple documents of similar themes.\n",
    "\n",
    "```\n",
    "context_strategies:\n",
    "  theme_based:\n",
    "    enabled: true\n",
    "    weight: 0.33\n",
    "    max_sentences: 10\n",
    "    fallback_to_inter_document: true\n",
    "    description: \"Cross-document thematic context grouping.\"\n",
    "```\n",
    "\n",
    "---\n",
    "### `sequential_multi_hop` (used for the 50 question dataset)\n",
    "\n",
    "Starting at a random chunk, \"read\" forwards or backwards inside the current document (whichever is most similar). Then after reaching 5 sentences, do a `theme_based` hop to the most similar chunk in a different document. Then, \"read\" forwards or backwards again. We do this so that we don't **just** hop between documents during retrieval; we actually want to attempt to get our algorithms to \"read\" a little bit during retrieval, and this context grouping algorithm is designed to demonstrate that once we attempt retrieval.\n",
    "\n",
    "```\n",
    "context_strategies:\n",
    "  sequential_multi_hop:\n",
    "    enabled: true\n",
    "    weight: 0.33\n",
    "    num_reading_hops: 3\n",
    "    num_paragraph_sentences: 5\n",
    "    num_cross_doc_hops: 3\n",
    "    description: \"Cross-document thematic reading simulation: 3 docs √ó 5 sentences = 15-sentence narratives\"\n",
    "```\n",
    "\n",
    "---\n",
    "### `deepeval_native`\n",
    "\n",
    "Basic context grouping. Uses a few random or sequential chunks, basic sentence deduplication, and `deepeval`'s quality filtration. Useable as a baseline but isn't very robust.\n",
    "\n",
    "```\n",
    "context_strategies:\n",
    "  deepeval_native:\n",
    "    enabled: true\n",
    "    weight: 0.2\n",
    "    max_sentences: 10\n",
    "    extraction_mode: \"random\"\n",
    "    chunks_per_group: 3\n",
    "    ensure_document_diversity: true\n",
    "    description: \"Simple random/sequential extraction with DeepEval FiltrationConfig quality filtering - no semantic traversal\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a14df-eac5-4357-a5db-adc3dbdfbbd9",
   "metadata": {},
   "source": [
    "## 7.3 Dataset Generation: Goldens, Filtration, and Evolutions\n",
    "\n",
    "Here is the relevant section in the `config.yaml`:\n",
    "\n",
    "```\n",
    "deepeval:\n",
    "  dataset:\n",
    "    generation:\n",
    "      num_goldens: 50\n",
    "      max_goldens_per_context: 1\n",
    "      include_expected_output: true\n",
    "\n",
    "    filtration:\n",
    "      enabled: true  # DeepEval FiltrationConfig only supports OpenAI models, not Ollama\n",
    "      critic_model: \"gpt-4o-mini\"  # Only used if enabled=true\n",
    "      synthetic_input_quality_threshold: 0.8\n",
    "      max_quality_retries: 5\n",
    "\n",
    "    evolution:\n",
    "      enabled: true\n",
    "      num_evolutions: 1\n",
    "      evolution_types:\n",
    "        - \"REASONING\"\n",
    "        - \"COMPARATIVE\"\n",
    "        - \"MULTICONTEXT\"\n",
    "      evolution_distribution:\n",
    "        REASONING: 0.4\n",
    "        COMPARATIVE: 0.2\n",
    "        MULTICONTEXT: 0.4\n",
    "\n",
    "    output:\n",
    "      save_path: \"data/synthetic_dataset.json\"\n",
    "      cache_enabled: true\n",
    "      force_regenerate: true\n",
    "      push_to_dashboard: false\n",
    "      dataset_alias: \"50qa-seq-multihop-gpt4o-reasoning-comparative-multicontext\"\n",
    "      pull_from_dashboard: true\n",
    "      generate_csv: true\n",
    "      csv_context_delimiter: \" | \"\n",
    "```\n",
    "\n",
    "### Relevant Terms:\n",
    "\n",
    "- `num_goldens`: A *golden* is a question/answer/context entry in a synthetic dataset generated by DeepEval. We can choose how many we want to generate based on our goals. For this research, we used 50.\n",
    "- `include_expected_output`: This ensures we actually generate an expected answer in the dataset to compare against during evaluation.\n",
    "- `filtration -> enabled: true` *Filtration* takes each generated golden and sends it to the `critic_model` to be analyzed for quality. If the question/answer/context grouping is below the `synthetic_input_quality_threshold`, we retry up to the `max_quality_retries`.\n",
    "- `evolution`: *Evolution*, unique to DeepEval, takes the generated question and re-generates it according to a specific `evolution_type`. They could be mixed and matched but for our 50 question dataset in this research, we only used 1 evolution per golden, weighted at the `evolution_distribution` above.\n",
    "- `output`: These settings allow configuration for synthetic dataset generation. You can either pull a previously built dataset from the DeepEval dashboard, or you can push one to it directly (you have to pay for DeepEval starter plan to push). By default, the dataset is saved as a `.json` but you can optionally export a `.csv` to the same directory to upload to DeepEval if you don't want to pay for it.\n",
    "- `dataset_alias`: The name of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1c1b8-2a94-4d05-8cae-25ecf6fff75f",
   "metadata": {},
   "source": [
    "## 7.4 Synthetic Datasets\n",
    "\n",
    "This repository contains *three* datasets that have been pre-generated for convenience:\n",
    "\n",
    "- `1q-intradoc-reasoning-multicontext`: Single question for testing. Uses intradoc context grouping, with a dual reasoning/multicontext evolution question.\n",
    "- `20q-themes-gpt4omini-reasoning`: 20 questions for testing. Uses theme context grouping, with `gpt-4o-mini` question generation and filtration, with only reasoning evolutions.\n",
    "- `50qa-seq-multihop-gpt4o-reasoning-comparative-multicontext`: Full 50 question dataset. Uses sequential multihop context grouping, `gpt-4o` for question generation and filteration, and using 40% reasoning, 40% multicontext, and 20% comparative evolutions.\n",
    "\n",
    "You may generate your own if you wish for your own testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ebeac-332b-4721-ad37-81233ca0f36e",
   "metadata": {},
   "source": [
    "## 7.5 Models\n",
    "\n",
    "There are multiple different model configs in the `config.yaml`, since different models are used at different parts of the benchmarking process:\n",
    "\n",
    "```\n",
    "models:\n",
    "  embedding_models:\n",
    "    - \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "\n",
    "dataset:\n",
    "  filtration:\n",
    "    enabled: true  # DeepEval FiltrationConfig only supports OpenAI models, not Ollama\n",
    "    critic_model: \"gpt-4o-mini\"  # Only used if enabled=true\n",
    "    synthetic_input_quality_threshold: 0.8\n",
    "    max_quality_retries: 5\n",
    "\n",
    "deepeval:\n",
    "  models:\n",
    "    question_generation: \n",
    "      provider: \"openai\"  \n",
    "      model_name: \"gpt-4o\"  \n",
    "      temperature: 0.1\n",
    "      max_tokens: 20000\n",
    "\n",
    "    answer_generation:\n",
    "      provider: \"openai\" \n",
    "      model_name: \"gpt-5-nano\"  \n",
    "      temperature: 0.1\n",
    "      max_tokens: 5000\n",
    "\n",
    "    evaluation_judge:\n",
    "      provider: \"openai\"  \n",
    "      model_name: \"gpt-5-nano\" \n",
    "      temperature: 0.0\n",
    "      max_tokens: 50000\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### Relevant Terms:\n",
    "\n",
    "- `embedding_models:`: Embedding model for entire project. Strongly recommend keeping as is, this model is 1024 dimensions and is very effective.\n",
    "- `provider:` Can choose any from `\"openai\"`, `\"openrouter\"`, or `\"ollama\"`. Strongly recommend `\"openai\"` due to issues with `openrouter` during long evaluation sessions but the options are available to anyone looking to experiment.\n",
    "- `critic_model`: Critic model for dataset filtration. This model identifies if a generated golden is sufficient, if not we retry.\n",
    "- `question_generation`: Model for actually generating questions through evolutions. Recommend keeping this a high-quality model.\n",
    "- `answer_generation`: Model for generating answers during evaluation, as well as generating the `expected_output` in a golden (expected answer).\n",
    "- `evaluation_judge`: Model evaluates the precision, recall, relevancy, and faithfulness of each golden, returning scores and reasoning for each. This gets *very expensive*, as each golden requires multiple API calls with large input token volume. `gpt-5-nano` is a good sweet spot for cost to performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23099ab-e43a-4cd3-8975-db46a01b4f0d",
   "metadata": {},
   "source": [
    "# 8.0 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a62e546-e96f-4d7c-bdcb-ff56c8e1b01d",
   "metadata": {},
   "source": [
    "Running a full DeepEval evaluation in this notebook would be unreasonable. But the resources to do so are available in the README for this project. If you want to run a full evaluation from scratch using the resources in this notebook, carefully modify the `config.yaml` and then run the `benchmark.py` script. It will take care of everything for you.\n",
    "\n",
    "For now, let's take a look at the results for all of the algorithms, with the relevant parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8afb3f-88d6-458c-897d-6335461dd97e",
   "metadata": {},
   "source": [
    "### Global Parameters:\n",
    "\n",
    "- `embed_model`: \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "- `reranking`: False\n",
    "- `models -> provider`: openai\n",
    "- `models -> question_generation`: gpt-4o\n",
    "- `models -> answer_generation`: gpt-5-nano\n",
    "- `models -> evaluation_judge`: gpt-5-nano\n",
    "- `chunking -> strategy`: \"sliding_window\"\n",
    "- `chunking -> window_size`: 3\n",
    "- `chunking -> overlap`: 2\n",
    "- `wikipedia -> topics`: \"Machine Learning, Artificial Intelligence\"\n",
    "- `wikipedia -> articles_per_topic`: 5\n",
    "- `wikipedia -> max_article_length`: 5000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b10b8ee-b61d-4af2-86cc-0b0c2369f07d",
   "metadata": {},
   "source": [
    "### 20 Question Dataset:\n",
    "\n",
    "```\n",
    "deepeval:\n",
    "  models:\n",
    "    question_generation: \n",
    "      provider: \"openai\"  \n",
    "      model_name: \"gpt-4o-mini\"  \n",
    "      temperature: 0.1\n",
    "      max_tokens: 20000\n",
    "\n",
    "    answer_generation:\n",
    "      provider: \"openai\"  \n",
    "      model_name: \"gpt-4o-mini\" \n",
    "      temperature: 0.1\n",
    "      max_tokens: 5000\n",
    "\n",
    "    evaluation_judge:\n",
    "      provider: \"openai\"  \n",
    "      model_name: \"gpt-4o-mini\"  \n",
    "      temperature: 0.0\n",
    "      max_tokens: 50000\n",
    "\n",
    "  dataset:\n",
    "    generation:\n",
    "      num_goldens: 20\n",
    "      max_goldens_per_context: 1\n",
    "      include_expected_output: true\n",
    "\n",
    "    filtration:\n",
    "      enabled: true  \n",
    "      critic_model: \"gpt-4o-mini\"  \n",
    "      synthetic_input_quality_threshold: 0.8\n",
    "      max_quality_retries: 5\n",
    "\n",
    "    evolution:\n",
    "      enabled: true\n",
    "      num_evolutions: 1\n",
    "      evolution_types:\n",
    "        - \"REASONING\"\n",
    "        # - \"COMPARATIVE\"\n",
    "        # - \"MULTICONTEXT\"\n",
    "      evolution_distribution:\n",
    "        REASONING: 1.0\n",
    "        COMPARATIVE: 0.0\n",
    "        MULTICONTEXT: 0.0\n",
    "\n",
    "    output:\n",
    "      save_path: \"data/synthetic_dataset.json\"\n",
    "      cache_enabled: true\n",
    "      force_regenerate: true\n",
    "      push_to_dashboard: false\n",
    "      dataset_alias: \"20qa-themes-gpt4omini-reasoning\"\n",
    "      pull_from_dashboard: false\n",
    "      generate_csv: true\n",
    "      csv_context_delimiter: \" | \"\n",
    "\n",
    "context_strategies:\n",
    "  theme_based:\n",
    "    enabled: false\n",
    "    weight: 0.33\n",
    "    max_sentences: 10\n",
    "    fallback_to_inter_document: true\n",
    "    description: \"Cross-document theme overlap traversal with semantic fallback\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20e8c2-cfe1-455f-8242-9ab6cbcd1f9b",
   "metadata": {},
   "source": [
    "### 50 Question Dataset:\n",
    "\n",
    "```\n",
    "deepeval:\n",
    "  models:\n",
    "    question_generation:\n",
    "      provider: \"openai\"  \n",
    "      model_name: \"gpt-4o\" \n",
    "      temperature: 0.1\n",
    "      max_tokens: 20000\n",
    "\n",
    "    answer_generation:\n",
    "      provider: \"openai\"\n",
    "      model_name: \"gpt-5-nano\" \n",
    "      temperature: 0.1\n",
    "      max_tokens: 5000\n",
    "\n",
    "    evaluation_judge:\n",
    "      provider: \"openai\"  \n",
    "      model_name: \"gpt-5-nano\" \n",
    "      temperature: 0.0\n",
    "      max_tokens: 50000\n",
    "\n",
    "  dataset:\n",
    "    generation:\n",
    "      num_goldens: 50\n",
    "      max_goldens_per_context: 1\n",
    "      include_expected_output: true\n",
    "\n",
    "    filtration:\n",
    "      enabled: true  \n",
    "      critic_model: \"gpt-4o-mini\" \n",
    "      synthetic_input_quality_threshold: 0.8\n",
    "      max_quality_retries: 5\n",
    "\n",
    "    evolution:\n",
    "      enabled: true\n",
    "      num_evolutions: 1\n",
    "      evolution_types:\n",
    "        - \"REASONING\"\n",
    "        - \"COMPARATIVE\"\n",
    "        - \"MULTICONTEXT\"\n",
    "      evolution_distribution:\n",
    "        REASONING: 0.4\n",
    "        COMPARATIVE: 0.2\n",
    "        MULTICONTEXT: 0.4\n",
    "\n",
    "    output:\n",
    "      save_path: \"data/synthetic_dataset.json\"\n",
    "      cache_enabled: true\n",
    "      force_regenerate: true\n",
    "      push_to_dashboard: false\n",
    "      dataset_alias: \"50qa-seq-multihop-gpt4o-reasoning-comparative-multicontext\"\n",
    "      pull_from_dashboard: true\n",
    "      generate_csv: true\n",
    "      csv_context_delimiter: \" | \"\n",
    "\n",
    "context_strategies:\n",
    "  sequential_multi_hop:\n",
    "    enabled: true\n",
    "    weight: 0.33\n",
    "    num_reading_hops: 3\n",
    "    num_paragraph_sentences: 5\n",
    "    num_cross_doc_hops: 3\n",
    "    description: \"Structured reading simulation: 3 docs √ó 5 sentences = 15-sentence narratives\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bfde8c-2fa1-49d3-9396-dd40daedbbb7",
   "metadata": {},
   "source": [
    "---\n",
    "## $$\\text{20qa-themes-gpt4omini-reasoning}$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{|l|c|c|c|c|c|}\n",
    "\\hline\n",
    "\\textbf{Algorithm} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{Answer Relevancy} & \\textbf{Faithfulness} & \\textbf{Test Cases} \\\\\n",
    "\\hline\n",
    "\\text{basic\\_retrieval} & 0.87 & 0.74 & 0.91 & 0.93 & 16/20 \\\\\n",
    "\\hline\n",
    "\\text{query\\_traversal} & 0.83 & 0.83 & 0.91 & 1.00 & 16/20 \\\\\n",
    "\\hline\n",
    "\\text{kg\\_traversal} & 0.73 & 0.72 & 0.98 & 0.92 & 15/20 \\\\\n",
    "\\hline\n",
    "\\text{triangulation\\_average} & 0.84 & 0.77 & 0.96 & 1.00 & 16/20 \\\\\n",
    "\\hline\n",
    "\\text{triangulation\\_geometric\\_3d} & 0.86 & 0.77 & 0.96 & 1.00 & 16/20 \\\\\n",
    "\\hline\n",
    "\\text{triangulation\\_fulldim} & 0.90 & 0.78 & 0.95 & 0.99 & 17/20 \\\\\n",
    "\\hline\n",
    "\\text{llm\\_guided\\_traversal} & 0.88 & 0.82 & 0.95 & 1.00 & 17/20 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94af850-3c35-44c6-bc7c-b597115493d4",
   "metadata": {},
   "source": [
    "---\n",
    "## $$\\text{50qa-seq-multihop-gpt4o-reasoning-comparative-multicontext}$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{|l|c|c|c|c|c|}\n",
    "\\hline\n",
    "\\textbf{Algorithm} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{Answer Relevancy} & \\textbf{Faithfulness} & \\textbf{Test Cases} \\\\\n",
    "\\hline\n",
    "\\text{basic\\_retrieval} & 0.93 & 0.88 & 0.99 & 0.99 & 48/50 \\\\\n",
    "\\hline\n",
    "\\text{query\\_traversal} & 0.91 & 0.91 & 0.98 & 1.00 & 50/50 \\\\\n",
    "\\hline\n",
    "\\text{kg\\_traversal} & 0.93 & 0.87 & 0.99 & 0.99 & 49/50 \\\\\n",
    "\\hline\n",
    "\\text{triangulation\\_average} & 0.92 & 0.87 & 0.98 & 0.99 & 49/50 \\\\\n",
    "\\hline\n",
    "\\text{triangulation\\_geometric\\_3d} & 0.93 & 0.85 & 0.98 & 1.00 & 48/50 \\\\\n",
    "\\hline\n",
    "\\text{triangulation\\_fulldim} & 0.93 & 0.87 & 1.00 & 0.97 & 47/50 \\\\\n",
    "\\hline\n",
    "\\text{llm\\_guided\\_traversal} & 0.91 & 0.94 & 0.99 & 0.99 & 49/50 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
