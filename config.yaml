# Semantic RAG Pipeline Configuration - Enhanced Multi-Granularity Architecture
# =============================================================================

# Experiment Configuration
experiment:
  name: "semantic_rag_pipeline"
  description: "Enhanced multi-granularity semantic graph traversal RAG system"
  version: "2.0.0"
  tags: ["multi_granularity", "graph_traversal", "rag", "ollama"]

  # Experiment Tracking for Dashboard
  tracking:
    notes: "Added reranking implementation with hybrid strategy"
    run_type: "reranking_comparison"
    dataset_type: "sequential_multi_hop"
    baseline_comparison: true
    expected_improvements: ["contextual_relevancy", "precision"]

# System Configuration
system:
  device: "mps"  # auto, cpu, cuda, mps
  max_memory_gb: 8
  min_disk_space_gb: 5
  num_workers: 4

# Directory Structure
directories:
  data: "data"
  embeddings: "embeddings"
  visualizations: "visualizations"
  logs: "logs"
  configs: "configs"

# Logging Configuration
logging:
  level: "INFO"
  log_to_file: true
  log_to_console: true
  max_log_file_mb: 100
  backup_count: 5

# Pipeline Execution
execution:
  mode: "full_pipeline"
  skip_phases: []
  force_recompute: []

# Enhanced Models Configuration - Multi-Granularity Embeddings
models:
  embedding_models:
    - "sentence-transformers/all-mpnet-base-v2"
  embedding_batch_size: 32
  embedding_cache: true
  
  # Multi-granularity embedding configuration
  granularity_types:
    chunks:
      enabled: true
      description: "3-sentence sliding windows (existing)"
    sentences:
      enabled: true
      description: "Individual sentences for fine-grained navigation"
    doc_summaries:
      enabled: true
      description: "Document-level summaries for high-level navigation"
      method: "extractive"  # extractive, ollama
      max_sentences: 5

# Wikipedia Configuration (unchanged)
wikipedia:
  use_cached_articles: true
  topics:
    - "Neuroscience"
    - "Psychology"
    - "Computer"
    - "Neural Networks"
    - "Machine Learning"
  articles_per_topic: 20
  max_article_length: 5000000
  min_article_length: 1000
  language: "en"
  rate_limit_delay: 1.0
  retry_attempts: 3
  timeout_seconds: 30

# Text Processing (unchanged)
text_processing:
  clean_html: true
  remove_references: true
  remove_navigation: true
  remove_tables: true
  fix_encoding: true
  normalize_whitespace: true
  min_sentence_length: 10
  max_sentence_length: 500

# Chunking Configuration (unchanged)
chunking:
  strategy: "sliding_window"
  window_size: 3
  overlap: 2
  window_visualization_buffer_size: 3  # Number of chunks to extend visualization window in each direction

# Enhanced Retrieval Configuration - Hybrid Query-Aware Traversal
retrieval:
  algorithm: "semantic_traversal"
  cache_graphs: true

  semantic_traversal:
    num_anchors: 1  # Single anchor mode based on convergence insight
    max_hops: 3  # Deprecated - now uses dynamic termination
    similarity_threshold: 0.3
    max_results: 10
    min_sentence_threshold: 10  # Minimum sentences to extract
    max_safety_hops: 20  # Safety limit to prevent infinite traversal
    enable_reranking: true   # Enable reranking of extracted content
    single_anchor_mode: true  # Use single anchor for efficiency

    # KG Algorithm
    causal_threshold: 0.6
    path_coherence_threshold: 0.7
    mst_max_edges: 5
    cot_segment_length: 3

  baseline_vector:
    top_k: 10
    similarity_threshold: 0.5

# Reranking Configuration - Standardizes Output and Optimizes Relevance
reranking:
  strategy: "semantic"  # Options: "tfidf", "semantic", "hybrid"
  target_count: 10      # Standardized output: exactly 10 sentences per algorithm

  # Neural semantic reranking (HuggingFace cross-encoders)
  model_name: "cross-encoder/ms-marco-MiniLM-L-6-v2"  # MS MARCO trained cross-encoder

  # TF-IDF reranking parameters (fallback)
  max_features: 5000
  ngram_range: [1, 2]

  # Hybrid reranking weights (when strategy is "hybrid")
  hybrid_weights:
    tfidf: 0.6           # Primary relevance signal
    length: 0.2          # Moderate-length sentence preference
    question_overlap: 0.2 # Bonus for question keyword overlap

  # Quality control
  enable_fallback: true
  log_reranking_decisions: false  # Set to true for debugging

# Enhanced Similarity Matrix Configuration - Multi-Granularity
similarities:
  use_cached: true
  similarity_metric: "cosine"
  batch_size: 1000
  max_memory_gb: 4
  
  # Multi-granularity similarity computation
  granularity_types:
    # Existing chunk-to-chunk similarities
    chunk_to_chunk:
      enabled: true
      intra_document:
        enabled: true
        top_k: 15  # Per chunk, within same document
        min_threshold: 0.4
      inter_document:
        enabled: true
        top_x: 5   # Per chunk, across different documents
        min_threshold: 0.3
    
    # New document-to-document similarities
    doc_to_doc:
      enabled: true
      top_k: 10  # Per document, most similar documents
      min_threshold: 0.2
      
    # New sentence-to-sentence similarities
    sentence_to_sentence:
      enabled: true
      semantic:
        enabled: true
        top_k: 3  # Per sentence, semantically similar sentences
        min_threshold: 0.4
      sequential:
        enabled: true
        description: "Adjacent sentence connections for reading flow"
        
    # New cross-granularity similarities
    cross_granularity:
      enabled: true
      sentence_to_chunk:
        enabled: true
        top_k: 3  # Per sentence, containing chunks
        min_threshold: 0.3
      chunk_to_doc:
        enabled: true
        description: "Chunk to parent document summary similarity"

# Phase 5: Entity/Theme Extraction Configuration
entity_theme_extraction:
  use_cached: true

  # Theme extraction settings
  theme_extraction:
    num_themes: 5                           # Number of themes to extract per document
    method: "ollama"                        # "ollama" or "fallback"
    fallback_method: "keyword_patterns"     # Fallback when Ollama unavailable

    # Ollama-specific settings (inherits from main ollama config)
    prompt_template: "structured_json"      # Template style for theme extraction
    max_theme_length: 50                    # Maximum characters per theme name

  # Quality filters
  quality_filters:
    min_entities_per_item: 0               # Minimum entities required (0 = no filter)
    min_themes_per_document: 1             # Minimum themes required per document
    filter_common_words: true              # Filter out common words from entities

# Phase 6: Knowledge Graph Assembly Configuration
knowledge_graph_assembly:
  use_cached: true

  # Theme bridging configuration (NEW)
  theme_bridging:
    enabled: true
    top_k_bridges: 20                    # Number of cross-document theme connections per theme
    min_bridge_similarity: 0.1          # Minimum similarity for theme bridges
    exclude_intra_document: true        # Only create cross-document bridges

# Simplified Knowledge Graph Configuration
knowledge_graph:
  use_cached: true
  architecture: "multi_granularity_three_tier"  # Document → Chunk → Sentence hierarchy
  
  # Note: Sparsity is now controlled in similarities section
  # This section focuses on graph structure and entity extraction
  extractors:
    ner:
      enabled: true
      entity_types: ["PERSON", "ORG", "GPE", "PRODUCT", "EVENT", "MISC"]
    keyphrases:
      enabled: true
      max_features: 15
    summary:
      enabled: true
      max_sentences: 2

# Question Generation Models Configuration (Phase 7)
question_generation:
  # Model selection: "ollama" or "openai"
  generator_model_type: "ollama"  # Primary model for question generation
  critic_model_type: "ollama"     # Secondary model for question critique
  
  # Question type distribution (must sum to 1.0)
  question_distribution:
    single_hop: 0.3        # Same document, raw similarity
    sequential_flow: 0.25  # Same document, 1-10 chunks
    multi_hop: 0.2         # 3 documents, raw KG similarity
    theme_hop: 0.15        # 3 documents, theme similarity  
    hierarchical: 0.1      # doc_summary → chunk → sentence
  
  # Generation parameters
  num_questions: 50        # Total number of questions to generate
  max_hops: 3              # Maximum hops for multi-hop questions
  max_sentences: 10        # Maximum sentences to pull
  enable_early_stopping: false  # Disabled for now
  cache_questions: true    # Cache generated questions
  
  # Quality control
  retry_failed_questions: true
  max_retries: 3
  min_question_length: 20
  max_question_length: 200

# Ollama Configuration
ollama:
  model: "llama3.1:8b"
  base_url: "http://localhost:11434"
  options:
    temperature: 0.1
    num_predict: 150
    stop: ["\n\n", "Text:", "Summary:", "Topics:"]

# OpenAI Configuration (for interchangeable use)
openai:
  generator_model: "gpt-4o-mini"     # Cheaper model for generation
  critic_model: "gpt-4o"             # More expensive model for critique
  api_key: ""  # Set via environment variable OPENAI_API_KEY
  options:
    temperature: 0.1
    max_tokens: 150

# Evaluation Configuration
evaluation:
  include_custom_metrics: true
  include_baseline_comparison: true
  baselines:
    - "bm25"
    - "traditional_vector"
    - "dense_passage_retrieval"
  metrics:
    standard: ["f1", "exact_match", "mrr", "ndcg"]
    custom: ["traversal_efficiency", "multi_hop_success", "semantic_coherence"]
  
  # Benchmarking configuration
  benchmarking:
    enable_reranking: true     # Apply reranking before evaluation
    standardize_output: true   # Ensure all algorithms produce same output count
    include_content_preview: true  # Add content previews to YAML output
    detailed_logging: true     # Enhanced logging for algorithm behavior analysis

# Visualization Configuration (unchanged)
visualization:
  enabled: true
  similarity_matrices: true
  traversal_paths: true
  chunk_boundaries: true
  question_distribution: true
  knowledge_graph_structure: true

# Output Configuration (unchanged)
output:
  create_report: true
  export_data: true
  save_models: false
  include_question_analysis: true

# Cleanup Configuration (unchanged)
cleanup:
  remove_temp_files: false
  compress_large_files: true

# Notifications (unchanged)
notifications:
  enabled: false

# DeepEval Configuration - Synthetic Dataset Generation and Evaluation
# ==================================================================
deepeval:
  # API Rate Limiting Configuration
  api_rate_limiting:
    enabled: true                    # Enable API call delays
    delay_between_calls: 3.0         # Seconds to wait between API calls
    delay_after_failure: 5.0         # Extra delay after API failures
    delay_for_large_requests: 5.0    # Extra delay for requests >30k tokens

  models:
    # Model provider for question generation
    question_generation:
      provider: "openrouter"              # ollama, openai, anthropic, openrouter
      model_name: "meta-llama/llama-3.3-70b-instruct"      # Provider-specific model identifier
      temperature: 0.1
      max_tokens: 20000

    # Model provider for evaluation (LLM-as-a-judge)
    evaluation_judge:
      provider: "openrouter"              # Using Ollama for local evaluation
      model_name: "meta-llama/llama-3.3-70b-instruct"      # Same model as question generation for consistency
      temperature: 0.0                # Low temperature for consistent evaluation
      max_tokens: 50000

  # Project configuration for DeepEval dashboard organization
  project:
    name: "semantic-rag-chunking-research"
    id: "cmfpz4kpj03i62ad3v3a098kv"  # DeepEval Project ID for dashboard integration
    description: "Benchmarking semantic RAG chunking strategies using knowledge graph traversal algorithms"
    version: "1.0.0"
    tags: ["rag", "knowledge-graph", "semantic-chunking", "retrieval"]

  dataset:
    generation:
      num_goldens: 5                # Number of synthetic questions to generate
      max_goldens_per_context: 1     # Questions per knowledge graph chunk group
      include_expected_output: true   # Generate ideal answers
      
    # Quality filtering configuration (for DeepEval native strategy)
    filtration:
      enabled: true                  # Enable quality filtering with LLM judge
      critic_model: "gpt-4o"         # Model for quality assessment
      synthetic_input_quality_threshold: 0.7  # Minimum quality score (0-1, default 0.5)
      max_quality_retries: 5         # Number of retries for low-quality questions
      
    evolution:
      enabled: true
      num_evolutions: 1               # Number of evolution steps per question
      evolution_types:                # Specific evolution strategies
        - "REASONING"                 # Multi-step logical reasoning
        - "COMPARATIVE"               # Comparison-based questions  
        - "IN_BREADTH"                # Broader scope questions
        - "MULTICONTEXT"              # Multi-chunk questions
      evolution_distribution:
        REASONING: 0.4                # 40% reasoning questions
        COMPARATIVE: 0.0              # 30% comparison questions
        IN_BREADTH: 0.0               # 20% broad questions
        MULTICONTEXT: 0.0             # 10% multi-context questions
    
    output:
      save_path: "data/synthetic_dataset.json"
      cache_enabled: true
      force_regenerate: true         # Set to true to regenerate existing datasets

      # DeepEval dashboard integration
      push_to_dashboard: false        # Push dataset to DeepEval dashboard (must be on paid plan for this to work)
      dataset_alias: "5q-deepeval-filtered-reasoning"  # Unique name for dataset on dashboard
      pull_from_dashboard: true      # Load dataset from dashboard instead of local file

      # CSV conversion for manual upload (free tier workaround)
      generate_csv: true               # Generate CSV file for manual dashboard upload
      csv_context_delimiter: " | "    # Delimiter for context arrays in CSV
  
  evaluation:
    # Async configuration for deepeval evaluation execution
    async_config:
      run_async: false               # Set to false for sequential execution (prevents API rate limiting)
      max_concurrent: 1              # Maximum concurrent evaluations (only used when run_async=true)
      throttle_value: 3.0            # Delay in seconds between test case evaluations

    metrics:
      # Core RAG metrics for retrieval quality assessment
      rag_metrics:
        - "answer_relevancy"          # Output relevance to input
        - "contextual_recall"         # Context completeness  
        - "contextual_precision"      # Context ranking quality
        - "contextual_relevancy"      # Context relevance to input
        - "faithfulness"              # Hallucination detection
      
      # Custom G-Eval metrics for semantic traversal assessment
      custom_metrics:
        - name: "semantic_coherence"
          criteria: "Evaluate whether the retrieved content demonstrates logical semantic connections and coherent reasoning flow across knowledge graph nodes"
          evaluation_params: ["ACTUAL_OUTPUT", "RETRIEVAL_CONTEXT"]
          threshold: 0.7
        
        - name: "traversal_efficiency" 
          criteria: "Assess whether the retrieval strategy efficiently navigates the knowledge graph to find relevant information without excessive or irrelevant hops"
          evaluation_params: ["ACTUAL_OUTPUT", "RETRIEVAL_CONTEXT", "INPUT"]
          threshold: 0.6
    
    algorithms:
      # List of retrieval algorithms to benchmark
      test_algorithms:
        - "basic_retrieval"
        # - "query_traversal"
        # - "kg_traversal"
        - "triangulation_centroid"
      
      # Algorithm hyperparameters (configurable for dashboard visibility)
      algorithm_hyperparameters:
        basic_retrieval:
          top_k: 10
          similarity_threshold: 0.5
        query_traversal:
          max_hops: 5
          similarity_threshold: 0.3
          min_sentence_threshold: 10
        kg_traversal:
          max_hops: 5
          similarity_threshold: 0.3
          min_sentence_threshold: 10
        triangulation_centroid:
          max_hops: 5
          similarity_threshold: 0.3
          min_sentence_threshold: 10
          max_results: 10
    
    output:
      results_directory: "benchmark_results"
      save_individual_results: true   # Save per-algorithm results
      save_comparison_report: true    # Generate comparison across algorithms
      include_visualizations: true    # Generate existing matplotlib/plotly visualizations

# Advanced Context Grouping Strategies Configuration
# =================================================
# Configurable strategies for generating semantically coherent context groups
# that mirror retrieval algorithm capabilities for self-validating benchmarks.
context_strategies:
  # Strategy 1: Intra-Document Semantic Traversal
  # Creates context groups by traversing semantically similar chunks within documents
  intra_document:
    enabled: false
    weight: 0.33                    # 20% of context groups
    max_sentences: 10              # Maximum sentences per context group
    description: "Semantic coherence within single documents using similarity-based chunk traversal"
  
  # Strategy 2: Inter-Document Cross-Domain Navigation  
  # Creates context groups by traversing across document boundaries using similarity
  inter_document:
    enabled: false
    weight: 0.2                    # 30% of context groups (primary focus for triangulation testing)
    max_sentences: 10              # Slightly larger for cross-domain contexts
    description: "Cross-document similarity navigation perfect for testing triangulation algorithms"
  
  # Strategy 3: Theme-Based Cross-Document Traversal
  # Creates context groups using theme overlap with similarity-based fallback
  theme_based:
    enabled: false
    weight: 0.33                    # 20% of context groups
    max_sentences: 10              # Larger for thematic reasoning
    fallback_to_inter_document: true # Use inter-document strategy if no theme overlap
    description: "Cross-document theme overlap traversal with semantic fallback"
  
  # Strategy 4: Sequential Multi-Hop Structured Reading
  # Creates predictable reading patterns with configurable cross-document hops
  sequential_multi_hop:
    enabled: false
    weight: 0.33                    # 20% of context groups
    num_reading_hops: 3            # Sequential reading steps within documents
    num_paragraph_sentences: 5     # Target sentences per paragraph/document
    num_cross_doc_hops: 3          # Number of cross-document jumps
    description: "Structured reading simulation: 3 docs × 5 sentences = 15-sentence narratives"
  
  # Strategy 5: Knowledge Graph Similarity Exploration
  # Pure similarity-based traversal without constraints
  knowledge_graph_similarity:
    enabled: false
    weight: 0.2                    # 10% of context groups
    max_sentences: 10              # Larger for unrestricted exploration
    allow_cross_document: true     # Enable cross-document traversal
    description: "Unrestricted similarity-based exploration across entire knowledge graph"
  
  # Strategy 6: DeepEval Native Simple Extraction
  # Simple chunk extraction relying on DeepEval's LLM-based quality filtering
  deepeval_native:
    enabled: true
    weight: 0.2                    # Baseline comparison weight
    max_sentences: 10              # Maximum sentences per context group
    extraction_mode: "random"      # 'random' or 'sequential' chunk selection
    chunks_per_group: 3            # Number of chunks per context group
    ensure_document_diversity: true # Ensure chunks come from different documents
    description: "Simple random/sequential extraction with DeepEval FiltrationConfig quality filtering - no semantic traversal"