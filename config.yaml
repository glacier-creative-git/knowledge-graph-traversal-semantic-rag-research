# Semantic RAG Pipeline Configuration - Local Implementation
# ========================================================

# Experiment Configuration
experiment:
  name: "semantic_rag_pipeline"
  description: "Semantic graph traversal RAG system with local Ollama extractors"
  version: "1.0.0"
  tags: ["semantic_chunking", "graph_traversal", "rag", "ollama"]

# System Configuration
system:
  device: "auto"  # auto, cpu, cuda, mps
  max_memory_gb: 8
  min_disk_space_gb: 5
  num_workers: 4

# Directory Structure
directories:
  data: "data"
  embeddings: "embeddings"
  visualizations: "visualizations"
  logs: "logs"
  configs: "configs"

# Logging Configuration
logging:
  level: "INFO"
  log_to_file: true
  log_to_console: true
  max_log_file_mb: 100
  backup_count: 5

# Pipeline Execution
execution:
  mode: "full_pipeline"
  skip_phases: []
  force_recompute: []

# Models Configuration
models:
  embedding_models:
    - "sentence-transformers/all-mpnet-base-v2"
  embedding_batch_size: 32
  embedding_cache: true

# Wikipedia Configuration
wikipedia:
  use_cached_articles: true
  topics:
    - "Machine learning"
    - "Psychology"
    - "Neural networks"
    - "Artifical intelligence"
    - "History of computers"
  articles_per_topic: 3
  max_article_length: 5000000
  min_article_length: 1000
  language: "en"
  rate_limit_delay: 1.0
  retry_attempts: 3
  timeout_seconds: 30

# Text Processing
text_processing:
  clean_html: true
  remove_references: true
  remove_navigation: true
  remove_tables: true
  fix_encoding: true
  normalize_whitespace: true
  min_sentence_length: 10
  max_sentence_length: 500

# Chunking Configuration
chunking:
  strategy: "sliding_window"
  window_size: 3
  overlap: 1

# Retrieval Configuration
retrieval:
  algorithm: "semantic_traversal"
  cache_graphs: true

  semantic_traversal:
    num_anchors: 3
    max_hops: 3
    similarity_threshold: 0.3
    max_results: 10

  baseline_vector:
    top_k: 10
    similarity_threshold: 0.5

# Similarity Matrix Configuration
similarities:
  use_cached: true
  similarity_metric: "cosine"
  intra_document:
    enabled: true
    top_k: 10
  inter_document:
    enabled: true
    top_x: 5
  batch_size: 1000
  max_memory_gb: 4

# Multi-Dimensional Knowledge Graph Configuration
knowledge_graph:
  use_cached: true
  architecture: "multi_dimensional_three_tier"  # Document → Chunk → Sentence hierarchy
  
  # Sparsity controls for multi-dimensional relationship construction
  sparsity:
    relationship_limits:
      cosine_similarity: 15      # Top-k cosine similarity relationships per chunk (from Phase 4)
      entity_overlap: 8          # Top-k entity overlap relationships per node
      # Note: hierarchical relationships are not limited (structural)
    min_thresholds:
      cosine_similarity: 0.4     # Minimum cosine similarity for relationships
      entity_overlap: 0.2        # Minimum Jaccard similarity for entity relationships

  # Domain-agnostic extractor configuration
  extractors:
    ner:
      enabled: true
      # Domain-agnostic entity types (no theme bias)
      entity_types: ["PERSON", "ORG", "GPE", "PRODUCT", "EVENT", "MISC"]
    keyphrases:
      enabled: true
      max_features: 15           # Adjusted for three-tier structure
    summary:
      enabled: true
      # Domain-agnostic extractive summarization (no LLM dependency)
      max_sentences: 2

# Ollama Configuration
ollama:
  model: "llama3.1:8b"
  base_url: "http://localhost:11434"
  options:
    temperature: 0.1
    num_predict: 100
    stop: ["\n\n", "Text:", "Summary:", "Topics:"]

# Knowledge Graph Question Generation Configuration
question_generation:
  target_questions: 50              # Total questions to generate
  
  # Question type distribution (weights should sum to 1.0)
  question_types:
    entity_bridge: 0.4              # 40% - Test entity-based traversal
    concept_similarity: 0.3         # 30% - Test cosine similarity traversal  
    hierarchical: 0.2               # 20% - Test multi-granularity navigation
    single_hop: 0.1                 # 10% - Test individual chunk/sentence retrieval
  
  # Quality control
  validation:
    min_question_length: 10
    max_question_length: 200
    require_ground_truth: true
    quality_threshold: 0.7

# Evaluation Configuration
evaluation:
  include_custom_metrics: true
  include_baseline_comparison: true
  
  # Baseline algorithms for comparison
  baselines:
    - "bm25"
    - "traditional_vector"
    - "dense_passage_retrieval"
  
  # Evaluation metrics
  metrics:
    standard: ["f1", "exact_match", "mrr", "ndcg"]
    custom: ["traversal_efficiency", "multi_hop_success", "semantic_coherence"]

# Visualization Configuration
visualization:
  enabled: true
  similarity_matrices: true
  traversal_paths: true
  chunk_boundaries: true
  question_distribution: true
  knowledge_graph_structure: true

# Output Configuration
output:
  create_report: true
  export_data: true
  save_models: false
  include_question_analysis: true

# Cleanup Configuration
cleanup:
  remove_temp_files: false
  compress_large_files: true

# Notifications
notifications:
  enabled: false