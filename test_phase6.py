#!/usr/bin/env python3
"""
Test Phase 6: Knowledge Graph Question Generation
================================================

Test script to verify Phase 6 (Knowledge Graph Question Generation) functionality.
Tests revolutionary knowledge graph-based intelligent node selection with Ollama
for domain-agnostic question synthesis.

Question Generation Strategies Tested:
- Entity Bridge Questions: Test entity-based traversal capabilities
- Concept Similarity Questions: Test cosine similarity traversal  
- Hierarchical Questions: Test multi-granularity navigation
- Single-Hop Questions: Test individual chunk/sentence retrieval

Run from project root:
    python test_phase6.py
"""

import sys
import os
from pathlib import Path

# Add utils to path
sys.path.append(str(Path(__file__).parent / "utils"))

from pipeline import SemanticRAGPipeline


def test_phase6():
    """Test Phase 6: Knowledge Graph Question Generation."""
    print("üß™ Testing Phase 6: Knowledge Graph Question Generation")
    print("üéØ INTELLIGENT NODE SELECTION: Using Multi-Dimensional Knowledge Graph for Question Synthesis")
    print("=" * 95)
    
    try:
        # Initialize pipeline
        config_path = "config.yaml"
        if not Path(config_path).exists():
            print(f"‚ùå Config file not found: {config_path}")
            print("üí° Make sure you're running from the project root directory")
            return False
        
        print(f"üìã Using config: {config_path}")
        pipeline = SemanticRAGPipeline(config_path)
        
        # Load config and setup
        pipeline._load_config()
        
        # Override config for testing the new question generation architecture
        print("üîß Configuring for Knowledge Graph Question Generation testing...")
        
        # Set mode to full pipeline to include Phase 6
        pipeline.config['execution']['mode'] = 'full_pipeline'
        pipeline.config['execution']['skip_phases'] = []
        
        # Use smaller models for faster testing
        pipeline.config['models']['embedding_models'] = [
            "sentence-transformers/all-MiniLM-L6-v2"  # Faster model for testing
        ]
        
        # Configure question generation for testing (smaller numbers)
        pipeline.config['question_generation'] = {
            'target_questions': 20,  # Reduced for testing
            'question_types': {
                'entity_bridge': 0.4,       # 40% - Test entity-based traversal
                'concept_similarity': 0.3,  # 30% - Test cosine similarity traversal
                'hierarchical': 0.2,        # 20% - Test multi-granularity navigation
                'single_hop': 0.1          # 10% - Test individual chunk/sentence retrieval
            },
            'max_themes_per_node': 8,  # RAGAS-style theme extraction
            'validation': {
                'min_question_length': 10,
                'max_question_length': 200,
                'require_ground_truth': True
            }
        }
        
        # Test Ollama integration
        try:
            import ollama
            models = ollama.list()
            ollama_available = True
            print(f"   ü§ñ Ollama available with models: {[m.model for m in models.models]}")
        except Exception:
            ollama_available = False
            print(f"   ‚ö†Ô∏è  Ollama not available - will use fallback question generation")
        
        # Enable force recompute to see fresh generation
        pipeline.config['execution']['force_recompute'] = ['questions']
        
        print(f"   üéØ Target questions: {pipeline.config['question_generation']['target_questions']}")
        print(f"   üìä Question type distribution: {pipeline.config['question_generation']['question_types']}")
        print(f"   ü§ñ Ollama integration: {'Enabled' if ollama_available else 'Disabled (fallback mode)'}")
        print(f"   üß† Intelligent node selection: Multi-dimensional knowledge graph strategies")
        
        # Run pipeline phases 1-6
        print("\nüöÄ Running pipeline phases 1-6...")
        
        # Phase 1: Setup
        pipeline._phase_1_setup_and_initialization()
        print(f"‚úÖ Phase 1: Setup completed (device: {pipeline.device})")
        
        # Phase 2: Data Acquisition (if needed)
        wiki_path = Path(pipeline.config['directories']['data']) / "wiki.json"
        if not wiki_path.exists():
            print("üì• No cached articles found, running Phase 2...")
            pipeline._phase_2_data_acquisition()
        else:
            print("üìÇ Loading cached articles...")
            from wiki import WikiEngine
            wiki_engine = WikiEngine(pipeline.config, pipeline.logger)
            pipeline.articles = wiki_engine._load_cached_articles(wiki_path)
            pipeline.corpus_stats = wiki_engine.get_corpus_statistics()
        
        print(f"‚úÖ Phase 2: Loaded {len(pipeline.articles)} articles")
        
        # Phase 3: Embedding Generation (if needed)
        if not pipeline.embeddings:
            print("üß† Running Phase 3: Embedding Generation...")
            pipeline._phase_3_embedding_generation()
        else:
            print("‚úÖ Phase 3: Embeddings already available")
        
        total_embeddings = sum(len(embeddings) for embeddings in pipeline.embeddings.values())
        print(f"‚úÖ Phase 3: {total_embeddings:,} chunks embedded")
        
        # Phase 4: Similarity Matrix Construction (if needed)
        if not pipeline.similarities:
            print("üï∞ Running Phase 4: Similarity Matrix Construction...")
            pipeline._phase_4_similarity_matrices()
        else:
            print("‚úÖ Phase 4: Similarity matrices already available")
        
        print(f"‚úÖ Phase 4: Similarity matrices for {len(pipeline.similarities)} models")
        
        # Phase 5: Knowledge Graph Construction (if needed)
        if not pipeline.knowledge_graph:
            print("üèóÔ∏è  Running Phase 5: Multi-Dimensional Knowledge Graph Construction...")
            pipeline._phase_5_knowledge_graph_construction()
        else:
            print("‚úÖ Phase 5: Knowledge graph already available")
        
        print(f"‚úÖ Phase 5: Multi-dimensional knowledge graph constructed")
        
        # Phase 6: Knowledge Graph Question Generation
        print("üéØ Running Phase 6: Knowledge Graph Question Generation...")
        pipeline._phase_6_question_generation()
        
        # Verify question generation results
        print("\nüîç Verifying Knowledge Graph Question Generation...")
        
        # Check questions
        if not pipeline.questions:
            print("‚ùå No questions were generated")
            return False
        
        print(f"‚úÖ Questions generated successfully: {len(pipeline.questions)} questions")
        
        # Test RAGAS-style question generation strategies
        print("\nüéØ Testing RAGAS-Style Question Generation Strategies:")
        
        # Group questions by generation strategy
        questions_by_strategy = {}
        for question in pipeline.questions:
            strategy = question.generation_strategy
            if strategy not in questions_by_strategy:
                questions_by_strategy[strategy] = []
            questions_by_strategy[strategy].append(question)
        
        for strategy, questions in questions_by_strategy.items():
            print(f"\n   üìä {strategy.upper()} Strategy:")
            print(f"      Generated: {len(questions)} questions")
            
            if questions:
                sample_question = questions[0]
                print(f"      Sample question: \"{sample_question.question}\"")
                print(f"      Persona used: {sample_question.persona_used}")
                print(f"      Themes used: {sample_question.themes_used[:3]}...")  # Show first 3 themes
                print(f"      Expected advantage: {sample_question.expected_advantage}")
                print(f"      Difficulty level: {sample_question.difficulty_level}")
                print(f"      Ground truth contexts: {len(sample_question.ground_truth_contexts)} nodes")
                print(f"      Reference contexts: {len(sample_question.reference_contexts)} contexts")
                print(f"      Relationship types tested: {sample_question.relationship_types}")
                print(f"      Generation time: {sample_question.generation_time:.3f}s")
                if sample_question.reference_answer:
                    print(f"      Reference answer: {sample_question.reference_answer[:100]}...")
        
        # Test RAGAS-style question quality indicators
        print("\nüîç Testing RAGAS-Style Question Quality:")
        
        valid_questions = 0
        questions_with_ground_truth = 0
        questions_with_relationships = 0
        questions_with_themes = 0
        questions_with_personas = 0
        questions_with_reference_answers = 0
        
        for question in pipeline.questions:
            # Check basic question format
            text = question.question.lower().strip()
            has_question_word = any(word in text for word in ['what', 'how', 'why', 'when', 'where', 'which', 'who'])
            has_question_mark = '?' in text
            has_imperative = any(word in text for word in ['explain', 'describe', 'compare', 'analyze'])
            
            if has_question_word or has_question_mark or has_imperative:
                valid_questions += 1
            
            # Check ground truth coverage
            if question.ground_truth_contexts:
                questions_with_ground_truth += 1
            
            # Check relationship testing
            if question.relationship_types:
                questions_with_relationships += 1
            
            # Check RAGAS-style features
            if question.themes_used:
                questions_with_themes += 1
            
            if question.persona_used:
                questions_with_personas += 1
            
            if question.reference_answer:
                questions_with_reference_answers += 1
        
        print(f"   üìù Valid question format: {valid_questions}/{len(pipeline.questions)} ({100*valid_questions/len(pipeline.questions):.1f}%)")
        print(f"   üéØ Ground truth coverage: {questions_with_ground_truth}/{len(pipeline.questions)} ({100*questions_with_ground_truth/len(pipeline.questions):.1f}%)")
        print(f"   üîó Relationship testing: {questions_with_relationships}/{len(pipeline.questions)} ({100*questions_with_relationships/len(pipeline.questions):.1f}%)")
        print(f"   üè∑Ô∏è  Theme utilization: {questions_with_themes}/{len(pipeline.questions)} ({100*questions_with_themes/len(pipeline.questions):.1f}%)")
        print(f"   üé≠ Persona usage: {questions_with_personas}/{len(pipeline.questions)} ({100*questions_with_personas/len(pipeline.questions):.1f}%)")
        print(f"   üìñ Reference answers: {questions_with_reference_answers}/{len(pipeline.questions)} ({100*questions_with_reference_answers/len(pipeline.questions):.1f}%)")
        
        # Test expected advantage distribution
        print("\nüèÜ Testing Expected Advantage Distribution:")
        
        advantage_counts = {}
        for question in pipeline.questions:
            advantage = question.expected_advantage
            advantage_counts[advantage] = advantage_counts.get(advantage, 0) + 1
        
        for advantage, count in advantage_counts.items():
            percentage = (count / len(pipeline.questions)) * 100
            print(f"   {advantage}: {count} questions ({percentage:.1f}%)")
        
        # Test persona distribution
        print("\nüé≠ Testing Persona Distribution:")
        
        persona_counts = {}
        for question in pipeline.questions:
            persona = question.persona_used
            persona_counts[persona] = persona_counts.get(persona, 0) + 1
        
        for persona, count in persona_counts.items():
            percentage = (count / len(pipeline.questions)) * 100
            print(f"   {persona}: {count} questions ({percentage:.1f}%)")
        
        # Verify semantic traversal bias
        semantic_questions = [q for q in pipeline.questions if q.expected_advantage == 'semantic_traversal']
        baseline_questions = [q for q in pipeline.questions if q.expected_advantage == 'baseline_vector']
        
        if semantic_questions:
            print(f"   ‚úÖ Generated {len(semantic_questions)} questions favoring semantic traversal")
            sample_semantic = semantic_questions[0]
            print(f"      Sample: \"{sample_semantic.question}\"")
            print(f"      Persona: {sample_semantic.persona_used}")
            print(f"      Themes: {sample_semantic.themes_used[:2]}")
        else:
            print(f"   ‚ö†Ô∏è  No questions favoring semantic traversal found")
        
        if baseline_questions:
            print(f"   ‚úÖ Generated {len(baseline_questions)} questions favoring baseline methods")
            sample_baseline = baseline_questions[0]
            print(f"      Sample: \"{sample_baseline.question}\"")
            print(f"      Persona: {sample_baseline.persona_used}")
        
        # Test knowledge graph integration with RAGAS-style features
        print("\nüèóÔ∏è  Testing Knowledge Graph Integration:")
        
        # Check if questions use different node types
        relationship_types_used = set()
        
        for question in pipeline.questions:
            relationship_types_used.update(question.relationship_types)
        
        print(f"   üîó Relationship types tested: {sorted(relationship_types_used)}")
        
        # Check theme extraction effectiveness
        all_themes = set()
        total_themes = 0
        for question in pipeline.questions:
            all_themes.update(question.themes_used)
            total_themes += len(question.themes_used)
        
        print(f"   üè∑Ô∏è  Theme extraction stats:")
        print(f"      Total themes used: {total_themes}")
        print(f"      Unique themes: {len(all_themes)}")
        if total_themes > 0:
            print(f"      Average themes per question: {total_themes / len(pipeline.questions):.1f}")
            print(f"      Theme reuse rate: {len(all_themes) / total_themes:.2f}")
        
        # Show sample themes
        if all_themes:
            sample_themes = list(all_themes)[:10]
            print(f"      Sample themes: {sample_themes}")
        
        # Check ground truth context diversity
        all_contexts = set()
        for question in pipeline.questions:
            all_contexts.update(question.ground_truth_contexts)
        
        print(f"   üéØ Ground truth context stats:")
        print(f"      Unique contexts: {len(all_contexts)}")
        context_usage = sum(len(q.ground_truth_contexts) for q in pipeline.questions)
        if context_usage > 0:
            print(f"      Context reuse rate: {len(all_contexts) / context_usage:.2f}")
            print(f"      Average contexts per question: {context_usage / len(pipeline.questions):.1f}")
        
        # Test RAGAS-style question statistics
        if pipeline.question_stats:
            stats = pipeline.question_stats
            print(f"\nüìä RAGAS-Style Question Generation Statistics:")
            print(f"   Total questions: {stats['total_questions']:,}")
            
            if 'by_generation_strategy' in stats:
                print(f"   By generation strategy:")
                for strategy, count in stats['by_generation_strategy'].items():
                    print(f"      {strategy}: {count}")
            
            if 'by_persona_used' in stats:
                print(f"   By persona used:")
                for persona, count in stats['by_persona_used'].items():
                    print(f"      {persona}: {count}")
            
            if 'by_expected_advantage' in stats:
                print(f"   By expected advantage:")
                for advantage, count in stats['by_expected_advantage'].items():
                    print(f"      {advantage}: {count}")
            
            if 'themes_coverage' in stats:
                coverage = stats['themes_coverage']
                print(f"   Theme utilization:")
                print(f"      Total themes used: {coverage['total_themes_used']}")
                print(f"      Average themes per question: {coverage['average_themes_per_question']:.1f}")
                print(f"      Unique themes: {coverage['unique_themes']}")
            
            if 'ground_truth_coverage' in stats:
                coverage = stats['ground_truth_coverage']
                print(f"   Ground truth coverage:")
                print(f"      Mean contexts per question: {coverage['mean_contexts_per_question']:.1f}")
                print(f"      Total unique contexts: {coverage['total_unique_contexts']}")
        
        # Test caching functionality
        print("\nüíæ Testing question caching...")
        try:
            # Clear force recompute and run again (should use cache)
            pipeline.config['execution']['force_recompute'] = []
            
            import time
            cache_start = time.time()
            pipeline._phase_6_question_generation()
            cache_end = time.time()
            cache_time = cache_end - cache_start
            
            print(f"   Cache load time: {cache_time:.3f}s")
            print(f"   Cached questions: {len(pipeline.questions)} questions")
            
            if cache_time < 1.0:  # Should be very fast
                print("   ‚úÖ Caching working effectively")
            else:
                print("   ‚ö†Ô∏è  Caching may not be working as expected")
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Cache test failed: {e}")
        
        # Check if questions were saved
        questions_path = Path(pipeline.config['directories']['data']) / "knowledge_graph_questions.json"
        if questions_path.exists():
            print(f"\n‚úÖ Questions saved successfully to {questions_path}")
            
            # Show file size
            file_size = questions_path.stat().st_size / 1024  # KB
            print(f"   File size: {file_size:.1f} KB")
            
            # Verify JSON structure
            try:
                import json
                with open(questions_path, 'r') as f:
                    questions_data = json.load(f)
                
                print(f"   ‚úÖ Valid JSON structure with {len(questions_data.get('questions', []))} questions")
                print(f"   üìä Metadata: {list(questions_data.get('metadata', {}).keys())}")
            except Exception as e:
                print(f"   ‚ùå Invalid JSON structure: {e}")
        else:
            print(f"\n‚ö†Ô∏è  Questions file not found at {questions_path}")
        
        print("\nüéâ Phase 6 RAGAS-Style Knowledge Graph Question Generation test completed successfully!")
        print(f"üìã Experiment ID: {pipeline.experiment_id}")
        print(f"üìÅ Logs saved to: logs/{pipeline.experiment_id}.log")
        print(f"üìä Questions saved to: {questions_path}")
        
        # Test persona effectiveness
        print("\nüé≠ Testing Persona Effectiveness:")
        researcher_questions = [q for q in pipeline.questions if "Research" in q.persona_used]
        googler_questions = [q for q in pipeline.questions if "Basic" in q.persona_used]
        
        if researcher_questions:
            print(f"   üë®‚Äçüî¨ Research Scientist questions: {len(researcher_questions)}")
            sample_research = researcher_questions[0]
            print(f"      Sample: \"{sample_research.question}\"")
            
        if googler_questions:
            print(f"   üîç Basic Googler questions: {len(googler_questions)}")
            sample_googler = googler_questions[0]
            print(f"      Sample: \"{sample_googler.question}\"")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Phase 6 test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Main test function."""
    print("üß™ Semantic RAG Pipeline - Phase 6 Test")
    print("üéØ Testing REVOLUTIONARY Knowledge Graph-Based Question Generation with Ollama")
    print("=" * 85)
    
    # Check if we're in the right directory
    if not Path("config.yaml").exists():
        print("‚ùå config.yaml not found!")
        print("üí° Make sure you're running this from the project root directory")
        print("üí° Expected directory structure:")
        print("   config.yaml")
        print("   utils/")
        print("   requirements.txt")
        return
    
    success = test_phase6()
    
    if success:
        print("\n‚úÖ All tests passed!")
        print("üöÄ Phase 6 RAGAS-Style Knowledge Graph Question Generation is ready for production use")
        print("\nüéØ REVOLUTIONARY FEATURES VERIFIED:")
        print("   ‚Ä¢ Multi-dimensional knowledge graph-based intelligent node selection")
        print("   ‚Ä¢ RAGAS-style structured prompting with personas and themes")
        print("   ‚Ä¢ Advanced theme extraction from entities and keyphrases")
        print("   ‚Ä¢ Multi-hop context formatting (<1-hop>, <2-hop>)")
        print("   ‚Ä¢ Ollama integration with structured output validation")
        print("   ‚Ä¢ Two specialized personas:")
        print("     - Research Scientist (complex, analytical questions)")
        print("     - Basic Googler (simple, factual questions)")
        print("   ‚Ä¢ Four strategic question generation approaches:")
        print("     - Entity Bridge Questions (test entity-based traversal)")
        print("     - Concept Similarity Questions (test cosine similarity traversal)")
        print("     - Hierarchical Questions (test multi-granularity navigation)")
        print("     - Single-Hop Questions (test individual retrieval)")
        print("   ‚Ä¢ Ground truth context tracking with reference answers")
        print("   ‚Ä¢ Expected advantage prediction for benchmarking")
        print("   ‚Ä¢ Intelligent caching and fallback systems")
        print("\nüéä RESEARCH BREAKTHROUGH:")
        print("   First question generation system that combines:")
        print("   ‚Ä¢ Multi-dimensional knowledge graph relationships")
        print("   ‚Ä¢ RAGAS-style structured prompting methodology")
        print("   ‚Ä¢ Domain-agnostic theme extraction")
        print("   ‚Ä¢ Persona-driven question diversity")
        print("   to create targeted evaluation datasets!")
    else:
        print("\n‚ùå Tests failed!")
        print("üîß Please check the error messages above")
        print("\nüí° Common issues:")
        print("   ‚Ä¢ Ollama not installed: curl -fsSL https://ollama.ai/install.sh | sh")
        print("   ‚Ä¢ Model not available: ollama pull llama3.1:8b")
        print("   ‚Ä¢ Knowledge graph missing: run test_phase5.py first")


if __name__ == "__main__":
    main()
