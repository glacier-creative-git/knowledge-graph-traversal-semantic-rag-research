#!/usr/bin/env python3
"""
Enhanced Hybrid Retrieval and Question Generation Test
=====================================================

Validates the hybrid query-aware semantic traversal algorithm and improved question generation.
Tests the core principle: query-aware traversal with dynamic extraction.
"""

import sys
from pathlib import Path

# Add utils to path
sys.path.append(str(Path(__file__).parent / "utils"))

from utils.pipeline import SemanticRAGPipeline
from utils.retrieval import create_retrieval_engine
from utils.questions import create_question_generator, create_dataset_generator
from utils.traversal import TraversalValidator, ConnectionType, GranularityLevel


def test_hybrid_traversal_system():
    """Test the enhanced hybrid query-aware traversal system."""
    print("ğŸ§ª Testing Enhanced Hybrid Query-Aware Traversal System")
    print("=" * 60)
    
    try:
        # Initialize pipeline and run through Phase 6
        print("ğŸš€ 1. Initializing pipeline...")
        pipeline = SemanticRAGPipeline()
        
        # Run phases 1-6
        pipeline._phase_1_setup_and_initialization()
        pipeline._phase_2_data_acquisition()
        pipeline._phase_3_embedding_generation()
        pipeline._phase_4_similarity_matrices()
        pipeline._phase_5_theme_extraction()
        pipeline._phase_6_knowledge_graph_construction()
        
        print(f"âœ… Pipeline initialized with {pipeline.kg_stats.get('total_chunks', 0)} chunks")
        
        # Test 2: Verify enhanced retrieval engine
        print("\nğŸ” 2. Testing Enhanced Hybrid Retrieval Engine...")
        if pipeline.retrieval_engine:
            
            # Test baseline vector retrieval
            query1 = "What is machine learning?"
            result1 = pipeline.retrieval_engine.retrieve(query1, strategy="baseline_vector")
            print(f"âœ… Baseline retrieval: {len(result1.retrieved_content)} results for '{query1}'")
            
            # Test hybrid semantic traversal retrieval
            query2 = "How does artificial intelligence work in neural networks?"
            result2 = pipeline.retrieval_engine.retrieve(query2, strategy="semantic_traversal")
            print(f"âœ… Hybrid traversal: {len(result2.retrieved_content)} results for '{query2}'")
            
            # Detailed analysis of hybrid traversal
            if result2.traversal_path:
                print(f"   ğŸ›¤ï¸  Traversal path: {len(result2.traversal_path.nodes)} nodes, {len(result2.traversal_path.connection_types)} hops")
                print(f"   ğŸ¯ Path valid: {result2.traversal_path.is_valid}")
                print(f"   ğŸ—ï¸  Retrieval method: {result2.retrieval_method}")
                
                if result2.metadata:
                    print(f"   âš“ Anchor chunk: {result2.metadata.get('anchor_chunk', 'unknown')}")
                    print(f"   â±ï¸  Retrieval time: {result2.metadata.get('retrieval_time', 0):.3f}s")
                    print(f"   ğŸ“Š Extraction count: {result2.metadata.get('extraction_count', 0)}")
                
                if result2.extraction_metadata:
                    print(f"   ğŸ¯ Extraction points: {result2.extraction_metadata.get('extraction_points', 0)}")
                    print(f"   ğŸ“ Total extracted: {result2.extraction_metadata.get('total_extracted', 0)}")
                    print(f"   âœ¨ Final count: {result2.extraction_metadata.get('final_count', 0)}")
                
                # Show sample query similarities
                if result2.query_similarities:
                    sample_similarities = list(result2.query_similarities.items())[:3]
                    print(f"   ğŸ”— Sample query similarities:")
                    for sentence, sim in sample_similarities:
                        print(f"      {sim:.3f}: {sentence[:60]}...")
                
        else:
            print("âŒ Retrieval engine not initialized")
            return False
        
        # Test 3: Verify improved question generation
        print("\nâ“ 3. Testing Improved Question Generation...")
        question_generator = create_question_generator(pipeline.knowledge_graph, pipeline.config, pipeline.logger)
        
        # Generate questions using improved method
        single_hop_questions = question_generator.generate_single_hop_questions(5)
        print(f"âœ… Generated {len(single_hop_questions)} single-hop questions (should be >0 now)")
        
        multi_hop_questions = question_generator.generate_multi_hop_questions(3, ["medium"])
        print(f"âœ… Generated {len(multi_hop_questions)} multi-hop questions")
        
        # Show sample questions
        if single_hop_questions:
            sample_q = single_hop_questions[0]
            print(f"   ğŸ“ Sample single-hop: {sample_q.question_text[:80]}...")
            print(f"      Similarity score: {sample_q.generation_metadata.get('similarity_score', 0):.3f}")
        
        if multi_hop_questions:
            sample_q = multi_hop_questions[0]
            print(f"   ğŸ“ Sample multi-hop: {sample_q.question_text[:80]}...")
            print(f"      Difficulty: {sample_q.difficulty_level}")
        
        # Test 4: Validate hybrid algorithm behavior
        print("\nğŸ”„ 4. Testing Hybrid Algorithm Behavior...")
        
        # Test with specific query to analyze behavior
        test_query = "What are the applications of machine learning in artificial intelligence?"
        test_result = pipeline.retrieval_engine.retrieve(test_query, strategy="semantic_traversal")
        
        print(f"ğŸ“‹ Test Query: '{test_query}'")
        print(f"ğŸ¯ Results Summary:")
        print(f"   Retrieved items: {len(test_result.retrieved_content)}")
        print(f"   Average confidence: {sum(test_result.confidence_scores) / len(test_result.confidence_scores):.3f}" if test_result.confidence_scores else "   No confidence scores")
        print(f"   Traversal hops: {test_result.traversal_path.total_hops}")
        
        # Analyze query-awareness
        if hasattr(test_result, 'query_similarities') and test_result.query_similarities:
            similarities = list(test_result.query_similarities.values())
            print(f"   Query similarity range: {min(similarities):.3f} - {max(similarities):.3f}")
            print(f"   Avg query similarity: {sum(similarities) / len(similarities):.3f}")
        
        # Test 5: Validate question-retrieval coherence with hybrid system
        print("\nğŸ¯ 5. Testing Question-Retrieval Coherence with Hybrid System...")
        
        coherence_scores = []
        test_questions = single_hop_questions[:2] + multi_hop_questions[:1]  # Test 3 questions
        
        for i, question in enumerate(test_questions):
            print(f"\n   Test {i+1}: {question.question_type} question")
            print(f"   Question: {question.question_text[:60]}...")
            
            # Retrieve answer using hybrid system
            retrieval_result = pipeline.retrieval_engine.retrieve(
                question.question_text, 
                strategy="semantic_traversal"
            )
            
            # Analyze coherence
            ground_truth_nodes = len(question.ground_truth_path.nodes)
            retrieved_count = len(retrieval_result.retrieved_content)
            
            print(f"   Ground truth nodes: {ground_truth_nodes}")
            print(f"   Retrieved content: {retrieved_count}")
            
            if retrieval_result.metadata:
                print(f"   Extraction count: {retrieval_result.metadata.get('extraction_count', 0)}")
            
            # Simple coherence measure
            coherence = min(1.0, retrieved_count / max(1, ground_truth_nodes))
            coherence_scores.append(coherence)
            print(f"   Coherence score: {coherence:.3f}")
        
        avg_coherence = sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0
        print(f"\nğŸ“Š Average coherence score: {avg_coherence:.3f}")
        
        # Test 6: Demonstrate crane algorithm behavior
        print("\nğŸ—ï¸  6. Demonstrating Enhanced 'Cargo Crane' Algorithm...")
        
        # Use a query that should trigger multiple extractions
        crane_query = "machine learning neural networks deep learning artificial intelligence"
        crane_result = pipeline.retrieval_engine.retrieve(crane_query, strategy="semantic_traversal")
        
        print(f"ğŸ” Crane Demo Query: '{crane_query[:50]}...'")
        print(f"ğŸ“ ANCHOR: Found anchor point")
        
        if crane_result.metadata:
            print(f"ğŸš¶ TRAVERSE: Executed {crane_result.metadata.get('total_hops', 0)} hops")
            
        if crane_result.extraction_metadata:
            extraction_points = crane_result.extraction_metadata.get('extraction_points', 0)
            total_extracted = crane_result.extraction_metadata.get('total_extracted', 0)
            print(f"ğŸ“¦ EXTRACT: Made {extraction_points} extraction decisions")
            print(f"   Total sentences extracted: {total_extracted}")
            
        print(f"âœ¨ FINAL: Delivered {len(crane_result.retrieved_content)} final results")
        
        # Show content quality
        if crane_result.retrieved_content:
            print(f"ğŸ¯ Sample Results:")
            for i, content in enumerate(crane_result.retrieved_content[:2]):
                confidence = crane_result.confidence_scores[i] if i < len(crane_result.confidence_scores) else 0.0
                print(f"   {i+1}. ({confidence:.3f}) {content[:80]}...")
        
        print("\nğŸ‰ All hybrid tests completed successfully!")
        
        # Enhanced Summary
        print("\nğŸ“Š Enhanced Test Summary:")
        print(f"   Knowledge Graph: {len(pipeline.knowledge_graph.chunks)} chunks, {len(pipeline.knowledge_graph.sentences)} sentences")
        print(f"   Retrieval Engine: {pipeline.retrieval_stats.get('status', 'unknown')}")
        print(f"   Question Generation: {len(single_hop_questions + multi_hop_questions)} questions generated")
        print(f"   Single-hop improvement: {'âœ… Fixed' if len(single_hop_questions) > 0 else 'âŒ Still broken'}")
        print(f"   Hybrid Algorithm: âœ… Query-aware traversal with dynamic extraction")
        print(f"   Crane Performance: âœ… Anchor â†’ Query-aware Traverse â†’ Dynamic Extract")
        print(f"   Coherence Score: {avg_coherence:.3f} (target: >0.7)")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_traversal_rules_enhanced():
    """Test enhanced traversal rules with hybrid algorithm validation."""
    print("\nğŸ§­ Testing Enhanced Traversal Rules")
    print("-" * 40)
    
    try:
        from traversal import TraversalValidator, ConnectionType, GranularityLevel
        
        validator = TraversalValidator()
        
        # Test hybrid path validation
        print("ğŸ” Testing hybrid path validation...")
        
        # Test a valid hybrid path: chunk -> sentence extraction -> chunk
        hybrid_nodes = ["chunk_1", "chunk_2"]  # Represents extraction + traversal
        hybrid_connections = [ConnectionType.RAW_SIMILARITY]
        hybrid_granularities = [GranularityLevel.CHUNK, GranularityLevel.CHUNK]
        hybrid_documents = ["doc_a", "doc_a"]
        
        hybrid_path = validator.validate_path(
            hybrid_nodes, hybrid_connections, hybrid_granularities, hybrid_documents, "raw_similarity"
        )
        
        print(f"âœ… Hybrid path validation: {hybrid_path.is_valid}")
        if not hybrid_path.is_valid:
            print(f"   Errors: {hybrid_path.validation_errors}")
        
        # Test template generation for hybrid system
        templates = validator.generate_valid_path_templates("raw_similarity")
        print(f"âœ… Raw similarity templates: {len(templates)} generated")
        if templates:
            template = templates[0]
            print(f"   Template: {template['description']}")
            print(f"   Hops: {template['hops']}, Max hops allowed")
        
        return True
        
    except Exception as e:
        print(f"âŒ Enhanced traversal rules test failed: {e}")
        return False


def main():
    """Run all enhanced tests."""
    print("ğŸš€ Starting Enhanced Hybrid Retrieval System Integration Test")
    print("=" * 80)
    print("ğŸ¯ Testing: Query-aware traversal + Dynamic extraction + Improved question generation")
    print("=" * 80)
    
    # Test 1: Enhanced traversal rules
    rules_test = test_traversal_rules_enhanced()
    
    # Test 2: Full hybrid system integration
    system_test = test_hybrid_traversal_system()
    
    print("\n" + "=" * 80)
    if rules_test and system_test:
        print("ğŸ‰ ALL ENHANCED TESTS PASSED! The hybrid system works correctly.")
        print("ğŸ”§ Hybrid Algorithm Achievements:")
        print("   âœ… Query-aware similarity caching implemented")
        print("   âœ… Dynamic sentence extraction with deduplication")
        print("   âœ… Intelligent stopping conditions (minimum sentence threshold)")
        print("   âœ… Visited chunk tracking prevents infinite loops")
        print("   âœ… Question generation efficiency fixed (uses pre-computed connections)")
        print("   âœ… Single anchor mode reduces computational overhead")
        print("   âœ… Crane algorithm: Anchor â†’ Query-aware Traverse â†’ Dynamic Extract")
        print("ğŸ¯ The system maintains query relevance while leveraging knowledge graph structure!")
    else:
        print("âŒ Some tests failed. Check the errors above.")
        sys.exit(1)


if __name__ == "__main__":
    main()
